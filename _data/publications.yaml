-
   title: Word Sense Filtering Improves Embedding-Based Lexical Substitution
   authors: Anne Cocos, Marianna Apidianaki and Chris Callison-Burch
   venue: Workshop on Sense, Concept and Entity Representations and their Applications
   type: workshop
   award: Best Paper Award
   year: 2017
   url: publications/word-sense-filtering-improves-lexical-substitution.pdf
   page_count: 9
   id: word-sense-filtering-improves-lexical-substitution
   abstract: The role of word sense disambiguation in lexical substitution has been questioned due to the high performance of vector space models which propose good substitutes without explicitly accounting for sense. We show that a filtering mechanism based on a sense inventory optimized for substitutability can improve the results of these models. Our sense inventory is constructed using a clustering method which generates paraphrase clusters that are congruent with lexical substitution annotations in a development set. The results show that lexical substitution can still benefit from senses which can improve the output of vector space paraphrase ranking models. 
   bibtex: |
      @inproceedings{Cocos-Apidianaki-Callison-Burch:2017:SENSE-WS,
        author    = {Anne Cocos and Marianna Apidianaki  and  Chris Callison-Burch},
        title     = {Word Sense Filtering Improves Embedding-Based Lexical Substitution},
        booktitle = {Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications},
        month     = {April},
        year      = {2017},
        address   = {Valencia, Spain},
        publisher = {Association for Computational Linguistics},
        pages     = {110--119},
        url       = {http://www.aclweb.org/anthology/E17-2016}
      }
-
   title: The Language of Place&colon; Semantic Value from Geospatial Context
   authors: Ann Cocos and Chris Callison-Burch
   venue: EACL
   type: conference
   year: 2017
   url: publications/language-of-place.pdf
   page_count: 5
   id: language-of-place
   abstract: There is a relationship between what we say and where we say it. Word embeddings are usually trained assuming that semantically-similar words occur within the same textual contexts. We investigate the extent to which semantically-similar words occur within the same geospatial contexts. We enrich a corpus of geolocated Twitter posts with physical data derived from Google Places and OpenStreetMap, and train word embeddings using the resulting geospatial contexts. Intrinsic evaluation of the resulting vectors shows that geographic context alone does provide useful information about semantic relatedness.
   bibtex: |
    @InProceedings{cocos-callisonburch:2017:EACLshort,
      author    = {Cocos, Anne  and  Callison-Burch, Chris},
      title     = {The Language of Place: Semantic Value from Geospatial Context},
      booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
      month     = {April},
      year      = {2017},
      address   = {Valencia, Spain},
      publisher = {Association for Computational Linguistics},
      pages     = {99--104},
      url       = {http://www.aclweb.org/anthology/E17-2016}
    }
-
   title: Crowd Control&colon; Effectively Utilizing Unscreened Crowd Workers for Biomedical Data Annotation
   authors: Anne Cocos, Ting Qiana, Chris Callison-Burch, and Aaron J. Masino
   venue: Journal of Biomedical Informatics
   type: journal
   year: 2017
   url: http://www.sciencedirect.com/science/article/pii/S1532046417300746
   page_count: 22
   id: optimizing-machine-translation-for-text-simplifciation
   abstract: Annotating unstructured texts in Electronic Health Records data is usually a necessary step for conducting machine learning research on such datasets. Manual annotation by domain experts provides data of the best quality, but has become increasingly impractical given the rapid increase in the volume of EHR data. In this article, we examine the effectiveness of crowdsourcing with unscreened online workers as an alternative for transforming unstructured texts in EHRs into annotated data that are directly usable in supervised learning models. We find the crowdsourced annotation data to be just as effective as expert data in training a sentence classification model to detect the mentioning of abnormal ear anatomy in radiology reports of audiology. Furthermore, we have discovered that enabling workers to self-report a confidence level associated with each annotation can help researchers pinpoint less-accurate annotations requiring expert scrutiny. Our findings suggest that even crowd workers without specific domain knowledge can contribute effectively to the task of annotating unstructured EHR datasets.
   bibtex: |
      @article{Xu-EtAl:2016:TACL,
         author = {Anne Cocos and Ting Qiana and Chris Callison-Burch and Aaron Masino},
         title = {Crowd Control: Effectively Utilizing Unscreened Crowd Workers for Biomedical Data Annotation},
         journal = {Journal of Biomedical Informatics},
         volume = {},
         number = {},
         year = {2017},
         url = {http://www.sciencedirect.com/science/article/pii/S1532046417300746}
       }
-
   title: The Gun Violence Database
   authors: Ellie Pavlick and Chris Callison-Burch
   venue: Bloomberg Data for Good Exchange
   type: workshop
   year: 2016
   url: publications/gvdb-d4gx.pdf
   page_count: 6
   id: gvdb-d4gx
   abstract: We describe the Gun Violence Database (GVDB), a large and growing database of gun violence incidents in the United States. The GVDB is built from the detailed information found in local news reports about gun violence, and is constructed via a large-scale crowdsourced annotation effort through our web site, http://gun-violence.org/. We argue that centralized and publicly available data about gun violence can facilitate scientific, fact-based discussion about a topic that is often dominated by politics and emotion. We describe our efforts to automate the construction of the database using state-of-the-art natural language processing (NLP) technologies, eventually enabling a fully-automated, highly-scalable resource for research on this important public health problem.
-
   title: The Gun Violence Database&colon; A new task and data set for NLP
   authors: Ellie Pavlick, Heng Ji, Xiaoman Pan and Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2016
   url: publications/gun-violence-database.pdf
   page_count: 6
   id: gun-violence-database
   abstract: We argue that NLP researchers are especially well-positioned to contribute to the national discussion about gun violence. Reasoning about the causes and outcomes of gun violence is typically dominated by politics and emotion, and data-driven research on the topic is stymied by a shortage of data and a lack of federal funding. However, data abounds in the form of unstructured text from news articles across the country. This is an ideal application of NLP technologies, such as relation extraction, coreference resolution, and event detection. We introduce a new and growing dataset, the Gun Violence Database, in order to facilitate the adaptation of current NLP technologies to the domain of gun violence, thus enabling better social science research on this important and under-resourced problem.
   bibtex: |
      @inproceedings{Pavlick-EtAl:2016:EMNLP,
       author = {Ellie Pavlick and Heng Ji and Xiaoman Pan and Chris Callison-Burch},
       title = {The Gun Violence Database: A new task and data set for {NLP}},
       booktitle = {Proceedings of The 2016 Conference on Empirical Methods on Natural Language Processing (EMNLP)},
       month = {November},
       year = {2016},
       address = {Austin, TX},
       url = {http://www.cis.upenn.edu/~ccb/publications/gun-violence-database.pdf}
       } 
-
   title: Tense Manages to Predict Implicative Behavior in Verbs
   authors: Ellie Pavlick and Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2016
   url: publications/tense-predicts-implicative-verbs.pdf
   page_count: 5
   id: tense-predicts-implicative-verbs
   abstract: Implicative verbs (e.g. manage) entail their compliment clauses, while non-implicative verbs (e.g. want) do not. For example, while managing to solve the problem entails solving the problem, no such inference follows from wanting to solve the problem. Differentiating between implicative and non-implicative verbs is therefore an essential component of natural language understanding, relevant to applications such as textual entailment and summarization. We present a simple method for predicting implicativeness which exploits known constraints on the tense of implicative verbs and their compliments. We show that this yields an effective, data-driven way of capturing this nuanced property in verbs.
   bibtex: |
      @inproceedings{Pavlick-Callison-Burch:2016:EMNLP,
       author = {Ellie Pavlick and Chris Callison-Burch},
       title = {Tense Manages to Predict Implicative Behavior in Verbs},
       booktitle = {Proceedings of The 2016 Conference on Empirical Methods on Natural Language Processing (EMNLP)},
       month = {November},
       year = {2016},
       address = {Austin, TX},
       url = {http://www.cis.upenn.edu/~ccb/publications/tense-predicts-implicative-verbs.pdf}
       } 
-
   title: So-Called Non-Subsective Adjectives
   authors: Ellie Pavlick and Chris Callison-Burch
   venue: STARSEM
   type: conference
   award: Best Paper Award
   year: 2016
   url: publications/non-subsective-adjectives.pdf
   page_count: 6
   id: non-subsective-adjectives
   abstract: The interpretation of adjective-noun pairs plays a crucial role in tasks such as recognizing textual entailment. Formal semantics often places adjectives into a taxonomy which should dictate adjectives’ entailment behavior when placed in adjective-noun compounds. However, we show experimentally that the behavior of subsective adjectives (e.g. red) versus non-subsective adjectives (e.g. fake) is not as cut and dry as often assumed. For example, inferences are not always symmetric&colon; while ID is generally considered to be mutually exclusive with fake ID, fake ID is considered to entail ID. We discuss the implications of these findings for automated natural language understanding.
   bibtex: |
      @inproceedings{Pavlick-Callison-Burch:2016:ACL,
       author = {Ellie Pavlick and Chris Callison-Burch},
       title = {So-Called Non-Subsective Adjectives},
       booktitle = {*SEM 2016: The Fifth Joint Conference on Lexical and Computational Semantics},
       month = {August},
       year = {2016},
       address = {Berlin, Germany},
       url = {http://www.cis.upenn.edu/~ccb/publications/non-subsective-adjectives.pdf}
       } 
-
   title: Most babies are little and most problems are huge&colon; Compositional Entailment in Adjective-Nouns
   authors: Ellie Pavlick and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2016
   url: publications/compositional-entailment-in-adjective-nouns.pdf
   page_count: 11
   id: compositional-entailment-in-adjective-nouns
   abstract: We examine adjective-noun (AN) composition in the task of recognizing textual entailment (RTE). We analyze behavior of ANs in large corpora and show that, despite conventional wisdom, adjectives do not always restrict the denotation of the nouns they modify. We use natural logic to characterize the variety of entailment relations that can result from AN composition. Predicting these relations depends on context and on common-sense knowledge, making AN composition especially challenging for current RTE systems. We demonstrate the inability of current state-of-the-art systems to handle AN composition in a simplified RTE task which involves the insertion of only a single word.
   bibtex: |
      @inproceedings{Pavlick-Callison-Burch:2016:ACL,
       author = {Ellie Pavlick and Chris Callison-Burch},
       title = {Most babies are little and most problems are huge&colon; Compositional Entailment in Adjective-Nouns},
       booktitle = {The 54th Annual Meeting of the Association for Computational Linguistics (ACL)},
       month = {August},
       year = {2016},
       address = {Berlin, Germany},
       url = {http://www.cis.upenn.edu/~ccb/publications/compositional-entailment-in-adjective-nouns.pdf}
       } 
-
   title: Simple PPDB&colon; A Paraphrase Database for Simplification
   authors: Ellie Pavlick and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2016
   url: publications/simple-ppdb.pdf
   page_count: 6
   id: simple-ppdb
   abstract: We release the Simple Paraphrase Database, a subset of of the Paraphrase Database (PPDB) adapted for the task of text simplification. We train a supervised model to associate simplification scores with each phrase pair, producing rankings competitive with state-of-the-art lexical simplification models. Our new simplification database contains 4.4 million paraphrase rules, making it the largest available resource for lexical simplification.
   bibtex: |
      @inproceedings{Pavlick-Callison-Burch:2016:ACL,
       author = {Ellie Pavlick and Chris Callison-Burch},
       title = {Simple {PPDB}&colon; A Paraphrase Database for Simplification},
       booktitle = {The 54th Annual Meeting of the Association for Computational Linguistics (ACL)},
       month = {August},
       year = {2016},
       address = {Berlin, Germany},
       url = {http://www.cis.upenn.edu/~ccb/publications/simple-ppdb.pdf}
       } 
-
   title: Clustering Paraphrases by Word Sense
   authors: Anne Cocos and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2016
   url: publications/clustering-paraphrases-by-word-sense.pdf
   page_count: 10
   id: clustering-paraphrases-by-word-sense
   abstract: Automatically generated databases of English paraphrases have the drawback that they return a single list of paraphrases for an input word or phrase. This means that all senses of polysemous words are grouped together, unlike WordNet which partitions different senses into separate synsets. We present a new method for clustering paraphrases by word sense, and apply it to the Paraphrase Database (PPDB). We investigate the performance of hierarchical and spectral clustering algorithms, and systematically explore different ways of defining the similarity matrix that they use as input. Our method produces sense clusters that are qualitatively and quantitatively good, and that represent a substantial improvement to the PPDB resource.
   bibtex: |
      @inproceedings{Cocos-Callison-Burch:2016:NAACL,
       author = {Anne Cocos and Chris Callison-Burch},
       title = {Clustering Paraphrases by Word Sense},
       booktitle = {The 2016 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016)},
       month = {June},
       year = {2016},
       address = {San Diego, California},
       url = {http://www.cis.upenn.edu/~ccb/publications/clustering-paraphrases-by-word-sense.pdf}
       } 
-
   title: Sentential Paraphrasing as Black-Box Machine Translation
   authors: Courtney Napoles, Chris Callison-Burch, and Matt Post
   venue: NAACL
   type: conference
   year: 2016
   url: publications/sentential-paraphrasing-demo-paper.pdf
   page_count: 5
   id: sentential-paraphrasing-demo-paper
   abstract: We present a simple, prepackaged solution to generating paraphrases of English sentences. We use the Paraphrase Database (PPDB) for monolingual sentence rewriting and provide machine translation language packs&colon; prepackaged, tuned models that can be downloaded and used to generate paraphrases on a standard Unix environment. The language packs can be treated as a black box or customized to specific tasks. In this demonstration, we will explain how to use the included interactive web-based tool to generate sentential paraphrases.
   bibtex: |
      @inproceedings{Napoles-et-al:2016:NAACL-demos,
       author = {Courtney Napoles and Chris Callison-Burch and Matt Post},
       title = {Sentential Paraphrasing as Black-Box Machine Translation},
       booktitle = {The 2016 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2016)},
       month = {June},
       year = {2016},
       address = {San Diego, California},
       url = {http://www.cis.upenn.edu/~ccb/publications/sentential-paraphrasing-demo-paper.pdf}
       } 
-
   title: Optimizing Statistical Machine Translation for Text Simplification
   authors: Wei Xu, Courtney Napoles, Ellie Pavlick, Jim Chen, and Chris Callison-Burch
   venue: TACL
   type: journal
   year: 2016
   url: publications/optimizing-machine-translation-for-text-simplifciation.pdf
   page_count: 15
   id: optimizing-machine-translation-for-text-simplifciation
   abstract: Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality and quantity of manually simplified corpora, which are expensive to build. In this paper, we conduct an in-depth adaptation of statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and  evaluating simplification systems, which will facilitate iterative development for this task.
   bibtex: |
      @article{Xu-EtAl:2016:TACL,
         author = {Wei Xu and Courtney Napoles and Ellie Pavlick and Quanze Chen and Chris Callison-Burch},
         title = {Optimizing Statistical Machine Translation for Text Simplification},
         journal = {Transactions of the Association for Computational Linguistics},
         volume = {4},
         year = {2016},
         url = {http://www.cis.upenn.edu/~ccb/publications/optimizing-machine-translation-for-text-simplifciation.pdf},
         pages = {401--415}
       }
-
   title: A Comprehensive Analysis of Bilingual Lexicon Induction
   authors: Ann Irvine and Chris Callison-Burch
   venue: Computational Linguistics
   type: journal
   year: 2016
   url: publications/discriminative-bilingual-lexicon-induction.pdf
   page_count: 38
   id: discriminative-bilingual-lexicon-induction
   abstract: Bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages.  In this paper we present the most comprehensive analysis of bilingual lexicon induction to date.  We present experiments on a wide range of languages and data sizes.  We examine translation into English from 25 foreign languages -- Albanian, Azeri, Bengali, Bosnian, Bulgarian, Cebuano, Gujarati, Hindi, Hungarian, Indonesian, Latvian, Nepali, Romanian, Serbian, Slovak, Somali, Spanish, Swedish, Tamil, Telugu, Turkish, Ukrainian, Uzbek, Vietnamese and Welsh.  We analyze the behavior of bilingual lexicon induction on low frequency words, rather than testing solely on high frequency words, as previous research has done.  Low frequency words are more relevant to statistical machine translation, where systems typically lack translations of rare words that fall outside of their training data.  We systematically explore a wide range of features and phenomena that affect the quality of the translations discovered by bilingual lexicon induction. We give illustrative examples of the highest ranking translations for orthogonal signals of translation equivalence like contextual similarity and temporal similarity.  We analyze the effects of frequency and burstiness, and the sizes of the seed bilingual dictionaries and the monolingual training corpora.  Additionally, we introduce a novel discriminative approach to bilingual lexicon induction.  Our discriminative model is capable of combining a wide variety of features, which individually provide only weak indications of translation equivalence.  When feature weights are discriminatively set, these signals produce dramatically higher translation quality than previous approaches that combined signals in an unsupervised fashion (e.g. using minimum reciprocal rank).  We also directly compare our model's performance against a sophisticated generative approach, the matching canonical correlation analysis (MCCA) algorithm used by Haghighi et al (2008).  Our algorithm achieves an accuracy of 42% versus MCCA's 15%.
-
   title: The Shield of Heroic Memories (mp3)
   authors: Chris Callison-Burch
   venue: The Adventure Zone podcast
   type: unpublished
   year: 2015
   url: publications/shield-of-heroic-memories.mp3
   id: shield-of-heroic-memories
   abstract: I designed an item for The Adventure Zone, a comedy podcast about three brothers playing D&D with their dad.  The McElroy brothers were incredibly enthusiastic about my submission.  I want all of my paper reviews to say what they said, "That's already radical and then my boy Chris Callison-Burch kicked it up a notch. It's brilliant."       
-
   title: Adding Semantics to Data-Driven Paraphrasing
   authors: Ellie Pavlick, Johan Bos, Malvina Nissim, Charley Beller, Benjamin Van Durme, and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2015
   url: publications/adding-semantics-to-data-driven-paraphrasing.pdf
   page_count: 10
   id: adding-semantics-to-data-driven-paraphrasing
   figures:
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-1.jpg
         label: Figure 1
         caption: An example sentence pair for the RTE task. In order for a system to conclude that the premise (top) does not entail the hypothesis (bottom), it should recognize that sparked implies caused but that in Denmark precludes in Jordan. These phrase-level entailment relationships are modeled by natural logic.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-1.jpg
         label: Table 1
         caption: Examples of different types of entailment relations appearing in PPDB.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-2.jpg
         label: Figure 2
         caption: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent).
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-3.jpg
         label: Figure 3
         caption: Summary of features extracted for each phrase pair ⟨p1,p2⟩. Full descriptions of the features used are given in the supplementary material.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-2.jpg
         label: Table 2
         caption: Column 1 gives the semantics of each label under MacCartney’s Natural Logic. Column 2 gives the notation we use throughout the remainder of this paper. Column 3 gives the description that was shown to Turkers.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-3.jpg
         label: Table 3
         caption: Top scoring pairs (x/y) according to various similarity measures, along with their manually classified entailment labels. Column 1 is cosine similarity based on dependency contexts. Column 2 is based on Lin (1998), column 3 on Weeds (2004), and column 4 is a novel feature. Precise definitions of each metric are given in the supplementary material.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-4.jpg
         label: Table 4
         caption: Top paths associated with the ¬ class.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-5.jpg
         label: Table 5
         caption: F1 measure (×100) achieved by entailment classifier using 10-fold cross validation on the training data.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-4.jpg
         label: Figure 4
         caption: Confusion matrices for classifier trained using only monolingual features (distributional and path) versus bilingual features (paraphrase and translation). True labels are shown along rows, predicted along columns. The matrix is normalized along rows, so that the predictions for each (true) class sum to 100%.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-7.jpg
         label: Table 7
         caption: F1 measure (×100) achieved by entailment classifier on the held out phrase pairs from the sentences in SICK test.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-6.jpg
         label: Table 6
         caption: Example misclassifications from some of the most frequent and most interesting error categories.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-5.jpg
         label: Figure 5
         caption: ENTAILMENT
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-6.jpg
         label: Figure 6
         caption: CONTRADICTION
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-7.jpg
         label: Figure 7
         caption: NEUTRAL
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-figure-8.jpg
         label: Figure 8
         caption: F1 measures achieved by Nutcracker on SICK test data when using various KBs. Baselines are in gray, this work in blue, human references in gold. PPDB-XL refers to a run in which every pair which appears in PPDB is assumed to be equivalent. PPDB-H refers to a run in which manual labels were used to generate axioms. PPDB+ refers to runs in which the automatic classifications were used to generate axioms. In some cases, better proof coverage causes NC to find incorrect proofs, illustrated by the decreased performance on CONTRADICTION when using PPDB-H. For example, using PPDB-H, NC finds an inconsistency for the pair Someone is not playing piano./A person is playing a keyboard. Using the PPDB+, in which piano/keyboard is falsely classified as #, NC fails to find a proof and so correctly guesses NEUTRAL.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-8.jpg
         label: Table 8
         caption: Nutcracker’s overall system accuracy and proof coverage when using different sources of axioms. Coverage is measured as the percent of sentence pairs for which NC’s theorem prover or model builder is able to find a complete logical proof of either entailment or contradiction. When NC fails to find either type of proof, it guesses the most frequent class, NEUTRAL. NC alone uses no axioms. PPDB+ refers to the axioms generated automatically using the classifier described in this paper. PPDB-H refers axioms generated using the human labels on which the classifier was trained.
      -
         img: figures/adding-semantics-to-data-driven-paraphrasing/adding-semantics-to-data-driven-paraphrasing-table-9.jpg
         label: Table 9
         caption: Examples of T/H pairs for which the system’s prediction differed when using PPDB+ vs. WN.
   abstract: We add an interpretable semantics to the paraphrase database (PPDB). To date, the relationship between the phrase pairs in the database has been weakly defined as approximately equivalent. We show that in fact these pairs represent a variety of relations, including directed entailment (little girl/girl) and exclusion (nobody/someone). We automatically assign semantic entailment relations to entries in PPDB using features derived from past work on discovering inference rules from text and semantic taxonomy induction. We demonstrate that our model assigns these entailment relations with high accuracy. In a downstream RTE task, our labels rival relations from WordNet and improve the coverage of a proof-based RTE system by 17%. 
   bibtex: |
      @inproceedings{Pavlick-EtAl:2015:ACL,
       author = {Ellie Pavlick and Johan Bos and Malvina Nissim and Charley Beller and Benjamin Van Durme and Chris Callison-Burch},
       title = {Adding Semantics to Data-Driven Paraphrasing},
       booktitle = {The 53rd Annual Meeting of the Association for Computational
       Linguistics (ACL 2015)},
       month = {July},
       year = {2015},
       address = {Beijing, China},
       url = {http://www.cis.upenn.edu/~ccb/publications/adding-semantics-to-data-driven-paraphrasing.pdf}
       }       
-
   title: PPDB 2.0&colon; Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification
   authors: Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevich, Ben Van Durme, Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2015
   url: publications/ppdb-reranking.pdf
   page_count: 6
   id: ppdb-reranking
   abstract: We present a new release of the Paraphrase Database. PPDB 2.0 includes a discriminatively re-ranked set of paraphrases that achieve a higher correlation with human judgments than PPDB 1.0’s heuristic rankings. Each paraphrase pair in the database now also includes fine-grained entailment relations, word embedding similarities, and style annotations.
   bibtex: |
    @InProceedings{PavlickEtAl-2015:ACL:Semantics,
      author =  {Ellie Pavlick and Pushpendre Rastogi and Juri Ganitkevich and Ben Van Durme, Chris Callison-Burch},
      title =   {{PPDB} 2.0&colon; Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification}
      booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)},
      month     = {July},
      year      = {2015},
      address   = {Beijing, China},
      publisher = {Association for Computational Linguistics},
    }
-
   title: Domain-Specific Paraphrase Extraction
   authors: Ellie Pavlick, Juri Ganitkevich, Tsz Ping Chan, Xuchen Yao, Ben Van Durme, Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2015
   url: publications/domain-specific-paraphrases.pdf
   page_count: 6
   id: domain-specific-paraphrases
   abstract: The validity of applying paraphrase rules depends on the domain of the text that they are being applied to. We develop a novel method for extracting domain-specific paraphrases. We adapt the bilingual pivoting paraphrase method to bias the training data to be more like our target domain of biology. Our best model results in higher precision while retaining complete recall, giving a 10% relative improvement in AUC.
   bibtex: |
    @InProceedings{PavlickEtAl-2015:ACL:Domain,
      author =  {Ellie Pavlick and Juri Ganitkevich and Tsz Ping Chan and Xuchen Yao and Ben Van Durme, Chris Callison-Burch},
      title =   {Domain-Specific Paraphrase Extraction},
      booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)},
      month     = {July},
      year      = {2015},
      address   = {Beijing, China},
      publisher = {Association for Computational Linguistics},
    }
-
   title: FrameNet+&colon; Fast Paraphrastic Tripling of FrameNet
   authors: Ellie Pavlick, Travis Wolfe, Pushpendre Rastogi, Chris Callison-Burch, Mark Drezde, Ben Van Durme
   venue: ACL
   type: conference
   year: 2015
   url: publications/FrameNetPlus.pdf
   page_count: 6
   id: FrameNetPlus
   abstract: We increase the lexical coverage of FrameNet through automatic paraphrasing. We use crowdsourcing to manually filter out bad paraphrases in order to ensure a high-precision resource. Our expanded FrameNet contains an additional 22K lexical units, a 3-fold increase over the current FrameNet, and achieves 40% better coverage when evaluated in a practical setting on New York Times data.
   bibtex: |
    @InProceedings{PavlickEtAl-2015:ACL:FNPlus,
      author =  {Ellie Pavlick and Travis Wolfe and Pushpendre Rastogi and Chris Callison-Burch and Mark Drezde and Benjamin Van Durme},
      title =   {FrameNet+: Fast Paraphrastic Tripling of FrameNet},
      booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)},
      month     = {July},
      year      = {2015},
      address   = {Beijing, China},
      publisher = {Association for Computational Linguistics},
    }
-
   title: Problems in Current Text Simplification Research&colon; New Data Can Help
   authors: Wei Xu, Chris Callison-Burch, and Courtney Napoles
   venue: TACL
   type: journal
   year: 2015
   url: publications/new-data-for-text-simplification.pdf
   page_count: 16
   id: new-data-for-text-simplification
   figures:
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-1.jpg
         label: Table 1
         caption: Example sentence pairs (NORM-SIMP) aligned between English Wikipedia and Simple English Wikipedia. The breakdown in percentages is obtained through manual examination of 200 randomly sampled sentence pairs in the Parallel Wikipedia Simplification (PWKP) corpus.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-2.jpg
         label: Table 2
         caption: The vocabulary size of the Parallel Wikipedia Simplification (PWKP) corpus and the vocabulary difference between its normal and simple sides (as a 2×2 matrix). Only words consisting of the 26 English letters are counted.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-3.jpg
         label: Table 3
         caption: Example of sentences written at multiple levels of text complexity from the Newsela data set. The Lexile readability score and grade level apply to the whole article rather than individual sentences, so the same sentences may receive different scores, e.g. the above sentences for the 6th and 7th grades. The bold font highlights the parts of sentence that are different from the adjacent version(s).
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-figure-1.jpg
         label: Figure 1
         caption: Manual classification of aligned sentence pairs from the Newsela corpus. We categorize randomly sampled 50 sentence pairs drawn from the Original-Simp2 and 50 sentences from the Original-Simp4.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-4.jpg
         label: Table 4
         caption: Basic statistics of the Newsela Simplification corpus vs. the Parallel Wikipedia Simplification (PWKP) corpus. The Newsela corpus consists of 1130 articles with original and 4 simplified versions each. Simp-1 is of the least simplified level, while Simp-4 is the most simplified. The numbers marked by * are slightly different from previously reported, because of the use of different tokenizers.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-5.jpg
         label: Table 5
         caption: This table shows the vocabulary changes between different levels of simplification in the Newsela corpus (as a 5×5 matrix). Each cell shows the number of unique word types that appear in the corpus listed in the column but do not appear in the corpus listed in the row. We also list the average frequency of those vocabulary items. For example, in the cell marked *, the Simp-4 version contains 583 unique words that do not appear in the Original version. By comparing the cells marked **, we see about half of the words (19,197 out of 39,046) in the Original version are not in the Simp-4 version. Most of the vocabulary that is removed consists of low-frequency words (with an average frequency of 2.6 in the Original).
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-6.jpg
         label: Table 6
         caption: Top 50 tokens associated with the complex text, computed using the Monroe et al. (2008) method. Bold words are shared by the complex version of Newsela and the complex version of Wikipedia.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-7.jpg
         label: Table 7
         caption: Top 50 tokens associated with the simplified text.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-8.jpg
         label: Table 8
         caption: Frequency of example words from Table 6. These complex words are reduced at a much greater rate in the simplified Newsela than they are in the Simple English Wikipedia. A smaller odds ratio indicates greater reduction.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-table-9.jpg
         label: Table 9
         caption: Top 30 syntax patterns associated with the complex text (left) and simplified text (right). Bold patterns are the top patterns shared by Newsela and Wikipedia.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-figure-2.jpg
         label: Figure 2
         caption: Distribution of document-level compression ratio, displayed as a histogram smoothed by kernel density estimation. The Newsela corpus is more normally distributed, suggesting more consistent quality.
      -
         img: figures/new-data-for-text-simplification/new-data-for-text-simplification-figure-3.jpg
         label: Figure 3
         caption: A radar chart that visualizes the odds ratio (radius axis) of discourse connectives in simple side vs. complex side. An odds ratio larger than 1 indicates the word is more likely to occur in the simplified text than in the complex text, and vice versa. Simple cue words (in the shaded region), except “hence”, are more likely to be added during Newsela’s simplification process than in Wikipedia’s. Complex conjunction connectives (in the unshaded region) are more likely to be retained in Wikipedia’s simplifications than in Newsela’s.
   abstract: Simple Wikipedia has dominated simplification research in the past 5 years. In this opinion paper, we argue that focusing on Wikipedia limits simplification research. We back up our arguments with corpus analysis and by highlighting statements that other researchers have made in the simplification literature. We introduce a new simplification dataset that is a significant improvement over Simple Wikipedia, and present a novel quantitative-comparative approach to study the quality of simplification data resources. 
   bibtex: |
      @article{Xu-EtAl:2015:TACL,
         author = {Wei Xu and Chris Callison-Burch and Courtney Napoles},
         title = {Problems in Current Text Simplification Research: New Data Can
       Help},
         journal = {Transactions of the Association for Computational Linguistics},
         volume = {3},
         year = {2015},
         url = {http://www.cis.upenn.edu/~ccb/publications/new-data-for-text-simplification.pdf},
         pages = {283--297}
       }
-
   title: Cost Optimization for Crowdsourcing Translation
   authors: Mingkun Gao, Wei Xu, and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2015
   url: publications/cost-optimization-for-crowdsourcing-translation.pdf
   page_count: 9
   id: cost-optimization-for-crowdsourcing-translation
   figures:
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-figure-1.jpg
         label: Figure 1
         caption: Example bilingual features for two crowdsourced translations of an Urdu sentence. The numbers are alignment probabilities for each aligned word. The bilingual feature is the average of these probabilities, thus 0.240 for the good translation and 0.043 for the bad translation. Some words are not aligned if potential word pairs don’t exist in bilingual training corpus.
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-table-1.jpg
         label: Table 1
         caption: The relationship between _ (the allowable deviation from the expected upper bound on BLEU score), the BLEU score for translations selected by models from partial sets and the average number of translation candidates set for each source sentence (# Trans).
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-figure-2.jpg
         label: Figure 2
         caption: A time-series plot of all of the translations produced by Turkers (identified by their WorkerID serial number). Turkers are sorted with the best translator at the top of the y-axis. Each tick represent a single translation and black means better than average quality.
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-table-2.jpg
         label: Table 2
         caption: Pearson Correlations for calibration data in different proportion. The percentage column shows what proportion of the whole data set is used for calibration.
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-figure-3.jpg
         label: Figure 3
         caption: Correlation between gold standard ranking and ranking computed using the first 20 sentences as calibration. Each bubble represents a worker. The radius of each bubble shows the relative volume of translations completed by the worker. The weighted correlation is 0.94.
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-figure-4.jpg
         label: Figure 4
         caption: Correlation between gold standard ranking and our model’s ranking. The corresponding weighted correlation is 0.95.
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-table-3.jpg
         label: Table 3
         caption: Correlation (ρ) and translation quality for the various features used by our model. Translation quality is computed by selecting best translations based on model-predicted ranking for workers (rank) and model-predicted scores for translations (score). Here we do not filter out bad workers when selecting the best translation.
      -
         img: figures/cost-optimization-for-crowdsourcing-translation/cost-optimization-for-crowdsourcing-translation-table-4.jpg
         label: Table 4
         caption: A comparison of the translation quality when we retain the top translators under different rankings. The rankings shown are random, the model’s ranking (using all features from Table 3) and the gold ranking. ∆ is the difference between the BLEU scores for the gold ranking and the model ranking. # Trans is the average number of translations needed for each source sentence.
   abstract: Crowdsourcing makes it possible to create translations at much lower cost than hiring professional translators. However, it is still expensive to obtain the millions of translations that are needed to train statistical machine translation systems. We propose two mechanisms to reduce the cost of crowdsourcing while maintaining high translation quality. First, we develop a method to reduce redundant translations. We train a linear model to evaluate the translation quality on a sentence-by-sentence basis, and fit a threshold between acceptable and unacceptable translations. Unlike past work, which always paid for a fixed number of translations for each source sentence and then chose the best from them, we can stop earlier and pay less when we receive a translation that is good enough. Second, we introduce a method to reduce the pool of translators by quickly identifying bad translators after they have translated only a few sentences. This also allows us to rank translators, so that we re-hire only good translators to reduce cost. 
   bibtex: |
      @inproceedings{Gao-EtAl:2015:NAACL,
       author = {Mingkun Gao and Wei Xu and Chris Callison-Burch},
       title = {Cost Optimization in Crowdsourcing Translation},
       booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2015)},
       month = {June},
       year = {2015},
       address = {Denver, Colorado},
       url = {http://www.cis.upenn.edu/~ccb/publications/cost-optimization-for-crowdsourcing-translation.pdf}
       }
-
   title: SemEval-2015 Task 1&colon; Paraphrase and Semantic Similarity in Twitter
   authors: Wei Xu, Chris Callison-Burch, and Bill Dolan
   venue: SemEval
   type: workshop
   year: 2015
   url: publications/paraphrase-and-semantic-similarity-in-twitter.pdf
   page_count: 11
   id: paraphrase-and-semantic-similarity-in-twitter
   figures:
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-table-1.jpg
         label: Table 1
         caption: Representative examples from PIT-2015 Twitter Paraphrase Corpus
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-table-2.jpg
         label: Table 2
         caption: Statistics of PIT-2015 Twitter Paraphrase Corpus. Debatable cases are those received a medium-score from annotators. The percentage of paraphrases is lower in the test set because it was constructed without topic selection.
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-figure-1.jpg
         label: Figure 1
         caption: A heat-map showing overlap between expert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are non-paraphrases; 4,5 are paraphrases. Medium-scored cases (2 for crowdsourcing; 3 for expert annotation) are discarded in the system evaluation of the PI sub-task.
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-figure-2.jpg
         label: Figure 2
         caption: The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield.
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-figure-3.jpg
         label: Figure 3
         caption: Numbers of paraphrases collected by different methods. The annotation efficiency (3,4,5 are regarded as paraphrases) is significantly improved by the sentence filtering and Multi-Armed Bandits (MAB) based topic selection.
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-figure-4.jpg
         label: Figure 4
         caption: PINC scores of paraphrases collected. The higher the PINC, the more significant the rewording. Our proposed annotation strategy quadruples paraphrase yield, while not greatly reducing diversity as measured by PINC.
      -
         img: figures/paraphrase-and-semantic-similarity-in-twitter/paraphrase-and-semantic-similarity-in-twitter-table-3.jpg
         label: Table 3
         caption: Evaluation results. The first column presents the rank of each team in the two tasks based on each team’s best system. The superscripts are the ranks of systems, ordered by F1 for Paraphrase Identification (PI) task and Pearson for Semantic Similarity (SS) task. ⇧ indicates unsupervised or semi-supervised system. In total, 19 teams participated in the PI task, of which 14 teams also participated in the SS task. Note that although the two sub-tasks share the same test set of 972 sentence pairs, the PI task ignores 134 debatable cases (received a medium-score from expert annotator) and uses only 838 pairs (663 paraphrases and 175 non-paraphrases) in evaluation, while SS task uses all 972 pairs. This causes that the F1-score in the PI task can be higher than the maximum F1-score in the SS task. Also note that the F1-scores of the baselines in the PI task are higher than reported in the Table 2 of (Xu et al., 2014), because the later reported maximum F1-scores on the PI task, ignoring the debatable cases.
   abstract: In this shared task, we present evaluations on two related tasks Paraphrase Identification (PI) and Semantic Textual Similarity (SS) systems for the Twitter data. Given a pair of sentences, participants are asked to produce a binary yes/no judgement or a graded score to measure their semantic equivalence. The task features a newly constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs. A total of 19 teams participated, submitting 36 runs to the PI task and 26 runs to the SS task. The evaluation shows encouraging results and open challenges for future research. The best systems scored a F1-measure of 0.674 for the PI task and a Pearson correlation of 0.619 for the SS task respectively, comparing to a strong baseline using logistic regression model of 0.589 F1 and 0.511 Pearson; while the best SS systems can often reach >0.80 Pearson on well-formed text. This shared task also provides insights into the relation between the PI and SS tasks and suggests the importance to bringing these two research areas together. We make all the data, baseline systems and evaluation scripts publicly available. 
   bibtex: |
      @inproceedings{Xu-EtAl:2015:semeval,
         author    = {Wei Xu and Chris Callison-Burch and William B. Dolan},
         title     = {{SemEval-2015 Task} 1: Paraphrase and Semantic Similarity in {Twitter} ({PIT})},
         booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval)},
         year      = {2015},
         url = 
       }
       
-
   title: Effectively Crowdsourcing Radiology Report Annotations
   authors: Anne Cocos, Aaron J. Masino, Ting Qian, Ellie Pavlick, and Chris Callison-Burch
   venue: Sixth International Workshop on Health Text Mining and Information Analysis
   type: workshop
   year: 2015
   url: publications/crowdsourcing-radiology.pdf
   page_count: 6
   id: crowdsourcing-radiology
   abstract: Crowdsourcing platforms are a popular choice for researchers to gather text annotations quickly at scale. We investigate whether crowdsourced annotations are useful when the labeling task requires medical domain knowledge. Comparing a sentence classification model trained with expert-annotated sentences to the same model trained on crowd-labeled sentences, we find the crowdsourced training data to be just as effective as the manually produced dataset. We can improve the accuracy of the crowd-fueled model without collecting further labels by filtering out worker labels applied with low confidence.
   bibtex: |
    @InProceedings{CocosEtAl-2015:LOUHI:Radiology,
      author =  {Anne Cocos and Aaron J. Masino and Ting Qian and Ellie Pavlick and Chris Callison-Burch},
      title =   {Effectively Crowdsourcing Radiology Report Annotations}
      booktitle = {Sixth International Workshop on Health Text Mining and Information Analysis},
      month     = {November},
      year      = {2015},
      address   = {Lisbon, Portugal},
    }
-
   title: Automatically Scoring Freshman Writing&colon; A Preliminary Investigation
   authors: Courtney Napoles and Chris Callison-Burch
   venue: Workshop on Innovative Use of NLP for Building Educational Applications
   type: workshop
   year: 2015
   url: publications/automatically-scoring-freshman-writing.pdf
   page_count: 10
   id: automatically-scoring-freshman-writing
   abstract: In this work, we explore applications of automatic essay scoring (AES) to a corpus of essays written by college freshmen and discuss the challenges we faced. While most AES systems evaluate highly constrained writing, we developed a system that handles open-ended, long-form writing. We present a novel corpus for this task, containing more than 3,000 essays and drafts written for a freshman writing course. We describe statistical analysis of the corpus and identify problems with automatically scoring this type of data. Finally, we demonstrate how to overcome grader bias by using a multi-task setup, and predict scores as well as human graders on a different dataset. Finally, we discuss how AES can help teachers assign more uniform grades.
   bibtex: |
    @InProceedings{napoles-callisonburch:2015:bea,
      author    = {Napoles, Courtney  and  Callison-Burch, Chris},
      title     = {Automatically Scoring Freshman Writing: A Preliminary Investigation},
      booktitle = {Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications},
      month     = {June},
      year      = {2015},
      address   = {Denver, Colorado},
      publisher = {Association for Computational Linguistics},
      pages     = {254--263}
    }

-
   title: Extracting Structured Information via Automatic + Human Computation
   authors: Ellie Pavlick and Chris Callison-Burch
   venue: HCOMP
   type: workshop
   year: 2015
   url: publications/gun-violence-db.pdf
   page_count: 2
   id: gun-violence-db
   abstract: We present a system for extracting structured information from unstructured text using a combination of information retrieval, natural language processing, machine learning, and crowdsourcing. We test our pipeline by building a structured database of gun violence incidents in the United States. The results of our pilot study demonstrate that the proposed methodology is a viable way of collecting large-scale, up-to-date data for public health, public policy, and social science research.
   bibtex: |
    @InProceedings{PavlickAndCallisonBurch-2015:HCOMP:GVDB,
      author =  {Ellie Pavlick and Chris Callison-Burch},
      title =   {Extracting Structured Information via Automatic + Human Computation}
      booktitle = {HCOMP},
      month     = {November},
      year      = {2015},
      address   = {San Diego, California},
    }
-
   title: Ideological Perspective Detection Using Semantic Features
   authors: Heba Elfardy, Mona Diab and Chris Callison-Burch
   venue: STARTSEM
   type: conference
   year: 2015
   url: publications/ideological-perspective-detection.pdf
   page_count: 10
   id: ideological-perspective-detection
   abstract: In this paper, we propose the use of word sense disambiguation and latent semantic features to automatically identify a person’s perspective from his/her written text. We run an Amazon Mechanical Turk experiment where we ask Turkers to answer a set of constrained and open-ended political questions drawn from the American National Election Studies (ANES). We then extract the proposed features from the answers to the open-ended questions and use them to predict the answer to one of the constrained questions, namely, their preferred Presidential Candidate. In addition to this newly created dataset, we also evaluate our proposed approach on a second standard dataset of "Ideological-Debates". This latter dataset contains topics from four domains&colon; Abortion, Creationism, Gun Rights and Gay Rights. Experimental results show that using word sense disambiguation and latent semantics, whether separately or combined, beats the majority and random baselines on the cross-validation and held-out-test sets for both the ANES and the four domains of the "Ideological Debates" datasets. Moreover combining both feature sets outperforms a stronger unigram-only classification system.
   bibtex: |
    @InProceedings{elfardy-diab-callisonburch:2015:*SEM2015,
      author    = {Elfardy, Heba  and  Diab, Mona  and  Callison-Burch, Chris},
      title     = {Ideological Perspective Detection Using Semantic Features},
      booktitle = {Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics},
      month     = {June},
      year      = {2015},
      address   = {Denver, Colorado},
      publisher = {Association for Computational Linguistics},
      pages     = {137--146},
      url       = {http://www.aclweb.org/anthology/S15-1015}
    }
-
   title: End-to-End Statistical Machine Translation with Zero or Small Parallel Texts
   authors: Ann Irvine and Chris Callison-Burch
   venue: Journal of Natural Language Engineering
   pages: 34
   type: journal
   year: 2016
   url: publications/end-to-end-smt-with-zero-or-small-bitexts.pdf
   page_count: 34
   id: end-to-end-smt-with-zero-or-small-bitexts
   abstract: We use bilingual lexicon induction techniques, which learn translations from monolingual texts in two languages, to build an end-to-end statistical machine translation (SMT) system without the use of any bilingual sentence-aligned parallel corpora. We present detailed analysis of the accuracy of bilingual lexicon induction, and show how a discriminative model can be used to combine various signals of translation equivalence (like contextual similarity, temporal similarity, orthographic similarity and topic similarity). Our discriminative model produces higher accuracy translations than previous bilingual lexicon induction techniques. We reuse these signals of translation equivalence as features on a phrase-based SMT system. These monolingually-estimated features enhance low resource SMT systems in addition to allowing end-to-end machine translation without parallel corpora.
   bibtex: |
      @article{Irvine-Callison-Burch:2015:JNLE,
         author = {Ann Irvine and Chris Callison-Burch},
         title = {End-to-End Statistical Machine Translation with Zero or Small Parallel Texts},
         journal = {Journal of Natural Language Engineering},
         volume = {22},
         issue = {4},
         year = {2016},
         url = {http://www.cis.upenn.edu/~ccb/publications/end-to-end-smt-with-zero-or-small-bitexts.pdf},
         pages = {517-548}
       }
-
   title: Arabic Dialect Identification
   authors: Omar Zaidan and Chris Callison-Burch
   venue: Computational Linguistics
   type: journal
   year: 2014
   url: publications/arabic-dialect-id.pdf
   page_count: 36
   id: arabic-dialect-id
   figures:
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-1.jpg
         label: Figure 1
         caption: One possible breakdown of spoken Arabic into dialect groups&colon; Maghrebi, Egyptian, Levantine, Gulf, and Iraqi. Habash (2010) and Versteegh (2001) give a breakdown along mostly the same lines. Note that this is a relatively coarse breakdown, and further division of the dialect groups is possible, especially in large regions such as the Maghreb.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-1.jpg
         label: Table 1
         caption: A few examples illustrating similarities and differences across MSA and three Arabic dialects&colon; Levantine, Gulf, and Egyptian. Even when a word is spelled the same across two or more varieties, the pronunciation might differ due to differences in short vowels (which are not spelled out). Also, due to the lack of orthography standardization, and variance in pronunciation even within a single dialect, some dialectal words could have more than one spelling (e.g. Egyptian “I drink” could be bAšrb, Levantine “He drinks” could be byšrb). (We use the Habash-Soudi-Buckwalter transliteration scheme to represent Arabic orthography, which maps each Arabic letter to a single, distinct character. We provide a table with the character mapping in Appendix A.)
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-2.jpg
         label: Figure 2
         caption: Two roughly equivalent Arabic sentences, one in MSA and one in Levantine Arabic, translated by the same MT system (Google Translate) into English. An acceptable translation would be When will we see this group of criminals undergo trial (or tried)?. The MSA variant is handled well, while the dialectal variant is mostly transliterated.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-3.jpg
         label: Figure 3
         caption: Two roughly equivalent Arabic sentences, one in MSA and one in Egyptian Arabic, translated by the same MT system (Google Translate) into English. An acceptable translation would be What is this that is happening? What is this that I’m seeing?. As in Figure 2, the dialectal variant is handles quite poorly.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-4.jpg
         label: Figure 4
         caption: The output of a Spanish-to-English system when given a Portuguese sentence as input, compared to the output of a Portuguese-to-English system, which performs well. The behavior is very similar to that in Figures 2 and 3, namely the failure to translate out-of-vocabulary words when there is a language mismatch.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-2.jpg
         label: Table 2
         caption: A summary of the different components of the AOC dataset. Overall, 1.4M comments were harvested from 86.1K articles, corresponding to 52.1M words.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-5.jpg
         label: Figure 5
         caption: Three sentences that were identified by our annotators as dialectical, even thought they do not contain individually dialectal words. A word-based OOV-detection approach would fail to classify these sentences as being dialectal, since all these words could appear in an MSA corpus. One might argue that a distinction should be drawn between informal uses of MSA versus dialectical sentences, but annotators consistently classify these sentences as dialect.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-6.jpg
         label: Figure 6
         caption: The dialectal sentences of Figure 5, with MSA equivalents.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-7.jpg
         label: Figure 7
         caption: The interface for the dialect identification task. This example, and the full interface, can be viewed at the URL http&colon;//bit.ly/eUtiO3.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-3.jpg
         label: Table 3
         caption: Some statistics over the labels provided by three spammers. Compared to the typical worker (right-most column), all workers perform terribly on the MSA control items, and also usually fail to recognize dialectal content in commentary sentences. Other red flags, such as geographic location and ‘identifying’ unrepresented dialects, are further proof of the spammy behavior.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-8.jpg
         label: Figure 8
         caption: The distribution of labels provided by the workers for the dialect identification task, over all three news sources (a) and over each individual news source (b–d). Al-Ghad is published in Jordan, Al-Riyadh in Saudi Arabia, and Al-Youm Al-Sabe’ in Egypt. Their local readerships are reflected in the higher proportion of corresponding dialects. Note that this is not a breakdown on the sentence level, and does not reflect any kind of majority voting. For example, most of the LEV labels on sentences from the Saudi newspaper are trumped by GLF labels when taking a majority vote, making the proportion of LEV-majority sentences smaller than what might be deduced by looking at the label distribution in (c).
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-4.jpg
         label: Table 4
         caption: The specific-dialect label distribution (given that a dialect label was provided), shown for each speaker group.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-9.jpg
         label: Figure 9
         caption: A bubble chart showing workers’ MSA and dialect recall. Each data point (or ‘bubble’) in the graph represents one annotator, with the bubble size corresponding to the number of Assignments completed by that annotator.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-5.jpg
         label: Table 5
         caption: Two annotators with a General label bias, one who uses the label liberally, and one who is more conservative. Note that in both cases, there is a noticeably smaller percentage of General labels in the Egyptian newspaper than in the Jordanian and Saudi newspapers.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-10.jpg
         label: Figure 10
         caption: Learning curves for the general MSA vs. dialect task, with all three news sources pooled together. Learning curves for the individual news sources can be found in Figure 11. The 83% line has no significance, and is provided to ease comparison with Figure 11.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-figure-11.jpg
         label: Figure 11
         caption: Learning curves for the MSA vs. dialect task, for each of the three news sources. The 83% line has no significance, and is provided to ease comparison across the three components, and with Figure 10.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-6.jpg
         label: Table 6
         caption: Accuracy rates (%) on several 2-way classification tasks (MSA vs. dialect) for various models. Models in the top part of the table do not utilize the dialect-annotated data, while models in the bottom part do. (For the latter kind of models, the accuracy rates reported are based on a training set size of 90% of the available data.)
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-7.jpg
         label: Table 7
         caption: Confusion matrix in the 4-way classification setup. Rows correspond to actual labels, and columns correspond to predicted labels. For instance, 6.7% of MSA sentences were given a GLF label (first row, third column). Note that entries within a single row sum to 100%.
      -
         img: figures/arabic-dialect-id/arabic-dialect-id-table-8.jpg
         label: Table 8
         caption: Predicted label breakdown for the crawled data, over the four varieties of Arabic. All varieties were given equal priors.
   abstract: The written form of the Arabic language, Modern Standard Arabic (MSA), differs in a non-trivial manner from the various spoken regional dialects of Arabic – the true “native languages” of Arabic speakers. Those dialects, in turn, differ quite a bit from each other. However, due to MSA’s prevalence in written form, almost all Arabic datasets have predominantly MSA content. In this article, we describe the creation of a novel Arabic resource with dialect annotations. We have created a large monolingual dataset rich in dialectal Arabic content, called the Arabic Online Commentary Dataset (Zaidan and Callison-Burch 2011). We describe our annotation effort to identify the dialect level (and dialect itself) in each of more than 100,000 sentences from the dataset by crowdsourcing the annotation task, and delve into interesting annotator behaviors (like over-identification of one’s own dialect). Using this new annotated dataset, we consider the task of Arabic dialect identification&colon; given the word sequence forming an Arabic sentence, determine the variety of Arabic in which it is written. We use the data to train and evaluate automatic classifiers for dialect identification, and establish that classifiers using dialectal data significantly and dramatically outperform baselines that use MSA-only data, achieving near-human classification accuracy. Finally, we apply our classifiers to discover dialectical data from a large web crawl consisting of 3.5 million pages mined from online Arabic newspapers. 
   bibtex: |
      @article{zaidan-callisonburch:CL:2013,
         author    = {Omar F. Zaidan and Chris Callison-Burch},
         title =   {Arabic Dialect Identification},
         journal = {Computational Linguistics},
         year =    {2014},
         volume = {40},
         number = {1},
         pages = {171-202}
       }

-
   title: Extracting Lexically Divergent Paraphrases from Twitter
   authors: Wei Xu, Alan Ritter, Chris Callison-Burch, William B. Dolan and Yangfeng Ji
   venue: TACL
   type: journal
   year: 2014
   url: publications/extracting-paraphrases-from-twitter.pdf
   page_count: 14
   id: extracting-paraphrases-from-twitter
   figures:
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-1.jpg
         label: Figure 1
         caption: (a) a plate representation of the MULTIP model (b) an example instantiation of MULTIP for the pair of sentences “Manti bout to be the next Junior Seau” and “Teo is the little new Junior Seau”, in which a new American football player Manti Te’o was being compared to a famous former player Junior Seau. Only 4 out of the total 6 × 5 word pairs, z1 - z30, are shown here.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-table-1.jpg
         label: Table 1
         caption: Representative examples from paraphrase corpora. The average sentence length is 11.9 words in Twitter vs. 18.6 in the news corpus.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-2.jpg
         label: Figure 2
         caption: MULTIP Learning Algorithm
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-table-2.jpg
         label: Table 2
         caption: Performance of different paraphrase identification approaches on Twitter version that uses additional 1.6 million sentences from Twitter. ** Reimplementation of a strong baseline used by Das and Smith (2009).
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-table-3.jpg
         label: Table 3
         caption: Feature ablation by removing each individual feature group from the full set.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-table-4.jpg
         label: Table 4
         caption: Example system outputs; rank is the position in the list of all candidate paraphrase pairs in the test set ordered by model score. MULTIP discovers lexically divergent paraphrases while LEXLATENT prefers more overall sentence similarity. Underline marks the word pair(s) with highest estimated probability as paraphrastic anchor(s) for each sentence pair.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-3.jpg
         label: Figure 3
         caption: Precision and recall curves. Our MULTIP model alone achieves competitive performance with the LEXLATENT system that combines latent space model and feature-based supervised classifier. The two approaches have complementary strengths, and achieves significant improvement when combined together (MULTIP-PE).
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-4.jpg
         label: Figure 4
         caption: A heat-map showing overlap between expert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are non-paraphrases; 4,5 are paraphrases. Medium-scored cases are discarded in training and testing in our experiments.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-5.jpg
         label: Figure 5
         caption: The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-6.jpg
         label: Figure 6
         caption: Numbers of paraphrases collected by different methods. The annotation efficiency (3,4,5 are regarded as paraphrases) is significantly improved by the sentence filtering and Multi-Armed Bandits (MAB) based topic selection.
      -
         img: figures/extracting-paraphrases-from-twitter/extracting-paraphrases-from-twitter-figure-7.jpg
         label: Figure 7
         caption: PINC scores of paraphrases collected. The higher the PINC, the more significant the rewording. Our proposed annotation strategy quadruples paraphrase yield, while not greatly reducing diversity as measured by PINC.
   abstract: We present MULTIP (Multi-instance Learning Paraphrase Model), a new model suited to identify paraphrases within the short messages on Twitter. We jointly model paraphrase relations between word and sentence pairs and assume only sentence-level annotations during learning. Using this principled latent variable model alone, we achieve the performance competitive with a state-of-the-art method which combines a latent space model with a feature-based supervised classifier. Our model also captures lexically divergent paraphrases that differ from yet complement previous methods; combining our model with previous work significantly outperforms the state-of-the-art. In addition, we present a novel annotation methodology that has allowed us to crowdsource a paraphrase corpus from Twitter. We make this new dataset available to the research community. 
   bibtex: |
      @article{Xu-EtAl-2014:TACL,
         author =  {Wei Xu and Alan Ritter and Chris Callison-Burch and William B. Dolan and Yangfeng Ji},
         title =   {Extracting Lexically Divergent Paraphrases from {Twitter}},
         journal = {Transactions of the Association for Computational Linguistics},
         volume =  {2},
         number =  {},
         year =    {2014},
         pages = {435--448},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/extracting-paraphrases-from-twitter.pdf}
       }
       
-
   title: Translations of the CALLHOME Egyptian Arabic corpus for conversational speech translation
   authors: Gaurav Kumar, Yuan Cao, Ryan Cotterell, Chris Callison-Burch, Daniel Povey, and Sanjeev Khudanpur
   venue: IWSLT
   type: workshop
   year: 2014
   url: publications/callhome-egyptian-arabic-speech-translations.pdf
   page_count: 5
   id: callhome-egyptian-arabic-speech-translations
   figures:
      -
         img: figures/callhome-egyptian-arabic-speech-translations/callhome-egyptian-arabic-speech-translations-table-1.jpg
         label: Table 1
         caption: Sizes (in # conversations) of the Callhome Egyptian Arabic corpus, supplements and evaluation datasets. The conversations last between 5-30 minutes.
      -
         img: figures/callhome-egyptian-arabic-speech-translations/callhome-egyptian-arabic-speech-translations-table-2.jpg
         label: Table 2
         caption: Partition statistics for the Callhome Egyptian Arabic corpus, supplements and evaluation datasets. Column 2,3 and 4 represent number of utterances, numbers of words and average number of words per utterance respectively.
      -
         img: figures/callhome-egyptian-arabic-speech-translations/callhome-egyptian-arabic-speech-translations-table-3.jpg
         label: Table 3
         caption: A sample of the special symbols using in the Arabic transcripts. These represent non-conventional speech segments such as non-verbal vocalizations, disfluencies, background noise and distortion.
      -
         img: figures/callhome-egyptian-arabic-speech-translations/callhome-egyptian-arabic-speech-translations-table-4.jpg
         label: Table 4
         caption: The results of the translation task described in section 4. Each utterance in the original partitions has about four redundant translations. The number of utterances in column 2 has hence effectively been multiplied by 4. The last column represents the number of words per utterance in the translations.
      -
         img: figures/callhome-egyptian-arabic-speech-translations/callhome-egyptian-arabic-speech-translations-table-5.jpg
         label: Table 5
         caption: A sample of the translations obtained using the translation task described in section 4. The translations are lower-cased, tokenized and punctuation has been normalized.
   abstract: Translation of the output of automatic speech recognition (ASR) systems, also known as speech translation, has received a lot of research interest recently. This is especially true for programs such as DARPA BOLT which focus on improving spontaneous human-human conversation across languages. However, this research is hindered by the dearth of datasets developed for this explicit purpose. For Egyptian Arabic-English, in particular, no parallel speech-transcription-translation dataset exists in the same domain. In order to support research in speech translation, we introduce the Callhome Egyptian Arabic-English Speech Translation Corpus. This supplements the existing LDC corpus with four reference translations for each utterance in the transcripts. The result is a three-way parallel dataset of Egyptian Arabic Speech, transcriptions and English translations. 
   bibtex: |
      @InProceedings{kumar-EtAl:2014:IWSLT,
         author    = {Matt Post and Gaurav Kumar and Adam Lopez and Damianos Karakos and Chris Callison-Burch and Sanjeev Khudanpur},
         title     = {Translations of the {CALLHOME} {Egyptian} {Arabic} corpus for conversational speech translation},
         booktitle = {Proceedings of the International Workshop on Spoken Language Translation (IWSLT)}
         month     = {December},
         year      = {2014},
         address   = {Lake Tahoe, USA},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/callhome-egyptian-arabic-speech-translations.pdf}
       }
       
-
   title: Poetry of the Crowd&colon; A Human Computation Algorithm to Convert Prose into Rhyming Verse
   authors: Quanze Chen, Chenyang Lei, Wei Xu, Ellie Pavlick and Chris Callison-Burch
   venue: HCOMP Poster
   type: workshop
   year: 2014
   url: publications/poetry-generation-with-crowdsourcing.pdf
   page_count: 3
   id: poetry-generation-with-crowdsourcing
   figures:
      -
         img: figures/poetry-generation-with-crowdsourcing/poetry-generation-with-crowdsourcing-figure-1.jpg
         label: Figure 1
         caption: We combine the creative powers and language skills of people with computational algorithms for expanding paraphrase, identifying rhymes and calculating meter
      -
         img: figures/poetry-generation-with-crowdsourcing/poetry-generation-with-crowdsourcing-figure-2.jpg
         label: Figure 2
         caption: Appropriate lexical substitutions (in shaded boxes) selected by crowdsourcing workers regarding to the context.
      -
         img: figures/poetry-generation-with-crowdsourcing/poetry-generation-with-crowdsourcing-figure-3.jpg
         label: Figure 3
         caption: Figure 3&colon; A crowd worker composes a sentence path with the “- + - + - + - + - +” stress pattern (iambic pentameter). The ending word stalls rhymes with other words that terminate sentences constructed in another worker’s task.
   abstract: Poetry composition is a very complex task that requires a poet to satisfy multiple constraints concurrently. We believe that the task can be augmented by combining the creative abilities of humans with computational algorithms that efficiently constrain and permute available choices. We present a hybrid method for generating poetry from prose that combines crowdsourcing with natural language processing (NLP) machinery. We test the ability of crowd workers to accomplish the technically challenging and creative task of composing poems. 
   bibtex: |
      @InProceedings{Chen-et-al:HCOMP:2014,
         author    = {Quanze Chen and Chenyang Lei and Wei Xu and Ellie Pavlick and Chris Callison-Burch},
         title     = {Poetry of the Crowd: A Human Computation Algorithm to Convert Prose into Rhyming Verse},
         booktitle = {The Second AAAI Conference on Human Computation and Crowdsourcing (HCOMP-2014)},
         month     = {November},
         year      = {2014},
         url       = {http://cis.upenn.edu/~ccb/publications/poetry-generation-with-crowdsourcing.pdf}
       }
       
-
   title: Crowd-Workers&colon; Aggregating Information Across Turkers To Help Them Find Higher Paying Work
   authors: Chris Callison-Burch
   venue: HCOMP Poster
   type: workshop
   year: 2014
   url: publications/crowd-workers.pdf
   page_count: 2
   id: crowd-workers
   figures:
      -
         img: figures/crowd-workers/crowd-workers-figure-1.jpg
         label: Figure 1
         caption: The crowd-workers.com web service allows users to sort HITs based on hourly rate. The hourly rate is estimated by tracking how long it took other workers to complete the task (using our browser extension), along with the reward amount (which MTurk makes available).
      -
         img: figures/crowd-workers/crowd-workers-figure-2.jpg
         label: Figure 2
         caption: The hourly earnings of the 65 Turkers in our pilot study who completed more than 100 HITs using the Crowd-Workers browser plugin.
   abstract: The Mechanical Turk crowdsourcing platform currently fails to provide the most basic piece of information to enable workers to make informed decisions about which tasks to undertake&colon; what is the expected hourly pay? Mechanical Turk advertises a reward amount per assignment, but does not give any indication of how long each assignment will take. We have developed a browser plugin that tracks the length of time it takes to complete a task, and a web service that aggregates the information across many workers. Crowd-Workers. com allows workers to discovery higher paying work by sorting tasks by estimated hourly rate. 
   bibtex: |
      @InProceedings{Chen-et-al:HCOMP:2014,
         author    = {Chris Callison-Burch},
         title     = {Crowd-Workers: Aggregating Information Across Turkers To Help Them Find Higher Paying Work},
         booktitle = {The Second AAAI Conference on Human Computation and Crowdsourcing (HCOMP-2014)},
         month     = {November},
         year      = {2014},
         url       = {http://cis.upenn.edu/~ccb/publications/crowd-workers.pdf}
       }
       
-
   title: The Language Demographics of  Amazon Mechanical Turk
   authors: Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev, and Chris Callison-Burch
   venue: TACL
   type: journal
   year: 2014
   url: publications/language-demographics-of-mechanical-turk.pdf
   page_count: 13
   id: language-demographics-of-mechanical-turk
   figures:
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-figure-1.jpg
         label: Figure 1
         caption: The number of workers per country. This map was generated based on geolocating the IP address of 4,983 workers in our study. Omitted are 60 workers who were located in more than one country during the study, and 238 workers who could not be geolocated. The size of the circles represents the number of workers from each country. The two largest are India (1,998 workers) and the United States (866). To calibrate the sizes&colon; the Philippines has 142 workers, Egypt has 25, Russia has 10, and Sri Lanka has 4.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-table-1.jpg
         label: Table 1
         caption: Self-reported native language of 3,216 bilingual Turkers. Not shown are 49 languages with 20 speakers. We omit 1,801 Turkers who did not report their native language, 243 who reported 2 native languages, and 83 with
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-table-2.jpg
         label: Table 2
         caption: A list of the languages that were used in our study, grouped by the number of Wikipedia articles in the language. Each language’s code is given in parentheses. These language codes are used in other figures throughout this paper.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-figure-3.jpg
         label: Figure 3
         caption: An example of the Turkers’ translations of a Hindi sentence. The translations are unedited and contain fixable spelling, capitalization and grammatical errors.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-figure-2.jpg
         label: Figure 2
         caption: Days to complete the translation HITs for 40 of the languages. Tick marks represent the completion of individual assignments.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-figure-4.jpg
         label: Figure 4
         caption: Translation quality for languages with at least 50 Turkers. The dark blue bars indicate the proportion of translations which exactly matched gold standard translations, and light blue indicate translations which were judged to be correct synonyms. Error bars show the 95% confidence intervals for each language.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-figure-5.jpg
         label: Figure 5
         caption: (a) Individual workers’ overlap with Google Translate. We removed the 500 workers with the highest overlap (shaded region on the left) from our analyses, as it is reasonable to assume these workers are cheating by submitting translations from Google. Workers with no overlap (shaded region on the right) are also likely to be cheating, e.g. by submitting random text. (b) Cumulative distribution of overlap with Google translate for workers and translations. We see that eliminating all workers with >70% overlap with google translate still preserves 90% of translations and >90% of workers.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-table-3.jpg
         label: Table 3
         caption: Translation quality when partitioning the translations into two groups, one containing translations submitted by Turkers whose location is within regions that plausibly speak the foreign language, and the other containing translations from Turkers outside those regions. In general, in-region Turkers provide higher quality translations. (**) indicates differences significant at p=0.05, (*) at p=0.10.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-figure-6.jpg
         label: Figure 6
         caption: The total volume of translations (measured in English words) as a function of elapsed days.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-table-4.jpg
         label: Table 4
         caption: Size of parallel corpora and bilingual dictionaries collected for each language.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-table-5.jpg
         label: Table 5
         caption: BLEU scores for translating into English using bilingual parallel corpora by themselves, and with the addition of single-word dictionaries. Scores are calculated using four reference translations and represent the mean of three MERT runs.
      -
         img: figures/language-demographics-of-mechanical-turk/language-demographics-of-mechanical-turk-table-6.jpg
         label: Table 6
         caption: The green box shows the best languages to target on MTurk. These languages have many workers who generate high quality results quickly. We defined many workers as 50 or more active in-region workers, high quality as
   abstract: We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk). We establish a methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying. We validate workers' self-reported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages. Our study ran for several months, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it. Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk. We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to develop crowdsourced, multilingual technologies. To further demonstrate the value of creating data via crowdsourcing, we hire workers to create bilingual parallel corpora in six Indian languages, and use them to train statistical machine translation systems. 
   bibtex: |
      @article{Pavlick-EtAl-2014:TACL,
         author =  {Ellie Pavlick and Matt Post and Ann Irvine and Dmitry Kachaev and Chris Callison-Burch},
         title =   {The Language Demographics of {Amazon Mechanical Turk}},
         journal = {Transactions of the Association for Computational Linguistics},
         volume =  {2},
         number =  {Feb},
         year =    {2014},
         pages = {79--92},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/language-demographics-of-mechanical-turk.pdf}
       }
       
-
   title: Hallucinating Phrase Translations for Low Resource MT
   authors: Ann Irvine and Chris Callison-Burch
   venue: CoNLL
   type: conference
   year: 2014
   url: publications/hallucinating-phrase-translations.pdf
   page_count: 11
   id: hallucinating-phrase-translations
   figures:
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-figure-1.jpg
         label: Figure 1
         caption: Example of loosely composed translations for the Spanish input in A, la casa linda. In B, we remove the stop word la. Then, in C, we enumerate the cartesian product of all unigram translations in the bilingual dictionary and sort the words within each alphabetically. Finally, we look up each list of words in C in the inverted index, and corresponding target phrases are enumerated in D. The inverted index contains all phrasal combinations and permutations of the word lists in C which also appear monolingually with some frequency and with, optionally, any number of stop words.
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-figure-2.jpg
         label: Figure 2
         caption: Example output from motivating experiment&colon; a comparison of the baseline and full oracle translations of Spanish no hab ́ıa nadie en los centros electorales, which translates correctly as there was nobody at the voting offices. The full oracle is augmented with translations composed from the seed model as well as induced unigram translations. The phrase was no one is composeable from hab ́ıa nadie given the seed model. In contrast, the phrase polling stations is composeable from centros electorales using induced translations. For each translation, the phrase segmentations used by the decoder are highlighted.
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-table-1.jpg
         label: Table 1
         caption: Motivating Experiment&colon; BLEU results using the baseline SMT model and composeable oracle translations with and without induced unigram translations.
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-table-2.jpg
         label: Table 2
         caption: Top five induced translations for several source words. Correct translations are bolded. aceite translates as oil.
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-table-3.jpg
         label: Table 3
         caption: Top three compositional translations for several source phrases and their model scores. Correct translations are bolded.
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-figure-3.jpg
         label: Figure 3
         caption: Precision Recall curve with BLEU scores for the top-k scored hallucinated translations. k varies from 1 to 200. Baseline model performance is shown with a red triangle.
      -
         img: figures/hallucinating-phrase-translations/hallucinating-phrase-translations-table-4.jpg
         label: Table 4
         caption: Experimental results. First, the baseline models are augmented with monolingual phrase table features and then also with the top-5 induced translations for all OOV unigrams. Then, we append the top-k hallucinated phrase translations to the third baseline models. BLEU scores are aver- aged over three tuning runs. We measure the statistical significance of each +Phrase Trans model in comparison with the highest performing (bolded) baseline for each language; * indicates statistical significance with p ă0.01.
   abstract: We demonstrate that “hallucinating” phrasal translations can significantly improve the quality of machine translation in low resource conditions. Our hallucinated phrase tables consist of entries composed from multiple unigram translations drawn from the baseline phrase table and from translations that are induced from monolingual corpora. The hallucinated phrase table is very noisy. Its translations are low precision but high recall. We counter this by introducing 30 new feature functions (including a variety of monolingually-estimated features) and by aggressively pruning the phrase table. Our analysis evaluates the intrinsic quality of our hallucinated phrase pairs as well as their impact in end-to-end Spanish-English and Hindi-English MT. 
   bibtex: |
      @InProceedings{irvine-callisonburch:2014:W14-16,
         author    = {Irvine, Ann  and  Callison-Burch, Chris},
         title     = {Hallucinating Phrase Translations for Low Resource MT},
         booktitle = {Proceedings of the Eighteenth Conference on Computational Natural Language Learning},
         month     = {June},
         year      = {2014},
         pages     = {160--170},
         url       = {http://www.aclweb.org/anthology/W14-1617}
       }
       
-
   title: Using Comparable Corpora to Adapt MT Models to New Domains
   authors: Ann Irvine and Chris Callison-Burch
   venue: WMT
   type: workshop
   year: 2014
   url: publications/using-comparable-corpora-for-mt-adaptation.pdf
   page_count: 8
   id: using-comparable-corpora-for-mt-adaptation
   figures:
      -
         img: figures/using-comparable-corpora-for-mt-adaptation/using-comparable-corpora-for-mt-adaptation-table-1.jpg
         label: Table 1
         caption: Summary of the size of each corpus of text used in this work in terms of the number of source and target word tokens.
      -
         img: figures/using-comparable-corpora-for-mt-adaptation/using-comparable-corpora-for-mt-adaptation-table-2.jpg
         label: Table 2
         caption: Top 10 Wikipedia articles ranked by their similarity to large new-domain English monolingual corpora.
      -
         img: figures/using-comparable-corpora-for-mt-adaptation/using-comparable-corpora-for-mt-adaptation-table-3.jpg
         label: Table 3
         caption: Comparison between the performance of baseline old-domain translation models and domain-adapted models in translating science and medical domain text. We experiment with two language models&colon; old, trained on the English side of our Hansard old-domain training corpus and new, trained on the English side of the parallel training data in each new domain. We use comparable corpora of 5, 000 (1) random, and (2) the most new-domain-like document pairs to score phrase tables. All results are averaged over three tuning runs, and we perform statistical significance testing comparing each system augmented with additional features with the baseline system that uses the same language model(s). * indicates that the BLEU scores are statistically significant with p † 0.01.
   abstract: In previous work we showed that when using an SMT model trained on old-domain data to translate text in a new-domain, most errors are due to unseen source words, unseen target translations, and inaccurate translation model scores (Irvine et al., 2013a). In this work, we target errors due to inaccurate translation model scores using new-domain comparable corpora, which we mine from Wikipedia. We assume that we have access to a large olddomain parallel training corpus but only enough new-domain parallel data to tune model parameters and do evaluation. We use the new-domain comparable corpora to estimate additional feature scores over the phrase pairs in our baseline models. Augmenting models with the new features improves the quality of machine translations in the medical and science domains by up to 1.3 BLEU points over very strong baselines trained on the 150 million word Canadian Hansard dataset. 
   bibtex: |
      @InProceedings{irvine-callisonburch:2014:W14-33,
         author    = {Irvine, Ann  and  Callison-Burch, Chris},
         title     = {Using Comparable Corpora to Adapt MT Models to New Domains},
         booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2014},
         pages     = {437--444},
         url       = {http://www.aclweb.org/anthology/W14-3357}
       }
       
-
   title: Are Two Heads are Better than One? Crowdsourced Translation via a Two-Step Collaboration between Translators and Editors
   authors: Rui Yan, Mingkun Gao, Ellie Pavlick, and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2014
   url: publications/crowdsourced-translation-via-collaboration-between-translators-and-editors.pdf
   page_count: 11
   id: crowdsourced-translation-via-collaboration-between-translators-and-editors
   figures:
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-table-1.jpg
         label: Table 1
         caption: Different versions of translations.
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-1.jpg
         label: Figure 1
         caption: Relationship between editor aggressiveness and effectiveness. Each point represents an editor/translation pair. Aggressiveness (x-axis) is measured as the TER between the pre-edit and post-edit version of the translation, and effective- ness (y-axis) is measured as the average amount by which the editing reduces the translation’s TERgold. While many editors make only a few changes, those who make many changes can bring the translation substantially closer to professional quality.
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-2.jpg
         label: Figure 2
         caption: Effect of editing on translations of varying quality. Rows reflect bins of editors, with the worse editors (those whose changes result in increased TERgold) on the top and the most effective editors (those whose changes result in the largest reduction in TERgold) on the bottom. Bars reflect bins of translations, with the highest TERgold translations on the left, and the lowest on the right. We can see from the consistently negative
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-3.jpg
         label: Figure 3
         caption: Three alternative translations (left) and the edited versions of each (right). Each edit on the right was produced by a different editor. Order reflects the TERgold of each translation, with the lowest TERgold on the top. Some translators receive low TERgold scores due to superficial errors, which can be easily improved through editing. In the above example, the middle-ranked translation (green) becomes the best translation after being revised by a good editor.
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-4.jpg
         label: Figure 4
         caption: 2-step collaborative crowdsourcing translation model based on graph ranking framework including three sub-networks. The undirected links between users denotes translation-editing collaboration. The undirected links between candidate translations indicate lexical similarity between candidates. A bipartite graph ties candidate and Turker networks together by authorship (to make the figure clearer, some linkage is omitted). A dashed circle indicates the group of candidate translations for a single source sentence to translate.
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-table-2.jpg
         label: Table 2
         caption: Overall BLEU performance for all methods (with and without post-editing). The highlighted result indicates the best performance, which is based on both candidate sentences and Turker information.
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-figure-5.jpg
         label: Figure 5
         caption: Effect of candidate-Turker coupling (
      -
         img: figures/crowdsourced-translation-via-collaboration-between-translators-and-editors/crowdsourced-translation-via-collaboration-between-translators-and-editors-table-3.jpg
         label: Table 3
         caption: Variations of all component settings.
   abstract: Crowdsourcing is a viable mechanism for creating training data for machine translation. It provides a low cost, fast turn-around way of processing large volumes of data. However, when compared professional translation, naive collection of translations from non-professionals yields low-quality results. Careful quality control is necessary for crowdsourcing to work well. In this paper, we examine the challenges of a two-step collaboration process with translation and post-editing by non-professionals. We develop graph-based ranking models that automatically select the best output from multiple redundant versions of translations and edits, and improves translation quality closer to professionals. 
   bibtex: |
      @InProceedings{Yan-EtAl-2014:ACL,
         author =  {Rui Yan and Mingkun Gao and Ellie Pavlick and Chris Callison-Burch},
         title =   {Are Two Heads are Better than One? Crowdsourced Translation via a Two-Step Collaboration between Translators and Editors},
         booktitle = {The 52nd Annual Meeting of the Association for Computational Linguistics},
         month     = {June},
         year      = {2014},
         address   = {Baltimore, Maryland},
         publisher = {Association for Computional Linguistics},
         url = {http://www.cis.upenn.edu/~ccb/publications/crowdsourced-translation-via-collaboration-between-translators-and-editors.pdf}
       }
       
-
   title: PARADIGM&colon; Paraphrase Diagnostics through Grammar Matching
   authors: Jonathan Weese, Juri Ganitkevitch, and Chris Callison-Burch
   venue: EACL
   type: conference
   year: 2014
   url: publications/paradigm-paraphrase-evaluation.pdf
   page_count: 10
   id: paradigm-paraphrase-evaluation
   figures:
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-figure-1.jpg
         label: Figure 1
         caption: PARADIGM extracts lexical, phrasal and syntactic paraphrases from parsed, word-aligned sentence pairs.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-figure-2.jpg
         label: Figure 2
         caption: Four examples each of lexical, phrasal, and syntactic paraphrases that can be extracted from the sentence pair in Figure 1.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-figure-3.jpg
         label: Figure 3
         caption: We measure the goodness of paraphrase grammars by determine how often they can be used to synchronously parse gold-standard sentential paraphrases. Note we do not require the synchronous derivation to match a gold-standard parse tree.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-1.jpg
         label: Table 1
         caption: Amount of English–English parallel data. LDC data has 4 parallel translations per sentence. Literature data is from Barzilay and McKeown (2001). MSR data is from Quirk et al. (2004) and Dolan et al. (2004). ParaMertic data is from Callison-Burch et al. (2008).
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-2.jpg
         label: Table 2
         caption: Size of various paraphrase grammars.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-3.jpg
         label: Table 3
         caption: Size of strict overlap (number of rules and % of the gold standard) of each grammar with a syntactic grammar derived from ParaMetric. freq. ≥ 2 means we first removed all rules that appeared only once from the ParaMetric grammar. The number in parentheses shows the percentage of ParaMetric rules that are present in the overlap.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-4.jpg
         label: Table 4
         caption: Size of non-strict overlap of each grammar with the syntactic grammar derived from ParaMetric. The number in parentheses shows the percentage of ParaMetric rules that are present in the overlap.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-figure-4.jpg
         label: Figure 4
         caption: Precision lower bound and relative recall when overlapping different sizes of PPDB with the syntactic ParaMetric grammar.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-5.jpg
         label: Table 5
         caption: Number of paraphrases of each type in each grammar’s strict overlap with the syntactic ParaMetric grammar. Numbers in parentheses show the percentage of ParaMetric rules of each type.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-6.jpg
         label: Table 6
         caption: Parse coverage on held-out LDC data. The all column considers every possible sentential paraphrase in the test set. The any column considers a sentence parsed if any of its paraphrases was able to parsed.
      -
         img: figures/paradigm-paraphrase-evaluation/paradigm-paraphrase-evaluation-table-7.jpg
         label: Table 7
         caption: The correlation (Spearman’s ρ) of different automatic evaluation metrics with human judgments of paraphrase quality for the text-to- text generation task of sentence compression.
   abstract: Paraphrase evaluation is typically done either manually or through indirect, task-based evaluation. We introduce an intrinsic evaluation PARADIGM which measures the goodness of paraphrase collections that are represented using synchronous grammars. We formulate two measures that evaluate these paraphrase grammars using gold standard sentential paraphrases drawn from a monolingual parallel corpus. The first measure calculates how often a paraphrase grammar is able to synchronously parse the sentence pairs in the corpus. The second measure enumerates paraphrase rules from the monolingual parallel corpus and calculates the overlap between this reference paraphrase collection and the paraphrase resource being evaluated. We demonstrate the use of these evaluation metrics on paraphrase collections derived from three different data types&colon; multiple translations of classic French novels, comparable sentence pairs drawn from different newspapers, and bilingual parallel corpora. We show that PARADIGM correlates with human judgments more strongly than BLEU on a task-based evaluation of paraphrase quality. 
   bibtex: |
      @InProceedings{Weese-EtAl:2014:EACL,
         author    = {Jonathan Weese and Juri Ganitkevitch and Chris Callison-Burch},
         title     = {PARADIGM: Paraphrase Diagnostics through Grammar Matching},
         booktitle = {14th Conference of the European Chapter of the Association for Computational Linguistics},
         month     = {April},
         year      = {2014},
         address   = {Gothenburg, Sweden},
         publisher = {Association for Computional Linguistics},
         url       = {http://cis.upenn.edu/~ccb/publications/paradigm-paraphrase-evaluation.pdf}}
       }
       
-
   title: Crowdsourcing for Grammatical Error Correction
   authors: Ellie Pavlick, Rui Yan, and Chris Callison-Burch
   venue: CSCW Poster
   type: conference
   year: 2014
   url: publications/crowdsourcing-for-grammatical-error-correction.pdf
   page_count: 4
   id: crowdsourcing-for-grammatical-error-correction
   figures:
      -
         img: figures/crowdsourcing-for-grammatical-error-correction/crowdsourcing-for-grammatical-error-correction-table-1.jpg
         label: Table 1
         caption: Multiple ways of producing a correct sentence for the same input.
      -
         img: figures/crowdsourcing-for-grammatical-error-correction/crowdsourcing-for-grammatical-error-correction-table-2.jpg
         label: Table 2
         caption: Edit distance may favor lazy workers over workers who make a concientious effort.
      -
         img: figures/crowdsourcing-for-grammatical-error-correction/crowdsourcing-for-grammatical-error-correction-figure-1.jpg
         label: Figure 1
         caption: F1s (x100) of automated systems in CoNLL 2013 shared task (blue) and of Turkers (red). Turker performance is measured by taking the edits produced by the single highest-scoring Turker for each sentence. Turkers edited a subset of the training data, not the final test data.
      -
         img: figures/crowdsourcing-for-grammatical-error-correction/crowdsourcing-for-grammatical-error-correction-figure-2.jpg
         label: Figure 2
         caption: F1 scores of each Turker vs. # of sentences corrected. Red lines show F1 scores of best CoNLL systems, black line is the average F1 of CoNLL systems. Omitted are 7 Turkers with >1000 sentences corrected. All had F1<0.15.
      -
         img: figures/crowdsourcing-for-grammatical-error-correction/crowdsourcing-for-grammatical-error-correction-figure-3.jpg
         label: Figure 3
         caption: Data structure allows us to meausre agreement on a specific edit, even if final versions of the sentence vary considerable. Here, we are able to tell that the two workers agree that ’into’ should be changed to ’in’, even though they each perform the edit on a different version of the sentence.
   abstract: We discuss the problem of grammatical error correction, which has gained attention for its usefulness both in the development of tools for learners of foreign languages and as a component of statistical machine translation systems. We believe the task of suggesting grammar and style corrections in writing is well suited to a crowdsourcing solution but is currently hindered by the difficulty of automatic quality control. In this proposal, we motivate the problem of grammatical error correction and outline the challenges of ensuring quality in a setting where traditional methods of aggregation (e.g. majority vote) fail to produce the desired results. We then propose a design for quality control and present preliminary results indicating the potential of crowd workers to provide a scalable solution. 
   bibtex: |
      @InProceedings{Pavlick-EtAl:2014:CSCW,
         author    = {Ellie Pavlick and Rui Yan and Chris Callison-Burch},
         title     = {Crowdsourcing for Grammatical Error Correction},
         booktitle = {17th ACM Conference on Computer Supported Cooperative Work and Social Computing, Companion Volume},
         month     = {February},
         year      = {2014},
         address   = {Baltimore, Maryland},
         publisher = {Association for Computing Machinery},
         pages     = {209--213},
         url       = {http://cis.upenn.edu/~ccb/publications/crowdsourcing-for-grammatical-error-correction.pdf}}
       }
       
-
   title: The Multilingual Paraphrase Database
   authors: Juri Ganitkevitch and Chris Callison-Burch
   venue: LREC
   type: conference
   year: 2014
   url: publications/ppdb-multilingual.pdf
   page_count: 8
   id: ppdb-multilingual
   figures:
      -
         img: figures/ppdb-multilingual/ppdb-multilingual-figure-1.jpg
         label: Figure 1
         caption: German paraphrases are extracted by pivoting over a shared English translation.
      -
         img: figures/ppdb-multilingual/ppdb-multilingual-figure-2.jpg
         label: Figure 2
         caption: In addition to extracting lexical and phrasal paraphrases, we also extract syntactic paraphrases. These have nonterminal symbols that act as slots that can be filled by other paraphrases that match that syntactic type. The syntactic labels are drawn from parse trees of the English sen- tences in our bitexts.
      -
         img: figures/ppdb-multilingual/ppdb-multilingual-table-1.jpg
         label: Table 1
         caption: An example paraphrase rule for German. The four fields are the left hand size nonterminal, the phrase, the paraphrase and the features associated with the rule.
      -
         img: figures/ppdb-multilingual/ppdb-multilingual-figure-3.jpg
         label: Figure 3
         caption: An overview of paraphrase collection size per language, measured in millions of paraphrase pairs.
      -
         img: figures/ppdb-multilingual/ppdb-multilingual-table-2.jpg
         label: Table 2
         caption: The sizes of the bilingual training data used to extract each language-specific version of PPDB.
   abstract: We release a massive expansion of the paraphrase database (PPDB) that now includes a collection of paraphrases in 23 different languages. The resource is derived from large volumes of bilingual parallel data. Our collection is extracted and ranked using state of the art methods. The multilingual PPDB has over a billion paraphrase pairs in total, covering the following languages&colon; Arabic, Bulgarian, Chinese, Czech, Dutch, Estonian, Finnish, French, German, Greek, Hungarian, Italian, Latvian, Lithuanian, Polish, Portugese, Romanian, Russian, Slovak, Slovenian, and Swedish. 
   bibtex: |
      @InProceedings{Ganitkevitch-Callison-Burch-2014:LREC,
         author =  {Juri Ganitkevitch and Chris Callison-Burch},
         title =   {The Multilingual Paraphrase Database},
         booktitle = {The 9th edition of the Language Resources and Evaluation Conference},
         month     = {May},
         year      = {2014},
         address   = {Reykjavik, Iceland},
         pages     = {},
         publisher = {European Language Resources Association},
         url = {http://cis.upenn.edu/~ccb/publications/ppdb-multilingual.pdf}
       }
       
-
   title: The American Local News Corpus
   authors: Ann Irvine, Joshua Langfus, and Chris Callison-Burch
   venue: LREC
   type: conference
   year: 2014
   url: publications/american-local-news-corpus.pdf
   page_count: 4
   id: american-local-news-corpus
   figures:
      -
         img: figures/american-local-news-corpus/american-local-news-corpus-figure-1.jpg
         label: Figure 1
         caption: Millions of words of newspaper data contained in the ALNC, by state.
      -
         img: figures/american-local-news-corpus/american-local-news-corpus-figure-2.jpg
         label: Figure 2
         caption: Relative frequency of words related to different sports across time, for all locations. The beginning of professional sports leagues’ seasons are indicated with dotted vertical lines and the end of seasons with dashed vertical lines. For example, the National Football League’s season began in early September and ended in early February.
      -
         img: figures/american-local-news-corpus/american-local-news-corpus-figure-3.jpg
         label: Figure 3
         caption: Ratio of football words to hockey words, by state. The darkest shade indicates that football words are mentioned 80 times as often as hockey words; the lightest that football words are mentioned only about 20 times as often.
      -
         img: figures/american-local-news-corpus/american-local-news-corpus-figure-4.jpg
         label: Figure 4
         caption: Relative frequency (x10k) of ‘murder,’ by state. The lightest shade indicates that ‘murder’ appears once in every 20, 000 words; the darkest, about four times as often.
      -
         img: figures/american-local-news-corpus/american-local-news-corpus-figure-5.jpg
         label: Figure 5
         caption: Relative frequency (x10k) of ‘murder’ vs. the number of murders per 100, 000 people.
   abstract: We present the American Local News Corpus (ALNC), containing over 4 billion words of text from 2, 652 online newspapers in the United States. Each article in the corpus is associated with a timestamp, state, and city. All 50 U.S. states and 1, 924 cities are represented. We detail our method for taking daily snapshots of thousands of local and national newspapers and present two example corpus analyses. The first explores how different sports are talked about over time and geography. The second compares per capita murder rates with news coverage of murders across the 50 states. The ALNC is about the same size as the Gigaword corpus and is growing continuously. Version 1.0 is available for research use. 
   bibtex: |
      @InProceedings{Irvine-EtAl-2014:LREC,
         author =  {Ann Irvine and Joshua Langfus and Chris Callison-Burch},
         title =   {The {American} Local News Corpus},
         booktitle = {The 9th edition of the Language Resources and Evaluation Conference},
         month     = {May},
         year      = {2014},
         address   = {Reykjavik, Iceland},
         pages     = {},
         publisher = {European Language Resources Association},
         url = {http://cis.upenn.edu/~ccb/publications/american-local-news-corpus.pdf}
       }
       
-
   title: A Multi-Dialect, Multi-Genre Corpus of Informal Written Arabic
   authors: Ryan Cotterell and Chris Callison-Burch
   venue: LREC
   type: conference
   year: 2014
   url: publications/arabic-dialect-corpus-2.pdf
   page_count: 5
   id: arabic-dialect-corpus-2
   figures:
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-1.jpg
         label: Figure 1
         caption: Arabic Map
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-2.jpg
         label: Figure 2
         caption: Screenshot of HIT
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-3.jpg
         label: Figure 3
         caption: Newspaper commentary annotated on MTurk as having high dialectal content
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-4.jpg
         label: Figure 4
         caption: Tweets annotated on MTurk as having high dialectal content
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-6.jpg
         label: Figure 6
         caption: Workers’ Performance on Arabic Commentary HIT
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-5.jpg
         label: Figure 5
         caption: Workers’ Performance on Twitter HIT
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-7.jpg
         label: Figure 7
         caption: Sample Commentary
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-8.jpg
         label: Figure 8
         caption: Experiments on Extended AOC (accuracy reported)
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-9.jpg
         label: Figure 9
         caption: Experiments on Twitter (accuracy reported)
      -
         img: figures/arabic-dialect-corpus-2/arabic-dialect-corpus-2-figure-10.jpg
         label: Figure 10
         caption: Experiments on Extended AOC
   abstract: This paper presents a multi-dialect, multi-genre, human annotated corpus of dialectal Arabic with data obtained from both online newspaper commentary and Twitter. Most Arabic corpora are small and focus on Modern Standard Arabic (MSA). There has been recent interest, however, in the construction of dialectal Arabic corpora. This work differs from previously constructed corpora in two ways. First, we include coverage of five dialects of Arabic&colon; Egyptian, Gulf, Levantine, Maghrebi and Iraqi. This is the most complete coverage of any dialectal corpus known to the authors. In addition to data, we provide results for the Arabic dialect identification task that outperform those reported in Zaidan and Callison-Burch (2011). 
   bibtex: |
      @InProceedings{Cotterell-Callison-Burch-2014:LREC,
         author =  {Ryan Cotterell and Chris Callison-Burch},
         title =   {A Multi-Dialect, Multi-Genre Corpus of Informal Written {Arabic}},
         booktitle = {The 9th edition of the Language Resources and Evaluation Conference},
         month     = {May},
         year      = {2014},
         address   = {Reykjavik, Iceland},
         pages     = {},
         publisher = {European Language Resources Association},
         url = {http://cis.upenn.edu/~ccb/publications/arabic-dialect-corpus-2.pdf}
       }
       
-
   title: An Algerian Arabic-French Code-Switched Corpus
   authors: Ryan Cotterell, Adithya Renduchintala, Naomi Saphra, and Chris Callison-Burch
   venue: LREC Workshop on Free/Open-Source Arabic Corpora and Corpora Processing Tools
   type: workshop
   year: 2014
   url: publications/arabic-french-codeswitching.pdf
   page_count: 4
   id: arabic-french-codeswitching
   abstract: Arabic is not just one language, but rather a collection of dialects in addition to Modern Standard Arabic (MSA). While MSA is used in formal situations, dialects are the language of every day life. Until recently, there was very little dialectal Arabic in written form. With the advent of social-media, however, the landscape has changed. We provide the first romanized code-switched Algerian Arabic-French corpus annotated for word-level language id. We review the history and sociological factors that make the linguistic situation in Algerian unique and highlight the value of this corpus to the natural language processing and linguistics communities. To build this corpus, we crawled an Algerian newspaper and extracted the comments from the news story. We discuss the informal nature of the language in the corpus and the challenges it will present. Additionally, we provide a preliminary analysis of the corpus. We then discuss some potential uses of our corpus of interest to the computational linguistics community. 
   bibtex: |
      @InProceedings{Cotterell-EtAl-2014:LREC-WS,
         author =  {Ryan Cotterell and Adithya Renduchintala and Naomi Saphra and Chris Callison-Burch},
         title =   {An {Algerian Arabic-French} Code-Switched Corpus},
         booktitle = {Workshop on Free/Open-Source Arabic Corpora and Corpora Processing Tools},
         month     = {May},
         year      = {2014},
         address   = {Reykjavik, Iceland},
         pages     = {},
         publisher = {European Language Resources Association},
         url = {http://cis.upenn.edu/~ccb/publications/arabic-french-codeswitching.pdf}
       }
       
-
   title: Open letter to President Obama
   authors: Chris Callison-Burch
   venue: Unpublished
   type: unpublished
   year: 2013
   url: publications/letter-to-the-president.pdf
   page_count: 2
   id: letter-to-the-president
   abstract: I wrote an open letter to President Obama about my former PhD student, Omar Zaidan, who had his student visa revoked on the eve of his PhD defense, and who has not been allowed to return to the US in 1.5 years. The letter was read by over 35,000 people in the first week after I published it. 
-
   title: Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus
   authors: Matt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch and Sanjeev Khudanpur
   venue: IWSLT
   type: workshop
   year: 2013
   url: publications/improved-speech-to-speech-translation.pdf
   page_count: 7
   id: improved-speech-to-speech-translation
   figures:
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-table-1.jpg
         label: Table 1
         caption: Corpus size and cost. Counts of segments and words were computed after pre-processing (§2).
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-table-2.jpg
         label: Table 2
         caption: Data splits for Fisher Spanish (top), Callhome Spanish (middle), and Europarl + News Commentary (bottom; for comparison). Words is the number of Spanish word tokens (after tokenization). The mean number of words per sentences ranges from 11.8 to 13.1.
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-table-3.jpg
         label: Table 3
         caption: Lattice statistics for the three Fisher and two Callhome test sets. Word error rates correspond to the 1-best and oracle paths from the lattice, and # Paths denotes the average number of distinct paths through each lattice. The average node density (the number of outgoing arcs) is 1.3 for Fisher and 1.4 for Callhome.
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-table-4.jpg
         label: Table 4
         caption: BLEU scores (four references) on Fisher/Dev2. The columns vary the data used to train the MT system, and the rows alter the interface between the ASR and MT systems.
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-table-5.jpg
         label: Table 5
         caption: BLEU scores (one reference) on Callhome/Evltest.
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-figure-1.jpg
         label: Figure 1
         caption: A subgraph of a lattice (sentence 17 of Fisher/Dev2) representing an ASR ambiguity. The oracle path is in bold. With access to the lattice, the MT system avoids the untranslatable word incorporamos, found in the 1-best output, producing a better translation. Above the line are inputs and the reference, with the Lattice line denoting the path selected by the MT system. The Google line is suggestive of the general difficulty in translating conversational speech.
      -
         img: figures/improved-speech-to-speech-translation/improved-speech-to-speech-translation-figure-2.jpg
         label: Figure 2
         caption: Conversation-level WER and BLEU, for conversations found in Fisher/Dev (open points) and Fisher/Dev2 (solid points). The Pearson’s correlation coefficient is -0.72.
   abstract: Research into the translation of the output of automatic speech recognition (ASR) systems is hindered by the dearth of datasets developed for that explicit purpose. For Spanish-English translation, in particular, most parallel data available exists only in vastly different domains and registers. In order to support research on cross-lingual speech applications, we introduce the Fisher and Callhome Spanish-English Speech Translation Corpus, supplementing existing LDC audio and transcripts with (a) ASR 1-best, lattice, and oracle output produced by the Kaldi recognition system and (b) English translations obtained on Amazon’s Mechanical Turk. The result is a four-way parallel dataset of Spanish audio, transcriptions, ASR lattices, and English translations of approximately 38 hours of speech, with defined training, development, and held-out test sets. We conduct baseline machine translation experiments using models trained on the provided training data, and validate the dataset by corroborating a number of known results in the field, including the utility of in-domain (information, conversational) training data, increased performance translating lattices (instead of recognizer 1-best output), and the relationship between word error rate and BLEU score. 
   bibtex: |
      @InProceedings{post-EtAl:2013:IWSLT,
         author    = {Matt Post and Gaurav Kumar and Adam Lopez and Damianos Karakos and Chris Callison-Burch and Sanjeev Khudanpur},
         title     = {Improved Speech-to-Text Translation with the Fisher and Callhome Spanish–English Speech Translation Corpus},
         booktitle = {Proceedings of the International Workshop on Spoken Language Translation (IWSLT)}
         month     = {December},
         year      = {2013},
         address   = {Heidelberg, Germany},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/improved-speech-to-speech-translation.pdf}
       }
       
-
   title: Semi-Markov Phrase-based Monolingual Alignment
   authors: Xuchen Yao, Ben Van Durme, Chris Callison-Burch and Peter Clark
   venue: EMNLP
   type: conference
   year: 2013
   url: publications/semi-markov-phrase-based-monolingual-alignment.pdf
   page_count: 11
   id: semi-markov-phrase-based-monolingual-alignment
   figures:
      -
         img: figures/semi-markov-phrase-based-monolingual-alignment/semi-markov-phrase-based-monolingual-alignment-figure-1.jpg
         label: Figure 1
         caption: A semi-Markov phrase-based model example and the desired Viterbi decoding path. Shaded horizontal circles represent the source sentence (Shops are closed up for now until March) and hollow vertical circles represent the hidden states with state IDs for the target sentence (Shops are temporarily closed down). State 0, a NULL state, is designated for deletion. One state (e.g. state 3 and 15) can span multiple consecutive source words (a semi-Markov property) for aligning phrases on the source side. States with an ID larger than the target sentence length indicate “phrasal states” (states 6-15 in this example), where consecutive target tokens are merged for aligning phrases on the target side. Combining the semi-Markov property and phrasal states yields for instance, a 2x2 alignment between closed up in the source and closed down in the target.
      -
         img: figures/semi-markov-phrase-based-monolingual-alignment/semi-markov-phrase-based-monolingual-alignment-table-1.jpg
         label: Table 1
         caption: Statistics of the two manually aligned corpora, divided into training and test in sentence pairs. The length column shows average lengths of source and target sentences in a pair. %align. is the percentage of aligned tokens.
      -
         img: figures/semi-markov-phrase-based-monolingual-alignment/semi-markov-phrase-based-monolingual-alignment-table-2.jpg
         label: Table 2
         caption: Percentage of various alignment sizes (undirectional, e.g., 1x2 and 2x1 are merged) after synthesizing phrasal alignment from token alignment in the training portion of two corpora.
      -
         img: figures/semi-markov-phrase-based-monolingual-alignment/semi-markov-phrase-based-monolingual-alignment-table-3.jpg
         label: Table 3
         caption: Results on original (mostly token) and phrasal (P) alignment corpora, where (x%) indicates how much alignment is identical alignment, such as New$New. E% stands for exact (perfect) match rate. Subscript i stands for corresponding scores for “identical” alignment and n for “non-identical”. *&colon; scores of MANLI-joint were for the original Edinburgh corpus instead of Edinburgh++ (with hand corrections) so it is not a direct comparison.
      -
         img: figures/semi-markov-phrase-based-monolingual-alignment/semi-markov-phrase-based-monolingual-alignment-table-4.jpg
         label: Table 4
         caption: Same results on the phrasal Edinburgh++ corpus but with scores divided by token-only alignment (subscript t) and phrase-only alignment (subscript p).
      -
         img: figures/semi-markov-phrase-based-monolingual-alignment/semi-markov-phrase-based-monolingual-alignment-table-5.jpg
         label: Table 5
         caption: Results (Accuracy, Precision, Recall, Mean Average Precision, Mean Reciprocal Rank) on the tasks of RTE, PP and QA
   abstract: We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF. Our model achieves state-of-the-art alignment accuracy on two phrase=based alignment datasets (RTE and paraphrase), while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment. Additional experiments highlight the potential benefit of our alignment model to RTE, paraphrase identification and question answering, where even a naive application of our model’s alignment score approaches the state of the art. 
   bibtex: |
      @InProceedings{yao-EtAl:2013:EMNLP,
         author    = {Xuchen Yao and Benjamin {Van Durme} and Chris Callison-Burch and Peter Clark},
         title     = {Semi-Markov Phrase-based Monolingual Alignment},
         booktitle = {Proceedings of EMNLP}
         month     = {October},
         year      = {2013},
         address   = {Seattle, Washington},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/semi-markov-phrase-based-monolingual-alignment.pdf}
       }
       
-
   title: Findings of the 2013 Workshop on Statistical Machine Translation
   authors: Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia
   venue: WMT
   type: workshop
   year: 2013
   url: publications/findings-of-the-wmt13-shared-tasks.pdf
   page_count: 44
   id: findings-of-the-wmt13-shared-tasks
   figures:
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-1.jpg
         label: Figure 1
         caption: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words (case-insensitive) is based on the provided tokenizer.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-1.jpg
         label: Table 1
         caption: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-2.jpg
         label: Figure 2
         caption: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and the outputs of five systems (anonymized and randomly-ordered) and has to rank these according to their translation quality, ties are allowed. For technical reasons, annotators on Amazon’s Mechanical Turk received all three ranking tasks for a single HIT on a single page, one upon the other.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-2.jpg
         label: Table 2
         caption: Amount of data collected in the WMT13 manual evaluation. The final two rows report summary information from the previous two workshops.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-3.jpg
         label: Table 3
         caption: κ scores measuring inter-annotator agreement. The WMT13r and WMT13m columns provide breakdowns for re- searcher annotations and MTurk annotations, respectively. See Table 4 for corresponding intra-annotator agreement scores.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-4.jpg
         label: Table 4
         caption: κ scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the human evaluation. The WMT13r and WMT13m columns provide breakdowns for researcher annotations and MTurk annota- tions, respectively. The perfect inter-annotator agreement for Spanish-English is a result of there being very little data for that language pair.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-5.jpg
         label: Table 5
         caption: Agreement as a function of threshold for Turkers on the Russian–English task. The threshold is the percentage of controls a Turker must pass for her rankings to be accepted.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-6.jpg
         label: Table 6
         caption: Official results for the WMT13 translation task. Systems are ordered by the expected win score. Lines between systems indicate clusters according to bootstrap resampling at p-level p ≤ .05. This method is also used to determine the range of ranks into which system falls. Systems with grey background indicate use of resources that fall outside the constraints provided for the shared task.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-3.jpg
         label: Figure 3
         caption: In this screen, the annotator is expected to correct the MT output given only the context of at most two neighbouring machine-translated sentences.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-4.jpg
         label: Figure 4
         caption: In this screen, the annotator is expected to validate the monolingual edit, correcting it if necessary. The annotator is expected to add the prefix ‘OK&colon;’ if the correction was more or less cosmetic.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-7.jpg
         label: Table 7
         caption: Distribution of review statuses.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-8.jpg
         label: Table 8
         caption: Annotator agreement when reviewing monolingual edits.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-9.jpg
         label: Table 9
         caption: Understandability of English!Czech systems. The _ values indicate empirical confidence bounds at 95%. Rank ranges were also obtained in the same resampling&colon; in 95% of observations, the system was ranked in the given range
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-10.jpg
         label: Table 10
         caption: Number of source sentences with the given number of distinct reference translations.
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-5.jpg
         label: Figure 5
         caption: Correlation of BLEU and WMT13 manual ranks for English→Czech translation
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-6.jpg
         label: Figure 6
         caption: Correlation of NIST and WMT13 manual ranks for English→Czech translation
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-7.jpg
         label: Figure 7
         caption: Projections from Figure 5 of BLEU and WMT13 manual ranks for English→Czech translation
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-figure-8.jpg
         label: Figure 8
         caption: Projections from Figure 6 of NIST and WMT13 manual ranks for English→Czech translation
      -
         img: figures/findings-of-the-wmt13-shared-tasks/findings-of-the-wmt13-shared-tasks-table-11.jpg
         label: Table 11
         caption: Participants in the WMT13 Quality Estimation shared task.
   abstract: We present the results of the WMT13 shared tasks, which included a translation task, a task for run-time estimation of machine translation quality, and an unofficial metrics task. This year, 143 machine translation systems were submitted to the ten translation tasks from 23 institutions. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually, in our largest manual evaluation to date. The quality estimation task had four subtasks, with a total of 14 teams, submitting 55 entries. 
   bibtex: |
      @InProceedings{bojar-EtAl:2013:WMT,
         author    = {Bojar, Ond\v{r}ej  and  Buck, Christian  and  Callison-Burch, Chris  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Monz, Christof  and  Post, Matt  and  Soricut, Radu  and  Specia, Lucia},
         title     = {Findings of the 2013 {Workshop on Statistical Machine Translation}},
         booktitle = {Proceedings of the Eighth Workshop on Statistical Machine Translation},
         month     = {August},
         year      = {2013},
         address   = {Sofia, Bulgaria},
         publisher = {Association for Computational Linguistics},
         pages     = {1--44},
         url       = {http://www.aclweb.org/anthology/W13-2201}
       }
       
-
   title: Joshua 5.0&colon; Sparser, better, faster, server
   authors: Matt Post, Juri Ganitkevitch, Luke Orland, Jonathan Weese, Yuan Cao, and Chris Callison-Burch
   venue: WMT
   type: workshop
   year: 2013
   url: publications/joshua-5.0.pdf
   page_count: 7
   id: joshua-5
   figures:
      -
         img: figures/joshua-5.0/joshua-5.0-figure-1.jpg
         label: Figure 1
         caption: End-to-end runtime as a function of the number of threads. Each data point is the minimum of at least fifteen different runs.
      -
         img: figures/joshua-5.0/joshua-5.0-figure-2.jpg
         label: Figure 2
         caption: Decoding time alone.
      -
         img: figures/joshua-5.0/joshua-5.0-figure-3.jpg
         label: Figure 3
         caption: Here, position-aware lexical and part-of-speech n-gram features, labeled dependency links, and features reflecting the phrase’s CCG-style label NP/NN are included in the context vector.
      -
         img: figures/joshua-5.0/joshua-5.0-table-1.jpg
         label: Table 1
         caption: Comparing Hadoop’s intermediate disk space use and extraction time on a selection of Europarl v.7 Hiero grammar extractions. Disk space was measured at its maximum, at the input of Thrax’s final grammar aggregation stage. Runtime was measured on our Hadoop cluster with a capacity of 52 mappers and 26 reducers. On average Thrax 2.0, bundled with Joshua 5.0, is up to 300% faster and more compact.
   abstract: We describe improvements made over the past year to Joshua, an open-source translation system for parsing-based machine translation. The main contributions this past year are significant improvements in both speed and usability of the grammar extraction and decoding steps. We have also rewritten the decoder to use a sparse feature representation, enabling training of large numbers of features with discriminative training methods. 
   bibtex: |
      @InProceedings{post-EtAl:2013:WMT,
         author    = {Post, Matt  and  Ganitkevitch, Juri  and  Orland, Luke  and  Weese, Jonathan  and  Cao, Yuan  and  Callison-Burch, Chris},
         title     = {Joshua 5.0: Sparser, Better, Faster, Server},
         booktitle = {Proceedings of the Eighth Workshop on Statistical Machine Translation},
         month     = {August},
         year      = {2013},
         address   = {Sofia, Bulgaria},
         publisher = {Association for Computational Linguistics},
         pages     = {206--212},
         url       = {http://www.aclweb.org/anthology/W13-2226}
       }
       
-
   title: Combining Bilingual and Comparable Corpora for Low Resource Machine Translation
   authors: Ann Irvine and Chris Callison-Burch
   venue: WMT
   type: workshop
   year: 2013
   url: publications/combining-bilingual-and-comparable-corpora.pdf
   page_count: 9
   id: combining-bilingual-and-comparable-corpora
   figures:
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-table-1.jpg
         label: Table 1
         caption: Information about datasets released by Post et al. (2012)&colon; thousands of words in the source language parallel sentences and dictionaries, and percent of development set word types (unique word tokens) and word tokens that are OOV (do not appear in either section of the training data).
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-table-2.jpg
         label: Table 2
         caption: Millions of words of time-stamped web crawls and Wikipedia text, by language.
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-figure-1.jpg
         label: Figure 1
         caption: Examples of OOV Bengali words, our top-3 ranked induced translations, and their correct translations.
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-table-3.jpg
         label: Table 3
         caption: Percent of word types in a held out portion of the training data which are translated correctly by our bilingual lexicon induction technique. Evaluation is over the top-1 and top-10 outputs in the ranked lists for each source word.
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-table-4.jpg
         label: Table 4
         caption: BLEU performance gains that target coverage (+OOV Trans.) and accuracy (+Features), and both (+Feats & OOV). OOV oracle uses OOV translations from automatic word alignments. Hiero and SAMT results are reported in Post et al. (2012).
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-table-5.jpg
         label: Table 5
         caption: Varying minimum training data frequency of source words for which new translations are induced and included in the phrase-based model. In all cases, the top-1 induced translation is added to the phrase table and features estimated over comparable corpora are included (i.e. +Feats & Trans model).
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-figure-2.jpg
         label: Figure 2
         caption: Comparison of learning curves over lines of parallel training data for four SMT systems&colon; our baseline phrase-based model (baseline), model that supplements the baseline with translations of OOV words induced using our supervised bilingual lexicon induction framework (+Trans), model that supplements the baseline with additional phrase table features estimated over comparable corpora (+Feats), and a system that supplements the baseline with both OOV translations and additional features (+Trans & Feats).
      -
         img: figures/combining-bilingual-and-comparable-corpora/combining-bilingual-and-comparable-corpora-figure-3.jpg
         label: Figure 3
         caption: English to Urdu translation results using varying amounts of comparable corpora to estimate features and induce translations.
   abstract: Statistical machine translation (SMT) performance suffers when models are trained on only small amounts of parallel data. The learned models typically have both low accuracy (incorrect translations and feature scores) and low coverage (high out-of-vocabulary rates). In this work, we use an additional data resource, comparable corpora, to improve both. Beginning with a small bitext and corresponding phrase-based SMT model, we improve coverage by using bilingual lexicon induction techniques to learn new translations from comparable corpora. Then, we supplement the model’s feature space with translation scores estimated over comparable corpora in order to improve accuracy. We observe improvements between 0.5 and 1.7 BLEU translating Tamil, Telugu, Bengali, Malayalam, Hindi, and Urdu into English. 
   bibtex: |
      @InProceedings{irvine-callisonburch:2013:WMT,
         author    = {Irvine, Ann  and  Callison-Burch, Chris},
         title     = {Combining Bilingual and Comparable Corpora for Low Resource Machine Translation},
         booktitle = {Proceedings of the Eighth Workshop on Statistical Machine Translation},
         month     = {August},
         year      = {2013},
         address   = {Sofia, Bulgaria},
         publisher = {Association for Computational Linguistics},
         pages     = {262--270},
         url       = {http://www.aclweb.org/anthology/W13-2233}
       }
       
-
   title: A Lightweight and High Performance Monolingual Word Aligner
   authors: Xuchen Yao, Peter Clark, Ben Van Durme and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2013
   url: publications/monolingual-word-aligner.pdf
   page_count: 6
   id: monolingual-word-aligner
   figures:
      -
         img: figures/monolingual-word-aligner/monolingual-word-aligner-table-1.jpg
         label: Table 1
         caption: Results on the 800 pairs of test data. E% stands for exact (perfect) match rate. Systems marked with ⇤ are reported by MacCartney et al. (2008), with / by Thadani and McKeown (2011).
      -
         img: figures/monolingual-word-aligner/monolingual-word-aligner-table-2.jpg
         label: Table 2
         caption: Alignment runtime in seconds per sentence pair on two corpora&colon; RTE2 (Cohn et al., 2008) and FUSION (McKeown et al., 2010). The MANLI-* results are from Thadani and McKeown (2011), on a Xeon 2.0GHz with 6MB Cache. The runtime for this work takes the longest timing from S2T and T2S, on a Xeon 2.2GHz with 4MB cache (the closest we can find to match their hardware). Horizontally in a real- world application where sentences have similar length, this work is roughly 20x faster (0.096 vs. 2.45). Vertically, the decoding time for our work increases less dramatically when sentence length increases (0.025!0.096 vs. 0.08!2.45).
      -
         img: figures/monolingual-word-aligner/monolingual-word-aligner-table-3.jpg
         label: Table 3
         caption: Performance without POS and/or Word-Net features.
   abstract: Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system. 
   bibtex: |
      @InProceedings{yao-EtAl:2013:ACL,
         author    = {Xuchen Yao and Peter Clark and Benjamin {Van Durme} and Chris Callison-Burch},
         title     = {A Lightweight and High Performance Monolingual Word Aligner},
         booktitle = {Proceedings of the 2013 Conference of the Association for Computational Linguistics (ACL 2013)},
         month     = {July},
         year      = {2013},
         address   = {Sofia, Bulgaria},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/monolingual-word-aligner.pdf}
       }
       
-
   title: PARMA&colon; A Predicate Argument Aligner
   authors: Travis Wolfe, Benjamin Van Durme, Mark Dredze, Nicholas Andrews, Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder, Jonathan Weese, Tan Xu and Xuchen Yao
   venue: ACL
   type: conference
   year: 2013
   url: publications/parma.pdf
   page_count: 6
   id: parma
   figures:
      -
         img: figures/parma/parma-figure-1.jpg
         label: Figure 1
         caption: Example of gold-standard alignment pairs from Roth and Frank’s data set and our data set created from the LDC’s Multiple Translation Corpora. The RF data set exhibits high lexical overlap, where most of the alignments are between identical words like police-police and said-said. The LDC MTC was constructed to increase lexical diversity, balcony and tent-camp may relax this assumption.
      -
         img: figures/parma/parma-figure-2.jpg
         label: Figure 2
         caption: We plotted the PARMA’s performance on each of the document pairs. Red squares show the F1 for individual document pairs drawn from Roth and Frank’s data set, and black circles show F1 for our Multiple Translation Corpora test set. The x-axis represents the cosine similarity between the document pairs. On the RF data set, performance is correlated with lexical similarity. On our more lexically diverse set, this is not the case. This could be due to the fact that some of the documents in the RF sets are minor re-writes of the same newswire story, making them easy to align.
      -
         img: figures/parma/parma-table-1.jpg
         label: Table 1
         caption: PARMA outperforms the baseline lemma matching system on the three test sets, drawn from the Extended Event Coreference Bank, Roth and Frank’s data, and our set created from the Multiple Translation Corpora. PARMA achieves a higher F1 and recall score than Roth and Frank’s reported result.
   abstract: We introduce PARMA, a system for crossdocument, semantic predicate and argument alignment. Our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering, integrating them into a simple discriminative model. PARMA achieves state of the art results on an existing and a new dataset. We suggest that previous efforts have focussed on data that is biased and too easy, and we provide a more difficult dataset based on translation data which has a low baseline which we beat by 17% F1. 
   bibtex: |
      @InProceedings{wolfe-EtAl:2013:ACL,
         author    = {Travis Wolfe and Benjamin {Van Durme} and Mark Dredze and Nicholas Andrews and Charley Beller and Chris Callison-Burch and Jay DeYoung and Justin Snyder and Jonathan Weese and Tan Xu and Xuchen Yao},
         title     = {{PARMA}: A Predicate Argument Aligner},
         booktitle = {Proceedings of the 2013 Conference of the Association for Computational Linguistics (ACL 2013)},
         month     = {July},
         year      = {2013},
         address   = {Sofia, Bulgaria},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/parma.pdf}
       }
-
   title: Learning to translate with products of novices&colon; a suite of open-ended challenge problems for teaching MT
   authors: Adam Lopez, Matt Post, Chris Callison-Burch, Jonathan Weese, Juri Ganitkevitch, Narges Ahmidi, Olivia Buzek, Leah Hanson, Beenish Jamil, Matthias Lee, Ya-Ting Lin, Henry Pao, Fatima Rivera, Leili Shahriyari, Debu Sinha, Adam Teichert, Stephen Wampler, Michael Weinberger, Daguang Xu, Lin Yang, and Shang Zhao
   venue: TACL
   type: journal
   year: 2013
   url: publications/teaching-machine-translation.pdf
   page_count: 13
   id: teaching-machine-translation
   figures:
      -
         img: figures/teaching-machine-translation/teaching-machine-translation-figure-1.jpg
         label: Figure 1
         caption: Submission history for the alignment challenge. Dashed lines represent the default and baseline system performance. Each colored line represents a student, and each dot represents a submission. For clarity, we show only submissions that improved the student’s AER.
      -
         img: figures/teaching-machine-translation/teaching-machine-translation-figure-2.jpg
         label: Figure 2
         caption: Submission history for the decoding challenge. The dotted green line represents the oracle over submissions.
      -
         img: figures/teaching-machine-translation/teaching-machine-translation-figure-3.jpg
         label: Figure 3
         caption: Submission history for the evaluation challenge.
      -
         img: figures/teaching-machine-translation/teaching-machine-translation-table-1.jpg
         label: Table 1
         caption: Response to student survey questions on a Likert scale from 1 (strongly disagree) to 5 (strongly agree).
   abstract: Machine translation (MT) draws from several different disciplines, making it a complex subject to teach. There are excellent pedagogical texts, but problems in MT and current algorithms for solving them are best learned by doing. As a centerpiece of our MT course, we devised a series of open-ended challenges for students in which the goal was to improve performance on carefully constrained instances of four key MT tasks&colon; alignment, decoding, evaluation, and reranking. Students brought a diverse set of techniques to the problems, including some novel solutions which performed remarkably well. A surprising and exciting outcome was that student solutions or their combinations fared competitively on some tasks, demonstrating that even newcomers to the field can help improve the state-of-the-art on hard NLP problems while simultaneously learning a great deal. The problems, baseline code, and results are freely available. 
   bibtex: |
      @article{Lopez-etal:TACL:2013,
         author    = {Adam Lopez and Matt Post and Chris Callison-Burch and Jonathan Weese and Juri Ganitkevitch and Narges Ahmidi and Olivia Buzek and Leah Hanson and Beenish Jamil and Matthias Lee and Ya-Ting Lin and Henry Pao and Fatima Rivera and Leili Shahriyari and Debu Sinha and Adam Teichert and Stephen Wampler and Michael Weinberger and Daguang Xu and Lin Yang and and Shang Zhao},
         title =   {Learning to translate with products of novices: a suite of open-ended challenge problems for teaching {MT}},
         journal = {Transactions of the Association for Computational Linguistics},
         year =    {2013},
         volume = {1},
         number = {May},
         pages = {166--177}
       }
       
-
   title: Dirt Cheap Web-Scale Parallel Text from the Common Crawl
   authors: Jason Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch and Adam Lopez
   venue: ACL
   type: conference
   year: 2013
   url: publications/bitexts-from-common-crawl.pdf
   page_count: 10
   id: bitexts-from-common-crawl
   figures:
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-1.jpg
         label: Table 1
         caption: The amount of parallel data mined from CommonCrawl for each language paired with English. Source tokens are counts of the foreign language tokens, and target tokens are counts of the English language tokens.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-2.jpg
         label: Table 2
         caption: Manual evaluation of precision (by sen- tence pair) on the extracted parallel data for Span- ish, French, and German (paired with English).
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-3.jpg
         label: Table 3
         caption: Automatic evaluation of precision through language identification for several lan- guages paired with English.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-4.jpg
         label: Table 4
         caption: The top five domains from the Spanish-English portion of the data. The domains are ranked by the combined number of source and target tokens.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-5.jpg
         label: Table 5
         caption: A list of 20 topics generated using the MALLET toolkit (McCallum, 2002) and their most likely tokens.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-6.jpg
         label: Table 6
         caption: A sample of topics along with the number of Europarl and CommonCrawl documents where they are the most likely topic in the mixture. We include topics that are mostly found in Europarl or CommonCrawl, and some that are somewhat prominent in both.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-7.jpg
         label: Table 7
         caption: Percentage of useful (non-boilerplate) sentences found by domain and language pair. hotel.info was not found in our German-English data.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-8.jpg
         label: Table 8
         caption: BLEU scores for several language pairs before and after adding the mined parallel data to systems trained on data from WMT data.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-9.jpg
         label: Table 9
         caption: BLEU scores for French-English and English-French before and after adding the mined parallel data to systems trained on data from WMT data including the French-English Giga- word (Callison-Burch et al., 2011).
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-10.jpg
         label: Table 10
         caption: The size (in English tokens) of the training corpora used in the SMT experiments from Tables 8 and 9 for each language pair.
      -
         img: figures/bitexts-from-common-crawl/bitexts-from-common-crawl-table-11.jpg
         label: Table 11
         caption: n-gram coverage percentages (up to 4-grams) of the source side of our test sets given our different parallel training corpora computed at the type level.
   abstract: Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon’s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource. 
   bibtex: |
      @InProceedings{smith-EtAl:2013:ACL,
         author    = {Jason Smith and Herve Saint-Amand and Magdalena Plamada and Philipp Koehn and Chris Callison-Burch and Adam Lopez},
         title     = {Dirt Cheap Web-Scale Parallel Text from the {Common Crawl}},
         booktitle = {Proceedings of the 2013 Conference of the Association for Computational Linguistics (ACL 2013)},
         month     = {July},
         year      = {2013},
         address   = {Sofia, Bulgaria},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/bitexts-from-common-crawl.pdf}
       }
       
-
   title: PPDB&colon; The Paraphrase Database
   authors: Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2013
   url: publications/ppdb.pdf
   page_count: 7
   id: ppdb
   figures:
      -
         img: figures/ppdb/ppdb-figure-1.jpg
         label: Figure 1
         caption: Phrasal paraphrases are extracted via bilingual pivoting.
      -
         img: figures/ppdb/ppdb-figure-2.jpg
         label: Figure 2
         caption: Features extracted for the phrase the long term from the n-gram corpus (2a) and Annotated Gigaword (2b). (a) The n-gram corpus records the long-term as preceded by revise (43 times), and followed by plans (97 times). We add corresponding features to the phrase’s distributional signature retaining the counts of the original n-grams. (b) Here, position-aware lexical and part-of-speech n-gram features, labeled dependency links , and features reflecting the phrase’s CCG-style label NP/NN are included in the context vector.
      -
         img: figures/ppdb/ppdb-table-1.jpg
         label: Table 1
         caption: A breakdown of PPDB&colon;Eng size by paraphrase type. We distinguish lexical (i.e. one-word) paraphrases, phrasal paraphrases and syntactically labeled paraphrase patterns.
      -
         img: figures/ppdb/ppdb-table-2.jpg
         label: Table 2
         caption: An overview of PPDB&colon;Spa. Again, we partition the resource into lexical (i.e. one-word) paraphrases, phrasal paraphrases and syntactically labeled paraphrase patterns.
      -
         img: figures/ppdb/ppdb-figure-3.jpg
         label: Figure 3
         caption: To inspect our coverage, we use the Penn Treebank’s parses to map from Propbank annotations to PPDB’s syntactic patterns. For the above annotation predicate, we extract VBP ! expect, which is matched by paraphrase rules like VBP ! expect | anticipate and VBP ! expect | hypothesize. To search for the entire relation, we replace the argument spans with syntactic nonterminals. Here, we obtain S ! NP expect S, for which PPDB has matching rules like S ! NP expect S | NP would hope S, and S ! NP expect S | NP trust S. This allows us to apply sophisticated paraphrases to the predicate while capturing its arguments in a generalized fashion.
      -
         img: figures/ppdb/ppdb-figure-4.jpg
         label: Figure 4
         caption: An illustration of PPDB’s coverage of the manually annotated Propbank predicate phrases (4a) and binary relations with argument non-terminals (4b). The curves indicate the coverage on tokens (solid) and types (dotted), as well as the average number of paraphrases per covered type (dashed) at the given pruning level. (a) PPDB&colon;Eng coverage of Propbank predicates (top), and average human judgment score (bottom) for varying pruning thresholds. (b) PPDB&colon;Eng’s coverage of Propbank predicates with up to two arguments. Here we consider rules that paraphrase the full predicate-argument expression.
   abstract: We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB&colon;Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB&colon;Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff. 
   bibtex: |
      @InProceedings{ganitkevitch-EtAl:2013:NAACL,
         author    = {Juri Ganitkevitch and Benjamin VanDurme and Chris Callison-Burch},
         title     = {{PPDB}: The Paraphrase Database},
         booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013)},
         month     = {June},
         year      = {2013},
         address   = {Atlanta, Georgia},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/ppdb.pdf}
       }
       
-
   title: Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals
   authors: Ann Irvine and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2013
   url: publications/supervised-bilingual-lexicon-induction.pdf
   page_count: 6
   id: supervised-bilingual-lexicon-induction
   figures:
      -
         img: figures/supervised-bilingual-lexicon-induction/supervised-bilingual-lexicon-induction-table-1.jpg
         label: Table 1
         caption: Millions of monolingual web crawl and Wikipedia word tokens
      -
         img: figures/supervised-bilingual-lexicon-induction/supervised-bilingual-lexicon-induction-figure-1.jpg
         label: Figure 1
         caption: Each box-and-whisker plot summarizes performance on the development set using the given feature(s) across all 22 languages. For each source word in our development sets, we rank all English target words according to the monolingual similarity metric(s) listed. All but the last plot show the performance of individual features. Discrim-All uses supervised data to train classifiers for each language based on all of the features.
      -
         img: figures/supervised-bilingual-lexicon-induction/supervised-bilingual-lexicon-induction-figure-2.jpg
         label: Figure 2
         caption: Performance on the development set goes up as features are greedily added to the feature space. Mean performance is slightly higher using this subset of six features (second to last bar) than using all features (last bar).
      -
         img: figures/supervised-bilingual-lexicon-induction/supervised-bilingual-lexicon-induction-figure-3.jpg
         label: Figure 3
         caption: Learning curves over number of positive training instances, up to 1250. For some languages, 1250 positive training instances are not available. In all cases, evaluation is on the development data and the number of negative training instances is three times the number of positive. For all languages, performance is fairly stable after about 300 positive training instances.
      -
         img: figures/supervised-bilingual-lexicon-induction/supervised-bilingual-lexicon-induction-table-2.jpg
         label: Table 2
         caption: Top-10 Accuracy on test set. Performance increases for all languages moving from the baseline (MRR) to discriminative training (Supv).
      -
         img: figures/supervised-bilingual-lexicon-induction/supervised-bilingual-lexicon-induction-figure-4.jpg
         label: Figure 4
         caption: Millions of monolingual word tokens vs. Lexicon Induction Top-10 Accuracy
   abstract: Prior research into learning translations from source and target language monolingual texts has treated the task as an unsupervised learning problem. Although many techniques take advantage of a seed bilingual lexicon, this work is the first to use that data for supervised learning to combine a diverse set of signals derived from a pair of monolingual corpora into a single discriminative model. Even in a low resource machine translation setting, where induced translations have the potential to improve performance substantially, it is reasonable to assume access to some amount of data to perform this kind of optimization. Our work shows that only a few hundred translation pairs are needed to achieve strong performance on the bilingual lexicon induction task, and our approach yields an average relative gain in accuracy of nearly 50% over an unsupervised baseline. Large gains in accuracy hold for all 22 languages (low and high resource) that we investigate. 
   bibtex: |
      @InProceedings{irvine-callisonburch:2013:NAACL,
         author    = {Ann Irvine and Chris Callison-Burch},
         title     = {Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals},
         booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013)},
         month     = {June},
         year      = {2013},
         address   = {Atlanta, Georgia},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/supervised-bilingual-lexicon-induction.pdf}
       }
       
-
   title: Answer Extraction as Sequence Tagging with Tree Edit Distance
   authors: Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch and Peter Clark
   venue: NAACL
   type: conference
   year: 2013
   url: publications/answer-extraction-as-sequence-tagging.pdf
   page_count: 10
   id: answer-extraction-as-sequence-tagging
   figures:
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-table-1.jpg
         label: Table 1
         caption: Features for ranking QA pairs.
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-figure-1.jpg
         label: Figure 1
         caption: Edits transforming a source sentence (left) to a question (right). Each node consists of&colon; lemma, POS tag and dependency relation, with root nodes and punctuation not shown. Shown includes deletion (⇥ and strikethrough on the left), alignment (arrows) and insertion (shaded area). Order of operations is not displayed. The standard TED model does not capture the alignment between tennis and sport (see Section 2.2).
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-table-2.jpg
         label: Table 2
         caption: Distribution of data, with imbalance towards negative examples (sentences without an answer).
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-table-3.jpg
         label: Table 3
         caption: Results on the QA Sentence Ranking task.
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-figure-2.jpg
         label: Figure 2
         caption: An example of linear-chain CRF for an- swer sequence tagging.
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-table-4.jpg
         label: Table 4
         caption: Features based on edit script for answer se- quence tagging.
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-figure-3.jpg
         label: Figure 3
         caption: A sample sequence tagging output that fails to predict an answer. From line 2 on, the first column is the reference output and the second column is the model output with the marginal probability for predicated labels. Note that World War II has much lower probabilities as an O than others.
      -
         img: figures/answer-extraction-as-sequence-tagging/answer-extraction-as-sequence-tagging-figure-4.jpg
         label: Figure 4
         caption: Impact of adding features based on chunking and question-type (CHUNKING) and tree edits (TED), e.g., EDIT and ALIGN.
   abstract: Our goal is to extract answers from pre-retrieved sentences for Question Answering (QA). We construct a linear-chain Conditional Random Field based on pairs of questions and their possible answer sentences, learning the association between questions and answer types. This casts answer extraction as an answer sequence tagging problem for the first time, where knowledge of shared structure between question and source sentence is incorporated through features based on Tree Edit Distance (TED). Our model is free of manually created question and answer templates, fast to run (processing 200 QA pairs per second excluding parsing time), and yields an F1 of 63.3% on a new public dataset based on prior TREC QA evaluations. The developed system is open-source, and includes an implementation of the TED model that is state of the art in the task of ranking QA pairs. 
   bibtex: |
      @InProceedings{yao-EtAl:2013:NAACL,
         author    = {Xuchen Yao and Benjamin {Van Durme} and Chris Callison-Burch and Peter Clark},
         title     = {Answer Extraction as Sequence Tagging with Tree Edit Distance},
         booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013)},
         month     = {June},
         year      = {2013},
         address   = {Atlanta, Georgia},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/answer-extraction-as-sequence-tagging.pdf}
       }
       
-
   title: Findings of the 2012 Workshop on Statistical Machine Translation
   authors: Chris Callison-Burch, Philipp Koehn, Christof  Monz, Matt Post, Radu Soricut, and Lucia Specia
   venue: WMT
   type: workshop
   year: 2012
   url: publications/findings-of-the-wmt12-shared-tasks.pdf
   page_count: 42
   id: findings-of-the-wmt12-shared-tasks
   figures:
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-figure-1.jpg
         label: Figure 1
         caption: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words (case-insensitive) is based on the provided tokenizer.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-1.jpg
         label: Table 1
         caption: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial, online, and rule-based systems were crawled by us, not submitted by the respective companies, and are therefore anonymized. Anonymized identifiers were chosen so as to correspond with the WMT11 systems.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-2.jpg
         label: Table 2
         caption: A summary of the WMT12 ranking task, showing the number of systems and number of labels (rankings) collected for each of the language translation tasks.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-3.jpg
         label: Table 3
         caption: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al. (2011), Table 7).
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-7.jpg
         label: Table 7
         caption: System-level Spearman’s rho correlation of the automatic evaluation metrics with the human judgments for translation into English, ordered by average absolute value.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-4.jpg
         label: Table 4
         caption: Official results for the WMT12 translation task. Systems are ordered by their > others score, reflecting how often their translations won in pairwise comparisons. For detailed head-to-head comparisons, see Appendix A.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-5.jpg
         label: Table 5
         caption: Overall ranking with different methods (English–German)
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-figure-2.jpg
         label: Figure 2
         caption: Ratio of statistically significant pairwise comparisons at different p-levels, based on number of pair-wise judgments collected.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-6.jpg
         label: Table 6
         caption: Participants in the metrics task.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-8.jpg
         label: Table 8
         caption: System-level Spearman’s rho correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average absolute value.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-9.jpg
         label: Table 9
         caption: Segment-level Kendall’s tau correlation of the automatic evaluation metrics with the human judgments for translation into English, ordered by average correlation.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-10.jpg
         label: Table 10
         caption: Segment-level Kendall’s tau correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average correlation.
      -
         img: figures/findings-of-the-wmt12-shared-tasks/findings-of-the-wmt12-shared-tasks-table-11.jpg
         label: Table 11
         caption: Participants in the WMT12 Quality Evaluation shared task.
   abstract: This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality. We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. We introduced a new quality estimation task this year, and evaluated submissions from 11 teams. 
   bibtex: |
      @InProceedings{callisonburch-EtAl:2012:WMT,
         author    = {Callison-Burch, Chris  and  Koehn, Philipp  and  Monz, Christof  and  Post, Matt  and  Soricut, Radu  and  Specia, Lucia},
         title     = {Findings of the 2012 Workshop on Statistical Machine Translation},
         booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2012},
         address   = {Montr{\'e}al, Canada},
         publisher = {Association for Computational Linguistics},
         pages     = {10--51},
         url = {http://cis.upenn.edu/~ccb/publications/findings-of-the-wmt12-shared-tasks.pdf}
       }
       
-
   title: Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing
   authors: Matt Post, Chris Callison-Burch, and Miles Osborne
   venue: WMT
   type: workshop
   year: 2012
   url: publications/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing.pdf
   page_count: 9
   id: constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing
   figures:
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-figure-1.jpg
         label: Figure 1
         caption: An example of SOV word ordering in Tamil. Translation&colon; The senator prepared her remarks.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-figure-2.jpg
         label: Figure 2
         caption: An example of the morphology of the Bengali word হাত্সিলাম, meaning [I] was walking. CONT denotes the continuous aspect, while PAST denotes past tense.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-3.jpg
         label: Table 3
         caption: Dictionary statistics. Entries is the number of source-language types, while translations lists the number of words or phrases they translated to (i.e., the number of pairs in the dictionary). Controls for Hindi were obtained using Google translate, the only one of these languages that were available at the outset of this project.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-figure-3.jpg
         label: Figure 3
         caption: The total volume of translations (measured in English words) as a function of elapsed days. For Malayalam, we collected half a million words of translations in just under a week.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-4.jpg
         label: Table 4
         caption: Data set sizes for each language pair&colon; words in the first row, parallel sentences in the second. (The dictionaries contains short phrases in addition to words, which accounts for the difference in dictionary word and line counts.)
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-5.jpg
         label: Table 5
         caption: BLEU scores translating into English (four references). BLEU scores are the mean of three MERT runs.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-6.jpg
         label: Table 6
         caption: Some example translations.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-7.jpg
         label: Table 7
         caption: BLEU scores translating into English on a quarter of the training data (plus dictionary), selected in two ways&colon; best (result of vote), and random. There is little difference, suggesting quality control may not be terribly important. We did not collect votes for Malayalam.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-8.jpg
         label: Table 8
         caption: Misspellings of japanese (947) in the training portion of the Urdu-English data, along with their counts.
      -
         img: figures/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing/constructing-parallel-corpora-for-six-indian-languages-via-crowdsourcing-table-9.jpg
         label: Table 9
         caption: Hiero translation results using Berkeley align- ments instead of GIZA++ heuristics. The gain columns denotes improvements relative to the Hiero systems in Ta- ble 5. In many cases (bold gains), the BLEU scores are at or above even the SAMT models from that table.
   abstract: Recent work has established the efficacy of Amazon's Mechanical Turk for constructing parallel corpora for machine translation research. We apply this to building a collection of parallel corpora between English and six languages from the Indian subcontinent&colon; Bengali, Hindi, Malayalam, Tamil, Telugu, and Urdu. These languages are low-resource, under-studied, and exhibit linguistic phenomena that are difficult for machine translation. We conduct a variety of baseline experiments and analysis, and release the data to the community. 
   bibtex: |
      @InProceedings{post-callisonburch-osborne:2012:WMT,
         author    = {Post, Matt  and  Callison-Burch, Chris  and  Osborne, Miles},
         title     = {Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing},
         booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2012},
         address   = {Montr{\'e}al, Canada},
         publisher = {Association for Computational Linguistics},
         pages     = {401--409},
         url       = {http://www.aclweb.org/anthology/W12-3152}
       }
       
-
   title: Using Categorial Grammar to Label Translation Rules
   authors: Jonathan Weese, Chris Callison-Burch, and Adam Lopez
   venue: WMT
   type: workshop
   year: 2012
   url: publications/using-categorial-grammar-to-label-translation-rules.pdf
   page_count: 10
   id: using-categorial-grammar-to-label-translation-rules
   figures:
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-table-1.jpg
         label: Table 1
         caption: An example lexicon, mapping words to categories.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-figure-1.jpg
         label: Figure 1
         caption: An example CCG derivation for the sentence “They own properties in various cities and villages” using the lexicon from Table 1. Φ indicates a conjunction operation; > and < are forward and backward function application, respectively.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-figure-2.jpg
         label: Figure 2
         caption: A word-aligned sentence pair fragment, with a box indicating a consistent phrase pair.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-figure-3.jpg
         label: Figure 3
         caption: A consistent phrase pair with a sub-phrase that is also consistent. We may extract a hierarchical SCFG rule from this training example.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-figure-4.jpg
         label: Figure 4
         caption: A portion of the parse chart for a sentence starting with “For most people . . . .” Note that the gray chart cell is not included in the 1-best derivation of this fragment in Section 3.5.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-table-2.jpg
         label: Table 2
         caption: Number of translation rules and non- terminal labels in an Urdu–English grammar under various models.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-figure-5.jpg
         label: Figure 5
         caption: Histograms of label frequency for each model, illustrating the sparsity of each model.
      -
         img: figures/using-categorial-grammar-to-label-translation-rules/using-categorial-grammar-to-label-translation-rules-table-3.jpg
         label: Table 3
         caption: Results of translation experiments on Urdu–English. Higher BLEU scores are better. BLEU’s brevity penalty is reported in parentheses.
   abstract: Adding syntactic labels to synchronous context-free translation rules can improve performance, but labeling with phrase structure constituents, as in GHKM (Galley et al., 2004), excludes potentially useful translation rules. SAMT (Zollmann and Venugopal, 2006) introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar. We introduce a labeling scheme based on categorial grammar, which allows syntactic labeling of many rules with a minimal, well-motivated label set. We show that our labeling scheme performs comparably to SAMT on an Urdu–English translation task, yet the label set is an order of magnitude smaller, and translation is twice as fast. 
   bibtex: |
      @InProceedings{weese-callisonburch-lopez:2012:WMT,
         author    = {Weese, Jonathan  and  Callison-Burch, Chris  and  Lopez, Adam},
         title     = {Using Categorial Grammar to Label Translation Rules},
         booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2012},
         address   = {Montr{\'e}al, Canada},
         publisher = {Association for Computational Linguistics},
         pages     = {222--231},
         url = {http://cis.upenn.edu/~ccb/publications/using-categorial-grammar-to-label-translation-rules.pdf}
       }
       
-
   title: Joshua 4.0&colon; Packing, PRO, and Paraphrases
   authors: Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post, and Chris Callison-Burch
   venue: WMT
   type: workshop
   year: 2012
   url: publications/joshua-4.0.pdf
   page_count: 9
   id: joshua-4
   figures:
      -
         img: figures/joshua-4.0/joshua-4.0-figure-1.jpg
         label: Figure 1
         caption: An illustration of our packed grammar data structures. The source sides of the grammar rules are stored in a packed trie. Each node may contain n children and the symbols linking to them, and m entries for rules that share the same source side. Each rule entry links to a node in the target-side trie, where the full target string can be retrieved by walking up the trie until the root is reached. The rule entries also contain a data block id, which identifies feature data attached to the rule. The features are encoded according to a type/quantization specification and stored as variable-length blocks of data in a byte buffer.
      -
         img: figures/joshua-4.0/joshua-4.0-table-1.jpg
         label: Table 1
         caption: Decoding-time memory use for the packed grammar versus the standard grammar format. Even without lossy quantization the packed grammar representation yields significant savings in memory consumption. Adding 8-bit quantization for the real- valued features in the grammar reduces even large syntactic grammars to a manageable size.
      -
         img: figures/joshua-4.0/joshua-4.0-figure-2.jpg
         label: Figure 2
         caption: A visualization of the loading and decoding speed on the WMT12 French-English development set contrasting the packed grammar representation with the standard format. Grammar loading for the packed grammar representation is substantially faster than that for the baseline setup. Even with a slightly slower decoding speed (note the difference in the slopes) the packed grammar finishes in less than half the time, compared to the standard format.
      -
         img: figures/joshua-4.0/joshua-4.0-figure-3.jpg
         label: Figure 3
         caption: Experimental results on the development and test sets. The x-axis is the number of iterations (up to 30) and the y-axis is the BLEU score. The three curves in each figure correspond to three classifiers. Upper row&colon; results trained using only dense features (10 features); Lower row&colon; results trained using dense+sparse features (1026 features). Left column&colon; development set (MT03); Middle column&colon; test set (MT04); Right column&colon; test set (MT05).
      -
         img: figures/joshua-4.0/joshua-4.0-table-2.jpg
         label: Table 2
         caption: Comparison between the results given by Z-MERT and J-PRO (trained with 10 features).
      -
         img: figures/joshua-4.0/joshua-4.0-table-3.jpg
         label: Table 3
         caption: Extraction times and grammar sizes for Hiero grammars using the Europarl and News Commentary training data for each listed language pair.
      -
         img: figures/joshua-4.0/joshua-4.0-table-4.jpg
         label: Table 4
         caption: Extraction times and grammar sizes for the SAMT grammars using the Europarl and News Commentary training data for each listed language pair.
      -
         img: figures/joshua-4.0/joshua-4.0-table-5.jpg
         label: Table 5
         caption: Large paraphrase grammars extracted from EuroParl data using Thrax. The sentence and word counts refer to the English side of the bitexts used.
   abstract: We present Joshua 4.0, the newest version of our open-source decoder for parsing-based statistical machine translation. The main contributions in this release are the introduction of a compact grammar representation based on packed tries, and the integration of our implementation of pairwise ranking optimization, J-PRO. We further present the extension of the Thrax SCFG grammar extractor to pivot-based extraction of syntactically informed sentential paraphrases. 
   bibtex: |
      @InProceedings{ganitkevitch-EtAl:2012:WMT,
         author    = {Ganitkevitch, Juri  and  Cao, Yuan  and  Weese, Jonathan  and  Post, Matt  and  Callison-Burch, Chris},
         title     = {Joshua 4.0: Packing, PRO, and Paraphrases},
         booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2012},
         address   = {Montr{\'e}al, Canada},
         publisher = {Association for Computational Linguistics},
         pages     = {283--291},
         url = {http://cis.upenn.edu/~ccb/publications/joshua-4.0.pdf}
       }
       
-
   title: Expectations of Word Sense in Parallel Corpora
   authors: Xuchen Yao, Benjamin Van Durme and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2012
   url: publications/expectations-of-word-sense-in-parallel-corpora.pdf
   page_count: 5
   id: expectations-of-word-sense-in-parallel-corpora
   figures:
      -
         img: figures/expectations-of-word-sense-in-parallel-corpora/expectations-of-word-sense-in-parallel-corpora-table-1.jpg
         label: Table 1
         caption: MTurk result on testing Turker reliability. Krippendorff’s Alpha is used to measure agreement. ↵- Turker&colon; how Turkers agree among themselves, ↵-maj.&colon; how the majority agrees with true value, maj.-agr.&colon; agreement between the majority vote and true value. ↵-maj. indicates the confidence level about the maj.-agr. value. Subscripts denote either 5 Turkers, or 3 randomly selected of the 5.
      -
         img: figures/expectations-of-word-sense-in-parallel-corpora/expectations-of-word-sense-in-parallel-corpora-table-2.jpg
         label: Table 2
         caption: Statistics for words sampled from parallel corpora. Average #senses observed over all words&colon; 2.6 (French-English), and 2.4 (Chinese-English). The sampled word keep has 18 senses in OntoNotes, with 5 observed.
      -
         img: figures/expectations-of-word-sense-in-parallel-corpora/expectations-of-word-sense-in-parallel-corpora-figure-1.jpg
         label: Figure 1
         caption: French-English values, by number of senses.
      -
         img: figures/expectations-of-word-sense-in-parallel-corpora/expectations-of-word-sense-in-parallel-corpora-figure-2.jpg
         label: Figure 2
         caption: French-English values, by number of senses.
   abstract: Given a parallel corpus, if two distinct words in language A, a and a2, are aligned to the same word b in language B, then this might signal that b is polysemous, or it might signal a and a2 are synonyms. Both assumptions with successful work have been put forward in the literature. We investigate these assumptions, along with other questions of word sense, by looking at sampled parallel sentences containing tokens of the same type in English, asking how often they mean the same thing when they are&colon; 1. aligned to the same foreign type; and 2. aligned to different foreign types. Results for French-English and Chinese-English parallel corpora show similar behavior&colon; Synonymy is only very weakly the more prevalent scenario, where both cases regularly occur. 
   bibtex: |
      @InProceedings{yao-vandurme-callisonburch:2012:NAACL-HLT,
         author    = {Yao, Xuchen, {Van Durme}, Benjamin and Callison-Burch, Chris},
         title     = {Expectations of Word Sense in Parallel Corpora},
         booktitle = {The 2012 Conference of the North American Chapter of the Association for Computational Linguistics},
         month     = {June},
         year      = {2012},
         address   = {Montr{\'e}al, Canada},
         publisher = {Association for Computational Linguistics},
         pages     = {621--625},
         url       = {http://www.aclweb.org/anthology/N12-1078}
       }
       
       
-
   title: Processing Informal, Romanized Pakistani Text Messages
   authors: Ann Irvine, Jonathan Weese, and Chris Callison-Burch
   venue: the NAACL Workshop on Language in Social Media
   type: workshop
   year: 2012
   url: publications/pakistani-SMS-corpus.pdf
   page_count: 4
   id: pakistani-SMS-corpus
   figures:
      -
         img: figures/pakistani-SMS-corpus/pakistani-SMS-corpus-figure-1.jpg
         label: Figure 1
         caption: Example of SMS with MTurk annotations
      -
         img: figures/pakistani-SMS-corpus/pakistani-SMS-corpus-figure-2.jpg
         label: Figure 2
         caption: Productivity vs. percent of annotations voted best among three deromanizations gathered on MTurk.
      -
         img: figures/pakistani-SMS-corpus/pakistani-SMS-corpus-figure-3.jpg
         label: Figure 3
         caption: Urdu words romanized in multiple ways. The Urdu word for “2” is pronounced approximately “du.”
      -
         img: figures/pakistani-SMS-corpus/pakistani-SMS-corpus-figure-4.jpg
         label: Figure 4
         caption: Illustration of HMM with an example from SMS data. English translation&colon; “What’s the situation?”
      -
         img: figures/pakistani-SMS-corpus/pakistani-SMS-corpus-table-1.jpg
         label: Table 1
         caption: Deromanization and normalization results on 500 SMS test set. Evaluation is by character (CER) and word error rate (WER); lower scores are better. “LM” indicates the data used to estimate the language model probabilities&colon; News refers to Urdu news corpus and SMS to deromanized side of our SMS training data. “Translit” column refers to the training data that was used to train the transliterator&colon; SMS; SMS training data; Eng; English-Urdu transliterations. α refers to the data used to estimate emissions&colon; transliterations, dictionary entries, or both.
      -
         img: figures/pakistani-SMS-corpus/pakistani-SMS-corpus-table-2.jpg
         label: Table 2
         caption: Results on tokens in the test set, binned by training frequency or difference in character length with their reference. Length differences are number of characters in romanized token minus the number of characters in its deromanization. α = 0.5 for all.
   abstract: Regardless of language, the standard character set for text messages (SMS) and many other social media platforms is the Roman alphabet. There are romanization conventions for some character sets, but they are used inconsistently in informal text, such as SMS. In this work, we convert informal, romanized Urdu messages into the native Arabic script and normalize non-standard SMS language. Doing so prepares the messages for existing downstream processing tools, such as machine translation, which are typically trained on well-formed, native script text. Our model combines information at the word and character levels, allowing it to handle out-of-vocabulary items. Compared with a baseline deterministic approach, our system reduces both word and character error rate by over 50%. 
   bibtex: |
      @InProceedings{irvine-weese-callisonburch:2012:LSM,
         author    = {Irvine, Ann  and  Weese, Jonathan  and  Callison-Burch, Chris},
         title     = {Processing Informal, Romanized Pakistani Text Messages},
         booktitle = {Proceedings of the Second Workshop on Language in Social Media},
         month     = {June},
         year      = {2012},
         address   = {Montr{\'e}al, Canada},
         publisher = {Association for Computational Linguistics},
         pages     = {75--78},
         url       = {http://www.aclweb.org/anthology/W12-2109}
       }
       
-
   title: Monolingual Distributional Similarity for Text-to-Text Generation
   authors: Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch
   venue: STARSEM
   type: conference
   year: 2012
   url: publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf
   page_count: 9
   id: monolingual-distributional-similarity-for-text-to-text-generation
   figures:
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-1.jpg
         label: Figure 1
         caption: Pivot-based paraphrase extraction for contiguous phrases. Two phrases translating to the same phrase in the foreign language are assumed to be paraphrases of one another.
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-2.jpg
         label: Figure 2
         caption: Extraction of syntactic paraphrases via the pivoting approach&colon; We aggregate over different sur- face realizations, matching the lexicalized portions of the rule and generalizing over the nonterminals.
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-3.jpg
         label: Figure 3
         caption: An example of a synchronous paraphrastic derivation, here a sentence compression. Shaded words are deleted in the indicated rule applications.
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-4.jpg
         label: Figure 4
         caption: Scoring a rule by extracting and scoring contiguous phrases consistent with the alignment. The overall score of the rule is determined by averaging across all pairs of contiguous subphrases
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-5.jpg
         label: Figure 5
         caption: An example of the n-gram feature extraction on an n-gram corpus. Here, “the long-term” is seen preceded by “revise” (43 times) and followed by “plans” (97 times). The corresponding left- and right-side features are added to the phrase signature with the counts of the n-grams that gave rise to them.
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-6.jpg
         label: Figure 6
         caption: An example of the syntactic feature-set. The phrase “the long-term” is annotated with position-aware lexical and part-of-speech n-gram features (e.g. “on to” on the left, and “investment” and “NN” to its right), labeled dependency links (e.g. amod − investment) and features derived from the phrase’s CCG label NP /NN .
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-table-1.jpg
         label: Table 1
         caption: Results of the human evaluation on longer compressions&colon; pairwise compression rates (CR), meaning and grammaticality scores. Bold indicates a statistically significance difference at p < 0.05.
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-figure-7.jpg
         label: Figure 7
         caption: A pairwise breakdown of the human judgments comparing the systems. Dark grey regions show the number of times the two systems were tied, and light grey shows how many times one system was judged to be better than the other.
      -
         img: figures/monolingual-distributional-similarity-for-text-to-text-generation/monolingual-distributional-similarity-for-text-to-text-generation-table-2.jpg
         label: Table 2
         caption: Example compressions produced by our systems and the baselines Table 1 for three input sentences from our test data.
   abstract: Previous work on paraphrase extraction and application has relied on either parallel datasets, or on distributional similarity metrics over large text corpora. Our approach combines these two orthogonal sources of information and directly integrates them into our paraphrasing system’s log-linear model. We compare different distributional similarity feature-sets and show significant improvements in grammaticality and meaning retention on the example text-to-text generation task of sentence compression, achieving state-of-the-art quality. 
   bibtex: |
      @InProceedings{Ganitkevitch-etal:2012:StarSEM,
         author =  {Juri Ganitkevitch and Benjamin {Van Durme} and Chris Callison-Burch},
         title     = {Monolingual Distributional Similarity for Text-to-Text Generation},
         booktitle = {*SEM First Joint Conference on Lexical and Computational Semantics},
         month     = {June},
         year      = {2012},
         address   = {Montreal},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/monolingual-distributional-similarity-for-text-to-text-generation.pdf}
       }
       
-
   title: Machine Translation of Arabic Dialects
   authors: Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard Schwartz, John Makhoul, Omar F. Zaidan and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2012
   url: publications/machine-translation-of-arabic-dialects.pdf
   page_count: 11
   id: machine-translation-of-arabic-dialects
   figures:
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-figure-1.jpg
         label: Figure 1
         caption: One possible breakdown of spoken Arabic into dialect groups&colon; Maghrebi, Egyptian, Levantine, Gulf and Iraqi. Habash (2010) gives a breakdown along mostly the same lines. We used this map as an illustration for annotators in our dialect classification task (Section 3.1), with Arabic names for the dialects instead of English
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-1.jpg
         label: Table 1
         caption: The total costs for the three MTurk subtasks involved with the creation of our Dialectal Arabic-English parallel corpus.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-2.jpg
         label: Table 2
         caption: Statistics about the training/tuning/test datasets used in our experiments. The token counts are calculated before MADA segmentation.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-3.jpg
         label: Table 3
         caption: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-4.jpg
         label: Table 4
         caption: A comparison of translation quality of Egyptian, Levantine, and MSA web text, using various training corpora. The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian), since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the dialectal data results in marginally worse translations.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-5.jpg
         label: Table 5
         caption: The most frequent OOV’s (with counts ≥ 10) of our test sets against the MSA training data.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-figure-2.jpg
         label: Figure 2
         caption: Examples of improvement in MT output when training on our Dialectal Arabic-English parallel corpus instead of an MSA-English parallel corpus.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-figure-3.jpg
         label: Figure 3
         caption: Examples of ambiguous words that are translated incorrectly by the MSA-English system, but correctly by the Dialectal Arabic-English system.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-figure-4.jpg
         label: Figure 4
         caption: Learning curves showing the effects of increasing the size of dialectal training data, when combined with the 150M-word MSA parallel corpus, and when used alone. Adding the MSA training data is only useful when the dialectal data is scarce (200k words).
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-6.jpg
         label: Table 6
         caption: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable&colon; +2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
      -
         img: figures/machine-translation-of-arabic-dialects/machine-translation-of-arabic-dialects-table-7.jpg
         label: Table 7
         caption: A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English, versus translating directly from Levantine into English. The mapping from Levantine to MSA was done manually, so it is an optimistic estimate of what might be done automatically. Although initially helpful to the MSA baseline system, the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance.
   abstract: Arabic dialects present many challenges for machine translation, not least of which is the lack of data resources. We use crowdsourcing to cheaply and quickly build Levantine-English and Egyptian-English parallel corpora, consisting of 1.1M words and 380k words, respectively. The dialect sentences are selected from a large corpus of Arabic web text, and translated using Mechanical Turk. We use this data to build Dialect Arabic MT systems. Small amounts of dialect data have a dramatic impact on the quality of translation. When translating Egyptian and Levantine test sets, our Dialect Arabic MT system performs 5.8 and 6.8 BLEU points higher than a Modern Standard Arabic MT system trained on a 150 million word Arabic-English parallel corpus -- over 100 times the amount of data as our dialect corpora. 
   bibtex: |
      @InProceedings{Zbib-etal:2012:NAACL,
         author =  {Rabih Zbib and Erika Malchiodi and Jacob Devlin and David Stallard and Spyros Matsoukas and Richard Schwartz and John Makhoul and Omar F. Zaidan and Chris Callison-Burch},
         title     = {Machine Translation of Arabic Dialects},
         booktitle = {The 2012 Conference of the North American Chapter of the Association for Computational Linguistics},
         month     = {June},
         year      = {2012},
         address   = {Montreal},
         publisher = {Association for Computational Linguistics},
         url = {http://cis.upenn.edu/~ccb/publications/machine-translation-of-arabic-dialects.pdf}
       }
       
-
   title: Toward Statistical Machine Translation without Parallel Corpora
   authors: Alex Klementiev, Ann Irvine, Chris Callison-Burch, and David Yarowsky
   venue: EACL
   type: conference
   year: 2012
   url: publications/toward-statistical-machine-translation-without-parallel-corpora.pdf
   page_count: 11
   id: toward-statistical-machine-translation-without-parallel-corpora
   figures:
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-1.jpg
         label: Figure 1
         caption: The reordering probabilities from the phrasebased models are estimated from bilingual data by calculating how often in the parallel corpus a phrase pair (f; e) is orientated with the preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous).
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-2.jpg
         label: Figure 2
         caption: Accuracy of single-word translations induced using contextual similarity as a function of the source word corpus frequency. Accuracy is the proportion of the source words with at least one correct (bilingual dictionary) translation in the top 1 and top 10 candidate lists.
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-3.jpg
         label: Figure 3
         caption: Scoring contextual similarity of phrases&colon; first, contextual vectors are projected using a small seed dictionary and then compared with the target language candidates.
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-4.jpg
         label: Figure 4
         caption: Temporal histograms of the English phrase terrorist, its Spanish translation terrorista, and riqueza (wealth) collected from monolingual texts spanning a 13 year period. While the correct translation has a good temporal match, the non-translation riqueza has a distinctly different signature.
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-5.jpg
         label: Figure 5
         caption: Algorithm for estimating reordering probabilities from monolingual data.
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-6.jpg
         label: Figure 6
         caption: Collecting phrase orientation statistics for a English-German phrase pair (“profile”, “Profils”) from non-parallel sentences (the German sentence translates as “Creating a Facebook profile is easy”).
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-table-1.jpg
         label: Table 1
         caption: Statistics about the monolingual training data and the phrase table that was used in all of the experiments.
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-7.jpg
         label: Figure 7
         caption: Much of the loss in BLEU score when bilingually estimated features are removed from a Spanish-English translation system (experiments 1-4) can be recovered when they are replaced with monolingual equivalents estimated from monolingual Europarl data (experiments 5-10). The labels indicate how the different types of parameters are estimated, the first part is for phrase-table features, the second is for reordering probabilities.
      -
         img: figures/toward-statistical-machine-translation-without-parallel-corpora/toward-statistical-machine-translation-without-parallel-corpora-figure-8.jpg
         label: Figure 8
         caption: Performance of monolingual features derived from truly monolingual corpora. Over 82% of the BLEU score loss can be recovered.
   abstract: We estimate the parameters of a phrase-based statistical machine translation system from monolingual corpora instead of a bilingual parallel corpus. We extend existing research on bilingual lexicon induction to estimate both lexical and phrasal translation probabilities for MT-scale phrase-tables. We propose a novel algorithm to estimate re-ordering probabilities from monolingual data. We report translation results for an end-to-end translation system using these monolingual features alone. Our method only requires monolingual corpora in source and target languages, a small bilingual dictionary, and a small bitext for tuning feature weights. In this paper, we examine an idealization where a phrase-table is given. We examine the degradation in translation performance when bilingually estimated translation probabilities are removed, and show that 82%+ of the loss can be recovered with monolingually estimated features alone. We further show that our monolingual features add 1.5 BLEU points when combined with standard bilingually estimated phrase table features. 
   bibtex: |
      @InProceedings{klementiev-etal:2012:EACL,
         author =  {Alex Klementiev and Ann Irvine and Chris Callison-Burch and David Yarowsky},
         title     = {Toward Statistical Machine Translation without Parallel Corpora},
         booktitle = {Proceedings of the 13th Conference of the European Chapter of the Association for computational Linguistics},
         month     = {April},
         year      = {2012},
         address   = {Avignon, France}
         publisher = {Association for Computational Linguistics},
       }
       
-
   title: Use of Modality and Negation in Semantically-Informed Syntactic MT
   authors: Kathryn Baker, Bonnie Dorr, Michael Bloodgood, Chris Callison-Burch, Wes Filardo, Christine Piatko, Lori Levin, and Scott Miller
   venue: Computational Linguistics
   type: journal
   year: 2012
   url: publications/modality-and-negation-in-semantically-informed-syntactic-mt.pdf
   page_count: 28
   id: modality-and-negation-in-semantically-informed-syntactic-mt
   figures:
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-1.jpg
         label: Figure 1
         caption: Modality/Negation Tagging Examples
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-2.jpg
         label: Figure 2
         caption: An example of Urdu-English translation. Shown are an Urdu source document, a reference translation produced by a professional human translator, and machine translation output from a phrase-based model (Moses) without linguistic information, which is representative of state-of-the-art MT quality before the SIMT effort
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-table-1.jpg
         label: Table 1
         caption: The size of the various data sets used for the experiments in this paper including the training, development (dev), incremental test set (devtest) and blind test set (test). The dev/devtest was a split of the NIST08 Urdu-English test set, and the blind test set was NIST09.
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-3.jpg
         label: Figure 3
         caption: The evolution of a semantically informed approach to our synchronous context free grammars (SCFGs). At the start of summer the decoder used translation rules with a single generic non-terminal symbol, later syntactic categories were used, and by the end of the summer the translation rules included semantic elements such as modalities and negation, as well as named entities.
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-4.jpg
         label: Figure 4
         caption: Eight Modalities Used for Tagging. H stands for the Holder of the modality, and P is the proposition over which the modality has scope.
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-5.jpg
         label: Figure 5
         caption: Thirteen Menu Choices for Modality/Negation Annotation
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-6.jpg
         label: Figure 6
         caption: Modality Lexicon Entry for need
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-7.jpg
         label: Figure 7
         caption: Sample output from the structure-based MN tagger
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-8.jpg
         label: Figure 8
         caption: Example of Embedded Target Head found inside VP must be found
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-9.jpg
         label: Figure 9
         caption: Example of Modality Composed with Negation&colon; TrigAble and TrigNegation combine to form NOTAble
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-10.jpg
         label: Figure 10
         caption: A sentence on the English side of the bilingual parallel training corpus is parsed with a syntactic parser, and also tagged with our modality tagger. The tags are then grafted onto the syntactic parse tree to form new categories like VP-TargNOTAble and VP-TargRequire. Grafting happens prior to extracting translation rules, which happens normally except for the use of the augmented trees.
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-figure-11.jpg
         label: Figure 11
         caption: Example translation rules with tags for modality, negation, and entities combined with syntactic categories.
      -
         img: figures/modality-and-negation-in-semantically-informed-syntactic-mt/modality-and-negation-in-semantically-informed-syntactic-mt-table-2.jpg
         label: Table 2
         caption: Modality Tags with their Negated Versions. Note that Require and Permit are in a dual relation, and thus RequireNegation is represented as NOTPermit and PermitNegation is represented as NOTRequire.
   abstract: This article describes the resource- and system-building efforts of an eight-week JHU Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically-Informed Machine Translation (SIMT). We describe a new modality/negation (MN) annotation scheme, a (publicly available) MN lexicon, and two automated MN taggers that we built using the annotation scheme and lexicon. Our annotation scheme isolates three components of modality and negation&colon; a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation) and a holder (an experiencer of modality). We describe how our MN lexicon was produced semi-automatically and we demonstrate that a structure-based MN tagger results in precision around 86% (depending on genre) for tagging of a standard LDC data set. We apply our MN annotation scheme to statistical machine translation using a syntactic framework that supports the inclusion of semantic annotations. Syntactic tags enriched with semantic annotations are assigned to parse trees in the target-language training texts through a process of tree grafting. While the focus of our work is modality and negation, the tree grafting procedure is general and supports other types of semantic information. We exploit this capability by including named entities, produced by a pre-existing tagger, in addition to the MN elements produced by the taggers described in this paper. The resulting system significantly outperformed a linguistically naïve baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English test set. This finding supports the hypothesis that both syntactic and semantic information can improve translation quality. 
   bibtex: |
      @article{baker-etal:2012:CL,
         author =  {Kathryn Baker and Bonnie Dorr and Michael Bloodgood and Chris Callison-Burch and Nathaniel Filardo and Christine Piatko and Lori Levin and Scott Miller},
         title =   {Use of Modality and Negation in Semantically-Informed Syntactic MT},
         journal = {Computational Linguistics},
         year =    {2012},
         volume = {38},
         number = {2},
         pages = {411-438}
       }
       
-
   title: Findings of the 2011 Workshop on Statistical Machine Translation
   authors: Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan
   venue: WMT
   type: workshop
   year: 2011
   url: publications/findings-of-the-wmt11-shared-tasks.pdf
   page_count: 43
   id: findings-of-the-wmt11-shared-tasks
   figures:
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-figure-1.jpg
         label: Figure 1
         caption: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words (case-insensitive) is based on the provided tokenizer.
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-1.jpg
         label: Table 1
         caption: Participants in the shared translation task (European language pairs; individual system track). Not all teams participated in all language pairs. The translations from commercial and online systems were crawled by us, not submitted by the respective companies, and are therefore anonymized.
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-2.jpg
         label: Table 2
         caption: Participants in the shared system combination task. Not all teams participated in all language pairs. ∗ The Quaero Project entry combined outputs they received directly from LIMSI, KIT, SYSTRAN, and RWTH.
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-3.jpg
         label: Table 3
         caption: Participants in the featured translation task (Haitian Creole SMS into English; individual system track). Not all teams participated in both the ‘Clean’ and ‘Raw’ tracks.
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-4.jpg
         label: Table 4
         caption: Examples of some of the Haitian Creole SMS messages that were sent to the 4636 short code along with their translations into English. Translations were done by volunteers who wanted to help with the relief effort. Prior to being distributed, the messages were anonymized to remove names, phone numbers, email addresses, etc. The anonymization guidelines specified that addresses be retained to facilitate work on mapping technologies.
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-5.jpg
         label: Table 5
         caption: Training data for the Haitian Creole-English featured translation task. The in-domain SMS data consists primarily of raw (noisy) SMS data. The in-domain data was provided by Mission 4636. The other data is out-of- domain. It comes courtesy of Carnegie Mellon University, Microsoft Research, Haitisurf.com, and Krengle.net.
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-6.jpg
         label: Table 6
         caption: A summary of the WMT11 ranking task, showing the number of systems and number of labels collected in each of the individual and system combination tracks. The system count does not include the reference translation, which was included in the evaluation, and so a value under “Labels per System” can be obtained only after adding 1 to the system count, before dividing the label count (e.g. in German-English, 4, 620/21 = 220.0).
      -
         img: figures/findings-of-the-wmt11-shared-tasks/findings-of-the-wmt11-shared-tasks-table-11.jpg
         label: Table 11
         caption: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics as baselines.
   abstract: This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics. We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics. This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake. We also conducted a pilot ‘tunable metrics’ task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality. 
   bibtex: |
      @InProceedings{callisonburch-EtAl:2011:WMT,
         author    = {Callison-Burch, Chris  and  Koehn, Philipp  and  Monz, Christof  and  Zaidan, Omar},
         title     = {Findings of the 2011 Workshop on Statistical Machine Translation},
         booktitle = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
         month     = {July},
         year      = {2011},
         address   = {Edinburgh, Scotland},
         publisher = {Association for Computational Linguistics},
         pages     = {22--64},
         url       = {http://www.aclweb.org/anthology/W11-2103}
       }
       
-
   title: Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation
   authors: Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme
   venue: EMNLP
   type: conference
   year: 2011
   url: publications/learning-sentential-paraphrases-from-bilingual-parallel-corpora.pdf
   page_count: 12
   id: learning-sentential-paraphrases-from-bilingual-parallel-corpora
   figures:
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-figure-1.jpg
         label: Figure 1
         caption: Synchronous grammar rules for translation are extracted from sentence pairs in a bixtext which have been automatically parsed and word-aligned. Extraction methods vary on whether they extract only minimal rules for phrases dominated by nodes in the parse tree, or more complex rules that include non-constituent phrases.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-figure-2.jpg
         label: Figure 2
         caption: An example derivation produced by a syntactic machine translation system. Although the synchronous trees are unlike the derivations found in the Penn Treebank, their yield is a good translation of the German.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-figure-3.jpg
         label: Figure 3
         caption: An example of a synchronous paraphrastic derivation. A few of the rules applied in the parse are show in the left column, with the pivot phrases that gave rise to them on the right.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-1.jpg
         label: Table 1
         caption: A selection of meaning-preserving transformations and hand-picked examples of syntactic paraphrases that our system extracts capturing these.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-2.jpg
         label: Table 2
         caption: Number and distribution of rules in our para- phrase grammar. Note the significant number of identity paraphrases and rules with complex nonterminal labels.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-4.jpg
         label: Table 4
         caption: Human evaluation for shorter compressions and for variations of our paraphrase system. +Feat. includes the compression features from Section 6.1, +Aug. includes optional deletion rules from Section 6.4.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-3.jpg
         label: Table 3
         caption: Results of the human evaluation on longer compressions&colon; pairwise compression rates (CR), meaning and grammaticality scores. Bold indicates a statistically significance difference at p < 0.05.
      -
         img: figures/learning-sentential-paraphrases-from-bilingual-parallel-corpora/learning-sentential-paraphrases-from-bilingual-parallel-corpora-table-5.jpg
         label: Table 5
         caption: Example compressions produced by the two systems in Table 3 for three input sentences from our test data.
   abstract: Previous work has shown that high quality phrasalparaphrases can be extracted from bilingual parallel corpora. However, it is not clear whether bitexts are an appropriate resource for extracting more sophisticated sentential paraphrases, which are more obviously learnable from monolingual parallel corpora. We extend bilingual paraphrase extraction to syntactic paraphrases and demonstrate its ability to learn a variety of general paraphrastic transformations, including passivization, dative shift, and topicalization. We discuss how our model can be adapted to many text generation tasks by augmenting its feature set, development data, and parameter estimation routine. We illustrate this adaptation by using our paraphrase model for the task of sentence compression and achieve results competitive with state-of-the-art compression systems. 
   bibtex: |
      @InProceedings{ganitkevitch-EtAl:2011:EMNLP,
         author    = {Ganitkevitch, Juri  and  Callison-Burch, Chris  and  Napoles, Courtney  and  {Van Durme}, Benjamin},
         title     = {Learning Sentential Paraphrases from Bilingual Parallel Corpora for Text-to-Text Generation},
         booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
         month     = {July},
         year      = {2011},
         address   = {Edinburgh, Scotland, UK.},
         publisher = {Association for Computational Linguistics},
         pages     = {1168--1179},
         url       = {http://www.aclweb.org/anthology/D11-1108}
       }
       
-
   title: Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity
   authors: Charley Chan, Chris Callison-Burch, and Benjamin Van Durme
   venue: GEMS
   type: workshop
   year: 2011
   url: publications/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity.pdf
   page_count: 10
   id: reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity
   figures:
      -
         img: figures/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-figure-1.jpg
         label: Figure 1
         caption: Using a bilingual parallel corpus to extract paraphrases.
      -
         img: figures/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-table-1.jpg
         label: Table 1
         caption: Paraphrases for huge amount of according to the bilingual pivoting (BiP), syntactic-constrainted bilingual pivoting (SyntBiP) translation score and the monolingual similarity score via LSH (MonoDS), ranked by corresponding scores listed next to each paraphrase. Syntactic type of the phrase is [JJ+NN+IN].
      -
         img: figures/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-table-2.jpg
         label: Table 2
         caption: Ordered reranked paraphrase candidates for the phrase reluctant according to monolingual distributional similarity (MonoDShand−selected) and bilingual pivoting paraphrase (BiP) method. Two hand-selected phrases are labeled with asterisks.
      -
         img: figures/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-table-3.jpg
         label: Table 3
         caption: Kendall’s Tau rank correlation coefficients be- tween human judgment of meaning and grammaticality for the different paraphrase scoring methods. Bottom panel&colon; SyntBiPmatched is the same as SyntBiP except paraphrases must match with the original phrase in syn- tactic type. SyntBiP* and MonoDS* are the same as before except they share the same phrase support with SyntBiPmatched . (‡&colon; MonoDS outperforms the corresponding BiP reranking at p-value ≤0.01, and † at ≤0.05)
      -
         img: figures/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity/reranking-bilingually-extracted-paraphrases-using-monolingual-distributional-similarity-figure-2.jpg
         label: Figure 2
         caption: Averaged scores in the top K paraphrase candidates as a function of K for different reranking metrics. All methods performs similarly in meaning preservation, but SyntBiP-MonoDS outperforms other scoring methods in grammaticality, as shown in the bottom graph.
   abstract: This paper improves an existing bilingual paraphrase extraction technique using monolingual distributional similarity to rerank candidate paraphrases. Raw monolingual data provides a complementary and orthogonal source of information that lessens the commonly observed errors in bilingual pivot-based methods. Our experiments reveal that monolingual scoring of bilingually extracted paraphrases has a significantly stronger correlation with human judgment for grammaticality than the probabilities assigned by the bilingual pivoting method does. The results also show that monolingual distribution similarity can serve as a threshold for high precision paraphrase selection. 
   bibtex: |
      @InProceedings{chan-callisonburch-vandurme:2011:GEMS,
         author    = {Chan, Tsz Ping  and  Callison-Burch, Chris  and  {Van Durme}, Benjamin},
         title     = {Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity},
         booktitle = {Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics},
         month     = {July},
         year      = {2011},
         address   = {Edinburgh, UK},
         publisher = {Association for Computational Linguistics},
         pages     = {33--42},
         url       = {http://www.aclweb.org/anthology/W11-2504}
       }
       
-
   title: Joshua 3.0&colon; Syntax-based Machine Translation with the Thrax Grammar Extractor
   authors: Jonathan Weese, Juri Ganitkevitch, Chris Callison-Burch, Matt Post and Adam Lopez
   venue: WMT
   type: workshop
   year: 2011
   url: publications/joshua-3.0.pdf
   page_count: 7
   id: joshua-3
   figures:
      -
         img: figures/joshua-3.0/joshua-3.0-figure-1.jpg
         label: Figure 1
         caption: An aligned sentence pair.
      -
         img: figures/joshua-3.0/joshua-3.0-table-1.jpg
         label: Table 1
         caption: A subset of the Hiero and SAMT rules extracted from the sentence pair of Figure 1.
      -
         img: figures/joshua-3.0/joshua-3.0-table-2.jpg
         label: Table 2
         caption: Training data size after subsampling.
      -
         img: figures/joshua-3.0/joshua-3.0-table-3.jpg
         label: Table 3
         caption: Single-reference BLEU-4 scores.
      -
         img: figures/joshua-3.0/joshua-3.0-figure-2.jpg
         label: Figure 2
         caption: An SAMT derivation. The shaded terminal symbols are the lexicalized part of a rule with terminals and non-terminals. The unshaded terminals are directly dominated by a nonterminal symbol.
   abstract: We present progress on Joshua, an open source decoder for hierarchical and syntax-based machine translation. The main focus is describing Thrax, a flexible, open source synchronous context-free grammar extractor. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. It is built on Apache Hadoop for efficient distributed performance, and can easily be extended with support for new grammars, feature functions, and output formats. 
   bibtex: |
      @InProceedings{weese-EtAl:2011:WMT,
         author    = {Weese, Jonathan  and  Ganitkevitch, Juri  and  Callison-Burch, Chris  and  Post, Matt  and  Lopez, Adam},
         title     = {Joshua 3.0: Syntax-based Machine Translation with the Thrax Grammar Extractor},
         booktitle = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
         month     = {July},
         year      = {2011},
         address   = {Edinburgh, Scotland},
         publisher = {Association for Computational Linguistics},
         pages     = {478--484},
         url       = {http://www.aclweb.org/anthology/W11-2160}
       }
       
-
   title: WikiTopics&colon; What is Popular on Wikipedia and Why
   authors: Byung Gyu Ahn, Ben Van Durme and Chris Callison-Burch
   venue: ACL Workshop on Automatic Summarization for Different Genres, Media, and Languages
   type: workshop
   year: 2011
   url: publications/wikitopics-what-is-popular-on-wikipedia-and-why.pdf
   page_count: 8
   id: wikitopics-what-is-popular-on-wikipedia-and-why
   figures:
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-1.jpg
         label: Figure 1
         caption: Automatically selected articles for Jan 27, 2009.
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-2.jpg
         label: Figure 2
         caption: Process diagram&colon; (a) Topic selection&colon; select interesting articles based on increase in pageviews. (b) Clustering&colon; cluster the articles according to relevant events using topic models or Wikipedia’s hyperlink structure. (c) Textualization&colon; select the sentence that best summarizes the relevant event
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-3.jpg
         label: Figure 3
         caption: Pageviews for all the hand-curated articles related to the inauguration of Barack Obama. Pageviews spike on the same day as the event took place–January 20, 2009.
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-4.jpg
         label: Figure 4
         caption: Log ratio of the increase in pageviews&colon; log􏰅i = 115dik/􏰅i = 1630. Zero means no change in pageviews. WikiTopics articles show pageviews increase in a few orders of magnitude as opposed to hand-curated articles.
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-5.jpg
         label: Figure 5
         caption: Illustrative articles for January 27, 2009. WikiTopics articles here do not appear in hand-curated articles within fifteen days before or after, and vice versa. The hand-curated articles shown here are all linked from a single event “Florida hedge fund manager Arthur Nadel is arrested by the United States Federal Bureau of Investigation and charged with fraud.”
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-table-1.jpg
         label: Table 1
         caption: Clustering evaluation&colon; F-scores are averaged across gold standard datasets. ConComp and OneHop are using the link structure. K-means clustering with tf-idf performs best. Manual clusters were evaluated against those of the other two annotators to determine inter-annotator agreement.
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-6.jpg
         label: Figure 6
         caption: Examples of clusters&colon; K-means clustering on the articles of January 27, 2009 and May 12, 2009. The centroid article for each cluster, defined as the closest article to the center of the cluster in vector space, is in bold.
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-figure-7.jpg
         label: Figure 7
         caption: Selected examples of temporal expressions identified by Serif from 247 such date and time expressions extracted from the article Abraham Lincoln.
      -
         img: figures/wikitopics-what-is-popular-on-wikipedia-and-why/wikitopics-what-is-popular-on-wikipedia-and-why-table-2.jpg
         label: Table 2
         caption: Textualization&colon; evaluation results of sentence selection schemes. Self fallback scheme first tries to select the best sentence as the Self scheme, and if it fails to select one it falls back to the Recent scheme.
   abstract: We establish a novel task in the spirit of news summarization and topic detection and tracking (TDT)&colon; daily determination of the topics newly popular with Wikipedia readers. Central to this effort is a new public dataset consisting of the hourly page view statistics of all Wikipedia articles over the last three years. We give baseline results for the tasks of&colon; discovering individual pages of interest, clustering these pages into coherent topics, and extracting the most relevant summarizing sentence for the reader. When compared to human judgements, our system shows the viability of this task, and opens the door to a range of exciting future work. 
   bibtex: |
      @InProceedings{ahn-vandurme-callisonburch:2011:SummarizationWorkshop,
         author    = {Ahn, Byung Gyu  and  {Van Durme}, Benjamin  and  Callison-Burch, Chris},
         title     = {WikiTopics: What is Popular on Wikipedia and Why},
         booktitle = {Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon},
         publisher = {Association for Computational Linguistics},
         pages     = {33--40},
         url       = {http://www.aclweb.org/anthology/W11-0505}
       }
       
-
   title: Evaluating sentence compression&colon; Pitfalls and suggested remedies
   authors: Courtney Napoles, Ben Van Durme
   venue: Workshop on Monolingual Text-To-Text Generation
   type: workshop
   year: 2011
   url: publications/evaluating-sentence-compression-pitfalls-and-suggested-remedies.pdf
   page_count: 7
   id: evaluating-sentence-compression-pitfalls-and-suggested-remedies
   figures:
      -
         img: figures/evaluating-sentence-compression-pitfalls-and-suggested-remedies/evaluating-sentence-compression-pitfalls-and-suggested-remedies-table-1.jpg
         label: Table 1
         caption: Three acceptable compressions of a sentence created by different annotators (the first is the original).
      -
         img: figures/evaluating-sentence-compression-pitfalls-and-suggested-remedies/evaluating-sentence-compression-pitfalls-and-suggested-remedies-figure-1.jpg
         label: Figure 1
         caption: Compression rate strongly correlates with human judgments of meaning and grammaticality. Gold represents gold-standard compression and Deletion the results of a leading deletion model. Gold.1 grammar judgments were made alongside the original sentence and Gold.2 were made in isolation.
      -
         img: figures/evaluating-sentence-compression-pitfalls-and-suggested-remedies/evaluating-sentence-compression-pitfalls-and-suggested-remedies-table-2.jpg
         label: Table 2
         caption: Mean quality ratings of two competing models once the compression rates have been standardized, and as reported in the original work (denoted ∗). There is no significant improvement, but the numerically better model changes.
   abstract: This work surveys existing evaluation methodologies for the task of sentence compression, identifies their shortcomings, and proposes alternatives. In particular, we examine the problems of evaluating paraphrastic compression and comparing the output of different models. We demonstrate that compression rate is a strong predictor of compression quality and that perceived improvement over other models is often a side effect of producing longer output. 
   bibtex: |
      @InProceedings{napoles-vandurme-callisonburch:2011:T2TW-2011,
         author    = {Napoles, Courtney  and  {Van Durme}, Benjamin  and  Callison-Burch, Chris},
         title     = {Evaluating Sentence Compression: Pitfalls and Suggested Remedies},
         booktitle = {Proceedings of the Workshop on Monolingual Text-To-Text Generation},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon},
         publisher = {Association for Computational Linguistics},
         pages     = {91--97},
         url       = {http://www.aclweb.org/anthology/W11-1611}
       }
       
-
   title: Paraphrastic Sentence Compression with a Character-based Metric&colon; Tightening without Deletion
   authors: Courtney Napoles, Chris Callison-Burch, Juri Ganitevitch, Ben Van Durme
   venue: Workshop on Monolingual Text-To-Text Generation
   type: workshop
   year: 2011
   url: publications/paraphrastic-sentence-compression.pdf
   page_count: 7
   id: paraphrastic-sentence-compression
   figures:
      -
         img: figures/paraphrastic-sentence-compression/paraphrastic-sentence-compression-figure-1.jpg
         label: Figure 1
         caption: Using a bilingual parallel corpus to extract paraphrases.
      -
         img: figures/paraphrastic-sentence-compression/paraphrastic-sentence-compression-table-1.jpg
         label: Table 1
         caption: Candidate paraphrases for study in detail with corresponding approximate cosine similarity (Monolingual) and translation model (Bilingual) scores.
      -
         img: figures/paraphrastic-sentence-compression/paraphrastic-sentence-compression-table-2.jpg
         label: Table 2
         caption: Mean ratings of compressions using just deletion or substitution at different paraphrase thresholds (Cos). Deletion performed better in all settings.
      -
         img: figures/paraphrastic-sentence-compression/paraphrastic-sentence-compression-table-3.jpg
         label: Table 3
         caption: Mean ratings of compressions generated by a substitution oracle, deletion only, deletion on the oracle compression, and the gold standard. Being able to choose the best paraphrases would enable our substitution model to outperform the deletion model.
   abstract: We present a substitution-only approach to sentence compression which “tightens” a sentence by reducing its character length. Replacing phrases with shorter paraphrases yields paraphrastic compressions as short as 60% of the original length. In support of this task, we introduce a novel technique for re-ranking paraphrases extracted from bilingual corpora. At high compression rates1 paraphrastic compressions outperform a state-of-the-art deletion model in an oracle experiment. For further compression, deleting from oracle paraphrastic compressions preserves more meaning than deletion alone. In either setting, paraphrastic compression shows promise for surpassing deletion-only methods. 
   bibtex: |
      @InProceedings{napoles-EtAl:2011:T2TW-2011,
         author    = {Napoles, Courtney  and  Callison-Burch, Chris  and  Ganitkevitch, Juri  and  {Van Durme}, Benjamin},
         title     = {Paraphrastic Sentence Compression with a Character-based Metric: Tightening without Deletion},
         booktitle = {Proceedings of the Workshop on Monolingual Text-To-Text Generation},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon},
         publisher = {Association for Computational Linguistics},
         pages     = {84--90},
         url       = {http://www.aclweb.org/anthology/W11-1610}
       }
       
-
   title: Paraphrase Fragment Extraction from Monolingual Comparable Corpora
   authors: Rui Wang and Chris Callison-Burch
   venue: BUCC
   type: workshop
   year: 2011
   url: publications/paraphrase-fragment-extraction-from-monolingual-comparable-corpora.pdf
   page_count: 9
   id: paraphrase-fragment-extraction-from-monolingual-comparable-corpora
   figures:
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-1.jpg
         label: Table 1
         caption: Previous work in paraphrase acquisition and machine translation.
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-figure-1.jpg
         label: Figure 1
         caption: A three stage pipeline is used to extract paraphrases from monolingual texts
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-figure-2.jpg
         label: Figure 2
         caption: Results of the sentence pair extraction. The x-axis is the threshold for the comparability scores; and the y-axis is the distribution of the annotations.
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-figure-3.jpg
         label: Figure 3
         caption: An example of fragment pair extraction. Stop words are all set to 1 initially. Zero is the threshold, and the underscored phrases are the outputs.
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-2.jpg
         label: Table 2
         caption: Distribution of the Extracted Fragment Pairs of our Corpus and MSR Corpus. We manually evaluated 1051 sentence pairs in all. We use LCS or word aligner as the initialization and apply n-gram-based or chunk-based phrase extraction. The first column serves as the baseline.
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-5.jpg
         label: Table 5
         caption: Some examples of the extracted paraphrase fragment pairs
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-3.jpg
         label: Table 3
         caption: The size of our corpus. We only used ca. 10% of the GIGAWORD corpus in the experiments and the size of the collection at each stage are shown in the table
      -
         img: figures/paraphrase-fragment-extraction-from-monolingual-comparable-corpora/paraphrase-fragment-extraction-from-monolingual-comparable-corpora-table-4.jpg
         label: Table 4
         caption: The (partial) distribution of N-grams (N=1-4) in different paraphrase collections
   abstract: We present a novel paraphrase fragment pair extraction method that uses a monolingual comparable corpus containing different articles about the same topics or events. The procedure consists of document pair extraction, sentence pair extraction, and fragment pair extraction. At each stage, we evaluate the intermediate results manually, and tune the later stages accordingly. With this minimally supervised approach, we achieve 62% of accuracy on the paraphrase fragment pairs we collected and 67% extracted from the MSR corpus. The results look promising, given the minimal supervision of the approach, which can be further scaled up. 
   bibtex: |
      @InProceedings{wang-callisonburch:2011:BUCC,
         author    = {Wang, Rui  and  Callison-Burch, Chris},
         title     = {Paraphrase Fragment Extraction from Monolingual Comparable Corpora},
         booktitle = {Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon},
         publisher = {Association for Computational Linguistics},
         pages     = {52--60},
         url       = {http://www.aclweb.org/anthology/W11-1208}
       }
       
-
   title: The Arabic Online Commentary Dataset&colon; An Annotated Dataset of Informal Arabic with High Dialectal Content
   authors: Omar Zaidan and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2011
   url: publications/arabic-dialect-corpus.pdf
   page_count: 5
   id: arabic-dialect-corpus
   figures:
      -
         img: figures/arabic-dialect-corpus/arabic-dialect-corpus-figure-1.jpg
         label: Figure 1
         caption: Two roughly equivalent Arabic sentences, one in MSA and one in Levantine Arabic, translated by the same MT system into English. An acceptable translation would be When will we see this group of criminals undergo trial (or tried)?. The MSA variant is handled well, while the dialectal variant is mostly transliterated
      -
         img: figures/arabic-dialect-corpus/arabic-dialect-corpus-figure-2.jpg
         label: Figure 2
         caption: One possible breakdown of spoken Arabic into dialect groups&colon; Maghrebi, Egyptian, Levantine, Gulf, and Iraqi. Habash (2010) also gives a very similar breakdown
      -
         img: figures/arabic-dialect-corpus/arabic-dialect-corpus-table-1.jpg
         label: Table 1
         caption: A summary of the different components of the AOC dataset. Overall, 1.4M comments were harvested from 86.1K articles, corresponding to 52.1M words.
      -
         img: figures/arabic-dialect-corpus/arabic-dialect-corpus-table-2.jpg
         label: Table 2
         caption: A breakdown of sentences for which ≥ 2 annotators agreed on whether dialectal content exists or not.
      -
         img: figures/arabic-dialect-corpus/arabic-dialect-corpus-table-3.jpg
         label: Table 3
         caption: Accuracy, dialect precision, and dialect recall (10-fold cross validation) for various classification tasks.
      -
         img: figures/arabic-dialect-corpus/arabic-dialect-corpus-figure-3.jpg
         label: Figure 3
         caption: Dialect precision vs. recall for the classification task over Al-Ghad sentences (MSA vs. Levantine). The square point corresponds to the first line in Table 3.
   abstract: The written form of Arabic, Modern Standard Arabic (MSA), differs quite a bit from the spoken dialects of Arabic, which are the true “native” languages of Arabic speakers used in daily life. However, due to MSA’s prevalence in written form, almost all Arabic datasets have predominantly MSA content. We present the Arabic Online Commentary Dataset, a 52M-word monolingual dataset rich in dialectal content, and we describe our long-term annotation effort to identify the dialect level (and dialect itself) in each sentence of the dataset. So far, we have labeled 108K sentences, 41% of which as having dialectal content. We also present experimental results on the task of automatic dialect identification, using the collected labels for training and evaluation. 
   bibtex: |
      @InProceedings{zaidan-callisonburch:2011:ACL-HLT2011,
         author    = {Zaidan, Omar F.  and  Callison-Burch, Chris},
         title     = {The Arabic Online Commentary Dataset: an Annotated Dataset of Informal Arabic with High Dialectal Content},
         booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon, USA},
         publisher = {Association for Computational Linguistics},
         pages     = {37--41},
         url       = {http://www.aclweb.org/anthology/P11-2007}
       }
       
-
   title: Crowdsourcing Translation&colon; Professional Quality from Non-Professionals
   authors: Omar Zaidan and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2011
   url: publications/crowdsourcing-translation.pdf
   page_count: 10
   id: crowdsourcing-translation
   figures:
      -
         img: figures/crowdsourcing-translation/crowdsourcing-translation-figure-1.jpg
         label: Figure 1
         caption: A comparison of professional translations provided by the LDC to non-professional translations created on Mechanical Turk.
      -
         img: figures/crowdsourcing-translation/crowdsourcing-translation-figure-2.jpg
         label: Figure 2
         caption: We redundantly translate each source sentence by soliciting multiple translations from different Turkers. These translations are put through a subsequent editing set, where multiple edited versions are produced. We select the best translation from the set using features that predict the quality of each translation and each translator.
      -
         img: figures/crowdsourcing-translation/crowdsourcing-translation-figure-3.jpg
         label: Figure 3
         caption: BLEU scores for different selection methods, measured against the reference sets. Each score is an average of four BLEU scores, each calculated against three LDC reference translations. The five right-most bars are colored in orange to indicate selection over a set that includes both original translations as well as edited versions of them.
      -
         img: figures/crowdsourcing-translation/crowdsourcing-translation-table-1.jpg
         label: Table 1
         caption: Correlation (± std. dev.) for different selection methods, compared against the reference sets.
      -
         img: figures/crowdsourcing-translation/crowdsourcing-translation-figure-4.jpg
         label: Figure 4
         caption: BLEU scores for the five right-most setups from Figure 3, constrained over the original translations.
      -
         img: figures/crowdsourcing-translation/crowdsourcing-translation-figure-5.jpg
         label: Figure 5
         caption: The effect of varying the amount of calibration data (and using only the calibration feature). The 10% point (BLEU = 37.82) and the dashed line (BLEU = 39.06) correspond to the two right-most bars of Figure 3.
   abstract: Naively collecting translations by crowdsourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-toEnglish evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional translators. The total cost is more than an order of magnitude lower than professional translation. 
   bibtex: |
      @InProceedings{zaidan-callisonburch:2011:ACL-HLT2011,
         author    = {Zaidan, Omar F.  and  Callison-Burch, Chris},
         title     = {Crowdsourcing Translation: Professional Quality from Non-Professionals},
         booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon, USA},
         publisher = {Association for Computational Linguistics},
         pages     = {1220--1229},
         url       = {http://www.aclweb.org/anthology/P11-1122}
       }
       
-
   title: Incremental Syntactic Language Models for Phrase-based Translation
   authors: Lane Schwartz, Chris Callison-Burch, William Schuler and Stephen Wu
   venue: ACL
   type: conference
   year: 2011
   url: publications/incremental-syntactic-language-models-for-phrase-based-translation.pdf
   page_count: 12
   id: incremental-syntactic-language-models-for-phrase-based-translation
   figures:
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-1.jpg
         label: Figure 1
         caption: Partial decoding lattice for standard phrase-based decoding stack algorithm translating the German sentence Der Pra ̈sident trifft am Freitag den Vorstand. Each node h in decoding stack t represents the application of a translation option, and includes the source sentence coverage vector, target language n-gram state, and syntactic language model state τ ̃ . Hypothesis combination is also shown, indicating where lattice paths with identical n-gram histories converge. We use the English translation The president meets the board on Friday as a running example throughout all Figures.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-2.jpg
         label: Figure 2
         caption: Sample binarized phrase structure tree.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-3.jpg
         label: Figure 3
         caption: Sample binarized phrase structure tree af- ter application of right-corner transform.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-4.jpg
         label: Figure 4
         caption: Graphical representation of the dependency structure in a standard Hierarchic Hidden Markov Model with D = 3 hidden levels that can be used to parse syntax. Circles denote random variables, and edges denote conditional dependencies. Shaded circles denote variables with observed values.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-5.jpg
         label: Figure 5
         caption: Graphical representation of the Hierarchic Hidden Markov Model after parsing input sentence The president meets the board on Friday. The shaded path through the parse lattice illustrates the recognized right-corner tree structure of Figure 3.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-6.jpg
         label: Figure 6
         caption: A hypothesis in the phrase-based decoding lattice from Figure 1 is expanded using translation option the board of source phrase den Vorstand. Syntactic language model state τ ̃31 contains random variables s1..3; likewise τ ̃51 contains s1..3. The intervening random variables r1..3, s1..3, and r1..3 are calculated by transition function δ (Eq. 6, as defined by §4.1), but are not stored. Observed random variables (e3..e5) are shown for clarity, but are not explicitly stored in any syntactic language model state.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-7.jpg
         label: Figure 7
         caption: Average per-word perplexity values. HHMM was run with beam size of 2000. Bold indicates best single-model results for LMs trained on WSJ sections 2-21. Best overall in italics.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-8.jpg
         label: Figure 8
         caption: Mean per-sentence decoding time (in seconds) for dev set using Moses with and without syntactic language model. HHMM parser beam sizes are indicated for the syntactic LM.
      -
         img: figures/incremental-syntactic-language-models-for-phrase-based-translation/incremental-syntactic-language-models-for-phrase-based-translation-figure-9.jpg
         label: Figure 9
         caption: Results for Ur-En devtest (only sentences with 1-20 words) with HHMM beam size of 2000 and Moses settings of distortion limit 10, stack size 200, and ttable limit 20.
   abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporating syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity. 
   bibtex: |
      @InProceedings{schwartz-EtAl:2011:ACL-HLT20111,
         author    = {Schwartz, Lane  and  Callison-Burch, Chris  and  Schuler, William  and  Wu, Stephen},
         title     = {Incremental Syntactic Language Models for Phrase-based Translation},
         booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
         month     = {June},
         year      = {2011},
         address   = {Portland, Oregon, USA},
         publisher = {Association for Computational Linguistics},
         pages     = {620--631},
         url       = {http://www.aclweb.org/anthology/P11-1063}
       }
       
-
   title: Predicting Human-Targeted Translation Edit Rate via Untrained Human Annotators
   authors: Omar Zaidan and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2010
   url: publications/predicting-HTER-from-untrained-annotators.pdf
   page_count: 4
   id: predicting-HTER-from-untrained-annotators
   figures:
      -
         img: figures/predicting-HTER-from-untrained-annotators/predicting-HTER-from-untrained-annotators-table-1.jpg
         label: Table 1
         caption: The 4 genres of the dataset.
      -
         img: figures/predicting-HTER-from-untrained-annotators/predicting-HTER-from-untrained-annotators-figure-1.jpg
         label: Figure 1
         caption: Rank correlation between predicted ranking and HTER ranking for different prediction schemes, across the four genres, and across various sizes of the worker verification set.
   abstract: In the field of machine translation, automatic metrics have proven quite valuable in system development for tracking progress and measuring the impact of incremental changes. However, human judgment still plays a large role in the context of evaluating MT systems. For example, the GALE project uses human-targeted translation edit rate (HTER), wherein the MT output is scored against a post-edited version of itself (as opposed to being scored against an existing human reference). This poses a problem for MT researchers, since HTER is not an easy metric to calculate, and would require hiring and training human annotators to perform the editing task. In this work, we explore soliciting those edits from untrained human annotators, via the online service Amazon Mechanical Turk. We show that the collected data allows us to predict HTER-ranking of documents at a significantly higher level than the ranking obtained using automatic metrics. 
   bibtex: |
      @InProceedings{zaidan-callisonburch:2010:NAACLHLT,
         author    = {Zaidan, Omar F.  and  Callison-Burch, Chris},
         title     = {Predicting Human-Targeted Translation Edit Rate via Untrained Human Annotators},
         booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles, California},
         publisher = {Association for Computational Linguistics},
         pages     = {369--372},
         url       = {http://www.aclweb.org/anthology/N10-1057}
       }
       
-
   title: Semantically-Informed Syntactic Machine Translation&colon; A Tree-Grafting Approach
   authors: Kathryn Baker, Michael Bloodgood, Chris Callison-Burch, Bonnie Dorr, Scott Miller, Christine Piatko, Nathaniel W. Filardo, and Lori Levin
   venue: AMTA
   type: conference
   year: 2010
   url: publications/semantically-informed-syntactic-machine-translation.pdf
   page_count: 10
   id: semantically-informed-syntactic-machine-translation
   figures:
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-1.jpg
         label: Figure 1
         caption: An example of Urdu-English translation. Shown are an Urdu source document, a reference translation produced by a professional human translator, and machine translation output from a phrase-based model (Moses) without linguistic information, which is representative of state-of-the-art MT quality before the SIMT effort.
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-2.jpg
         label: Figure 2
         caption: The evolution of a semantically informed approach to our synchronous context free grammars (SCFGs). At the start of summer the decoder used translation rules with a single generic non-terminal symbol, later syntactic categories were used, and by the end of the summer the translation rules included semantic elements such as named entities and modalities.
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-table-1.jpg
         label: Table 1
         caption: The size of the various data sets used for the experiments in this paper including the training, development (dev), incremental test set (devtest) and blind test set (test). The dev/devtest was a split of the NIST08 Urdu-English test set, and the blind test set was NIST09.
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-3.jpg
         label: Figure 3
         caption: Workflow for producing semantically-grafted parse trees. The English side of the parallel corpus is automatically parsed, and also tagged with modality and named-entity markers. These tags are then grafted onto the syntactic parse trees. The relation finder was designed for additional tagging but was not implemented in the current work. (Future work will test relations as another component of meaning that may contribute toward im- proved MT ouput.)
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-4.jpg
         label: Figure 4
         caption: A sentence on the English side of the bilingual parallel training corpus is parsed with a syntactic parser, and also tagged with a named entity tagger. The tags are then grafted onto the syntactic parse tree to form new categories like NP-GPE and NP-weapon. Grafting happens prior to extracting translation rules, which happens normally except for the use of the augmented trees.
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-5.jpg
         label: Figure 5
         caption: Example translation rules with named entity tags and modalities combined with syntactic categories.
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-table-2.jpg
         label: Table 2
         caption: Named entity tags
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-table-3.jpg
         label: Table 3
         caption: Modality tags with their negated versions
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-6.jpg
         label: Figure 6
         caption: Results for a range of experiments conducted during the SIMT effort. Results show scores for base- line systems, which here include a phrase-based model (Moses) and a hierarchical phrase-based model (Hiero), neither of which make use of syntactic information. These also show the substantial improvements when syn- tax is introduced, along with different numbers of feature functions (FFs), and further improvements from semantic elements. The scores are lowercased Bleu calculated on the held-out devtest set.
      -
         img: figures/semantically-informed-syntactic-machine-translation/semantically-informed-syntactic-machine-translation-figure-7.jpg
         label: Figure 7
         caption: An example of the improvements to Urdu-English translation before and after the SIMT effort. Output is from the baseline Hiero model, which does not use linguistic information, and from the final model, which incorporates syntactic and semantic information.
   abstract: We describe a unified and coherent syntactic framework for supporting a semantically-informed syntactic approach to statistical machine translation. Semantically enriched syntactic tags assigned to the target-language training texts improved translation quality. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English translation task. This finding supports the hypothesis (posed by many researchers in the MT community, e.g., in DARPA GALE) that both syntactic and semantic information are critical for improving translation quality—and further demonstrates that large gains can be achieved for low-resource languages with different word order than English. 
   bibtex: |
      @InProceedings{Baker-EtAl:2010:AMTA,
         author = {Kathryn Baker and Michael Bloodgood and Chris Callison-Burch and Bonnie J. Dorr and Nathaniel W. Filardo and Lori Levin and Scott Miller and Christine Piatko},
         title = {Semantically-Informed Machine Translation: A Tree-Grafting Approach},
         booktitle = {Proceedings of The Ninth Biennial Conference of the Association for Machine Translation in the Americas},
         address = {Denver, Colorado},
         url = {http://www.mt-archive.info/AMTA-2010-Baker.pdf},
         year = {2010}
       }
       
-
   title: Transliterating From All Languages
   authors: Ann Irvine, Alex Klementiev, and Chris Callison-Burch
   venue: AMTA
   type: conference
   year: 2010
   url: publications/transliterating-from-all-languages.pdf
   page_count: 8
   id: transliterating-from-all-languages
   figures:
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-1.jpg
         label: Table 1
         caption: Examples of Russian to English and Greek to English transliteration rules learned by Joshua along with the following associated log probabilities&colon; a character sequence mapping probability, a character substitution probability, and a character-based language model probability.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-2.jpg
         label: Table 2
         caption: The 100 languages with the largest number of name pairs with English. The counts are for Wikipedia pages describing people that have a inter-language link with English, and whose title is not identical to the English page title.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-3.jpg
         label: Table 3
         caption: Languages of interest and the number of harvested person names. There are many more English names than there are for other languages and, correspondingly, its overlap with other languages is relatively large. Consequently, the amount of training data for transliterating between English and other languages is greater than between any other pair of languages.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-4.jpg
         label: Table 4
         caption: Examples of multi-word Russian-English name pairs that require word alignments and filtering.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-5.jpg
         label: Table 5
         caption: A comparison of our performance against the systems submitted to the Russian and Hindi transliteration shared tasks at the 2009 Named Entities Workshop.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-6.jpg
         label: Table 6
         caption: Examples of candidate transliterations and their corresponding reference transliterations, and the edit distances and normalized edit distances between them. The normalized edit distance is the minimum number of insertions, deletions, and substitutions that must be made to transform one string into the other, normalized by the length of the reference string, and multiplied by 100.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-figure-1.jpg
         label: Figure 1
         caption: Number of training pairs vs. system performance as measured by average normalized edit distance from the reference. The normalized edit distance is the minimum number of insertions, deletions, and substitutions that must be made to transform one string into the other, normalized by the length of the reference string, and multiplied by 100.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-figure-2.jpg
         label: Figure 2
         caption: Learning curves resulting from holding out some training pairs from the models. The normalized edit distance is the minimum number of insertions, deletions, and substitutions that must be made to transform one string into the other, normalized by the length of the reference string, and multiplied by 100.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-figure-3.jpg
         label: Figure 3
         caption: The percent of perfect transliterations found in the n-best output vs. n in n-best.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-7.jpg
         label: Table 7
         caption: The 10 most common errors. The reference is on the left, and hypothesis is on the right. E indicates that the letter E is dropped from the hypothesis, and I indicates I is inserted.
      -
         img: figures/transliterating-from-all-languages/transliterating-from-all-languages-table-8.jpg
         label: Table 8
         caption: Examples of Russian to English transliteration output. The system produced the reference transliteration in the first three examples, and it produced a correct alternative English transliteration in the second three examples. It incorrectly transliterated the final three.
   abstract: Much of the previous work on transliteration has depended on resources and attributes specific to particular language pairs. In this work, rather than focus on a single language pair, we create robust models for transliterating from all languages in a large, diverse set to English. We create training data for 150 languages by mining name pairs from Wikipedia. We train 13 systems and analyze the effects of the amount of training data on transliteration performance. We also present an analysis of the types of errors that the systems make. Our analyses are particularly valuable for building machine translation systems for low resource languages, where creating and integrating a transliteration module for a language with few NLP resources may provide substantial gains in translation performance. 
   bibtex: |
      @InProceedings{Irvine-EtAl:2010:AMTA,
         author = {Ann Irvine and Chris Callison-Burch and Alexandre Klementiev}
         title = {Transliterating From All Languages},
         booktitle = {Proceedings of The Ninth Biennial Conference of the Association for Machine Translation in the Americas},
         address = {Denver, Colorado},
         url = {http://cis.upenn.edu/~ccb/publications/transliterating-from-all-languages.pdf},
         year = {2010}
       }
       
-
   title: Joshua 2.0&colon; A Toolkit for Parsing-Based Machine Translationwith Syntax, Semirings, Discriminative Training and Other Goodies
   authors: Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Ann Irvine, Lane Schwartz, Wren N. G. Thornton, Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
   venue: WMT
   type: workshop
   year: 2010
   url: publications/joshua-2.0.pdf
   page_count: 5
   id: joshua-2
   abstract: We describe the progress we have made in the past year on Joshua (Li et al., 2009a), an open source toolkit for parsing-based machine translation. The new functionality includes&colon; support for translation grammars with a rich set of syntactic nonterminals, the ability for external modules to posit constraints on how spans in the input sentence should be translated, lattice parsing for dealing with input uncertainty, a semiring framework that provides a unified way of doing various dynamic programming calculations, variational decoding for approximating the intractable MAP decoding, hypergraph-based discriminative training for better feature engineering, a parallelized MERT module, document-level and tail-based MERT, visualization of the derivation trees, and a cleaner pipeline for MT experiments. 
   bibtex: |
      @InProceedings{li-EtAl:2010:WMT,
         author    = {Li, Zhifei  and  Callison-Burch, Chris  and  Dyer, Chris  and  Ganitkevitch, Juri  and  Irvine, Ann  and  Khudanpur, Sanjeev  and  Schwartz, Lane  and  Thornton, Wren  and  Wang, Ziyuan  and  Weese, Jonathan  and  Zaidan, Omar},
         title     = {Joshua 2.0: A Toolkit for Parsing-Based Machine Translation with Syntax, Semirings, Discriminative Training and Other Goodies},
         booktitle = {Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR},
         month     = {July},
         year      = {2010},
         address   = {Uppsala, Sweden},
         publisher = {Association for Computational Linguistics},
         pages     = {133--137},
         url       = {http://www.aclweb.org/anthology/W10-1718}
       }
       
-
   title: Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation
   authors: Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, Omar Zaidan
   venue: WMT
   type: workshop
   year: 2010
   url: publications/findings-of-wmt10-and-metrics-matr.pdf
   page_count: 33
   id: findings-of-wmt10-and-metrics-matr
   figures:
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-figure-1.jpg
         label: Figure 1
         caption: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words is based on the provided tokenizer.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-1.jpg
         label: Table 1
         caption: Participants in the shared translation task. Not all groups participated in all language pairs.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-2.jpg
         label: Table 2
         caption: Participants in the system combination task.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-3.jpg
         label: Table 3
         caption: The number of items that were collected for each task during the manual evaluation. An item is defined to be a rank label in the ranking task, an edited sentence in the editing task, and a yes/no judgment in the judgment task.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-4.jpg
         label: Table 4
         caption: Inter- and intra-annotator agreement for the sentence ranking task. In this task, P(E) is 0.333.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-figure-2.jpg
         label: Figure 2
         caption: This screenshot shows what an annotator sees when beginning to edit the output of a machine translation system.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-5.jpg
         label: Table 5
         caption: Official results for the WMT10 translation task, based on the human evaluation (ranking translations relative to each other)
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-6.jpg
         label: Table 6
         caption: Official results for the WMT10 system combination task, based on the human evaluation (ranking translations relative to each other)
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-figure-3.jpg
         label: Figure 3
         caption: The percent of time that each system’s edited output was judged to be an acceptable translation. These numbers also include judgments of the system’s output when it was marked either incomprehen- sible or acceptable and left unedited. Note that the reference translation was edited alongside the system outputs. Error bars show one positive and one negative standard deviation for the systems in that lan- guage pair.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-7.jpg
         label: Table 7
         caption: System-level Spearman’s rho correlation of the automatic evaluation metrics with the human judgments for translation into English, ordered by average absolute value.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-9.jpg
         label: Table 9
         caption: Segment-level Kendall’s tau correlation of the automatic evaluation metrics with the human judgments for translation into English, ordered by average absolute value. Number of pairs included in comparison&colon; cz-en 3575, fr-en 5844, de-en 7585, es-en 7911.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-8.jpg
         label: Table 8
         caption: System-level Spearman’s rho correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average absolute value.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-10.jpg
         label: Table 10
         caption: Segment-level Kendall’s tau correlation of the automatic evaluation metrics with the human judgments for translation out of English, ordered by average absolute value. Number of pairs included in comparison&colon; en-cz 9613, en-fr 5904, en-de 10892, en-es 3813.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-table-11.jpg
         label: Table 11
         caption: Statistics for data collected on MTurk for the ranking task. In total, 55,082 rank labels were collected across the eight language pairs (145% of expert data). Each language pair had 600 sets, and we requested each set completed by 5 different workers. Since each set provides 5 labels, we could have potentially obtained 600 _ 5 _ 5 = 15,000 labels for each language pair. The Label count row indicates to what extent that potential was met (over the 30-day lifetime of our tasks), and the “Completed...” rows give a breakdown of redundancy. For instance, the right-most column indicates that, in the cz-en group, 2.0% of the 600 sets were completed by only one worker, while 67% of the sets were completed by 5 workers, with 100% of the sets completed at least once. The total cost of this data collection effort was roughly $200.
      -
         img: figures/findings-of-wmt10-and-metrics-matr/findings-of-wmt10-and-metrics-matr-figure-4.jpg
         label: Figure 4
         caption: The effect of removing an increasing number of MTurk workers. The order in which workers are removed is by Kexp(w), the kappa agreement coefficient with expert data (excluding references).
   abstract: This paper presents the results of the WMT10 and MetricsMATR10 shared tasks,1 which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics. This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon’s Mechanical Turk. 
   bibtex: |
      @InProceedings{callisonburch-EtAl:2010:WMT,
         author    = {Callison-Burch, Chris  and  Koehn, Philipp  and  Monz, Christof  and  Peterson, Kay  and  Przybocki, Mark  and  Zaidan, Omar},
         title     = {Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation},
         booktitle = {Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR},
         month     = {July},
         year      = {2010},
         address   = {Uppsala, Sweden},
         publisher = {Association for Computational Linguistics},
         pages     = {17--53},
         url       = {http://www.aclweb.org/anthology/W10-1703}
       }
       
-
   title: Large-Scale, Cost-Focused Active Learning for Statistical Machine Translation
   authors: Michael Bloodgood and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2010
   url: publications/cost-focused-active-learning-for-statistical-machine-translation.pdf
   page_count: 11
   id: cost-focused-active-learning-for-statistical-machine-translation
   figures:
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-1.jpg
         label: Figure 1
         caption: Syntax-based and Hierarchical Phrase-Based MT systems’ learning curves on the LDC Urdu-English language pack. The x-axis measures the number of sentence pairs in the training data. The y-axis measures BLEU score. Note the diminishing returns as more data is added. Also note how relatively early on in the process previous studies were terminated. In contrast, the focus of our main experiments doesn’t even be- gin until much higher performance has already been achieved with a period of diminishing returns firmly established.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-2.jpg
         label: Figure 2
         caption: The VG sentence selection algorithm
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-3.jpg
         label: Figure 3
         caption: Random vs VG selection. The x-axis measures the number of sentence pairs in the training data. The y-axis measures BLEU score.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-4.jpg
         label: Figure 4
         caption: Random vs VG selection. The x-axis measures the number of foreign words in the training data. The y-axis measures BLEU score.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-5.jpg
         label: Figure 5
         caption: Random vs Shortest vs Longest selection. The x-axis measures the number of sentence pairs in the training data. The y-axis measures BLEU score.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-6.jpg
         label: Figure 6
         caption: Random vs Shortest vs Longest selection. The x-axis measures the number of foreign words in the training data. The y-axis measures BLEU score.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-7.jpg
         label: Figure 7
         caption: VG vs MostNew vs ModerateNew selection. The x-axis measures the number of sentence pairs in the training data. The y-axis measures BLEU score.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-8.jpg
         label: Figure 8
         caption: Screenshot of the interface we used for soliciting translations for triggers.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-9.jpg
         label: Figure 9
         caption: HNG vs Random collection of new data via MTurk. y-axis measures BLEU. x-axis measures annotation time in seconds.
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-10.jpg
         label: Figure 10
         caption: Distribution of translation speeds (in seconds per word) for HNG postings versus complete sentence postings. The y-axis measures relative frequency. The x-axis measures translation speed in seconds per word (so farther to the left is faster).
      -
         img: figures/cost-focused-active-learning-for-statistical-machine-translation/cost-focused-active-learning-for-statistical-machine-translation-figure-11.jpg
         label: Figure 11
         caption: Bucking the trend&colon; performance of HNG-selected additional data from BBC web crawl data annotated via Amazon Mechanical Turk. y-axis measures BLEU. x-axis measures number of words annotated.
   abstract: We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement. 
   bibtex: |
      @InProceedings{bloodgood-callisonburch:2010:ACL,
         author    = {Bloodgood, Michael  and  Callison-Burch, Chris},
         title     = {Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation},
         booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
         month     = {July},
         year      = {2010},
         address   = {Uppsala, Sweden},
         publisher = {Association for Computational Linguistics},
         pages     = {854--864},
         url       = {http://www.aclweb.org/anthology/P10-1088}
       }
       
-
   title: Creating Speech and Language Data With Amazon’s Mechanical Turk
   authors: Chris Callison-Burch and Mark Dredze
   venue: NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk
   type: workshop
   year: 2010
   url: publications/creating-speech-and-language-data-with-amazon-mechanical-turk.pdf
   page_count: 12
   id: creating-speech-and-language-data-with-amazon-mechanical-turk
   figures:
      -
         img: figures/creating-speech-and-language-data-with-amazon-mechanical-turk/creating-speech-and-language-data-with-amazon-mechanical-turk-figure-1.jpg
         label: Figure 1
         caption: Time spent, HITs completed, and amount earned from a survey of 1,000 Turkers by Ipeirotis (2010).
   abstract: In this paper we give an introduction to using Amazon's Mechanical Turk crowdsourcing platform for the purpose of collecting data for human language technologies. We survey the papers published in the NAACL2010 Workshop. 24 researchers participated in the workshop's $100 challenge to create data for speech and language applications. 
   bibtex: |
      @InProceedings{callisonburch-dredze:2010:MTURK,
         author    = {Callison-Burch, Chris  and  Dredze, Mark},
         title     = {Creating Speech and Language Data With {Amazon's Mechanical Turk}},
         booktitle = {Proceedings of the {NAACL HLT} 2010 Workshop on Creating Speech and Language Data with {Amazon's Mechanical Turk}},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles},
         publisher = {Association for Computational Linguistics},
         pages     = {1--12},
         url       = {http://www.aclweb.org/anthology/W10-0701}
       }
       
-
   title: Using Mechanical Turk to Build Machine Translation Evaluation Sets
   authors: Michael Bloodgood and Chris Callison-Burch
   venue: NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk
   type: workshop
   year: 2010
   url: publications/using-mechanical-turk-to-build-machine-translation-evaluation-sets.pdf
   page_count: 4
   id: using-mechanical-turk-to-build-machine-translation-evaluation-sets
   figures:
      -
         img: figures/using-mechanical-turk-to-build-machine-translation-evaluation-sets/using-mechanical-turk-to-build-machine-translation-evaluation-sets-table-1.jpg
         label: Table 1
         caption: This table shows three MT systems evaluated on five different test sets. For each system-test set pair, two numbers are displayed. The top number is the BLEU score for that system when using that test set. For example, ISI-Syntax tested on the NIST-2009 test set has a BLEU score of 33.10. The bottom number is the percentage of baseline system performance that is achieved. ISI-Syntax (the highest-performing system on NIST2009 to our knowledge) is used as the baseline. Thus, it will always have 100% as the percentage performance for all of the test sets. To illustrate computing the percentage performance for the other systems, consider for JHU-Syntax tested on NIST2009, that its BLEU score of 32.77 divided by the BLEU score of the baseline system is 32.77/33.10 99.00%
      -
         img: figures/using-mechanical-turk-to-build-machine-translation-evaluation-sets/using-mechanical-turk-to-build-machine-translation-evaluation-sets-table-2.jpg
         label: Table 2
         caption: This table shows three MT systems evaluated using the official NIST2009 test set and the two test sets we constructed (MTurk-NoEditing and MTurk-Edited). For each system-test set pair, two numbers are displayed. The top number is the BLEU score for that system when using that test set. For example, ISI-Syntax tested on the NIST-2009 test set has a BLEU score of 33.10. The bottom number is the percentage of baseline system performance that is achieved. ISI-Syntax (the highest-performing system on NIST2009 to our knowledge) is used as the baseline.
   abstract: Building machine translation (MT) test sets is a relatively expensive task. As MT becomes increasingly desired for more and more language pairs and more and more domains, it becomes necessary to build test sets for each case. In this paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MT test sets cheaply. We find that MTurk can be used to make test sets much cheaper than professionally-produced test sets. More importantly, in experiments with multiple MT systems, we find that the MTurk-produced test sets yield essentially the same conclusions regarding system performance as the professionally-produced test sets yield. 
   bibtex: |
      @InProceedings{bloodgood-callisonburch:2010:MTURK,
         author    = {Bloodgood, Michael  and  Callison-Burch, Chris},
         title     = {Using {Mechanical Turk} to Build Machine Translation Evaluation Sets},
         booktitle = {Proceedings of the {NAACL HLT} 2010 Workshop on Creating Speech and Language Data with {Amazon's Mechanical Turk}},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles},
         publisher = {Association for Computational Linguistics},
         pages     = {208--211},
         url       = {http://www.aclweb.org/anthology/W10-0733}
       }
       
-
   title: Crowdsourced Accessibility&colon; Elicitation of Wikipedia Articles
   authors: Scott Novotoney and Chris Callison-Burch
   venue: NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk
   type: workshop
   year: 2010
   url: publications/crowdsourced-accessibility-elicitation-of-wikipedia-articles.pdf
   page_count: 4
   id: crowdsourced-accessibility-elicitation-of-wikipedia-articles
   abstract: Mechanical Turk is useful for generating complex speech resources like conversational speech transcription. In this work, we explore the next step of eliciting narrations of Wikipedia articles to improve accessibility for low-literacy users. This task proves a useful test-bed to implement qualitative vetting of workers based on difficult to define metrics like narrative quality. Working with the Mechanical Turk API, we collected sample narrations, had other Turkers rate these samples and then granted access to full narration HITs depending on aggregate quality. While narrating full articles proved too onerous a task to be viable, using other Turkers to perform vetting was very successful. Elicitation is possible on Mechanical Turk, but it should conform to suggested best practices of simple tasks that can be completed in a streamlined workflow. 
   bibtex: |
      @InProceedings{novotney-callisonburch:2010:MTURK,
         author    = {Novotney, Scott  and  Callison-Burch, Chris},
         title     = {Crowdsourced Accessibility: Elicitation of Wikipedia Articles},
         booktitle = {Proceedings of the {NAACL HLT} 2010 Workshop on Creating Speech and Language Data with {Amazon's Mechanical Turk}},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles},
         publisher = {Association for Computational Linguistics},
         pages     = {41--44},
         url       = {http://www.aclweb.org/anthology/W10-0706}
       }
       
-
   title: Cheap Facts and Counter-Facts
   authors: Rui Wang and Chris Callison-Burch
   venue: NAACL Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk
   type: workshop
   year: 2010
   url: publications/cheap-facts-and-counter-facts.pdf
   page_count: 5
   id: cheap-facts-and-counter-facts
   figures:
      -
         img: figures/cheap-facts-and-counter-facts/cheap-facts-and-counter-facts-table-1.jpg
         label: Table 1
         caption: The statistics of the (valid) data we collect. The Total column presents the number of extracted NEs and generated hypotheses and the Average column shows the average numbers per text respectively.
      -
         img: figures/cheap-facts-and-counter-facts/cheap-facts-and-counter-facts-table-2.jpg
         label: Table 2
         caption: The comparison between the generated (counter-)facts and the original hypotheses from the RTE dataset. The Ave. Length column represents the average number of words in each hypothesis; The Ave. BoW shows the average bag-of-words similarity compared with the text. The three columns on the right are all about the position of the NE appearing in the sentence, how likely it is at the head, middle, or tail of the sentence.
      -
         img: figures/cheap-facts-and-counter-facts/cheap-facts-and-counter-facts-table-4.jpg
         label: Table 4
         caption: Examples of facts and counter-facts, compared with the original texts and hypotheses. We ask the Turkers to write several (counter-)facts about the highlighted NEs, and only part of the results are shown here.
      -
         img: figures/cheap-facts-and-counter-facts/cheap-facts-and-counter-facts-table-3.jpg
         label: Table 3
         caption: The comparison of the generated (counter-)facts with the original hypotheses. The Valid column shows the percentage of the valid (counter-)facts; and other columns present the distribution of harder, easier cases than the original hypotheses or with the same difficulty.
      -
         img: figures/cheap-facts-and-counter-facts/cheap-facts-and-counter-facts-table-5.jpg
         label: Table 5
         caption: The results of baseline RTE systems on the data we collected, compared with the original RTE-5 dataset. The Counter-/Facts column shows the number of T-H pairs; and the other scores in percentage are accuracy of the systems.
   abstract: This paper describes our experiments of using Amazon's Mechanical Turk to generate (counter-)facts from texts for certain named entities. We give the human annotators a paragraph of text and a highlighted named entity. They will write down several (counter-)facts about this named entity in that context. The analysis of the results is performed by comparing the acquired data with the recognizing textual entailment (RTE) challenge dataset. 
   bibtex: |
      @InProceedings{wang-callisonburch:2010:MTURK,
         author    = {Wang, Rui  and  Callison-Burch, Chris},
         title     = {Cheap Facts and Counter-Facts},
         booktitle = {Proceedings of the {NAACL HLT} 2010 Workshop on Creating Speech and Language Data with {Amazon's Mechanical Turk}},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles},
         publisher = {Association for Computational Linguistics},
         pages     = {163--167},
         url       = {http://www.aclweb.org/anthology/W10-0725}
       }
       
-
   title: Stream-based Translation Models for Statistical Machine Translation
   authors: Abby Levenberg, Chris Callison-Burch, and Miles Osborne
   venue: NAACL
   type: conference
   year: 2010
   url: publications/stream-based-translation-models.pdf
   page_count: 9
   id: stream-based-translation-models
   figures:
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-figure-1.jpg
         label: Figure 1
         caption: Streaming coverage conditions. In traditional batch based modeling the coverage of a trained model never changes. Unbounded coverage operates without any memory constraints so the model is able to continually add data from the input stream. Bounded coverage uses just a fixed window.
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-figure-2.jpg
         label: Figure 2
         caption: Recency effects to SMT performance. Depicted are the differences in BLEU scores for multiple test points decoded by a static baseline system and a system batched retrained on a fixed sized window prior to the test point in question. The results are accentuated at the end of the timeline when more time has passed confirming that recent data impacts translation performance.
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-table-1.jpg
         label: Table 1
         caption: Date ranges, total sentence pairs, and source and target word counts encountered in the input stream for example epochs. Epoch 00 is baseline data that is also used as a seed corpus for the online models.
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-table-2.jpg
         label: Table 2
         caption: Test and Hiero grammar rules extracted for the corresponding test set. Translation model statistics for example epochs and the next test dates grouped by experimental condition. Train Sent. is the number of sentence pairs in test and training data respectively. Rules is the count of unique
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-figure-3.jpg
         label: Figure 3
         caption: Static vs. online TM performance. Gains in translation performance measured by BLEU are achieved when recent German-English sentence pairs are automatically incorporated into the TM. Shown are relative BLEU improvements for the online models against the static baseline.
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-table-3.jpg
         label: Table 3
         caption: Sample BLEU results for all baseline and online EM model conditions. The static baseline is a traditional model that is never retrained. The batch unbounded and batch bounded models incorporate new data from the stream but retraining is slow and computationally expensive (best results are bolded). In contrast both unbounded and bounded online models incrementally retrain only the mini-batch of new sentences collected from the incoming stream so quickly adopt the new data (best results are italicized).
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-figure-4.jpg
         label: Figure 4
         caption: Example sentences and improvements to their translation fluency by the adaptation of the TM with recent sentences. In both examples we get longer matching phrases in the online translation compared to the static one.
      -
         img: figures/stream-based-translation-models/stream-based-translation-models-table-4.jpg
         label: Table 4
         caption: Unbounded LM coverage improvements. Shown are the BLEU scores for each experimental conditional when we allow the LM coverage to increase.
   abstract: Typical statistical machine translation systems are trained with static parallel corpora. Here we account for scenarios with a continuous incoming stream of parallel training data. Such scenarios include daily governmental proceedings, sustained output from translation agencies, or crowd-sourced translations. We show incorporating recent sentence pairs from the stream improves performance compared with a static baseline. Since frequent batch retraining is computationally demanding we introduce a fast incremental alternative using an online version of the EM algorithm. To bound our memory requirements we use a novel data-structure and associated training regime. When compared to frequent batch retraining, our online time and space-bounded model achieves the same performance with significantly less computational overhead. 
   bibtex: |
      @InProceedings{levenberg-callisonburch-osborne:2010:NAACLHLT,
         author    = {Levenberg, Abby  and  Callison-Burch, Chris  and  Osborne, Miles},
         title     = {Stream-based Translation Models for Statistical Machine Translation},
         booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles, California},
         publisher = {Association for Computational Linguistics},
         pages     = {394--402},
         url       = {http://www.aclweb.org/anthology/N10-1062}
       }
       
-
   title: Cheap, Fast and Good Enough&colon; Automatic Speech Recognition with Non-Expert Transcription
   authors: Scott Novotney and Chris Callison-Burch
   venue: NAACL
   type: conference
   year: 2010
   url: publications/automatic-speech-recognition-with-non-expert-transcription.pdf
   page_count: 9
   id: automatic-speech-recognition-with-non-expert-transcription
   figures:
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-1.jpg
         label: Figure 1
         caption: Histogram of per-turker transcription rate for twenty hours of English CTS data. Historical estimates for high quality transcription are 50xRT. The 2004 Fisher transcription effort achieved 6xRT and the average here is 11xRT.
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-table-1.jpg
         label: Table 1
         caption: Quality of Non-Professional Transcription on 20 hours of English Switchboard. Even though disagreement for random selection without quality control has 23% disagreement with professional transcription, an ASR system trained on the data is only 2.5% worse than using LDC transcriptions. The upper bound for quality control (row 3) recovers only 50% of the total loss.
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-2.jpg
         label: Figure 2
         caption: WER with a varied amount of LM training data and a fixed 16hr acoustic model. MTurk transcription degrades WER by 0.8% absolute across LM size. When interpolated with 1M words of broadcast news, this degradation shrinks to 0.6%.
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-3.jpg
         label: Figure 3
         caption: Historical cost estimates are $150 per hour of transcription (blue cirlces). The company Casting Words uses Turkers to transcribe English at $90 per hour which we estimated to be high quality (green triangles). Transcription without quality control on Mechanical Turk (red squares) is drastically cheaper at $5 per hour. With a fixed budget, it is better to transcribe more data at lower quality than to improve quality. Contrast the oracle WER for 20 hours transcribed three times (red diamond) with 60 hours transcribed once (bottom red square).
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-4.jpg
         label: Figure 4
         caption: Each Turker was judged against professional and non-professional reference and assigned an overall disagreement. The distribution of Turker disagreement follows a gamma distribution, with a tight cluster of average Turkers and a long-tail of bad Turkers. Estimating with non-professionals (even though the reference is 23% wrong on average) is surprisingly well matched to professional estimate. Turker estimation over-estimated disagreement by only 2%.
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-5.jpg
         label: Figure 5
         caption: Boxplot of the difference of non-professional disagreement with a fixed number of utterances to professional disagreement over all utterances. While error is expectedly high with one utterance, 50% of the estimates are within 3% of the truth after ten utterances and 75% of the estimates are within 6% after fifteen utterances.
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-6.jpg
         label: Figure 6
         caption: Each Turker is a point with professional (X axis) plotted against non-professional (Y axis) disagreement. The non-professional disagreement correlates surprisingly well with professional disagreement even though the transcripts used as reference are 23% wrong on average. By setting a selection threshold, the space is divided into four quadrants. The bottom left are correctly accepted&colon; both non-professional and professional disagreement are below the threshold. The top left are incorrectly rejected&colon; using their transcripts would have helped, but they don’t hurt system performance, just waste money. The top right are correctly rejected for having high disagreement. The bottom right are the troublesome false positives that are included in training but actually may hurt performance. Luckily, the ratio of false negatives to false positives is usually much larger.
      -
         img: figures/automatic-speech-recognition-with-non-expert-transcription/automatic-speech-recognition-with-non-expert-transcription-figure-7.jpg
         label: Figure 7
         caption: It is difficult to find only good Turkers since the false positives outnumber the few good workers. However, rejecting bad Turkers becomes very easy once past the mean error rate of 23%. It is better to use disagreement estimation to reject poor workers instead of finding good workers.
   abstract: Deploying an automatic speech recognition system with reasonable performance requires expensive and time-consuming in-domain transcription. Previous work demonstrated that non-professional annotation through Amazon’s Mechanical Turk can match professional quality. We use Mechanical Turk to transcribe conversational speech for as little as one thirtieth the cost of professional transcription. The higher disagreement of non-professional transcribers does not have a significant effect on system performance. While previous work demonstrated that redundant transcription can improve data quality, we found that resources are better spent collecting more data. Finally, we describe a quality control method without needing professional transcription. 
   bibtex: |
      @InProceedings{novotney-callisonburch:2010:NAACLHLT,
         author    = {Novotney, Scott  and  Callison-Burch, Chris},
         title     = {Cheap, Fast and Good Enough: Automatic Speech Recognition with Non-Expert Transcription},
         booktitle = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
         month     = {June},
         year      = {2010},
         address   = {Los Angeles, California},
         publisher = {Association for Computational Linguistics},
         pages     = {207--215},
         url       = {http://www.aclweb.org/anthology/N10-1024}
       }
       
-
   title: Integrating Output from Specialized Modules in Machine Translation&colon; Transliteration in Joshua
   authors: Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton, and Chris Callison-Burch
   venue: PBML
   type: journal
   year: 2010
   url: publications/integrating-output-from-specialized-modules-in-machine-translation.pdf
   page_count: 10
   id: integrating-output-from-specialized-modules-in-machine-translation
   figures:
      -
         img: figures/integrating-output-from-specialized-modules-in-machine-translation/integrating-output-from-specialized-modules-in-machine-translation-table-1.jpg
         label: Table 1
         caption: Impact of transliteration on BLEU in submissions to NIST MT09 evaluation.
      -
         img: figures/integrating-output-from-specialized-modules-in-machine-translation/integrating-output-from-specialized-modules-in-machine-translation-table-2.jpg
         label: Table 2
         caption: Examples of improvements from transliteration.
      -
         img: figures/integrating-output-from-specialized-modules-in-machine-translation/integrating-output-from-specialized-modules-in-machine-translation-table-3.jpg
         label: Table 3
         caption: Impact of transliteration. Note that the location name “Swabia” was incorrectly transliterated to “Cuba.” This example indicates the future room for improvement.
   abstract: In many cases in SMT we want to allow specialized modules to propose translation fragments to the decoder and allow them to compete with translations contained in the phrase table. Transliteration is one module that may produce such specialized output. In this paper, as an example, we build a specialized Urdu transliteration module and integrate its output into an Urdu–English MT system. The module marks-up the test text using an XML format, and the decoder allows alternate translations (transliterations) to compete. 
   bibtex: |
      @article{Irvine-EtAl:2010:PBML,
       author = {Ann Irvine and Mike Kayser and Zhifei Li and Wren Thornton and Chris Callison-Burch },
       title = {Integrating Output from Specialized Modules in Machine Translation: Transliteration in {J}oshua},
       journal = {The Prague Bulletin of Mathematical Linguistics},
       volume = {93},
       pages = {107--116},
       year = {2010}
       }
       
-
   title: Visualizing Data Structures in Parsing-Based Machine Translation
   authors: Jonathan Weese and Chris Callison-Burch
   venue: PBML
   type: journal
   year: 2010
   url: publications/visualizing-data-structures-in-parsing-based-machine-translation.pdf
   page_count: 10
   id: visualizing-data-structures-in-parsing-based-machine-translation
   figures:
      -
         img: figures/visualizing-data-structures-in-parsing-based-machine-translation/visualizing-data-structures-in-parsing-based-machine-translation-figure-1.jpg
         label: Figure 1
         caption: A hypergraph showing two candidate translations of Je suis mon maître.
      -
         img: figures/visualizing-data-structures-in-parsing-based-machine-translation/visualizing-data-structures-in-parsing-based-machine-translation-figure-2.jpg
         label: Figure 2
         caption: The Derivation Tree browser’s sentence selection and tree-viewing windows.
      -
         img: figures/visualizing-data-structures-in-parsing-based-machine-translation/visualizing-data-structures-in-parsing-based-machine-translation-figure-3.jpg
         label: Figure 3
         caption: An example visualization of two derivation trees for SCFGs that use a Hiero-style grammar and a syntactically-motivated grammar.
      -
         img: figures/visualizing-data-structures-in-parsing-based-machine-translation/visualizing-data-structures-in-parsing-based-machine-translation-figure-4.jpg
         label: Figure 4
         caption: The visualization window for the hypergraph browser.
      -
         img: figures/visualizing-data-structures-in-parsing-based-machine-translation/visualizing-data-structures-in-parsing-based-machine-translation-figure-5.jpg
         label: Figure 5
         caption: An example of bad production rules that parse pieces of the source sentence without producing any target-side output.
   abstract: As machine translation (MT) systems grow more complex and incorporate more linguistic knowledge, it becomes more difficult to evaluate independent pieces of the MT pipeline. Being able to inspect many of the intermediate data structures used during MT decoding allows a more fine-grained evaluation of MT performance, helping to determine which parts of the current process are effective and which are not. In this article, we present an overview of the visualization tools that are currently distributed with the Joshua (Li et al., 2009) MT decoder. We explain their use and present an example of how visually inspecting the decoder’s data structures has led to useful improvements in the MT model. 
   bibtex: |
      @article{Weese-CallisonBurch:2010:PBML,
       author = {Jonathan Weese and Chris Callison-Burch},
       title = {Visualizing Data Structures in Parsing-based Machine Translation},
       journal = {The Prague Bulletin of Mathematical Linguistics},
       volume = {93},
       pages = {127--136},
       year = {2010}
       }
       
-
   title: Hierarchical Phrase-Based Grammar Extraction in Joshua&colon; Suffix Arrays and Prefix Trees
   authors: Lane Schwartz and Chris Callison-Burch
   venue: PBML
   type: journal
   year: 2010
   url: publications/hiero-grammar-extraction-with-suffix-arrays.pdf
   page_count: 10
   id: hiero-grammar-extraction-with-suffix-arrays
   figures:
      -
         img: figures/hiero-grammar-extraction-with-suffix-arrays/hiero-grammar-extraction-with-suffix-arrays-figure-1.jpg
         label: Figure 1
         caption: During suffix array creation, the contents of a corpus array are sorted using the element comparison function Compare_Elements
      -
         img: figures/hiero-grammar-extraction-with-suffix-arrays/hiero-grammar-extraction-with-suffix-arrays-figure-2.jpg
         label: Figure 2
         caption: Query intersection algorithm implemented in Joshua. This algorithm is adapted from a corrected version (Lopez, p.c.) of query intersection (Lopez, 2008).
   abstract: While example-based machine translation has long used corpus information at run-time, statistical phrase-based approaches typically include a preprocessing stage where an aligned parallel corpus is split into phrases, and parameter values are calculated for each phrase using simple relative frequency estimates. This paper describes an open source implementation of the crucial algorithms presented in (Lopez, 2008) which allow direct run-time calculation of SCFG translation rules in Joshua. 
   bibtex: |
      @article{Schwartz-CallisonBurch:2010:PBML,
       author = {Lane Schwartz and Chris Callison-Burch },
       title = {Hierarchical Phrase-Based Grammar Extraction in Joshua: Suffix Arrays and Prefix Tree},
       journal = {The Prague Bulletin of Mathematical Linguistics},
       volume = {93},
       pages = {157--166},
       year = {2010}
       }
       
-
   title: Semantically Informed Machine Translation (SIMT)
   authors: Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf Brown, Chris Callison-Burch, Glen Coppersmith, Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine, Mike Kayser, Lori Levin, Justin Martineau, Jim Mayﬁeld, Scott Miller, Aaron Phillips, Andrew Philpot, Christine Piatko, Lane Schwartz and David Zajic.  SCAL Summer Workshop Final Report
   venue: HLTCOE
   type: report
   year: 2009
   url: publications/scale-2009-report.pdf
   page_count: 152
   id: scale-2009-report
   figures:
      -
         img: figures/scale-2009-report/scale-2009-report-figure-1.jpg
         label: Figure 1
         caption: An example of Urdu-English translation. Shown are an Urdu source document, a reference translation produced by a professional human translator, and machine translation output from a state-of-the-art system before the SIMT SCALE.
      -
         img: figures/scale-2009-report/scale-2009-report-figure-2.jpg
         label: Figure 2
         caption: Translation Errors due to Missing or Incorrect Named Entities
      -
         img: figures/scale-2009-report/scale-2009-report-table-3.jpg
         label: Table 3
         caption: Example clusters derived from the Brown bigram mutual information clustering.
      -
         img: figures/scale-2009-report/scale-2009-report-table-4.jpg
         label: Table 4
         caption: The BBC English and Urdu data.
      -
         img: figures/scale-2009-report/scale-2009-report-table-6.jpg
         label: Table 6
         caption: Coverage of new Urdu phrases in the (relatively small) test set.
      -
         img: figures/scale-2009-report/scale-2009-report-table-7.jpg
         label: Table 7
         caption: Examples of English-to-English transfer rules learned via pivoting.
      -
         img: figures/scale-2009-report/scale-2009-report-figure-10.jpg
         label: Figure 10
         caption: PhysicallocationrelationforIn the West Bank, a passenger was wounded.
   abstract: This report describes the findings of the machine translation team from the first Summer Camp for Applied Language Exploration (SCALE) hosted at the Human Language Technology Center of Excellence located at Johns Hopkins University. This intensive, eight week workshop brought together 20 students, faculty and researchers to conduct research on the topic of Semantically Informed Machine Translation (SIMT). The type of semantics that were examined at the SIMT workshop were "High Information Value Elements," or HIVEs, which include named entities (such as people or organizations) and modalities (indications that a statement represents something that has taken place or is a belief or an intention). These HIVEs were examined in the context of machine translation between Urdu and English. The goal of the workshop was to identify and translate HIVEs from the foreign language, and to investigate whether incorporating this sort of structured semantic information into machine translation (MT) systems could produce better translations. 
   bibtex: |
      @techreport{Baker-EtAl:2010:HLTCOE,
           author = {Kathy Baker and Steven Bethard and Michael Bloodgood and Ralf Brown and Chris Callison-Burch and Glen Coppersmith and Bonnie Dorr and Wes Filardo and Kendall Giles and Anni Irvine and Mike Kayser and Lori Levin and Justin Martineau and Jim Mayﬁeld and Scott Miller and Aaron Phillips and Andrew Philpot and Christine Piatko and Lane Schwartz and David Zajic},
           title = {Semantically Informed Machine Translation},
           address = {Human Language Technology Center of Excellence},
           institution = {Johns Hopkins University, Baltimore, MD},
           number = {002},
           url = {http://web.jhu.edu/bin/u/l/HLTCOE-TechReport-002-SIMT.pdf}, 
           year = {2010}
       }
       
-
   title: Fast, Cheap, and Creative&colon; Evaluating Translation Quality Using Amazon's Mechanical Turk
   authors: Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2009
   url: publications/mechanical-turk-for-machine-translation-evaluation.pdf
   page_count: 10
   id: mechanical-turk-for-machine-translation-evaluation
   figures:
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-figure-1.jpg
         label: Figure 1
         caption: Agreement on ranking translated sentences increases as more non-experts vote. Weighting non-experts' votes based on agreement with either experts or other non-expert increases it up further. Five weighted non-experts reached the top line agreement between experts.
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-figure-2.jpg
         label: Figure 2
         caption: The agreement of individual Turkers with the experts. The most prolific Turker performed barely above chance, indicating random clicking. This suggests that users who contribute more tend to have lower quality.
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-figure-3.jpg
         label: Figure 3
         caption: Correlation with experts’ ranking of systems. All of the different ways of combining the non-expert judgments perform at the upper bound of expert-expert correlation. All correlate more strongly than Bleu.
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-figure-4.jpg
         label: Figure 4
         caption: Bleu scores quantifying the quality of Turkers’ translations. The chart shows the average Bleu score when one LDC translator is compared against the other 10 translators (or the other 2 translators in the case of Urdu). This gives an upper bound on the expected quality. The Turkers’ translation quality falls within a standard deviation of LDC translators for Spanish, German and Chinese. For all languages, Turkers produce significantly better translations than an online machine translation system.
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-table-1.jpg
         label: Table 1
         caption: Self-reported demographic information from Turkers who completed the translation HIT. The statistics on the left are for people who appeared to do the task honestly. The statistics on the right are for people who appeared to be using MT (marked as using it 20% or more in the Detect MT HIT).
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-table-2.jpg
         label: Table 2
         caption: HTER scores for five MT systems. The edit rate decreases as the number of editors in- creases from zero (where HTER is simply the TER score between the MT output and the reference translation) and five.
      -
         img: figures/mechanical-turk-for-machine-translation-evaluation/mechanical-turk-for-machine-translation-evaluation-table-3.jpg
         label: Table 3
         caption: The results of evaluating the MT output using a reading comprehension test
   abstract: Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive. We explore a fast and inexpensive way of doing it using Amazon’s Mechanical Turk to pay small sums to a large number of non-expert annotators. For $10 we redundantly recreate judgments from a WMT08 translation task. We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does. We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations. 
   bibtex: |
      @InProceedings{callisonburch:2009:EMNLP,
         author    = {Callison-Burch, Chris},
         title     = {Fast, Cheap, and Creative: Evaluating Translation Quality Using {Amazon's} {Mechanical Turk}},
         booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
         month     = {August},
         year      = {2009},
         address   = {Singapore},
         publisher = {Association for Computational Linguistics},
         pages     = {286--295},
         url       = {http://www.aclweb.org/anthology/D/D09/D09-1030}
       }
       
-
   title: Feasibility of Human-in-the-loop Minimum Error Rate Training
   authors: Omar Zaidan and Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2009
   url: publications/HMERT.pdf
   page_count: 10
   id: HMERT
   figures:
      -
         img: figures/HMERT/HMERT-figure-1.jpg
         label: Figure 1
         caption: Och’s method applied to a set of two foreign sentences. This figure is essentially a visualization of equation (1). We show here sufficient statistics for TER for simplicity, since there are only 2 of them, but the metric optimized in MERT is usually BLEU.
      -
         img: figures/HMERT/HMERT-figure-2.jpg
         label: Figure 2
         caption: The source parse tree (top) and the candidate derivation tree (bottom). Nodes in the parse tree with a thick border correspond to the frontier node set with maxLen = 4. The human annotator only sees the portion surrounded by the dashed rectangle, including the highlighting (though excluding the word alignment links).
      -
         img: figures/HMERT/HMERT-table-1.jpg
         label: Table 1
         caption: Ranking comparison results. The left half corresponds to the experiment (open to all workers) where the English reference was shown, whereas the right half corresponds to the experiment (open only to workers living in Germany) where the English reference was not shown.
      -
         img: figures/HMERT/HMERT-table-2.jpg
         label: Table 2
         caption: Ranking comparison results, grouped by sentence. This table corresponds to the left half of Table 1. 3 judgments were collected for each comparison, with the “aggregate” for a comparison calculated from these 3 judgments. For instance, an aggregate of “RYPT +3” means all 3 judgments favored RYPT’s choice, and “RYPT +1” means one more judgment favored RYPT than did BLEU.
      -
         img: figures/HMERT/HMERT-figure-3.jpg
         label: Figure 3
         caption: Label percolation under different maxLen values. The bottom two curves are the breakdown of the difference between the middle two. Accuracy is measured against majority votes.
   abstract: Minimum error rate training (MERT) involves choosing parameter values for a machine translation (MT) system that maximize performance on a tuning set as measured by an automatic evaluation metric, such as BLEU. The method is best when the system will eventually be evaluated using the same metric, but in reality, most MT evaluations have a human-based component. Although performing MERT with a human-based metric seems like a daunting task, we describe a new metric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time. In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU. 
   bibtex: |
      @InProceedings{zaidan-callisonburch:2009:EMNLP,
         author    = {Zaidan, Omar F.  and  Callison-Burch, Chris},
         title     = {Feasibility of Human-in-the-loop Minimum Error Rate Training},
         booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
         month     = {August},
         year      = {2009},
         address   = {Singapore},
         publisher = {Association for Computational Linguistics},
         pages     = {52--61},
         url       = {http://www.aclweb.org/anthology/D/D09/D09-1006}
       }
       
-
   title: Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases
   authors: Yuval Marton, Chris Callison-Burch and Philip Resnik
   venue: EMNLP
   type: conference
   year: 2009
   url: publications/improved-translation-with-monolingually-derived-paraphrases.pdf
   page_count: 10
   id: improved-translation-with-monolingually-derived-paraphrases
   figures:
      -
         img: figures/improved-translation-with-monolingually-derived-paraphrases/improved-translation-with-monolingually-derived-paraphrases-table-1.jpg
         label: Table 1
         caption: Training set sizes (million tokens).
      -
         img: figures/improved-translation-with-monolingually-derived-paraphrases/improved-translation-with-monolingually-derived-paraphrases-table-2.jpg
         label: Table 2
         caption: E2C Results&colon; character-based BLEU and TER scores. All models have one additional feature over baseline, except for the "1 + 2-6" models that have one feature for unigrams and another feature for bigrams to 6-grams. Paraphrases with score < &colon;3 were filtered out. *** = significance test over baseline with p < 0&colon;0001, using Koehn’s (2004) pair-wise bootstrap resampling test for BLEU with 95% confidence interval.
      -
         img: figures/improved-translation-with-monolingually-derived-paraphrases/improved-translation-with-monolingually-derived-paraphrases-table-3.jpg
         label: Table 3
         caption: English paraphrases from E2C 29Kbitext systems.
      -
         img: figures/improved-translation-with-monolingually-derived-paraphrases/improved-translation-with-monolingually-derived-paraphrases-table-4.jpg
         label: Table 4
         caption: S2E Results&colon; Lowercase BLEU and TER. Paraphrases with score < minScore were filtered out. *** = significance test over baseline with p < 0&colon;0001, using Koehn’s (2004) pair-wise bootstrap test for BLEU with 95% confidence interval.
      -
         img: figures/improved-translation-with-monolingually-derived-paraphrases/improved-translation-with-monolingually-derived-paraphrases-table-5.jpg
         label: Table 5
         caption: Comparison of Spanish paraphrases&colon; by pivoting, and by two monolingual corpora. Ordered from best to worst score.
      -
         img: figures/improved-translation-with-monolingually-derived-paraphrases/improved-translation-with-monolingually-derived-paraphrases-table-6.jpg
         label: Table 6
         caption: S2E translation examples on 10k-bitext systems. Some translation differences are in bold.
   abstract: Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts. Augmenting the training data with paraphrases generated by pivoting through other languages alleviates this problem, especially for the so-called “low density” languages. But pivoting requires additional parallel texts. We address this problem by deriving paraphrases monolingually, using distributional semantic similarity measures, thus providing access to larger training resources, such as comparable and unrelated monolingual corpora. We present what is to our knowledge the first successful integration of a collocational approach to untranslated words with an end-to-end, state of the art SMT system demonstrating significant translation improvements in a low-resource setting. 
   bibtex: |
      @InProceedings{marton-callisonburch-resnik:2009:EMNLP,
         author    = {Marton, Yuval  and  Callison-Burch, Chris  and  Resnik, Philip},
         title     = {Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases},
         booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
         month     = {August},
         year      = {2009},
         address   = {Singapore},
         publisher = {Association for Computational Linguistics},
         pages     = {381--390},
         url       = {http://www.aclweb.org/anthology/D/D09/D09-1040}
       }
       
-
   title: Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences
   authors: Nikesh Garera, Chris Callison-Burch and David Yarowsky
   venue: CoNLL
   type: conference
   year: 2009
   url: publications/improving-translation-lexicon-induction.pdf
   page_count: 9
   id: improving-translation-lexicon-induction
   figures:
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-figure-1.jpg
         label: Figure 1
         caption: Illustration of (Rapp, 1999) model for translating spanish word “crecimiento (growth)” via dependency context vectors extracted from respective monolingual corpora as explained in Section 3.1.2
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-figure-2.jpg
         label: Figure 2
         caption: Illustration of using dependency trees to model richer contexts for projection
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-table-1.jpg
         label: Table 1
         caption: Contrasting context words derived from the adjacent vs dependency models for the above example.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-table-2.jpg
         label: Table 2
         caption: Top 10 translation candidates for the spanish word “camino (way)” for the best adjacent context model (Adjbow) and best dependency context model (Depposn). The bold English terms show the acceptable translations.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-figure-3.jpg
         label: Figure 3
         caption: Precision/Recall curve showing superior performance of dependency context model as compared to adjacent context at different recall points. Precision is the fraction of tested Spanish words with Top 1 translation correct and Recall is fraction of the 1000 Spanish words tested upon.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-table-3.jpg
         label: Table 3
         caption: Performance of various context-based models learned from monolingual corpora and phrase-table learned from parallel corpora on Noun translation.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-table-4.jpg
         label: Table 4
         caption: List of 20 most confident mappings using the dependency context based model for noun translation. Note that although the first mapping is the correct one, it was not present in the lexicon used for evaluation and hence is marked as incorrect.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-table-5.jpg
         label: Table 5
         caption: Performance of dependency context-based model along with addition of part-of-speech mapping model on translating all word-types.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-figure-4.jpg
         label: Figure 4
         caption: Illustration of using part-of-speech tag mapping to restrict candidate space of translations
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-figure-5.jpg
         label: Figure 5
         caption: Illustration of mapping Spanish part-of-speech tagset to English tagset. The tagsets vary greatly in notation and the morphological/syntactic constituents represented and need to be mapped first, using the algorithm described in Section 6.1.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-figure-6.jpg
         label: Figure 6
         caption: Precision/Recall curve showing superior performance of using part-of-speech equivalences for translating all word-types. Precision is the fraction of tested Spanish words with Top 1 translation correct and Recall is fraction of the 1000 Spanish words tested upon.
      -
         img: figures/improving-translation-lexicon-induction/improving-translation-lexicon-induction-table-6.jpg
         label: Table 6
         caption: List of 20 most confident mappings using the dependency context with the part-of-speech mapping model translating all word-types. Note that although the second best mapping in Table4 for noun-translation is for xenofobia with score 0.87, xenofobia is not among the 1000 most frequent words (of all word-types) and thus is not in this test set.
   abstract: This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses. We introduce a dependency-based context model that incorporates long-range dependencies, variable context sizes, and reordering. It provides a 16% relative improvement over the baseline approach that uses a fixed context window of adjacent words. Its Top 10 accuracy for noun translation is higher than that of a statistical translation model trained on a Spanish-English parallel corpus containing 100,000 sentence pairs. We generalize the evaluation to other word-types, and show that the performance can be increased to 18% relative by preserving part-of-speech equivalencies during translation. 
   bibtex: |
      @InProceedings{garera-callisonburch-yarowsky:2009:CoNLL,
         author    = {Garera, Nikesh  and  Callison-Burch, Chris  and  Yarowsky, David},
         title     = {Improving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences},
         booktitle = {Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)},
         month     = {June},
         year      = {2009},
         address   = {Boulder, Colorado},
         publisher = {Association for Computational Linguistics},
         pages     = {129--137},
         url       = {http://www.aclweb.org/anthology/W09-1117}
       }
       
-
   title: Findings of the 2009 Workshop on Statistical Machine Translation
   authors: Chris Callison-Burch, Philipp Koehn, Christof Monz and  Josh Schroeder
   venue: WMT
   type: workshop
   year: 2009
   url: publications/findings-of-the-wmt09-shared-tasks.pdf
   page_count: 28
   id: findings-of-the-wmt09-shared-tasks
   figures:
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-figure-1.jpg
         label: Figure 1
         caption: Statistics for the training and test sets used in the translation task. The number of words is based on the provided tokenizer and the number of distinct words is the based on lowercased tokens.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-1.jpg
         label: Table 1
         caption: Participants in the shared translation task. Not all groups participated in all language pairs.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-2.jpg
         label: Table 2
         caption: Participants in the system combination task.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-3.jpg
         label: Table 3
         caption: The number of items that were judged for each task during the manual evaluation.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-figure-2.jpg
         label: Figure 2
         caption: This screenshot shows an annotator editing the output of a machine translation system.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-figure-3.jpg
         label: Figure 3
         caption: This screenshot shows an annotator judging the acceptability of edited translations.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-4.jpg
         label: Table 4
         caption: Inter- and intra-annotator agreement for the two types of manual evaluation
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-6.jpg
         label: Table 6
         caption: Official results for the WMT09 translation task, based on the human evaluation (ranking translations relative to each other)
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-5.jpg
         label: Table 5
         caption: A comparison between the best system combinations and the best individual systems. It was generally difficult to draw a statistically significant differences between the two groups, and between the combinations themselves.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-figure-6.jpg
         label: Figure 6
         caption: The percent of time that each system’s edited output was judged to be an acceptable translation. These numbers also include judgments of the system’s output when it was marked either incomprehensible or acceptable and left unedited. Note that the reference translation was edited alongside the system outputs. Error bars show one positive and one negative standard deviation for the systems in that language pair.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-7.jpg
         label: Table 7
         caption: The system-level correlation of the automatic evaluation metrics with the human judgments for translation into English.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-8.jpg
         label: Table 8
         caption: The system-level correlation of the automatic evaluation metrics with the human judgements for translation out of English.
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-9.jpg
         label: Table 9
         caption: The system-level correlation for automatic metrics ranking five English-Czech systems
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-10.jpg
         label: Table 10
         caption: Sentence-level consistency of the automatic metrics with human judgments for translations into English. Italicized numbers do not beat the random-choice baseline. (This table was corrected after publication.)
      -
         img: figures/findings-of-the-wmt09-shared-tasks/findings-of-the-wmt09-shared-tasks-table-11.jpg
         label: Table 11
         caption: Sentence-level consistency of the automatic metrics with human judgments for translations out of English. Italicized numbers do not beat the random-choice baseline. (This table was corrected after publication.)
   abstract: This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task. We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries. We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics. We present a new evaluation technique whereby system output is edited and judged for correctness. 
   bibtex: |
      @InProceedings{callisonburch-EtAl:2009:WMT,
         author    = {Callison-Burch, Chris  and  Koehn, Philipp  and  Monz, Christof  and  Schroeder, Josh},
         title     = {Findings of the 2009 {W}orkshop on {S}tatistical {M}achine {T}ranslation},
         booktitle = {Proceedings of the Fourth Workshop on Statistical Machine Translation},
         month     = {March},
         year      = {2009},
         address   = {Athens, Greece},
         publisher = {Association for Computational Linguistics},
         pages     = {1--28},
         url       = {http://www.aclweb.org/anthology/W09-0401}
       }
       
-
   title: Joshua&colon; An Open Source Toolkit for Parsing-based Machine Translation
   authors: Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese and Omar Zaidan
   venue: WMT
   type: workshop
   year: 2009
   url: publications/joshua-open-source-toolkit-for-statistical-machine-translation.pdf
   page_count: 5
   id: joshua-open-source-toolkit-for-statistical-machine-translation
   figures:
      -
         img: figures/joshua-open-source-toolkit-for-statistical-machine-translation/joshua-open-source-toolkit-for-statistical-machine-translation-table-1.jpg
         label: Table 1
         caption: The uncased BLEU scores on WMT-09 French-English Task. The test set consists of 2525 segments, each with one reference translation.
   abstract: We describe Joshua, an open source toolkit for statistical machine translation. Joshua implements all of the algorithms required for synchronous context free grammars (SCFGs)&colon; chart-parsing, ngram language model integration, beamand cube-pruning, and k-best extraction. The toolkit also implements suffix-array grammar extraction and minimum error rate training. It uses parallel and distributed computing techniques for scalability. We demonstrate that the toolkit achieves state of the art translation performance on the WMT09 French-English translation task. 
   bibtex: |
      @InProceedings{li-EtAl:2009:WMT1,
         author    = {Li, Zhifei  and  Callison-Burch, Chris  and  Dyer, Chris  and  Khudanpur, Sanjeev  and  Schwartz, Lane  and  Thornton, Wren  and  Weese, Jonathan  and  Zaidan, Omar},
         title     = {{Joshua}: An Open Source Toolkit for Parsing-Based Machine Translation},
         booktitle = {Proceedings of the Fourth Workshop on Statistical Machine Translation},
         month     = {March},
         year      = {2009},
         address   = {Athens, Greece},
         publisher = {Association for Computational Linguistics},
         pages     = {135--139},
         url       = {http://www.aclweb.org/anthology/W09-0424}
       }
       
-
   title: Decoding in Joshua&colon; Open Source, Parsing-Based Machine Translation
   authors: Zhifei Li, Chris Callison-Burch,  Sanjeev Khudanpur, and Wren Thornton
   venue: PBML
   type: journal
   year: 2009
   url: publications/decoding-in-joshua.pdf
   page_count: 10
   id: decoding-in-joshua
   figures:
      -
         img: figures/decoding-in-joshua/decoding-in-joshua-table-1.jpg
         label: Table 1
         caption: An example configuration file. For conciseness, this file neglects some standard configuration options (e.g. k-best size).
      -
         img: figures/decoding-in-joshua/decoding-in-joshua-table-2.jpg
         label: Table 2
         caption: Decoder Comparison&colon; Translation speed and quality on the 2003 and 2005 NIST MT benchmark tests.
      -
         img: figures/decoding-in-joshua/decoding-in-joshua-table-3.jpg
         label: Table 3
         caption: Distributed language model&colon; the 7-gram LM cannot be loaded alongside the SCFG on a single machine; via distributed computing, it yields significant improvement in BLEU-4 over a 5-gram.
   abstract: We describe a scalable decoder for parsing-based machine translation. Thee decoder is written in Java and implements all the essential algorithms described in (Chiang, 2007) and (Li and Khudanpur, 2008b)&colon; chart-parsing, n-gram language model integration, beamand cube-pruning, and k-best extraction. Additionally, parallel and distributed computing techniques are exploited to make it scalable. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in Python. 
   bibtex: |
      @article{Li-EtAl:2010:PBML,
          author = {Lane Schwartz and Chris Callison-Burch },
          title = {Hierarchical Phrase-Based Grammar Extraction in Joshua: Suffix Arrays and Prefix Tree},
          journal = {The Prague Bulletin of Mathematical Linguistics},
          volume = {91},
          pages = {47--56},
          year = {2009}
       }
       
-
   title: Syntactic Constraints on Paraphrases Extracted from Parallel Corpora
   authors: Chris Callison-Burch
   venue: EMNLP
   type: conference
   year: 2008
   url: publications/syntactic-constraints-on-paraphrases.pdf
   page_count: 10
   id: syntactic-constraints-on-paraphrases
   figures:
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-figure-1.jpg
         label: Figure 1
         caption: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words is based on the provided tokenizer
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-table-1.jpg
         label: Table 1
         caption: The baseline method’s paraphrases of equal and their probabilities (excluding items with p < .01).
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-table-2.jpg
         label: Table 2
         caption: The baseline’s paraphrases of create equal. Most are clearly bad, and the most probable e2 ̸= e1 is a sub- string of e1 .
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-table-3.jpg
         label: Table 3
         caption: Syntactically constrained paraphrases for equal when it is labeled as an adjective or adjectival phrase.
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-figure-2.jpg
         label: Figure 2
         caption: In addition to extracting phrases that are dominated by a node in the parse tree, we also generate labels for non-syntactic constituents. Three labels are possible for create equal.
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-table-4.jpg
         label: Table 4
         caption: Paraphrases and syntactic labels for the non- constituent phrase create equal.
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-table-5.jpg
         label: Table 5
         caption: Annotators rated paraphrases along two 5-point scales.
      -
         img: figures/syntactic-constraints-on-paraphrases/syntactic-constraints-on-paraphrases-table-6.jpg
         label: Table 6
         caption: The results of the manual evaluation for each of the eight conditions. Correct meaning is the percent of time that a condition was assigned a 3, 4, or 5, and correct grammar is the percent of time that it was given a 4 or 5, using the scales from Table 5.
   abstract: We improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type. This is achieved by parsing the English side of a parallel corpus and altering the phrase extraction algorithm to extract phrase labels alongside bilingual phrase pairs. In order to retain broad coverage of non-constituent phrases, complex syntactic labels are introduced. A manual evaluation indicates a 19% absolute improvement in paraphrase quality over the baseline method. 
   bibtex: |
      @InProceedings{callisonburch:2008:EMNLP,
         author    = {Callison-Burch, Chris},
         title     = {Syntactic Constraints on Paraphrases Extracted from Parallel Corpora},
         booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
         month     = {October},
         year      = {2008},
         address   = {Honolulu, Hawaii},
         publisher = {Association for Computational Linguistics},
         pages     = {196--205},
         url       = {http://www.aclweb.org/anthology/D08-1021}
       }
       
-
   title: ParaMetric&colon; An Automatic Evaluation Metric for Paraphrasing
   authors: Chris Callison-Burch, Trevor Cohn, Mirella Lapata
   venue: CoLing
   type: conference
   year: 2008
   url: publications/parametric.pdf
   page_count: 8
   id: parametric
   figures:
      -
         img: figures/parametric/parametric-figure-1.jpg
         label: Figure 1
         caption: Pairs of English sentences were aligned by hand. Black squares indicate paraphrase corre- spondences.
      -
         img: figures/parametric/parametric-table-1.jpg
         label: Table 1
         caption: Non-identical words and phrases which are identified as being in correspondence by the alignments in Figure 1.
      -
         img: figures/parametric/parametric-figure-2.jpg
         label: Figure 2
         caption: Pang et al. (2003) created word graphs by merging parse trees. Paths with the same start and end nodes are treated as paraphrases.
      -
         img: figures/parametric/parametric-figure-3.jpg
         label: Figure 3
         caption: Bannard and Callison-Burch (2005) extracted paraphrases by equating English phrases that share a common translation.
      -
         img: figures/parametric/parametric-table-2.jpg
         label: Table 2
         caption: Summary results for scoring the different paraphrasing techniques using our proposed automatic evaluations.
      -
         img: figures/parametric/parametric-table-3.jpg
         label: Table 3
         caption: Results for paraphrases of continuous subphrases of various lengths.
   abstract: We present ParaMetric, an automatic evaluation metric for data-driven approaches to paraphrasing. ParaMetric provides an objective measure of quality using a collection of multiple translations whose paraphrases have been manually annotated. ParaMetric calculates precision and recall scores by comparing the paraphrases discovered by automatic paraphrasing techniques against gold standard alignments of words and phrases within equivalent sentences. We report scores for several established paraphrasing techniques. 
   bibtex: |
      @InProceedings{callisonburch-cohn-lapata:2008:Coling,
         author    = {Callison-Burch, Chris  and  Cohn, Trevor  and  Lapata, Mirella},
         title     = {ParaMetric: An Automatic Evaluation Metric for Paraphrasing},
         booktitle = {Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008)},
         month     = {August},
         year      = {2008},
         address   = {Manchester, UK},
         publisher = {Coling 2008 Organizing Committee},
         pages     = {97--104},
         url       = {http://www.aclweb.org/anthology/C08-1013}
       }
       
-
   title: Further Meta-Evaluation of Machine Translation
   authors: Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz and  Josh Schroeder
   venue: WMT
   type: workshop
   year: 2008
   url: publications/further-meta-evaluation-of-machine-tranlsion.pdf
   page_count: 37
   id: further-meta-evaluation-of-machine-tranlsion
   figures:
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-1.jpg
         label: Table 1
         caption: Difficulty of the test set parts based on the original language. For each part, we average BLEU scores from the Edinburgh systems for 12 language pairs of the shared task.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-figure-1.jpg
         label: Figure 1
         caption: Properties of the training and test sets used in the shared task. The training data is drawn from the Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple languages. For Czech and Hungarian we use other available parallel corpora. Note that the number of words is computed based on the provided tokenizer and that the number of distinct words is the based on lowercased tokens.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-2.jpg
         label: Table 2
         caption: Participants in the shared translation task. Not all groups participated in all language pairs.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-figure-2.jpg
         label: Figure 2
         caption: In constituent-based evaluation, the source sentence was parsed, and automatically aligned with the reference translation and systems’ translations
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-3.jpg
         label: Table 3
         caption: The number of items that were judged for each task during the manual evaluation. The All-English judgments were reused in the News task for individual language pairs.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-4.jpg
         label: Table 4
         caption: Summary results for the sentence ranking judgments. The numbers report the percent of time that each system was judged to be greater than or equal to any other system. Bold indicates the highest score for that task.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-5.jpg
         label: Table 5
         caption: Summary results for the constituent ranking judgments. The numbers report the percent of time that each system was judged to be greater than or equal to any other system. Bold indicates the highest score for that task.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-6.jpg
         label: Table 6
         caption: Summary results for the Yes/No judgments for constituent translations judgments. The numbers report the percent of each system’s translations that were judged to be acceptable. Bold indicates the highest score for that task.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-7.jpg
         label: Table 7
         caption: The average number of times that each system was judged to be better than or equal to all other systems in the sentence ranking task for the All-English condition. The subscript indicates the source language of the system.
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-9.jpg
         label: Table 9
         caption: Average system-level correlations for the automatic evaluation metrics on translations into French, German and Spanish (This table was corrected after publication)
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-8.jpg
         label: Table 8
         caption: Average system-level correlations for the automatic evaluation metrics on translations into English (This table was corrected after publication)
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-10.jpg
         label: Table 10
         caption: The percent of time that each automatic metric was consistent with human judgments for translations into English
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-table-11.jpg
         label: Table 11
         caption: The percent of time that each automatic metric was consistent with human judgments for translations into other languages
      -
         img: figures/further-meta-evaluation-of-machine-tranlsion/further-meta-evaluation-of-machine-tranlsion-figure-3.jpg
         label: Figure 3
         caption: Distributions of the amount of time it took to judge single sentences for the three types of man- ual evaluation
   abstract: This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish. We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort. We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level. We validate our manual evaluation methodology by measuring intra- and inter-annotator agreement, and collecting timing information.Note&colon; This paper was corrected subsequent to publication.
   bibtex: |
      @InProceedings{callisonburch-EtAl:2008:WMT,
         author    = {Callison-Burch, Chris  and  Fordyce, Cameron  and  Koehn, Philipp  and  Monz, Christof  and  Schroeder, Josh},
         title     = {Further Meta-Evaluation of Machine Translation},
         booktitle = {Proceedings of the Third Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2008},
         address   = {Columbus, Ohio},
         publisher = {Association for Computational Linguistics},
         pages     = {70--106},
         url       = {http://www.aclweb.org/anthology/W/W08/W08-0309}
       }
       
-
   title: Constructing Corpora for the Development and Evaluation of Paraphrase Systems
   authors: Trevor Cohn, Chris Callison-Burch, Mirella Lapata
   venue: Computational Linguistics
   type: journal
   year: 2008
   url: publications/constructing-corpora-for-paraphrase-systems.pdf
   page_count: 18
   id: constructing-corpora-for-paraphrase-systems
   figures:
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-figure-1.jpg
         label: Figure 1
         caption: Manual alignment between two sentence pairs from the MTC corpus, displayed as a grid.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-figure-2.jpg
         label: Figure 2
         caption: Sample sentence pair showing the word alignments from two annotators.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-table-1.jpg
         label: Table 1
         caption: Single word pairs specified by the word alignments from Figure 2, for two annotators A and B. The column entries specify the alignment type for each annotator, either sure (S) or possible (P). Dashes indicate that the word-pair was not predicted by the annotator. Italics denote lexically identical word pairs.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-figure-3.jpg
         label: Figure 3
         caption: Validity of phrase pairs according to the phrase extraction heuristic. Only the leftmost phrase pair is valid. The others are inconsistent with the alignment or have an unaligned word on a boundary, respectively, indicated by a cross.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-table-2.jpg
         label: Table 2
         caption: Phrase pairs specified by the word alignments from Figure 2, using the possible alignments. The entire set of atomic phrase pairs for either annotator (labeled A or B) are shown and a selection of the remaining 57 composite phrase pairs. The italics denote lexically identical phrase pairs. ∗This phrase pair is atomic in A but composite in B.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-table-3.jpg
         label: Table 3
         caption: Inter-annotator agreement using precision, recall, F1, and Cˆ; the agreement is measured over words in the (monolingual) parallel corpus.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-table-4.jpg
         label: Table 4
         caption: Inter-annotator agreement using precision, recall, F1 and Cˆ; the agreement is measured over atomic phrase pairs in the (monolingual) parallel corpus.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-figure-4.jpg
         label: Figure 4
         caption: Agreement statistics plotted against sentence length for the three sub-corpora. Each group of three columns correspond to πˆ, πˆ0 and Cˆ, respectively. The statistics were measured over non-identical phrase pairs using all phrase pairs&colon; atomic and composite.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-table-5.jpg
         label: Table 5
         caption: Agreement between automatic Giza++ predicted word-alignments and our manually corrected alignments, measured over atomic phrase pairs.
      -
         img: figures/constructing-corpora-for-paraphrase-systems/constructing-corpora-for-paraphrase-systems-figure-5.jpg
         label: Figure 5
         caption: Synchronous grammar rules extracted from the MTC sub-corpus.
   abstract: Automatic paraphrasing is an important component in many natural language processing tasks. In this paper we present a new parallel corpus with paraphrase annotations. We adopt a definition of paraphrase based on word-alignments and show that it yields high inter-annotator agreement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall and F1) and also in developing linguistically rich paraphrase models based on syntactic structure 
   bibtex: |
      @article{cohn-callisonburch-lapata:2008:CL,
         author =  {Trevor Cohn and Chris Callison-Burch and Mirella Lapata},
         title =   {Constructing Corpora for the Development and Evaluation of Paraphrase Systems},
         journal = {Computational Linguistics},
         year =    {2008},
         volume = {34},
         number = {4},
         pages = {597--614}
       }
       
-
   title: Affinity Measures based on the Graph Laplacian
   authors: Delip Rao, David Yarowsky, Chris Callison-Burch
   venue: of the 3rd Textgraphs workshop on Graph-based Algorithms for Natural Language Processing at CoLing
   type: workshop
   year: 2008
   url: publications/graph-laplacian-affinity-measures.pdf
   page_count: 8
   id: graph-laplacian-affinity-measures
   figures:
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-figure-1.jpg
         label: Figure 1
         caption: Shortest path distances on graphs
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-table-1.jpg
         label: Table 1
         caption: Similarity using shortest-path measure
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-table-2.jpg
         label: Table 2
         caption: Similarity using bounded random walks (m = 20).
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-figure-2.jpg
         label: Figure 2
         caption: Effect of m in Bounded walk
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-table-3.jpg
         label: Table 3
         caption: Similarity via pagerank (® = 0.1).
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-table-5.jpg
         label: Table 5
         caption: Denoising graph Laplacian via SVD
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-table-4.jpg
         label: Table 4
         caption: Similarity via inverse Laplacian.
      -
         img: figures/graph-laplacian-affinity-measures/graph-laplacian-affinity-measures-figure-3.jpg
         label: Figure 3
         caption: Noise reduction via SVD.
   abstract: Several language processing tasks can be inherently represented by a weighted graph where the weights are interpreted as a measure of relatedness between two vertices. Measuring similarity between arbitrary pairs of vertices is essential in solving several language processing problems on these datasets. Random walk based measures perform better than other path based measures like shortest-path. We evaluate several random walk measures and propose a new measure based on commute time. We use the psuedo inverse of the Laplacian to derive estimates for commute times in graphs. Further, we show that this pseudo inverse based measure could be improved by discarding the least significant eigenvectors, corresponding to the noise in the graph construction process, using singular value decomposition. 
   bibtex: |
      @InProceedings{rao-yarowsky-callisonburch:2008:TG3,
         author    = {Rao, Delip  and  Yarowsky, David  and  Callison-Burch, Chris},
         title     = {Affinity Measures Based on the Graph {L}aplacian},
         booktitle = {Coling 2008: Proceedings of the 3rd Textgraphs workshop on Graph-based Algorithms for Natural Language Processing},
         month     = {August},
         year      = {2008},
         address   = {Manchester, UK},
         publisher = {Coling 2008 Organizing Committee},
         pages     = {41--48},
         url       = {http://www.aclweb.org/anthology/W08-2006}
       }
       
-
   title: Paraphrasing and Translation
   authors: Chris Callison-Burch
   venue: PhD Thesis, University of Edinburgh
   type: thesis
   year: 2007
   url: publications/callison-burch-thesis.pdf
   page_count: 
   id: callison-burch-thesis
   figures:
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-1.jpg
         label: Figure 1
         caption: The Spanish word cada ́veres can be used to discover that the English phrase dead bodies can be paraphrased as corpses.
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-2.jpg
         label: Figure 2
         caption: Barzilay and McKeown (2001) extracted paraphrases from multiple transla- tions using identical surrounding substrings
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-3.jpg
         label: Figure 3
         caption: A phrase can be aligned to many foreign phrases, which in turn can be aligned to multiple possible paraphrases
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-4.jpg
         label: Figure 4
         caption: In machine translation evaluation the following scales are used by judges to assign adequacy and fluency scores to each translation
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-5.jpg
         label: Figure 5
         caption: Percent of unique unigrams, bigrams, trigrams, and 4-grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-6.jpg
         label: Figure 6
         caption: Scatterplot of the length of each translation against its number of possible permutations due to bigram mismatches for an entry in the 2005 NIST MT Eval
      -
         img: figures/callison-burch-thesis/callison-burch-thesis-figure-7.jpg
         label: Figure 7
         caption: The decoder for the baseline system has translation options only for those words which have phrases that occur in the phrase table. In this case there are no translations for the source word votare ́.
   abstract: Paraphrasing and translation have previously been treated as unconnected natural language processing tasks. Whereas translation represents the preservation of meaning when an idea is rendered in the words in a different language, paraphrasing represents the preservation of meaning when an idea is expressed using different words in the same language. We show that the two are intimately related. The major contributions of this thesis are as follows&colon;We define a novel technique for automatically generating paraphrases using bilingual parallel corpora, which are more commonly used as training data for statistical models of translation.We show that paraphrases can be used to improve the quality of statistical machine translation by addressing the problem of coverage and introducing a degree of generalization into the models.We explore the topic of automatic evaluation of translation quality, and show that the current standard evaluation methodology cannot be guaranteed to correlate with human judgments of translation quality.Whereas previous data-driven approaches to paraphrasing were dependent upon either data sources which were uncommon such as multiple translation of the same source text, or language specific resources such as parsers, our approach is able to harness more widely parallel corpora and can be applied to any language which has a parallel corpus. The technique was evaluated by replacing phrases with their paraphrases, and asking judges whether the meaning of the original phrase was retained and whether the resulting sentence remained grammatical. Paraphrases extracted from a parallel corpus with manual alignments are judged to be accurate (both meaningful and grammatical) 75% of the time, retaining the meaning of the original phrase 85% of the time. Using automatic alignments, meaning can be retained at a rate of 70%.Being a language independent and probabilistic approach allows our method to be easily integrated into statistical machine translation. A paraphrase model derived from parallel corpora other than the one used to train the translation model can be used to increase the coverage of statistical machine translation by adding translations of previously unseen words and phrases. If the translation of a word was not learned, but a translation of a synonymous word has been learned, then the word is paraphrased and its paraphrase is translated. Phrases can be treated similarly. Results show that augmenting a state-of-the-art SMT system with paraphrases in this way leads to significantly improved coverage and translation quality. For a training corpus with 10,000 sentence pairs, we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches.
   bibtex: |
      @PhdThesis{callisonburch:2007:thesis,
         author =  {Chris Callison-Burch},
         title =   {Paraphrasing and Translation},
         school = {University of Edinburgh},
         address =   {Edinburgh, Scotland},
         year =    {2007},
         url = {http://cis.upenn.edu/~ccb/publications/callison-burch-thesis.pdf}
       }
       
-
   title: (Meta-) Evaluation of Machine Translation
   authors: Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz and  Josh Schroeder
   venue: WMT
   type: workshop
   year: 2007
   url: publications/meta-evaluation-of-machine-tranlsion.pdf
   page_count: 
   id: meta-evaluation-of-machine-tranlsion
   figures:
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-figure-1.jpg
         label: Figure 1
         caption: Properties of the training and test sets used in the shared task. The training data is drawn from the Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple languages.
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-1.jpg
         label: Table 1
         caption: Participants in the shared task. Not all groups participated in all translation directions.
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-figure-2.jpg
         label: Figure 2
         caption: In constituent-based evaluation, the source sentence was parsed, and automatically aligned with the reference translation and systems’ translations
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-figure-3.jpg
         label: Figure 3
         caption: For each of the types of evaluation, judges were shown screens containing up to five different system translations, along with the source sentence and reference translation.
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-2.jpg
         label: Table 2
         caption: The number of items that were judged for each task during the manual evaluation
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-3.jpg
         label: Table 3
         caption: The proportion of time that participants’ entries were top-ranked in the human evaluation
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-4.jpg
         label: Table 4
         caption: The proportion of time that participants’ entries were top-ranked by the automatic evaluation metrics
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-5.jpg
         label: Table 5
         caption: Kappa coefficient values representing the inter-annotator agreement for the different types of manual evaluation
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-6.jpg
         label: Table 6
         caption: Kappa coefficient values for intra-annotator agreement for the different types of manual evaluation
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-figure-4.jpg
         label: Figure 4
         caption: Distributions of the amount of time it took to judge single sentences for the three types of manual evaluation
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-7.jpg
         label: Table 7
         caption: Average corrections for the different automatic metrics when they are used to evaluate translations into English
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-8.jpg
         label: Table 8
         caption: Average corrections for the different automatic metrics when they are used to evaluate translations into the other languages
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-9.jpg
         label: Table 9
         caption: Human evaluation for German-English submissions
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-10.jpg
         label: Table 10
         caption: Human evaluation for Spanish-English submissions
      -
         img: figures/meta-evaluation-of-machine-tranlsion/meta-evaluation-of-machine-tranlsion-table-11.jpg
         label: Table 11
         caption: Human evaluation for French-English submissions
   abstract: This paper evaluates the translation quality of machine translation systems for 8 language pairs&colon; translating French, German, Spanish, and Czech to English and back. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured timing and intra- and inter-annotator agreement for three types of subjective evaluation. We measured the correlation of automatic evaluation metrics with human judgments. This meta-evaluation reveals surprising facts about the most commonly used methodologies. 
   bibtex: |
      @InProceedings{callisonburch-EtAl:2007:WMT,
         author    = {Callison-Burch, Chris  and  Fordyce, Cameron  and  Koehn, Philipp  and  Monz, Christof  and  Schroeder, Josh},
         title     = {(Meta-) Evaluation of Machine Translation},
         booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2007},
         address   = {Prague, Czech Republic},
         publisher = {Association for Computational Linguistics},
         pages     = {136--158},
         url       = {http://www.aclweb.org/anthology/W/W07/W07-0718}
       }
       
-
   title: Open Source Toolkit for Statistical Machine Translation&colon; Factored Translation Models and Confusion Network Decoding
   authors: Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, Chris Callison-Burch, Alexandra Constantin, Brooke Cowan, Chris Dyer, Marcello Federico, Evan Herbst, Hieu Hoang, Christine Moran, Wade Shen, and Richard Zens
   venue: CLSP Summer Workshop Final Report WS, Johns Hopkins University
   type: workshop
   year: 2007
   url: publications/open-source-toolkit-for-statistical-machine-translation.pdf
   page_count: 
   id: open-source-toolkit-for-statistical-machine-translation
   abstract: The 2006 Language Engineering Workshop Open Source Toolkit for Statistical Machine Translationhad the objective to advance the current state-of-the-art in statistical machine translation through richer input and richer annotation of the training data. The workshop focused on three topics&colon; factored translation models, confusion network decoding, and the development of an open source toolkit that incorporates this advancements. This report describes the scientific goals, the novel methods, and experimental results of the workshop. It also documents details of the implementation of the open source toolkit. 
   bibtex: |
      @techreport{Koehn-EtAl:2007:CLSP,
          author = { Philipp Koehn and Nicola Bertoldi and Ondrej Bojar and Chris Callison-Burch and Alexandra Constantin and  Brooke Cowan and Chris Dyer and Marcello Federico and Evan Herbst and Hieu Hoang and Christine Moran and Wade Shen and Richard Zens},
          title = {Open Source Toolkit for Statistical Machine Translation: Factored Translation Models and Confusion Network Decoding. },
          institution = {Johns Hopkins University},
          number = {WS-2006},
          type = {CLSP Summer Workshop Final Report},
          year = {2007}
       }
       
-
   title: Moses&colon; Open source toolkit for statistical machine translation
   authors: Philipp Koehn, Hieu Hoang,  Alexandra Birch,  Chris Callison-burch,    Marcello Federico,  Nicola Bertoldi,   Brooke Cowan,  Wade Shen,  Christine Moran,  Richard Zens, Chris Dyer, Ondřej Bojar,  Alexandra Constantin,  and Evan Herbst
   venue: ACL
   type: conference
   year: 2007
   url: publications/moses-toolkit.pdf
   page_count: 
   id: moses-toolkit
   figures:
      -
         img: figures/moses-toolkit/moses-toolkit-figure-1.jpg
         label: Figure 1
         caption: Non-factored translation
      -
         img: figures/moses-toolkit/moses-toolkit-figure-2.jpg
         label: Figure 2
         caption: Factored translation
      -
         img: figures/moses-toolkit/moses-toolkit-figure-3.jpg
         label: Figure 3
         caption: Example of graph of decoding steps
   abstract: We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. 
   bibtex: |
      @InProceedings{koehn-EtAl:2007:PosterDemo,
         author    = {Koehn, Philipp  and  Hoang, Hieu  and  Birch, Alexandra  and  Callison-Burch, Chris  and  Federico, Marcello  and  Bertoldi, Nicola  and  Cowan, Brooke  and  Shen, Wade  and  Moran, Christine  and  Zens, Richard  and  Dyer, Chris  and  Bojar, Ondrej  and  Constantin, Alexandra  and  Herbst, Evan},
         title     = {Moses: Open Source Toolkit for Statistical Machine Translation},
         booktitle = {Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions},
         month     = {June},
         year      = {2007},
         address   = {Prague, Czech Republic},
         publisher = {Association for Computational Linguistics},
         pages     = {177--180},
         url       = {http://www.aclweb.org/anthology/P07-2045}
       }
       
-
   title: Paraphrase Substitution for Recognizing Textual Entailment
   authors: Wauter Bosma and Chris Callison-Burch
   venue: Evaluation of Multilingual and Multimodalformation Retrieval, Lecture Notes in Computer Science, C Peters et al editors
   type: chapter
   year: 2007
   url: publications/paraphrase-substitution-for-recognizing-textual-entailment.pdf
   page_count: 
   id: paraphrase-substitution-for-recognizing-textual-entailment
   figures:
      -
         img: figures/paraphrase-substitution-for-recognizing-textual-entailment/paraphrase-substitution-for-recognizing-textual-entailment-table-1.jpg
         label: Table 1
         caption: Examples paraphrases and probabilities for the phrase dead bodies
      -
         img: figures/paraphrase-substitution-for-recognizing-textual-entailment/paraphrase-substitution-for-recognizing-textual-entailment-figure-1.jpg
         label: Figure 1
         caption: A bilingual parallel corpus can be used to extract paraphrases
   abstract: We describe a method for recognizing textual entailment that uses the length of the longest common subsequence (LCS) between two texts as its decision criterion. Rather than requiring strict word matching in the common subsequences, we perform a flexible match using automatically generated paraphrases. We find that the use of paraphrases over strict word matches represents an average F-measure improvement from 0.22 to 0.36 on the CLEF 2006 Answer Validation Exercise for 7 languages. 
   bibtex: |
      @InProceedings{bosma-callisonburch:2006:CLEF,
         author = {Wauter Bosma and Chris Callison-Burch},
         title = {Paraphrase Substitution for Recognizing Textual Entailment},
         booktitle = {Proceedings of CLEF},
         year = {2006}
         url = {http://cis.upenn.edu/~ccb/publications/paraphrase-substitution-for-recognizing-textual-entailment.pdf}
       }
       
-
   title: Improved Statistical Machine Translation Using Paraphrases
   authors: Chris Callison-Burch, Philipp Koehn and Miles Osborne
   venue: NAACL
   type: conference
   year: 2006
   url: publications/improved-statistical-machine-translation-using-paraphrases.pdf
   page_count: 
   id: improved-statistical-machine-translation-using-paraphrases
   figures:
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-figure-1.jpg
         label: Figure 1
         caption: Percent of unique unigrams, bigrams, trigrams, and 4-grams from the Europarl Spanish test sentences for which translations were learned in increasingly large training corpora
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-table-1.jpg
         label: Table 1
         caption: Example of automatically generated para- phrases for the Spanish words encargarnos and us- ado along with their English translations which were automatically learned from the Europarl corpus
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-figure-2.jpg
         label: Figure 2
         caption: Using a bilingual parallel corpus to extract paraphrases
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-figure-4.jpg
         label: Figure 4
         caption: Judges were asked whether the highlighted phrase retained the same meaning as the highlighted phrase in the reference translation (top)
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-figure-3.jpg
         label: Figure 3
         caption: Test sentences and reference translations were manually word-aligned. This allowed us to equate unseen phrases with their corresponding English phrase. In this case enumeradas with listed.
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-table-2.jpg
         label: Table 2
         caption: Bleu scores for the various training corpora, including baseline results without paraphrasing, results for only paraphrasing unknown words, and results for paraphrasing any unseen phrase. Corpus size is measured in sentences.
      -
         img: figures/improved-statistical-machine-translation-using-paraphrases/improved-statistical-machine-translation-using-paraphrases-table-3.jpg
         label: Table 3
         caption: Bleu scores for the various training corpora, when the paraphrase feature function is not included
   abstract: Parallel corpora are crucial for training SMT systems. However, for many language pairs they are available only in very limited quantities. For these language pairs a huge portion of phrases encountered at run-time will be unknown. We show how techniques from paraphrasing can be used to deal with these otherwise unknown source language phrases. Our results show that augmenting a stateof-the-art SMT system with paraphrases leads to significantly improved coverage and translation quality. For a training corpus with 10,000 sentence pairs we increase the coverage of unique test set unigrams from 48% to 90%, with more than half of the newly covered items accurately translated, as opposed to none in current approaches. 
   bibtex: |
      @InProceedings{callisonburch-koehn-osborne:2006:HLT-NAACL06-Main,
         author    = {Callison-Burch, Chris  and  Koehn, Philipp  and  Osborne, Miles},
         title     = {Improved Statistical Machine Translation Using Paraphrases},
         booktitle = {Proceedings of the Human Language Technology Conference of the NAACL, Main Conference},
         month     = {June},
         year      = {2006},
         address   = {New York City, USA},
         publisher = {Association for Computational Linguistics},
         pages     = {17--24},
         url       = {http://www.aclweb.org/anthology/N/N06/N06-1003}
       }
       
-
   title: Re-evaluating the Role of Bleu in Machine Translation Research
   authors: Chris Callison-Burch, Miles Osborne and Philipp Koehn
   venue: EACL
   type: conference
   year: 2006
   url: publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf
   page_count: 8
   id: re-evaluating-the-role-of-bleu-in-mt-research
   abstract: We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores. 
   bibtex: |
      @InProceedings{callisonburch-koehn-osborne:2006:HLT-NAACL06-Main,
         author    = {Callison-Burch, Chris  and  Osborne, Miles and  Koehn, Philipp},
         title     = {Re-evaluating the Role of BLEU in Machine Translation Research},
         booktitle = {11th Conference of the European Chapter of the Association for Computational Linguistics},
         month     = {April},
         year      = {2006},
         address   = {Trento, Italy},
         publisher = {Association for Computational Linguistics},
         pages     = {249--256},
         url       = {http://aclweb.org/anthology-new/E/E06/E06-1032}
       }
       
-
   title: Constraining the Phrase-Based, Joint Probability Statistical Translation Model
   authors: Alexandra Birch, Chris Callison-Burch and Miles Osborne
   venue: WMT
   type: workshop
   year: 2006
   url: publications/constraining-the-phrase-based-joint-probability-model.pdf
   page_count: 
   id: constraining-the-phrase-based-joint-probability-model
   figures:
      -
         img: figures/constraining-the-phrase-based-joint-probability-model/constraining-the-phrase-based-joint-probability-model-table-1.jpg
         label: Table 1
         caption: The number of possible phrasal alignments for sentence pairs calculated using Stirling numbers of the second kind.
      -
         img: figures/constraining-the-phrase-based-joint-probability-model/constraining-the-phrase-based-joint-probability-model-table-2.jpg
         label: Table 2
         caption: Bleu scores for the Joint Model with IBM constraints and prior counts, corpus size indicates number of sentence pairs
      -
         img: figures/constraining-the-phrase-based-joint-probability-model/constraining-the-phrase-based-joint-probability-model-table-3.jpg
         label: Table 3
         caption: Translation table size in millions of phrase pairs
   abstract: The Joint Probability Model proposed by Marcu and Wong (2002) provides a probabilistic framework for modeling phrase-based statistical machine translation (SMT). The model’s usefulness is, however, limited by the computational complexity of estimating parameters at the phrase level. We present a method of constraining the search space of the Joint Probability Model based on statistically and linguistically motivated word alignments. This method reduces the complexity and size of the Joint Model and allows it to display performance superior to the standard phrase-based models for small amounts of training material. 
   bibtex: |
      @InProceedings{birch-EtAl:2006:WMT,
         author    = {Birch, Alexandra  and  Callison-Burch, Chris  and  Osborne, Miles  and  Koehn, Philipp},
         title     = {Constraining the Phrase-Based, Joint Probability Statistical Translation Model},
         booktitle = {Proceedings on the Workshop on Statistical Machine Translation},
         month     = {June},
         year      = {2006},
         address   = {New York City},
         publisher = {Association for Computational Linguistics},
         pages     = {154--157},
         url       = {http://www.aclweb.org/anthology/W/W06/W06-3123}
       }
       
-
   title: Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases
   authors: Chris Callison-Burch, Colin Bannard and Josh Schroeder
   venue: ACL
   type: conference
   year: 2005
   url: publications/scaling-phrase-based-statistical-machine-translation.pdf
   page_count: 
   id: scaling-phrase-based-statistical-machine-translation
   figures:
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-table-1.jpg
         label: Table 1
         caption: Statistics about Arabic phrases in the NIST-2004 large data track.
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-table-2.jpg
         label: Table 2
         caption: Estimated size of lookup tables for the NIST-2004 Arabic-English data
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-table-3.jpg
         label: Table 3
         caption: Lengths of phrases from the training data that occur in the NIST-2004 test set
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-table-4.jpg
         label: Table 4
         caption: Coverage using only repeated phrases of the specified length
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-figure-2.jpg
         label: Figure 2
         caption: A sorted suffix array and its corresponding suffixes
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-figure-1.jpg
         label: Figure 1
         caption: An initialized, unsorted suffix array for a very small corpus
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-table-5.jpg
         label: Table 5
         caption: Examples of O and calculation times for phrases of different frequencies
      -
         img: figures/scaling-phrase-based-statistical-machine-translation/scaling-phrase-based-statistical-machine-translation-table-6.jpg
         label: Table 6
         caption: A comparison of retrieval times and translation quality when the number of translations is capped at various sample sizes
   abstract: In this paper we describe a novel data structure for phrase-based statistical machine translation which allows for the retrieval of arbitrarily long phrases while simultaneously using less memory than is required by current decoder implementations. We detail the computational complexity and average retrieval times for looking up phrase translations in our suffix array-based data structure. We show how sampling can be used to reduce the retrieval time by orders of magnitude with no loss in translation quality. 
   bibtex: |
      @InProceedings{callisonburch-bannard-schroeder:2005:ACL,
         author    = {Callison-Burch, Chris  and  Bannard, Colin  and  Schroeder, Josh},
         title     = {Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases},
         booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)},
         month     = {June},
         year      = {2005},
         address   = {Ann Arbor, Michigan},
         publisher = {Association for Computational Linguistics},
         pages     = {255--262},
         url       = {http://www.aclweb.org/anthology/P05-1032},
       }
       
-
   title: Paraphrasing with Bilingual Parallel Corpora
   authors: Colin Bannard and Chris Callison-Burch
   venue: ACL
   type: conference
   year: 2005
   url: publications/paraphrasing-with-bilingual-parallel-corpora.pdf
   page_count: 
   id: paraphrasing-with-bilingual-parallel-corpora
   figures:
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-figure-1.jpg
         label: Figure 1
         caption: Using a monolingual parallel corpus to extract paraphrases
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-figure-2.jpg
         label: Figure 2
         caption: Using a bilingual parallel corpus to extract paraphrases
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-table-1.jpg
         label: Table 1
         caption: Phrases that were selected to paraphrase
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-figure-3.jpg
         label: Figure 3
         caption: Phrases highlighted for manual alignment
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-figure-4.jpg
         label: Figure 4
         caption: Paraphrases substituted in for the original phrase
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-table-2.jpg
         label: Table 2
         caption: Paraphrases extracted from a manually word-aligned parallel corpus
      -
         img: figures/paraphrasing-with-bilingual-parallel-corpora/paraphrasing-with-bilingual-parallel-corpora-table-3.jpg
         label: Table 3
         caption: Paraphrase accuracy and correct meaning for the different data conditions
   abstract: Previous work has used monolingual parallel corpora to extract and generate paraphrases. We show that this task can be done using bilingual parallel corpora, a much more commonly available resource. Using alignment techniques from phrase-based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments. 
   bibtex: |
      @InProceedings{bannard-callisonburch:2005:ACL,
         author    = {Bannard, Colin  and  Callison-Burch, Chris},
         title     = {Paraphrasing with Bilingual Parallel Corpora},
         booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)},
         month     = {June},
         year      = {2005},
         address   = {Ann Arbor, Michigan},
         publisher = {Association for Computational Linguistics},
         pages     = {597--604},
         url       = {http://www.aclweb.org/anthology/P05-1074},
       }
       
-
   title: A Compact Data Structure for Searchable Translation Memories
   authors: Chris Callison-Burch, Colin Bannard and Josh Schroeder
   venue: EAMT
   type: workshop
   year: 2005
   url: publications/compact-data-structure-for-searchable-translation-memories.pdf
   page_count: 
   id: compact-data-structure-for-searchable-translation-memories
   figures:
      -
         img: figures/compact-data-structure-for-searchable-translation-memories/compact-data-structure-for-searchable-translation-memories-figure-1.jpg
         label: Figure 1
         caption: Search results for the English phrase “west bank”
      -
         img: figures/compact-data-structure-for-searchable-translation-memories/compact-data-structure-for-searchable-translation-memories-figure-2.jpg
         label: Figure 2
         caption: A word-level alignment for a sentence pair that occurs in our training data
      -
         img: figures/compact-data-structure-for-searchable-translation-memories/compact-data-structure-for-searchable-translation-memories-figure-3.jpg
         label: Figure 3
         caption: An initialized, unsorted suffix array for a very small corpus
      -
         img: figures/compact-data-structure-for-searchable-translation-memories/compact-data-structure-for-searchable-translation-memories-figure-4.jpg
         label: Figure 4
         caption: A sorted suffix array and its corresponding suffixes
      -
         img: figures/compact-data-structure-for-searchable-translation-memories/compact-data-structure-for-searchable-translation-memories-figure-5.jpg
         label: Figure 5
         caption: A word-level alignment for the sentence in the suffix array
   abstract: In this paper we describe searchable translation memories, which allow translators to search their archives for possible translations of phrases. We describe how statistical machine translation can be used to align sub-sentential units in a translation memory, and rank them by their probability. We detail a data structure that allows for memory-efficient storage of the index. We evaluate the accuracy of translations retrieved from a searchable translation memory built from 50,000 sentence pairs, and find a precision of 86.6% for the top ranked translations. 
   bibtex: |
      @InProceedings{callison-burch-EtAl:2005:EAMT,
         author =  {Chris Callison-Burch and Colin Bannard and Josh Schroeder},
         title =           {A Compact Data Structure for Searchable Translation Memories},
           booktitle = {European Association for Machine Translation}, 
       year =  {2005}
       }
       
-
   title: Linear B System Description for the 2005 NIST MT Evaluation Exercise
   authors: Chris Callison-Burch
   venue: Machine Translation Evaluation Workshop
   type: workshop
   year: 2005
   url: publications/linear-b-system-description-for-nist-mt-eval-2005.pdf
   page_count: 
   id: linear-b-system-description-for-nist-mt-eval-2005
   figures:
      -
         img: figures/linear-b-system-description-for-nist-mt-eval-2005/linear-b-system-description-for-nist-mt-eval-2005-figure-1.jpg
         label: Figure 1
         caption: In the simple editing condition subjects simply edited the output of a fully-automatic statistical machine translation system
      -
         img: figures/linear-b-system-description-for-nist-mt-eval-2005/linear-b-system-description-for-nist-mt-eval-2005-figure-2.jpg
         label: Figure 2
         caption: In the visualization condition subjects first constructed the translations by selecting from a set of probable translations of the phrases in each Arabic sentence
      -
         img: figures/linear-b-system-description-for-nist-mt-eval-2005/linear-b-system-description-for-nist-mt-eval-2005-figure-3.jpg
         label: Figure 3
         caption: A comparison of Linear B’s human-aided and Edinburgh University’s fully automatic translation for article AFP20041201.0189
      -
         img: figures/linear-b-system-description-for-nist-mt-eval-2005/linear-b-system-description-for-nist-mt-eval-2005-figure-4.jpg
         label: Figure 4
         caption: A comparison for article XIA20050101.0119
   abstract: This document describes Linear B’s entry for the 2005 NIST MT Evaluation exercise. Linear B examined the efficacy of human-aided statistical machine translation by looking at the improvements that could be had by involving non-Arabic speakers in the translation process. We examined two conditions&colon; one in which non-Arabic speakers edited the output of a statistical machine translation system, and one in which they were allowed to select phrasal translations from a chart of possible translations for an Arabic sentence, and then edit the text. 
   bibtex: |
      @InProceedings{callisonburch:2005:NIST,
         author =  {Chris Callison-Burch },
         title =           {A Compact Data Structure for Searchable Translation Memories},
           booktitle = {Proceedings of Machine Translation Evaluation Workshop}, 
       year =  {2005}
       }
       
-
   title: Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation
   authors: Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot
   venue: IWSLT
   type: workshop
   year: 2005
   url: publications/iwslt05-report.pdf
   page_count: 
   id: iwslt05-report
   figures:
      -
         img: figures/iwslt05-report/iwslt05-report-figure-1.jpg
         label: Figure 1
         caption: Phrase-based SMT&colon; Input is segmented into phrases, each is mapped into output phrase and may be reordered
      -
         img: figures/iwslt05-report/iwslt05-report-figure-2.jpg
         label: Figure 2
         caption: Obtaining a high precision, low recall word alignment by intersecting two GIZA++ alignments
      -
         img: figures/iwslt05-report/iwslt05-report-figure-3.jpg
         label: Figure 3
         caption: Adding additional alignment points. Potential points are points in the union of the two GIZA++ alignments (grey). In the growing step, neighbouring points are added, when they connect at least one unaligned word. In a final step outlying points may be added (see Section 2.3).
      -
         img: figures/iwslt05-report/iwslt05-report-figure-5.jpg
         label: Figure 5
         caption: Definition of consistent word alignments&colon; Words of an extracted phrase pair have to be aligned to each other and nothing else
      -
         img: figures/iwslt05-report/iwslt05-report-figure-4.jpg
         label: Figure 4
         caption: Figure 4&colon; Pseudo-code of the grow-diag-final method to symmetrise word alignments. See Section 2.3 for variations of this method.
      -
         img: figures/iwslt05-report/iwslt05-report-figure-6.jpg
         label: Figure 6
         caption: Possible orientations of phrases&colon; monotone (m), swap (s), or discontinuous (d)
      -
         img: figures/iwslt05-report/iwslt05-report-table-1.jpg
         label: Table 1
         caption: Different word alignment methods and the effect of the phrase table&colon; Since alignment points restrict possible phrase pairs, fewer alignment points lead to larger phrase tables.
      -
         img: figures/iwslt05-report/iwslt05-report-table-2.jpg
         label: Table 2
         caption: BLEU scores for systems trained using different alignment methods
      -
         img: figures/iwslt05-report/iwslt05-report-table-3.jpg
         label: Table 3
         caption: Best lexicalised reordering methods, compared against the baseline (using only distance-based reordering penalty)&colon; Improvements for all language pairs
      -
         img: figures/iwslt05-report/iwslt05-report-table-4.jpg
         label: Table 4
         caption: Optimising the reordering limit (maximum word distance for phrase movement). The table also shows the effect of dropping unknown words instead of passing them to the output.
      -
         img: figures/iwslt05-report/iwslt05-report-table-5.jpg
         label: Table 5
         caption: Official Results&colon; The scores for our official submission to the IWSLT’05 Evaluation Campaign (length penalty in parenthesis), and rank among participants according to the BLEU score.
      -
         img: figures/iwslt05-report/iwslt05-report-table-6.jpg
         label: Table 6
         caption: Optimisation to average reference sentence length instead of shortest reference length (length penalty in parenthesis)&colon; Note the improved length penalties and vastly improved NIST scores. 4 out of 5 BLEU scores are higher as well (exception is Chinese-English).
   abstract: Our participation in the IWSLT 2005 speech translation task is our first effort to work on limited domain speech data. We adapted our statistical machine translation system that performed successfully in previous DARPA competitions on open domain text translations. We participated in the supplied corpora transcription track. We achieved the highest BLEU score in 2 out of 5 language pairs and had competitive results for the other language pairs. 
   bibtex: |
      @InProceedings{Koehn-EtAl:2005:IWSLT,
         author =  {Philipp Koehn and Amittai Axelrod and Alexandra Birch and Chris Callison-Burch and Miles Osborne and David Talbot and Michael White},
         title =           {Edinburgh System Description for the 2005 {IWSLT} Speech Translation Evaluation},
           booktitle = {Proceedings of International Workshop on Spoken Language Translation},
       year =  {2005},
         url = {http://cis.upenn.edu/~ccb/publications/iwslt05-report.pdf}
       }
       
-
   title: Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora
   authors: Chris Callison-Burch, David Talbot and Miles Osborne
   venue: ACL
   type: conference
   year: 2004
   url: publications/smt-with-word-and-sentence-aligned-parallel-corpora.pdf
   page_count: 
   id: smt-with-word-and-sentence-aligned-parallel-corpora
   figures:
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-table-1.jpg
         label: Table 1
         caption: Alignment error rates for the various IBM Models trained with sentence-aligned data
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-table-2.jpg
         label: Table 2
         caption: Alignment error rates for the various IBM Models trained with word-aligned data
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-figure-1.jpg
         label: Figure 1
         caption: Example alignments using sentence-aligned training data (a), using word-aligned data (b), and a reference manual alignment (c)
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-table-3.jpg
         label: Table 3
         caption: The improved alignment error rates when using a dictionary instead of word-aligned data to constrain word translations
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-table-4.jpg
         label: Table 4
         caption: Improved AER leads to improved translation quality
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-table-5.jpg
         label: Table 5
         caption: The effect of weighting word-aligned data more heavily that its proportion in the training data (corpus size 16000 sentence pairs)
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-figure-2.jpg
         label: Figure 2
         caption: The effect on AER of varying λ for a train- ing corpus of 16K sentence pairs with various pro- portions of word-alignments
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-figure-3.jpg
         label: Figure 3
         caption: The effect on AER of varying the ratio of word-aligned to sentence-aligned data
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-figure-4.jpg
         label: Figure 4
         caption: The effect on Bleu of varying the ratio of word-aligned to sentence-aligned data
      -
         img: figures/smt-with-word-and-sentence-aligned-parallel-corpora/smt-with-word-and-sentence-aligned-parallel-corpora-table-6.jpg
         label: Table 6
         caption: Summary results for AER and translation quality experiments on Hansards data
   abstract: The parameters of statistical translation models are typically estimated from sentence-aligned parallel corpora. We show that significant improvements in the alignment and translation quality of such models can be achieved by additionally including word-aligned data during training. Incorporating word-level alignments into the parameter estimation of the IBM models reduces alignment error rate and increases the Bleu score when compared to training the same models only on sentence-aligned data. On the Verbmobil data set, we attain a 38% reduction in the alignment error rate and a higher Bleu score with half as many training examples. We discuss how varying the ratio of word-aligned to sentence-aligned data affects the expected performance gain. 
   bibtex: |
      @inproceedings{callisonburch-talbot-osborne:2004:ACL,
         author    = {Callison-Burch, Chris  and  Talbot, David  and  Osborne, Miles},
         title     = {Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora},
         booktitle = {Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL'04), Main Volume},
         year      = {2004},
         month     = {July},
         address   = {Barcelona, Spain},
         pages     = {175--182},
         url       = {http://www.aclweb.org/anthology/P04-1023},
       }
       
-
   title: Searchable Translation Memories
   authors: Chris Callison-Burch, Colin Bannard and Josh Schroeder
   venue: ASLIB Translating and the Computer
   type: workshop
   year: 2004
   url: publications/searchable-translation-memories.pdf
   page_count: 
   id: searchable-translation-memories
   figures:
      -
         img: figures/searchable-translation-memories/searchable-translation-memories-figure-1.jpg
         label: Figure 1
         caption: Search results for the French phrase “paiement initial”
      -
         img: figures/searchable-translation-memories/searchable-translation-memories-figure-2.jpg
         label: Figure 2
         caption: A word-level alignment for a sentence pair that occurs in our training data
      -
         img: figures/searchable-translation-memories/searchable-translation-memories-figure-3.jpg
         label: Figure 3
         caption: Extracting incrementally larger phrases from a word alignment
      -
         img: figures/searchable-translation-memories/searchable-translation-memories-figure-4.jpg
         label: Figure 4
         caption: A sample of the evaluation data used to produce precision and recall results
      -
         img: figures/searchable-translation-memories/searchable-translation-memories-figure-5.jpg
         label: Figure 5
         caption: A chart of the various sub-sentential segments that were retrieved for an Arabic sentence, along with their associated probabilities
   abstract: In this paper we introduce a technique for creating searchable translation memories. Linear B’s searchable translation memories allow a translator to type in a phrase and retrieve a ranked list of possible translations for that phrase, which is ordered based on the likelihood of the translations. The searchable translation memories use translation models similar to those used in statistical machine translation. In this paper we first describe the technical details of how the TMs are indexed and how translations are assigned probabilities, and then evaluate a searchable TM using precision and recall metrics. 
   bibtex: |
      @inproceedings{Callison-Burch:2004:ASLIB,
        author =  {Chris Callison-Burch and Colin Bannard and Josh Schroeder},
           title = {Searchable Translation Memories},
           booktitle = {Proceedings of ASLIB Translating and the Computer 26}, 
           year = {2004}
       }
       
-
   title: Improved Statistical Translation Through Editing
   authors: Chris Callison-Burch, Colin Bannard and Josh Schroeder
   venue: EAMT
   type: workshop
   year: 2004
   url: publications/improved-smt-through-editing.pdf
   page_count: 
   id: improved-smt-through-editing
   figures:
      -
         img: figures/improved-smt-through-editing/improved-smt-through-editing-figure-1.jpg
         label: Figure 1
         caption: A word-level alignment for a sentence pair that occurs in our training data
      -
         img: figures/improved-smt-through-editing/improved-smt-through-editing-figure-2.jpg
         label: Figure 2
         caption: Extracting incrementally larger phrases from a word alignment
      -
         img: figures/improved-smt-through-editing/improved-smt-through-editing-figure-3.jpg
         label: Figure 3
         caption: An example of the phrases that were used to translate two sentences
      -
         img: figures/improved-smt-through-editing/improved-smt-through-editing-figure-4.jpg
         label: Figure 4
         caption: Word alignment produced by the alignment server for an edited translation
      -
         img: figures/improved-smt-through-editing/improved-smt-through-editing-figure-5.jpg
         label: Figure 5
         caption: Word alignment produced by the alignment server for an edited translation
      -
         img: figures/improved-smt-through-editing/improved-smt-through-editing-figure-6.jpg
         label: Figure 6
         caption: A software tool which allows an advanced user to correct misaligned sentence pairs from the training data
   abstract: In this paper we introduce Linear B’s statistical machine translation system. We describe how Linear B’s phrase-based translation models are learned from a parallel corpus, and show how the quality of the translations produced by our system can be improved over time through editing. There are two levels at which our translations can be edited. The first is through a simple correction of the text that is produced by our system. The second is through a mechanism which allows an advanced user to examine the sentences that a particular translation was learned from. The learning process can be improved by correcting which phrases in the sentence should be considered translations of each other. 
   bibtex: |
      @inproceedings{Callison-Burch-EtAl:2004:EAMT,
        author =  {Chris Callison-Burch and Colin Bannard and Josh Schroeder},
           title = {Improved Statistical Translation Through Editing},
           booktitle = {European Association for Machine Translation}, 
           year = {2004}
       }
       
-
   title: Statistical Natural Language Processing
   authors: Chris Callison-Burch and Miles Osborne
   venue: A Handbook for Language Engineers, Ali Farghaly, Editor
   type: chapter
   year: 2003
   url: publications/statistical-natural-language-processing-chapter.pdf
   page_count: 
   id: statistical-natural-language-processing-chapter
   figures:
      -
         img: figures/statistical-natural-language-processing-chapter/statistical-natural-language-processing-chapter-figure-1.jpg
         label: Figure 1
         caption: Two possible parses (T1 and T2) for “Do you sell Apple laptops?”.
      -
         img: figures/statistical-natural-language-processing-chapter/statistical-natural-language-processing-chapter-figure-2.jpg
         label: Figure 2
         caption: A very simple probabilistic grammar for English
      -
         img: figures/statistical-natural-language-processing-chapter/statistical-natural-language-processing-chapter-figure-3.jpg
         label: Figure 3
         caption: Various commonly used corpora
      -
         img: figures/statistical-natural-language-processing-chapter/statistical-natural-language-processing-chapter-figure-4.jpg
         label: Figure 4
         caption: Two possible parses (T1 and T2) for \Do you sell Apple laptops?".
   abstract: Statistical natural language processing (SNLP) is a field lying in the intersection of natural language processing and machine learning. SNLP differs from traditional natural language processing in that instead of having a linguist manually construct some model of a given linguistic phenomenon, that model is instead (semi-) automatically constructed from linguistically annotated resources. Methods for assigning partof-speech tags to words, categories to texts, parse trees to sentences, and so on, are (semi-) automatically acquired using machine learning techniques.The recent trend of applying statistical techniques to natural language processing came largely from industrial speech recognition research groups at AT&T's Bell Laboratories and IBM's T.J. Watson Research Center. Statistical techniques in speech recognition have so vastly outstripped the performance of their non-statistical counterparts that rule-based speech recognition systems are essentially no longer an area of research. The success of machine learning techniques in speech processing led to an interest in applying them to a broader range of NLP applications. In addition to being useful from the perspective of producing high-quality results, as in speech recognition, SNLP systems are useful for a number of practical reasons. They are cheap and fast to produce, and they handle the wide variety of input required by a real-world application. SNLP is therefore especially useful in industry. In particular&colon;SNLP affords rapid prototyping. Whereas fully hand-crafted systems are extremely time consuming to build, statistical systems that are automatically trained using corpora can be produced more quickly. This allows many different approaches to be tried and evaluated in a short time-frame. As an example, Cucerzan and Yarowsky described how one might create a new part-of-speech tagger in a single day (Cucerzan and Yarowsky, 2002). An even more ambitious example is Al-Onaizan et al.'s "machine translation in a day" experiment wherein they used statistical techniques to develop a complete Chinese-English machine translation system in a 24-hour period (AlOnaizan et al., 1999).Statistical systems are "robust" (Junqua and van Noord, 2001). Although this has a wide variety of meanings, in SNLP it generally means that a system will always produce some output no matter how badly formed the input is, and no matter how novel it is. For example, a text classification system may be able to classify a text even if all of the words in that text are previously unseen. Handling all kinds of input is necessary in real-world applications; a system which fails to produce output when it is unable to analyze a sentence will not be useful.Statistical systems are often cheaper to produce than hand-crafted rule-based systems. Because the process of creating a statistical system is more automated than the process of creating a rule-based system, the actual number of participants needed to create a system will often be less. Furthermore, because they are learned from data, statistical systems require less knowledge of the particular language being analyzed. This becomes a budgetary issue on a multi-language project because of the expense of hiring language consultants or staff with specialized skills.A common theme with many early SNLP systems was a pride in minimizing the amount of linguistic knowledge used in the system. For example, Fred Jelinek, the then leader of IBM's speech recognition research group, purportedly said, "Every time I fire a linguist, my performance goes up." The sentiment is rather shocking. Should Jelinek's statement strike fear into the hearts of all linguists reading this chapter? Is there a strong opposition between theoretical linguistics and SNLP? Will SNLP put linguists out of work?We put forth a positive answer in this chapter&colon; there is a useful role for linguistic expertise in statistical systems. Jelinek's infamous quote represents biases of the early days of SNLP. While a decade's worth of research has shown that SNLP can be an extremely powerful tool and is able to produce impressive results, recent trends indicate that using naive approaches that are divorced from linguistics can only go so far. There is therefore a revival of interest in integrating more sophisticated linguistic information into statistical models. For example, language models for speech recognition are moving from being word-based "ngram" models towards incorporating statistical grammars (Chelba and Jelinek, 1998, Charniak, 2001). So there is indeed a role for the linguist. This chapter will provide an entry point for linguists entering into the field of SNLP so that they may apply their expertise to enhance an already powerful approach to natural language processing.Lest we represent SNLP as a completely engineering-oriented discipline, we point the interested reader to Abney (1996) which describes a number of ways in which SNLP might inform academic topics in linguistics. For example, SNLP can be useful for psycholinguistic research since systems typically encode graduated notions of well-formedness. This offers a more psychologically plausible alternative to the traditional binary grammatical/ungrammatical distinction. In a similarly academic vein, Johnson (1998) shows how Optimality Theory can be interpreted in terms of statistical models. This in turn suggests a number of interesting directions that OT might take.The rest of this chapter is as follows&colon; We begin by presenting a simple worked example designed to illustrate some of the aspects of SNLP in Section 1.2. After motivating the usefulness of SNLP, we then move onto the core methods used in SNLP&colon; modeling, learning, data and evaluation (Sections 1.3, 1.4, 1.5, and 1.6 respectively). These core methods are followed by a brief review of some of the many applications of SNLP (Section 1.7). We conclude with a discussion (Section 1.8) where we make some comments about the current state of SNLP and possible future directions it might take.
   bibtex: |
      @incollection{Callison-Burch2003b,
       author = {Chris Callison-Burch and Miles Osborne},
       title = {Statistical Natural Language Processing},
       booktitle = {A Handbook for Language Engineers},
       editor = {Ali Farghaly},
       publisher = {CSLI},
       year = {2003}
       }
       
-
   title: Bootstrapping Parallel Corpora
   authors: Chris Callison-Burch and Miles Osborne
   venue: NAACL workshop Building and Using Parallel Texts
   type: workshop
   year: 2003
   url: publications/bootstrapping-parallel-corpora.pdf
   page_count: 6
   id: bootstrapping-parallel-corpora
   figures:
      -
         img: figures/bootstrapping-parallel-corpora/bootstrapping-parallel-corpora-figure-1.jpg
         label: Figure 1
         caption: Translation accuracy plotted against training corpus size
      -
         img: figures/bootstrapping-parallel-corpora/bootstrapping-parallel-corpora-figure-2.jpg
         label: Figure 2
         caption: Co-training using German, French, and Spanish sources to produce English machine translations
      -
         img: figures/bootstrapping-parallel-corpora/bootstrapping-parallel-corpora-table-1.jpg
         label: Table 1
         caption: Co-training results over three rounds
      -
         img: figures/bootstrapping-parallel-corpora/bootstrapping-parallel-corpora-figure-3.jpg
         label: Figure 3
         caption: “Coaching” of German to English by a French to English translation model
      -
         img: figures/bootstrapping-parallel-corpora/bootstrapping-parallel-corpora-figure-4.jpg
         label: Figure 4
         caption: “Coaching” of German to English by multiple translation models
   abstract: We present two methods for the automatic creation of parallel corpora. Whereas previous work into the automatic construction of parallel corpora has focused on harvesting them from the web, we examine the use of existing parallel corpora to bootstrap data for new language pairs. First, we extend existing parallel corpora using co-training, wherein machine translations are selectively added to training corpora with multiple source texts. Retraining translation models yields modest improvements. Second, we simulate the creation of training data for a language pair for which a parallel corpus is not available. Starting with no human translations from German to English we produce a German to English translation model with 45% accuracy using parallel corpora in other languages. This suggests the method may be useful in the creation of parallel corpora for languages with scarce resources. 
   bibtex: |
      @inproceedings{CallisonBurch-Osborne:2003:PARALLEL,
         author = {Callison-Burch, Chris  and  Osborne, Miles},
         title  = {Bootstrapping Parallel Corpora},
         booktitle = {Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond},
         editor = {Rada Mihalcea and Ted Pedersen},
         url    = {http://www.aclweb.org/anthology/W03-0310},
         year   = 2003,
         pages  = {44--49}
       }
       
-
   title: Co-training for Statistical Machine Translation
   authors: Chris Callison-Burch and Miles Osborne
   venue: the 6th Annual CLUK Research Colloquium
   type: workshop
   year: 2003
   url: publications/co-training-for-smt.pdf
   page_count: 
   id: co-training-for-smt
   figures:
      -
         img: figures/co-training-for-smt/co-training-for-smt-figure-1.jpg
         label: Figure 1
         caption: Translation accuracy plotted against training corpus size.
      -
         img: figures/co-training-for-smt/co-training-for-smt-figure-2.jpg
         label: Figure 2
         caption: Co-training using German, French, and Spanish sources as views on English translations
      -
         img: figures/co-training-for-smt/co-training-for-smt-figure-3.jpg
         label: Figure 3
         caption: The co-training algorithm for machine translation
      -
         img: figures/co-training-for-smt/co-training-for-smt-table-1.jpg
         label: Table 1
         caption: Co-training results over three rounds
      -
         img: figures/co-training-for-smt/co-training-for-smt-figure-4.jpg
         label: Figure 4
         caption: “Coaching” of German to English by a French to English translation model
      -
         img: figures/co-training-for-smt/co-training-for-smt-figure-5.jpg
         label: Figure 5
         caption: “Coaching” of German to English by multiple translation models
   abstract: We present a novel co-training method for statistical machine translation. Since cotraining requires independent views on the data, with each view being sufficient for the labeling task, we use source strings in multiple languages as views on translation. Co-training for statistical machine translation is therefore a type of multi-source translation. We show that using five language pairs our approach can yield improvements of up to 2.5% in word error rates for translation models. Our experiments suggest that co-training is even more effective for languages with highly impoverished parallel corpora&colon; starting with no human translations from German to English we produce a German to English translation model with 45% accuracy using parallel corpora in other languages. 
   bibtex: |
      @inproceedings{CallisonBurch-Osborne:2003:CLUK,
         author = {Callison-Burch, Chris  and  Osborne, Miles},
         title  = {Co-Training For Statistical Machine Translation},
         booktitle = {Proceedings of the 6th Annual CLUK Research Colloquium},
         year   = {2003}
       }
       
-
   title: Evaluating Question Answering Systems Using FAQ Answer Injection
   authors: Jochen Leidner and Chris Callison-Burch
   venue: the 6th Annual CLUK Research Colloquium
   type: workshop
   year: 2003
   url: publications/evaluating-question-answering-systems-using-faq-answer-injection.pdf
   page_count: 
   id: evaluating-question-answering-systems-using-faq-answer-injection
   figures:
      -
         img: figures/evaluating-question-answering-systems-using-faq-answer-injection/evaluating-question-answering-systems-using-faq-answer-injection-figure-1.jpg
         label: Figure 1
         caption: FAQ Answer Injection.
      -
         img: figures/evaluating-question-answering-systems-using-faq-answer-injection/evaluating-question-answering-systems-using-faq-answer-injection-figure-2.jpg
         label: Figure 2
         caption: Detection of Components Responsible for Recall Drop.
   abstract: Question answering (NLQA) systems which retrieve a textual fragment from a document collection that represents the answer to a question are an active field of research. But evaluations currently involve a large amount of manual effort. We propose a new evaluation scheme that uses the insertion of answers from Frequently Asked Questions collections (FAQs) to measure the ability of a system to retrieve it from the corresponding question. We describe how the usefulness of the approach can be assessed and discuss advantages and problems. 
   bibtex: |
      @inproceedings{Leidner-CallisonBurch:2003:CLUK,
         author = {Jochen L. Leidner and Chris Callison-Burch},
         title  = {Evaluating Question Answering Systems Using FAQ Answer Injection},
         booktitle = {Proceedings of the 6th Annual CLUK Research Colloquium},
         year   = {2003}
       }
       
-
   title: Co-Training for Statistical Machine Translation
   authors: Chris Callison-Burch
   venue: Master's thesis, School offormatics, University of Edinburgh
   type: thesis
   year: 2002
   url: publications/msc-thesis.pdf
   page_count: 
   id: msc-thesis
   abstract: I propose a novel co-training method for statistical machine translation. As co-training requires multiple learners trained on views of the data which are disjoint and sufficient for the labeling task, I use multiple source documents as views on translation. Co-training for statistical machine translation is therefore a type of multi-source translation. Unlike previous mutli-source methods, it improves the overall quality of translations produced by a model, rather than single translations. This is achieved by augmenting the parallel corpora on which the statistical translation models are trained. Experiments suggest that co-training is especially effective for languages with highly impoverished parallel corpora. 
   bibtex: |
      @MastersThesis{Callison-Burch2002,
         author =       {Chris Callison-Burch},
         title =        {Co-training for Statistical Machine Translation},
         school =       {University of Edinburgh},
         year =         {2002}
       }
       
-
   title: Upping the Ante for "Best of Breed" Machine Translation Providers
   authors: Chris Callison-Burch
   venue: ASLIB Translating and the Computer
   type: workshop
   year: 2001
   url: publications/upping-the-ante.pdf
   page_count: 
   id: upping-the-ante
   figures:
      -
         img: figures/upping-the-ante/upping-the-ante-table-1.jpg
         label: Table 1
         caption: Japanese to English Chat Sentences
      -
         img: figures/upping-the-ante/upping-the-ante-table-2.jpg
         label: Table 2
         caption: French to English Web Page Sentences
      -
         img: figures/upping-the-ante/upping-the-ante-table-3.jpg
         label: Table 3
         caption: French to English Chat Sentences
      -
         img: figures/upping-the-ante/upping-the-ante-table-4.jpg
         label: Table 4
         caption: English to French Sentences (Chat and Web Page)
   abstract: The notion of "best of breed" among value-added machine translation technology providers is generally defined as providing access to the single best commercially available machine translation engine for each language pair. This paper describes the efforts of Amikai, Inc. to go beyond that definition of best of breed. Rather than relying on a single engine for each pair, we have written a program that automatically selects the best translation from a set of candidate translations generated by multiple commercial machine translation engines. The program is implemented using a simple statistical language modelling technique, and relies on the simplifying assumption that the most fluent item in the set is the best translation. The program was able to produce the best translation in human ranked data up to 19% more often than the single best performing engine. 
   bibtex: |
      @inproceedings{Callison-Burch:2001:ASLIB,
         title =               {Upping the Ante for "Best of Breed" Machine Translation Providers},
         author =              {Chris Callison-Burch},
         booktitle =   {Proceedings of ASLIB Translating and the Computer 23},
         year =                {2001},
       }
       
-
   title: A program for automatically selecting the best output from multiple machine translation engines
   authors: Chris Callison-Burch and Raymond Flournoy
   venue: MT Summit
   type: conference
   year: 2001
   url: publications/multi-engine-mt-with-language-models.pdf
   page_count: 
   id: multi-engine-mt-with-language-models
   figures:
      -
         img: figures/multi-engine-mt-with-language-models/multi-engine-mt-with-language-models-table-1.jpg
         label: Table 1
         caption: Japanese
      -
         img: figures/multi-engine-mt-with-language-models/multi-engine-mt-with-language-models-table-2.jpg
         label: Table 2
         caption: French
      -
         img: figures/multi-engine-mt-with-language-models/multi-engine-mt-with-language-models-table-3.jpg
         label: Table 3
         caption: French
      -
         img: figures/multi-engine-mt-with-language-models/multi-engine-mt-with-language-models-table-4.jpg
         label: Table 4
         caption: English
   abstract: This paper describes a program that automatically selects the best translation from a set of translations produced by multiple commercial machine translation engines. The program is simplified by assuming that the most fluent item in the set is the best translation. Fluency is determined using a trigram language model. Results are provided illustrating how well the program performs for human ranked data as compared to each of its constituent engines. 
   bibtex: |
      @inproceedings{Callison-Burch-Flournoy:2001:MTSummit,
         title =               {A Program for Automatically Selecting the Best Output from Multiple Machine Translation Engines},
         author =              {Chris Callison-Burch and Raymond S. Flournoy},
         booktitle =   {Proceedings of the Machine Translation Summit VIII},
         year =                {2001},
       }
       
-
   title: Secondary Benefits of Feedback and User Interaction in Machine Translation Tools
   authors: Raymond Flournoy and Chris Callison-Burch
   venue: MT Summit Workshop
   type: workshop
   year: 2001
   url: publications/secondary-benefits-of-user-feedback-in-mt.pdf
   page_count: 
   id: secondary-benefits-of-user-feedback-in-mt
   figures:
      -
         img: figures/secondary-benefits-of-user-feedback-in-mt/secondary-benefits-of-user-feedback-in-mt-figure-1.jpg
         label: Figure 1
         caption: The “Huh?” button in AmiChat
      -
         img: figures/secondary-benefits-of-user-feedback-in-mt/secondary-benefits-of-user-feedback-in-mt-figure-2.jpg
         label: Figure 2
         caption: AmiWeb
   abstract: User feedback has often been proposed as a method for improving the accuracy of machine translation systems, but useful feedback can also serve a number of secondary benefits, including increasing user confidence in the MT technology and expanding the potential audience of users. Amikai, Inc. has produced a number of communication tools which embed translation technology and which attempt to improve the user experience by maximizing useful user interaction and feedback. As MT continues to develop, further attention needs to be paid to developing the overall user experience, which can improve the utility of translation tools even when translation quality itself plateaus 
   bibtex: |
      @inproceedings{Flournoy-Callison-Burch:2001:MTSummit,
         title =               {Secondary Benefits of Feedback and User Interaction in Machine Translation Tools},
         author =              {Raymond S. Flournoy and Chris Callison-Burch},
         booktitle =   {Workshop paper for "MT2010: Towards a Roadmap for MT" of the MT Summit VIII},
         year =                {2001},
       }
       
-
   title: A Computer Model of a Grammar for English Questions
   authors: Chris Callison-Burch
   venue: Undergraduate thesis, Symbolic Systems Program, Stanford University
   type: thesis
   year: 2000
   url: publications/computer-model-of-a-grammar-for-english-questions.pdf
   page_count: 78
   id: computer-model-of-a-grammar-for-english-questions
   figures:
   abstract: This document describes my senior honors project, which is an implementation of a grammar for English questions. I have created a computer model of Ginzburg and Sag’s theory of English interrogative constructions using the parsing software developed at the Center for Study of Language and Information (CSLI). In this chapter I describe the LKB parsing software, give instructions on downloading the system, and comment on the process of grammar engineering. The next chapter gives a summary of Ginzburg and Sag (2000). Chapter 3 details the discrepancies between the Ginzburg and Sag theory and my implementation. Chapter 4 provides a detailed discussion of a set of key example sentences. The appendices contain tables describing all the grammar constructions, lexical rules, types, and example lexical entries used in my implementation. 
   bibtex: |
      @MISC{Callison-Burch2000,
         author =  {Chris Callison-Burch},
         title =   {A Computer Model of a Grammar for English Questions},
         school = {Stanford University},
         address =   {Palo Alto, California},
         note = {Undergraduate honors thesis},
         year =    {2000}
       }
       
