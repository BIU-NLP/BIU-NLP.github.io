"use strict";(self.webpackChunklab_website=self.webpackChunklab_website||[]).push([[179],{297:()=>{function te(n){return"function"==typeof n}function ca(n){const t=n(i=>{Error.call(i),i.stack=(new Error).stack});return t.prototype=Object.create(Error.prototype),t.prototype.constructor=t,t}const Oo=ca(n=>function(t){n(this),this.message=t?`${t.length} errors occurred during unsubscription:\n${t.map((i,r)=>`${r+1}) ${i.toString()}`).join("\n  ")}`:"",this.name="UnsubscriptionError",this.errors=t});function Ji(n,e){if(n){const t=n.indexOf(e);0<=t&&n.splice(t,1)}}class Je{constructor(e){this.initialTeardown=e,this.closed=!1,this._parentage=null,this._finalizers=null}unsubscribe(){let e;if(!this.closed){this.closed=!0;const{_parentage:t}=this;if(t)if(this._parentage=null,Array.isArray(t))for(const a of t)a.remove(this);else t.remove(this);const{initialTeardown:i}=this;if(te(i))try{i()}catch(a){e=a instanceof Oo?a.errors:[a]}const{_finalizers:r}=this;if(r){this._finalizers=null;for(const a of r)try{lp(a)}catch(o){e=null!=e?e:[],o instanceof Oo?e=[...e,...o.errors]:e.push(o)}}if(e)throw new Oo(e)}}add(e){var t;if(e&&e!==this)if(this.closed)lp(e);else{if(e instanceof Je){if(e.closed||e._hasParent(this))return;e._addParent(this)}(this._finalizers=null!==(t=this._finalizers)&&void 0!==t?t:[]).push(e)}}_hasParent(e){const{_parentage:t}=this;return t===e||Array.isArray(t)&&t.includes(e)}_addParent(e){const{_parentage:t}=this;this._parentage=Array.isArray(t)?(t.push(e),t):t?[t,e]:e}_removeParent(e){const{_parentage:t}=this;t===e?this._parentage=null:Array.isArray(t)&&Ji(t,e)}remove(e){const{_finalizers:t}=this;t&&Ji(t,e),e instanceof Je&&e._removeParent(this)}}Je.EMPTY=(()=>{const n=new Je;return n.closed=!0,n})();const op=Je.EMPTY;function sp(n){return n instanceof Je||n&&"closed"in n&&te(n.remove)&&te(n.add)&&te(n.unsubscribe)}function lp(n){te(n)?n():n.unsubscribe()}const vi={onUnhandledError:null,onStoppedNotification:null,Promise:void 0,useDeprecatedSynchronousErrorHandling:!1,useDeprecatedNextContext:!1},No={setTimeout(n,e,...t){const{delegate:i}=No;return null!=i&&i.setTimeout?i.setTimeout(n,e,...t):setTimeout(n,e,...t)},clearTimeout(n){const{delegate:e}=No;return((null==e?void 0:e.clearTimeout)||clearTimeout)(n)},delegate:void 0};function cp(n){No.setTimeout(()=>{const{onUnhandledError:e}=vi;if(!e)throw n;e(n)})}function wc(){}const M2=_c("C",void 0,void 0);function _c(n,e,t){return{kind:n,value:e,error:t}}let wi=null;function Fo(n){if(vi.useDeprecatedSynchronousErrorHandling){const e=!wi;if(e&&(wi={errorThrown:!1,error:null}),n(),e){const{errorThrown:t,error:i}=wi;if(wi=null,t)throw i}}else n()}class Dc extends Je{constructor(e){super(),this.isStopped=!1,e?(this.destination=e,sp(e)&&e.add(this)):this.destination=N2}static create(e,t,i){return new da(e,t,i)}next(e){this.isStopped?Cc(function A2(n){return _c("N",n,void 0)}(e),this):this._next(e)}error(e){this.isStopped?Cc(function I2(n){return _c("E",void 0,n)}(e),this):(this.isStopped=!0,this._error(e))}complete(){this.isStopped?Cc(M2,this):(this.isStopped=!0,this._complete())}unsubscribe(){this.closed||(this.isStopped=!0,super.unsubscribe(),this.destination=null)}_next(e){this.destination.next(e)}_error(e){try{this.destination.error(e)}finally{this.unsubscribe()}}_complete(){try{this.destination.complete()}finally{this.unsubscribe()}}}const P2=Function.prototype.bind;function Sc(n,e){return P2.call(n,e)}class L2{constructor(e){this.partialObserver=e}next(e){const{partialObserver:t}=this;if(t.next)try{t.next(e)}catch(i){jo(i)}}error(e){const{partialObserver:t}=this;if(t.error)try{t.error(e)}catch(i){jo(i)}else jo(e)}complete(){const{partialObserver:e}=this;if(e.complete)try{e.complete()}catch(t){jo(t)}}}class da extends Dc{constructor(e,t,i){let r;if(super(),te(e)||!e)r={next:null!=e?e:void 0,error:null!=t?t:void 0,complete:null!=i?i:void 0};else{let a;this&&vi.useDeprecatedNextContext?(a=Object.create(e),a.unsubscribe=()=>this.unsubscribe(),r={next:e.next&&Sc(e.next,a),error:e.error&&Sc(e.error,a),complete:e.complete&&Sc(e.complete,a)}):r=e}this.destination=new L2(r)}}function jo(n){vi.useDeprecatedSynchronousErrorHandling?function R2(n){vi.useDeprecatedSynchronousErrorHandling&&wi&&(wi.errorThrown=!0,wi.error=n)}(n):cp(n)}function Cc(n,e){const{onStoppedNotification:t}=vi;t&&No.setTimeout(()=>t(n,e))}const N2={closed:!0,next:wc,error:function O2(n){throw n},complete:wc},xc="function"==typeof Symbol&&Symbol.observable||"@@observable";function Qn(n){return n}function dp(n){return 0===n.length?Qn:1===n.length?n[0]:function(t){return n.reduce((i,r)=>r(i),t)}}let ye=(()=>{class n{constructor(t){t&&(this._subscribe=t)}lift(t){const i=new n;return i.source=this,i.operator=t,i}subscribe(t,i,r){const a=function B2(n){return n&&n instanceof Dc||function j2(n){return n&&te(n.next)&&te(n.error)&&te(n.complete)}(n)&&sp(n)}(t)?t:new da(t,i,r);return Fo(()=>{const{operator:o,source:s}=this;a.add(o?o.call(a,s):s?this._subscribe(a):this._trySubscribe(a))}),a}_trySubscribe(t){try{return this._subscribe(t)}catch(i){t.error(i)}}forEach(t,i){return new(i=up(i))((r,a)=>{const o=new da({next:s=>{try{t(s)}catch(l){a(l),o.unsubscribe()}},error:a,complete:r});this.subscribe(o)})}_subscribe(t){var i;return null===(i=this.source)||void 0===i?void 0:i.subscribe(t)}[xc](){return this}pipe(...t){return dp(t)(this)}toPromise(t){return new(t=up(t))((i,r)=>{let a;this.subscribe(o=>a=o,o=>r(o),()=>i(a))})}}return n.create=e=>new n(e),n})();function up(n){var e;return null!==(e=null!=n?n:vi.Promise)&&void 0!==e?e:Promise}const H2=ca(n=>function(){n(this),this.name="ObjectUnsubscribedError",this.message="object unsubscribed"});let _e=(()=>{class n extends ye{constructor(){super(),this.closed=!1,this.currentObservers=null,this.observers=[],this.isStopped=!1,this.hasError=!1,this.thrownError=null}lift(t){const i=new hp(this,this);return i.operator=t,i}_throwIfClosed(){if(this.closed)throw new H2}next(t){Fo(()=>{if(this._throwIfClosed(),!this.isStopped){this.currentObservers||(this.currentObservers=Array.from(this.observers));for(const i of this.currentObservers)i.next(t)}})}error(t){Fo(()=>{if(this._throwIfClosed(),!this.isStopped){this.hasError=this.isStopped=!0,this.thrownError=t;const{observers:i}=this;for(;i.length;)i.shift().error(t)}})}complete(){Fo(()=>{if(this._throwIfClosed(),!this.isStopped){this.isStopped=!0;const{observers:t}=this;for(;t.length;)t.shift().complete()}})}unsubscribe(){this.isStopped=this.closed=!0,this.observers=this.currentObservers=null}get observed(){var t;return(null===(t=this.observers)||void 0===t?void 0:t.length)>0}_trySubscribe(t){return this._throwIfClosed(),super._trySubscribe(t)}_subscribe(t){return this._throwIfClosed(),this._checkFinalizedStatuses(t),this._innerSubscribe(t)}_innerSubscribe(t){const{hasError:i,isStopped:r,observers:a}=this;return i||r?op:(this.currentObservers=null,a.push(t),new Je(()=>{this.currentObservers=null,Ji(a,t)}))}_checkFinalizedStatuses(t){const{hasError:i,thrownError:r,isStopped:a}=this;i?t.error(r):a&&t.complete()}asObservable(){const t=new ye;return t.source=this,t}}return n.create=(e,t)=>new hp(e,t),n})();class hp extends _e{constructor(e,t){super(),this.destination=e,this.source=t}next(e){var t,i;null===(i=null===(t=this.destination)||void 0===t?void 0:t.next)||void 0===i||i.call(t,e)}error(e){var t,i;null===(i=null===(t=this.destination)||void 0===t?void 0:t.error)||void 0===i||i.call(t,e)}complete(){var e,t;null===(t=null===(e=this.destination)||void 0===e?void 0:e.complete)||void 0===t||t.call(e)}_subscribe(e){var t,i;return null!==(i=null===(t=this.source)||void 0===t?void 0:t.subscribe(e))&&void 0!==i?i:op}}function fp(n){return te(null==n?void 0:n.lift)}function xe(n){return e=>{if(fp(e))return e.lift(function(t){try{return n(t,this)}catch(i){this.error(i)}});throw new TypeError("Unable to lift unknown Observable type")}}function ve(n,e,t,i,r){return new z2(n,e,t,i,r)}class z2 extends Dc{constructor(e,t,i,r,a,o){super(e),this.onFinalize=a,this.shouldUnsubscribe=o,this._next=t?function(s){try{t(s)}catch(l){e.error(l)}}:super._next,this._error=r?function(s){try{r(s)}catch(l){e.error(l)}finally{this.unsubscribe()}}:super._error,this._complete=i?function(){try{i()}catch(s){e.error(s)}finally{this.unsubscribe()}}:super._complete}unsubscribe(){var e;if(!this.shouldUnsubscribe||this.shouldUnsubscribe()){const{closed:t}=this;super.unsubscribe(),!t&&(null===(e=this.onFinalize)||void 0===e||e.call(this))}}}function H(n,e){return xe((t,i)=>{let r=0;t.subscribe(ve(i,a=>{i.next(n.call(e,a,r++))}))})}function _i(n){return this instanceof _i?(this.v=n,this):new _i(n)}function G2(n,e,t){if(!Symbol.asyncIterator)throw new TypeError("Symbol.asyncIterator is not defined.");var r,i=t.apply(n,e||[]),a=[];return r={},o("next"),o("throw"),o("return"),r[Symbol.asyncIterator]=function(){return this},r;function o(h){i[h]&&(r[h]=function(f){return new Promise(function(p,g){a.push([h,f,p,g])>1||s(h,f)})})}function s(h,f){try{!function l(h){h.value instanceof _i?Promise.resolve(h.value.v).then(c,d):u(a[0][2],h)}(i[h](f))}catch(p){u(a[0][3],p)}}function c(h){s("next",h)}function d(h){s("throw",h)}function u(h,f){h(f),a.shift(),a.length&&s(a[0][0],a[0][1])}}function U2(n){if(!Symbol.asyncIterator)throw new TypeError("Symbol.asyncIterator is not defined.");var t,e=n[Symbol.asyncIterator];return e?e.call(n):(n=function mp(n){var e="function"==typeof Symbol&&Symbol.iterator,t=e&&n[e],i=0;if(t)return t.call(n);if(n&&"number"==typeof n.length)return{next:function(){return n&&i>=n.length&&(n=void 0),{value:n&&n[i++],done:!n}}};throw new TypeError(e?"Object is not iterable.":"Symbol.iterator is not defined.")}(n),t={},i("next"),i("throw"),i("return"),t[Symbol.asyncIterator]=function(){return this},t);function i(a){t[a]=n[a]&&function(o){return new Promise(function(s,l){!function r(a,o,s,l){Promise.resolve(l).then(function(c){a({value:c,done:s})},o)}(s,l,(o=n[a](o)).done,o.value)})}}}const Ec=n=>n&&"number"==typeof n.length&&"function"!=typeof n;function bp(n){return te(null==n?void 0:n.then)}function yp(n){return te(n[xc])}function vp(n){return Symbol.asyncIterator&&te(null==n?void 0:n[Symbol.asyncIterator])}function wp(n){return new TypeError(`You provided ${null!==n&&"object"==typeof n?"an invalid object":`'${n}'`} where a stream was expected. You can provide an Observable, Promise, ReadableStream, Array, AsyncIterable, or Iterable.`)}const _p=function q2(){return"function"==typeof Symbol&&Symbol.iterator?Symbol.iterator:"@@iterator"}();function Dp(n){return te(null==n?void 0:n[_p])}function Sp(n){return G2(this,arguments,function*(){const t=n.getReader();try{for(;;){const{value:i,done:r}=yield _i(t.read());if(r)return yield _i(void 0);yield yield _i(i)}}finally{t.releaseLock()}})}function Cp(n){return te(null==n?void 0:n.getReader)}function ot(n){if(n instanceof ye)return n;if(null!=n){if(yp(n))return function Y2(n){return new ye(e=>{const t=n[xc]();if(te(t.subscribe))return t.subscribe(e);throw new TypeError("Provided object does not correctly implement Symbol.observable")})}(n);if(Ec(n))return function Q2(n){return new ye(e=>{for(let t=0;t<n.length&&!e.closed;t++)e.next(n[t]);e.complete()})}(n);if(bp(n))return function K2(n){return new ye(e=>{n.then(t=>{e.closed||(e.next(t),e.complete())},t=>e.error(t)).then(null,cp)})}(n);if(vp(n))return xp(n);if(Dp(n))return function X2(n){return new ye(e=>{for(const t of n)if(e.next(t),e.closed)return;e.complete()})}(n);if(Cp(n))return function J2(n){return xp(Sp(n))}(n)}throw wp(n)}function xp(n){return new ye(e=>{(function Z2(n,e){var t,i,r,a;return function W2(n,e,t,i){return new(t||(t=Promise))(function(a,o){function s(d){try{c(i.next(d))}catch(u){o(u)}}function l(d){try{c(i.throw(d))}catch(u){o(u)}}function c(d){d.done?a(d.value):function r(a){return a instanceof t?a:new t(function(o){o(a)})}(d.value).then(s,l)}c((i=i.apply(n,e||[])).next())})}(this,void 0,void 0,function*(){try{for(t=U2(n);!(i=yield t.next()).done;)if(e.next(i.value),e.closed)return}catch(o){r={error:o}}finally{try{i&&!i.done&&(a=t.return)&&(yield a.call(t))}finally{if(r)throw r.error}}e.complete()})})(n,e).catch(t=>e.error(t))})}function Tn(n,e,t,i=0,r=!1){const a=e.schedule(function(){t(),r?n.add(this.schedule(null,i)):this.unsubscribe()},i);if(n.add(a),!r)return a}function $e(n,e,t=1/0){return te(e)?$e((i,r)=>H((a,o)=>e(i,a,r,o))(ot(n(i,r))),t):("number"==typeof e&&(t=e),xe((i,r)=>function eD(n,e,t,i,r,a,o,s){const l=[];let c=0,d=0,u=!1;const h=()=>{u&&!l.length&&!c&&e.complete()},f=g=>c<i?p(g):l.push(g),p=g=>{a&&e.next(g),c++;let m=!1;ot(t(g,d++)).subscribe(ve(e,b=>{null==r||r(b),a?f(b):e.next(b)},()=>{m=!0},void 0,()=>{if(m)try{for(c--;l.length&&c<i;){const b=l.shift();o?Tn(e,o,()=>p(b)):p(b)}h()}catch(b){e.error(b)}}))};return n.subscribe(ve(e,f,()=>{u=!0,h()})),()=>{null==s||s()}}(i,r,n,t)))}function Zi(n=1/0){return $e(Qn,n)}const En=new ye(n=>n.complete());function Tp(n){return n&&te(n.schedule)}function kc(n){return n[n.length-1]}function ua(n){return Tp(kc(n))?n.pop():void 0}function kp(n,e=0){return xe((t,i)=>{t.subscribe(ve(i,r=>Tn(i,n,()=>i.next(r),e),()=>Tn(i,n,()=>i.complete(),e),r=>Tn(i,n,()=>i.error(r),e)))})}function Mp(n,e=0){return xe((t,i)=>{i.add(n.schedule(()=>t.subscribe(i),e))})}function Ip(n,e){if(!n)throw new Error("Iterable cannot be null");return new ye(t=>{Tn(t,e,()=>{const i=n[Symbol.asyncIterator]();Tn(t,e,()=>{i.next().then(r=>{r.done?t.complete():t.next(r.value)})},0,!0)})})}function He(n,e){return e?function lD(n,e){if(null!=n){if(yp(n))return function iD(n,e){return ot(n).pipe(Mp(e),kp(e))}(n,e);if(Ec(n))return function aD(n,e){return new ye(t=>{let i=0;return e.schedule(function(){i===n.length?t.complete():(t.next(n[i++]),t.closed||this.schedule())})})}(n,e);if(bp(n))return function rD(n,e){return ot(n).pipe(Mp(e),kp(e))}(n,e);if(vp(n))return Ip(n,e);if(Dp(n))return function oD(n,e){return new ye(t=>{let i;return Tn(t,e,()=>{i=n[_p](),Tn(t,e,()=>{let r,a;try{({value:r,done:a}=i.next())}catch(o){return void t.error(o)}a?t.complete():t.next(r)},0,!0)}),()=>te(null==i?void 0:i.return)&&i.return()})}(n,e);if(Cp(n))return function sD(n,e){return Ip(Sp(n),e)}(n,e)}throw wp(n)}(n,e):ot(n)}function Ap(...n){const e=ua(n),t=function nD(n,e){return"number"==typeof kc(n)?n.pop():e}(n,1/0),i=n;return i.length?1===i.length?ot(i[0]):Zi(t)(He(i,e)):En}function Mc(n,e,...t){if(!0===e)return void n();if(!1===e)return;const i=new da({next:()=>{i.unsubscribe(),n()}});return ot(e(...t)).subscribe(i)}function he(n){for(let e in n)if(n[e]===he)return e;throw Error("Could not find renamed property on target object.")}function Ic(n,e){for(const t in e)e.hasOwnProperty(t)&&!n.hasOwnProperty(t)&&(n[t]=e[t])}function fe(n){if("string"==typeof n)return n;if(Array.isArray(n))return"["+n.map(fe).join(", ")+"]";if(null==n)return""+n;if(n.overriddenName)return`${n.overriddenName}`;if(n.name)return`${n.name}`;const e=n.toString();if(null==e)return""+e;const t=e.indexOf("\n");return-1===t?e:e.substring(0,t)}function Ac(n,e){return null==n||""===n?null===e?"":e:null==e||""===e?n:n+" "+e}const cD=he({__forward_ref__:he});function Rc(n){return n.__forward_ref__=Rc,n.toString=function(){return fe(this())},n}function F(n){return function Pc(n){return"function"==typeof n&&n.hasOwnProperty(cD)&&n.__forward_ref__===Rc}(n)?n():n}class _ extends Error{constructor(e,t){super(function Bo(n,e){return`NG0${Math.abs(n)}${e?": "+e.trim():""}`}(e,t)),this.code=e}}function z(n){return"string"==typeof n?n:null==n?"":String(n)}function Ho(n,e){throw new _(-201,!1)}function St(n,e){null==n&&function ce(n,e,t,i){throw new Error(`ASSERTION ERROR: ${n}`+(null==i?"":` [Expected=> ${t} ${i} ${e} <=Actual]`))}(e,n,null,"!=")}function k(n){return{token:n.token,providedIn:n.providedIn||null,factory:n.factory,value:void 0}}function Te(n){return{providers:n.providers||[],imports:n.imports||[]}}function zo(n){return Pp(n,Wo)||Pp(n,Op)}function Pp(n,e){return n.hasOwnProperty(e)?n[e]:null}function Lp(n){return n&&(n.hasOwnProperty(Lc)||n.hasOwnProperty(yD))?n[Lc]:null}const Wo=he({\u0275prov:he}),Lc=he({\u0275inj:he}),Op=he({ngInjectableDef:he}),yD=he({ngInjectorDef:he});var O=(()=>((O=O||{})[O.Default=0]="Default",O[O.Host=1]="Host",O[O.Self=2]="Self",O[O.SkipSelf=4]="SkipSelf",O[O.Optional=8]="Optional",O))();let Oc;function Ft(n){const e=Oc;return Oc=n,e}function Np(n,e,t){const i=zo(n);return i&&"root"==i.providedIn?void 0===i.value?i.value=i.factory():i.value:t&O.Optional?null:void 0!==e?e:void Ho(fe(n))}function Kn(n){return{toString:n}.toString()}var Kt=(()=>((Kt=Kt||{})[Kt.OnPush=0]="OnPush",Kt[Kt.Default=1]="Default",Kt))(),Xt=(()=>{return(n=Xt||(Xt={}))[n.Emulated=0]="Emulated",n[n.None=2]="None",n[n.ShadowDom=3]="ShadowDom",Xt;var n})();const pe=(()=>"undefined"!=typeof globalThis&&globalThis||"undefined"!=typeof global&&global||"undefined"!=typeof window&&window||"undefined"!=typeof self&&"undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope&&self)(),er={},se=[],Vo=he({\u0275cmp:he}),Nc=he({\u0275dir:he}),Fc=he({\u0275pipe:he}),Fp=he({\u0275mod:he}),Mn=he({\u0275fac:he}),ha=he({__NG_ELEMENT_ID__:he});let wD=0;function mt(n){return Kn(()=>{const t=!0===n.standalone,i={},r={type:n.type,providersResolver:null,decls:n.decls,vars:n.vars,factory:null,template:n.template||null,consts:n.consts||null,ngContentSelectors:n.ngContentSelectors,hostBindings:n.hostBindings||null,hostVars:n.hostVars||0,hostAttrs:n.hostAttrs||null,contentQueries:n.contentQueries||null,declaredInputs:i,inputs:null,outputs:null,exportAs:n.exportAs||null,onPush:n.changeDetection===Kt.OnPush,directiveDefs:null,pipeDefs:null,standalone:t,dependencies:t&&n.dependencies||null,getStandaloneInjector:null,selectors:n.selectors||se,viewQuery:n.viewQuery||null,features:n.features||null,data:n.data||{},encapsulation:n.encapsulation||Xt.Emulated,id:"c"+wD++,styles:n.styles||se,_:null,setInput:null,schemas:n.schemas||null,tView:null},a=n.dependencies,o=n.features;return r.inputs=Hp(n.inputs,i),r.outputs=Hp(n.outputs),o&&o.forEach(s=>s(r)),r.directiveDefs=a?()=>("function"==typeof a?a():a).map(jp).filter(Bp):null,r.pipeDefs=a?()=>("function"==typeof a?a():a).map(lt).filter(Bp):null,r})}function jp(n){return de(n)||st(n)}function Bp(n){return null!==n}function Ae(n){return Kn(()=>({type:n.type,bootstrap:n.bootstrap||se,declarations:n.declarations||se,imports:n.imports||se,exports:n.exports||se,transitiveCompileScopes:null,schemas:n.schemas||null,id:n.id||null}))}function Hp(n,e){if(null==n)return er;const t={};for(const i in n)if(n.hasOwnProperty(i)){let r=n[i],a=r;Array.isArray(r)&&(a=r[1],r=r[0]),t[r]=i,e&&(e[r]=a)}return t}const Re=mt;function bt(n){return{type:n.type,name:n.name,factory:null,pure:!1!==n.pure,standalone:!0===n.standalone,onDestroy:n.type.prototype.ngOnDestroy||null}}function de(n){return n[Vo]||null}function st(n){return n[Nc]||null}function lt(n){return n[Fc]||null}function Ct(n,e){const t=n[Fp]||null;if(!t&&!0===e)throw new Error(`Type ${fe(n)} does not have '\u0275mod' property.`);return t}const U=11;function yt(n){return Array.isArray(n)&&"object"==typeof n[1]}function Zt(n){return Array.isArray(n)&&!0===n[1]}function Hc(n){return 0!=(8&n.flags)}function qo(n){return 2==(2&n.flags)}function Yo(n){return 1==(1&n.flags)}function en(n){return null!==n.template}function TD(n){return 0!=(256&n[2])}function Ti(n,e){return n.hasOwnProperty(Mn)?n[Mn]:null}class MD{constructor(e,t,i){this.previousValue=e,this.currentValue=t,this.firstChange=i}isFirstChange(){return this.firstChange}}function Ei(){return Vp}function Vp(n){return n.type.prototype.ngOnChanges&&(n.setInput=AD),ID}function ID(){const n=Up(this),e=null==n?void 0:n.current;if(e){const t=n.previous;if(t===er)n.previous=e;else for(let i in e)t[i]=e[i];n.current=null,this.ngOnChanges(e)}}function AD(n,e,t,i){const r=Up(n)||function RD(n,e){return n[Gp]=e}(n,{previous:er,current:null}),a=r.current||(r.current={}),o=r.previous,s=this.declaredInputs[t],l=o[s];a[s]=new MD(l&&l.currentValue,e,o===er),n[i]=e}Ei.ngInherit=!0;const Gp="__ngSimpleChanges__";function Up(n){return n[Gp]||null}function Oe(n){for(;Array.isArray(n);)n=n[0];return n}function Qo(n,e){return Oe(e[n])}function Tt(n,e){return Oe(e[n.index])}function Uc(n,e){return n.data[e]}function Et(n,e){const t=e[n];return yt(t)?t:t[0]}function Ko(n){return 64==(64&n[2])}function Xn(n,e){return null==e?null:n[e]}function $p(n){n[18]=0}function $c(n,e){n[5]+=e;let t=n,i=n[3];for(;null!==i&&(1===e&&1===t[5]||-1===e&&0===t[5]);)i[5]+=e,t=i,i=i[3]}const B={lFrame:ng(null),bindingsEnabled:!0};function Yp(){return B.bindingsEnabled}function w(){return B.lFrame.lView}function ee(){return B.lFrame.tView}function ze(){let n=Qp();for(;null!==n&&64===n.type;)n=n.parent;return n}function Qp(){return B.lFrame.currentTNode}function un(n,e){const t=B.lFrame;t.currentTNode=n,t.isParent=e}function qc(){return B.lFrame.isParent}function Yc(){B.lFrame.isParent=!1}function ct(){const n=B.lFrame;let e=n.bindingRootIndex;return-1===e&&(e=n.bindingRootIndex=n.tView.bindingStartIndex),e}function or(){return B.lFrame.bindingIndex++}function An(n){const e=B.lFrame,t=e.bindingIndex;return e.bindingIndex=e.bindingIndex+n,t}function QD(n,e){const t=B.lFrame;t.bindingIndex=t.bindingRootIndex=n,Qc(e)}function Qc(n){B.lFrame.currentDirectiveIndex=n}function Zp(){return B.lFrame.currentQueryIndex}function Xc(n){B.lFrame.currentQueryIndex=n}function XD(n){const e=n[1];return 2===e.type?e.declTNode:1===e.type?n[6]:null}function eg(n,e,t){if(t&O.SkipSelf){let r=e,a=n;for(;!(r=r.parent,null!==r||t&O.Host||(r=XD(a),null===r||(a=a[15],10&r.type))););if(null===r)return!1;e=r,n=a}const i=B.lFrame=tg();return i.currentTNode=e,i.lView=n,!0}function Jc(n){const e=tg(),t=n[1];B.lFrame=e,e.currentTNode=t.firstChild,e.lView=n,e.tView=t,e.contextLView=n,e.bindingIndex=t.bindingStartIndex,e.inI18n=!1}function tg(){const n=B.lFrame,e=null===n?null:n.child;return null===e?ng(n):e}function ng(n){const e={currentTNode:null,isParent:!0,lView:null,tView:null,selectedIndex:-1,contextLView:null,elementDepthCount:0,currentNamespace:null,currentDirectiveIndex:-1,bindingRootIndex:-1,bindingIndex:-1,currentQueryIndex:0,parent:n,child:null,inI18n:!1};return null!==n&&(n.child=e),e}function ig(){const n=B.lFrame;return B.lFrame=n.parent,n.currentTNode=null,n.lView=null,n}const rg=ig;function Zc(){const n=ig();n.isParent=!0,n.tView=null,n.selectedIndex=-1,n.contextLView=null,n.elementDepthCount=0,n.currentDirectiveIndex=-1,n.currentNamespace=null,n.bindingRootIndex=-1,n.bindingIndex=-1,n.currentQueryIndex=0}function dt(){return B.lFrame.selectedIndex}function Jn(n){B.lFrame.selectedIndex=n}function De(){const n=B.lFrame;return Uc(n.tView,n.selectedIndex)}function Xo(n,e){for(let t=e.directiveStart,i=e.directiveEnd;t<i;t++){const a=n.data[t].type.prototype,{ngAfterContentInit:o,ngAfterContentChecked:s,ngAfterViewInit:l,ngAfterViewChecked:c,ngOnDestroy:d}=a;o&&(n.contentHooks||(n.contentHooks=[])).push(-t,o),s&&((n.contentHooks||(n.contentHooks=[])).push(t,s),(n.contentCheckHooks||(n.contentCheckHooks=[])).push(t,s)),l&&(n.viewHooks||(n.viewHooks=[])).push(-t,l),c&&((n.viewHooks||(n.viewHooks=[])).push(t,c),(n.viewCheckHooks||(n.viewCheckHooks=[])).push(t,c)),null!=d&&(n.destroyHooks||(n.destroyHooks=[])).push(t,d)}}function Jo(n,e,t){ag(n,e,3,t)}function Zo(n,e,t,i){(3&n[2])===t&&ag(n,e,t,i)}function ed(n,e){let t=n[2];(3&t)===e&&(t&=2047,t+=1,n[2]=t)}function ag(n,e,t,i){const a=null!=i?i:-1,o=e.length-1;let s=0;for(let l=void 0!==i?65535&n[18]:0;l<o;l++)if("number"==typeof e[l+1]){if(s=e[l],null!=i&&s>=i)break}else e[l]<0&&(n[18]+=65536),(s<a||-1==a)&&(oS(n,t,e,l),n[18]=(4294901760&n[18])+l+2),l++}function oS(n,e,t,i){const r=t[i]<0,a=t[i+1],s=n[r?-t[i]:t[i]];if(r){if(n[2]>>11<n[18]>>16&&(3&n[2])===e){n[2]+=2048;try{a.call(s)}finally{}}}else try{a.call(s)}finally{}}class ya{constructor(e,t,i){this.factory=e,this.resolving=!1,this.canSeeViewProviders=t,this.injectImpl=i}}function es(n,e,t){let i=0;for(;i<t.length;){const r=t[i];if("number"==typeof r){if(0!==r)break;i++;const a=t[i++],o=t[i++],s=t[i++];n.setAttribute(e,o,s,a)}else{const a=r,o=t[++i];sg(a)?n.setProperty(e,a,o):n.setAttribute(e,a,o),i++}}return i}function og(n){return 3===n||4===n||6===n}function sg(n){return 64===n.charCodeAt(0)}function ts(n,e){if(null!==e&&0!==e.length)if(null===n||0===n.length)n=e.slice();else{let t=-1;for(let i=0;i<e.length;i++){const r=e[i];"number"==typeof r?t=r:0===t||lg(n,t,r,null,-1===t||2===t?e[++i]:null)}}return n}function lg(n,e,t,i,r){let a=0,o=n.length;if(-1===e)o=-1;else for(;a<n.length;){const s=n[a++];if("number"==typeof s){if(s===e){o=-1;break}if(s>e){o=a-1;break}}}for(;a<n.length;){const s=n[a];if("number"==typeof s)break;if(s===t){if(null===i)return void(null!==r&&(n[a+1]=r));if(i===n[a+1])return void(n[a+2]=r)}a++,null!==i&&a++,null!==r&&a++}-1!==o&&(n.splice(o,0,e),a=o+1),n.splice(a++,0,t),null!==i&&n.splice(a++,0,i),null!==r&&n.splice(a++,0,r)}function cg(n){return-1!==n}function sr(n){return 32767&n}function lr(n,e){let t=function uS(n){return n>>16}(n),i=e;for(;t>0;)i=i[15],t--;return i}let nd=!0;function ns(n){const e=nd;return nd=n,e}let hS=0;const hn={};function wa(n,e){const t=rd(n,e);if(-1!==t)return t;const i=e[1];i.firstCreatePass&&(n.injectorIndex=e.length,id(i.data,n),id(e,null),id(i.blueprint,null));const r=is(n,e),a=n.injectorIndex;if(cg(r)){const o=sr(r),s=lr(r,e),l=s[1].data;for(let c=0;c<8;c++)e[a+c]=s[o+c]|l[o+c]}return e[a+8]=r,a}function id(n,e){n.push(0,0,0,0,0,0,0,0,e)}function rd(n,e){return-1===n.injectorIndex||n.parent&&n.parent.injectorIndex===n.injectorIndex||null===e[n.injectorIndex+8]?-1:n.injectorIndex}function is(n,e){if(n.parent&&-1!==n.parent.injectorIndex)return n.parent.injectorIndex;let t=0,i=null,r=e;for(;null!==r;){if(i=yg(r),null===i)return-1;if(t++,r=r[15],-1!==i.injectorIndex)return i.injectorIndex|t<<16}return-1}function rs(n,e,t){!function fS(n,e,t){let i;"string"==typeof t?i=t.charCodeAt(0)||0:t.hasOwnProperty(ha)&&(i=t[ha]),null==i&&(i=t[ha]=hS++);const r=255&i;e.data[n+(r>>5)]|=1<<r}(n,e,t)}function hg(n,e,t){if(t&O.Optional||void 0!==n)return n;Ho()}function fg(n,e,t,i){if(t&O.Optional&&void 0===i&&(i=null),0==(t&(O.Self|O.Host))){const r=n[9],a=Ft(void 0);try{return r?r.get(e,i,t&O.Optional):Np(e,i,t&O.Optional)}finally{Ft(a)}}return hg(i,0,t)}function pg(n,e,t,i=O.Default,r){if(null!==n){if(1024&e[2]){const o=function vS(n,e,t,i,r){let a=n,o=e;for(;null!==a&&null!==o&&1024&o[2]&&!(256&o[2]);){const s=gg(a,o,t,i|O.Self,hn);if(s!==hn)return s;let l=a.parent;if(!l){const c=o[21];if(c){const d=c.get(t,hn,i);if(d!==hn)return d}l=yg(o),o=o[15]}a=l}return r}(n,e,t,i,hn);if(o!==hn)return o}const a=gg(n,e,t,i,hn);if(a!==hn)return a}return fg(e,t,i,r)}function gg(n,e,t,i,r){const a=function mS(n){if("string"==typeof n)return n.charCodeAt(0)||0;const e=n.hasOwnProperty(ha)?n[ha]:void 0;return"number"==typeof e?e>=0?255&e:bS:e}(t);if("function"==typeof a){if(!eg(e,n,i))return i&O.Host?hg(r,0,i):fg(e,t,i,r);try{const o=a(i);if(null!=o||i&O.Optional)return o;Ho()}finally{rg()}}else if("number"==typeof a){let o=null,s=rd(n,e),l=-1,c=i&O.Host?e[16][6]:null;for((-1===s||i&O.SkipSelf)&&(l=-1===s?is(n,e):e[s+8],-1!==l&&bg(i,!1)?(o=e[1],s=sr(l),e=lr(l,e)):s=-1);-1!==s;){const d=e[1];if(mg(a,s,d.data)){const u=gS(s,e,t,o,i,c);if(u!==hn)return u}l=e[s+8],-1!==l&&bg(i,e[1].data[s+8]===c)&&mg(a,s,e)?(o=d,s=sr(l),e=lr(l,e)):s=-1}}return r}function gS(n,e,t,i,r,a){const o=e[1],s=o.data[n+8],d=as(s,o,t,null==i?qo(s)&&nd:i!=o&&0!=(3&s.type),r&O.Host&&a===s);return null!==d?_a(e,o,d,s):hn}function as(n,e,t,i,r){const a=n.providerIndexes,o=e.data,s=1048575&a,l=n.directiveStart,d=a>>20,h=r?s+d:n.directiveEnd;for(let f=i?s:s+d;f<h;f++){const p=o[f];if(f<l&&t===p||f>=l&&p.type===t)return f}if(r){const f=o[l];if(f&&en(f)&&f.type===t)return l}return null}function _a(n,e,t,i){let r=n[t];const a=e.data;if(function sS(n){return n instanceof ya}(r)){const o=r;o.resolving&&function dD(n,e){const t=e?`. Dependency path: ${e.join(" > ")} > ${n}`:"";throw new _(-200,`Circular dependency in DI detected for ${n}${t}`)}(function oe(n){return"function"==typeof n?n.name||n.toString():"object"==typeof n&&null!=n&&"function"==typeof n.type?n.type.name||n.type.toString():z(n)}(a[t]));const s=ns(o.canSeeViewProviders);o.resolving=!0;const l=o.injectImpl?Ft(o.injectImpl):null;eg(n,i,O.Default);try{r=n[t]=o.factory(void 0,a,n,i),e.firstCreatePass&&t>=i.directiveStart&&function aS(n,e,t){const{ngOnChanges:i,ngOnInit:r,ngDoCheck:a}=e.type.prototype;if(i){const o=Vp(e);(t.preOrderHooks||(t.preOrderHooks=[])).push(n,o),(t.preOrderCheckHooks||(t.preOrderCheckHooks=[])).push(n,o)}r&&(t.preOrderHooks||(t.preOrderHooks=[])).push(0-n,r),a&&((t.preOrderHooks||(t.preOrderHooks=[])).push(n,a),(t.preOrderCheckHooks||(t.preOrderCheckHooks=[])).push(n,a))}(t,a[t],e)}finally{null!==l&&Ft(l),ns(s),o.resolving=!1,rg()}}return r}function mg(n,e,t){return!!(t[e+(n>>5)]&1<<n)}function bg(n,e){return!(n&O.Self||n&O.Host&&e)}class cr{constructor(e,t){this._tNode=e,this._lView=t}get(e,t,i){return pg(this._tNode,this._lView,e,i,t)}}function bS(){return new cr(ze(),w())}function yg(n){const e=n[1],t=e.type;return 2===t?e.declTNode:1===t?n[6]:null}function Da(n){return function pS(n,e){if("class"===e)return n.classes;if("style"===e)return n.styles;const t=n.attrs;if(t){const i=t.length;let r=0;for(;r<i;){const a=t[r];if(og(a))break;if(0===a)r+=2;else if("number"==typeof a)for(r++;r<i&&"string"==typeof t[r];)r++;else{if(a===e)return t[r+1];r+=2}}}return null}(ze(),n)}const ur="__parameters__";function fr(n,e,t){return Kn(()=>{const i=function od(n){return function(...t){if(n){const i=n(...t);for(const r in i)this[r]=i[r]}}}(e);function r(...a){if(this instanceof r)return i.apply(this,a),this;const o=new r(...a);return s.annotation=o,s;function s(l,c,d){const u=l.hasOwnProperty(ur)?l[ur]:Object.defineProperty(l,ur,{value:[]})[ur];for(;u.length<=d;)u.push(null);return(u[d]=u[d]||[]).push(o),l}}return t&&(r.prototype=Object.create(t.prototype)),r.prototype.ngMetadataName=n,r.annotationCls=r,r})}class E{constructor(e,t){this._desc=e,this.ngMetadataName="InjectionToken",this.\u0275prov=void 0,"number"==typeof t?this.__NG_ELEMENT_ID__=t:void 0!==t&&(this.\u0275prov=k({token:this,providedIn:t.providedIn||"root",factory:t.factory}))}get multi(){return this}toString(){return`InjectionToken ${this._desc}`}}function kt(n,e){void 0===e&&(e=n);for(let t=0;t<n.length;t++){let i=n[t];Array.isArray(i)?(e===n&&(e=n.slice(0,t)),kt(i,e)):e!==n&&e.push(i)}return e}function Rn(n,e){n.forEach(t=>Array.isArray(t)?Rn(t,e):e(t))}function wg(n,e,t){e>=n.length?n.push(t):n.splice(e,0,t)}function os(n,e){return e>=n.length-1?n.pop():n.splice(e,1)[0]}function xa(n,e){const t=[];for(let i=0;i<n;i++)t.push(e);return t}function Mt(n,e,t){let i=pr(n,e);return i>=0?n[1|i]=t:(i=~i,function SS(n,e,t,i){let r=n.length;if(r==e)n.push(t,i);else if(1===r)n.push(i,n[0]),n[0]=t;else{for(r--,n.push(n[r-1],n[r]);r>e;)n[r]=n[r-2],r--;n[e]=t,n[e+1]=i}}(n,i,e,t)),i}function ld(n,e){const t=pr(n,e);if(t>=0)return n[1|t]}function pr(n,e){return function Sg(n,e,t){let i=0,r=n.length>>t;for(;r!==i;){const a=i+(r-i>>1),o=n[a<<t];if(e===o)return a<<t;o>e?r=a:i=a+1}return~(r<<t)}(n,e,1)}const Ta={},dd="__NG_DI_FLAG__",ls="ngTempTokenPath",AS=/\n/gm,Cg="__source";let Ea;function gr(n){const e=Ea;return Ea=n,e}function PS(n,e=O.Default){if(void 0===Ea)throw new _(-203,!1);return null===Ea?Np(n,void 0,e):Ea.get(n,e&O.Optional?null:void 0,e)}function v(n,e=O.Default){return(function vD(){return Oc}()||PS)(F(n),e)}function me(n,e=O.Default){return"number"!=typeof e&&(e=0|(e.optional&&8)|(e.host&&1)|(e.self&&2)|(e.skipSelf&&4)),v(n,e)}function ud(n){const e=[];for(let t=0;t<n.length;t++){const i=F(n[t]);if(Array.isArray(i)){if(0===i.length)throw new _(900,!1);let r,a=O.Default;for(let o=0;o<i.length;o++){const s=i[o],l=LS(s);"number"==typeof l?-1===l?r=s.token:a|=l:r=s}e.push(v(r,a))}else e.push(v(i))}return e}function ka(n,e){return n[dd]=e,n.prototype[dd]=e,n}function LS(n){return n[dd]}const ei=ka(fr("Optional"),8),mr=ka(fr("SkipSelf"),4);var vt=(()=>((vt=vt||{})[vt.Important=1]="Important",vt[vt.DashCase=2]="DashCase",vt))();const md=new Map;let JS=0;const yd="__ngContext__";function tt(n,e){yt(e)?(n[yd]=e[20],function eC(n){md.set(n[20],n)}(e)):n[yd]=e}function wd(n,e){return undefined(n,e)}function Ra(n){const e=n[3];return Zt(e)?e[3]:e}function _d(n){return Ug(n[13])}function Dd(n){return Ug(n[4])}function Ug(n){for(;null!==n&&!Zt(n);)n=n[4];return n}function yr(n,e,t,i,r){if(null!=i){let a,o=!1;Zt(i)?a=i:yt(i)&&(o=!0,i=i[0]);const s=Oe(i);0===n&&null!==t?null==r?Xg(e,t,s):ki(e,t,s,r||null,!0):1===n&&null!==t?ki(e,t,s,r||null,!0):2===n?function Md(n,e,t){const i=us(n,e);i&&function DC(n,e,t,i){n.removeChild(e,t,i)}(n,i,e,t)}(e,s,o):3===n&&e.destroyNode(s),null!=a&&function xC(n,e,t,i,r){const a=t[7];a!==Oe(t)&&yr(e,n,i,a,r);for(let s=10;s<t.length;s++){const l=t[s];Pa(l[1],l,n,e,i,a)}}(e,n,a,t,r)}}function Cd(n,e,t){return n.createElement(e,t)}function qg(n,e){const t=n[9],i=t.indexOf(e),r=e[3];512&e[2]&&(e[2]&=-513,$c(r,-1)),t.splice(i,1)}function xd(n,e){if(n.length<=10)return;const t=10+e,i=n[t];if(i){const r=i[17];null!==r&&r!==n&&qg(r,i),e>0&&(n[t-1][4]=i[4]);const a=os(n,10+e);!function pC(n,e){Pa(n,e,e[U],2,null,null),e[0]=null,e[6]=null}(i[1],i);const o=a[19];null!==o&&o.detachView(a[1]),i[3]=null,i[4]=null,i[2]&=-65}return i}function Yg(n,e){if(!(128&e[2])){const t=e[U];t.destroyNode&&Pa(n,e,t,3,null,null),function bC(n){let e=n[13];if(!e)return Td(n[1],n);for(;e;){let t=null;if(yt(e))t=e[13];else{const i=e[10];i&&(t=i)}if(!t){for(;e&&!e[4]&&e!==n;)yt(e)&&Td(e[1],e),e=e[3];null===e&&(e=n),yt(e)&&Td(e[1],e),t=e&&e[4]}e=t}}(e)}}function Td(n,e){if(!(128&e[2])){e[2]&=-65,e[2]|=128,function _C(n,e){let t;if(null!=n&&null!=(t=n.destroyHooks))for(let i=0;i<t.length;i+=2){const r=e[t[i]];if(!(r instanceof ya)){const a=t[i+1];if(Array.isArray(a))for(let o=0;o<a.length;o+=2){const s=r[a[o]],l=a[o+1];try{l.call(s)}finally{}}else try{a.call(r)}finally{}}}}(n,e),function wC(n,e){const t=n.cleanup,i=e[7];let r=-1;if(null!==t)for(let a=0;a<t.length-1;a+=2)if("string"==typeof t[a]){const o=t[a+1],s="function"==typeof o?o(e):Oe(e[o]),l=i[r=t[a+2]],c=t[a+3];"boolean"==typeof c?s.removeEventListener(t[a],l,c):c>=0?i[r=c]():i[r=-c].unsubscribe(),a+=2}else{const o=i[r=t[a+1]];t[a].call(o)}if(null!==i){for(let a=r+1;a<i.length;a++)(0,i[a])();e[7]=null}}(n,e),1===e[1].type&&e[U].destroy();const t=e[17];if(null!==t&&Zt(e[3])){t!==e[3]&&qg(t,e);const i=e[19];null!==i&&i.detachView(n)}!function tC(n){md.delete(n[20])}(e)}}function Qg(n,e,t){return function Kg(n,e,t){let i=e;for(;null!==i&&40&i.type;)i=(e=i).parent;if(null===i)return t[0];if(2&i.flags){const r=n.data[i.directiveStart].encapsulation;if(r===Xt.None||r===Xt.Emulated)return null}return Tt(i,t)}(n,e.parent,t)}function ki(n,e,t,i,r){n.insertBefore(e,t,i,r)}function Xg(n,e,t){n.appendChild(e,t)}function Jg(n,e,t,i,r){null!==i?ki(n,e,t,i,r):Xg(n,e,t)}function us(n,e){return n.parentNode(e)}function Zg(n,e,t){return tm(n,e,t)}let Pd,tm=function em(n,e,t){return 40&n.type?Tt(n,t):null};function hs(n,e,t,i){const r=Qg(n,i,e),a=e[U],s=Zg(i.parent||e[6],i,e);if(null!=r)if(Array.isArray(t))for(let l=0;l<t.length;l++)Jg(a,r,t[l],s,!1);else Jg(a,r,t,s,!1)}function fs(n,e){if(null!==e){const t=e.type;if(3&t)return Tt(e,n);if(4&t)return kd(-1,n[e.index]);if(8&t){const i=e.child;if(null!==i)return fs(n,i);{const r=n[e.index];return Zt(r)?kd(-1,r):Oe(r)}}if(32&t)return wd(e,n)()||Oe(n[e.index]);{const i=im(n,e);return null!==i?Array.isArray(i)?i[0]:fs(Ra(n[16]),i):fs(n,e.next)}}return null}function im(n,e){return null!==e?n[16][6].projection[e.projection]:null}function kd(n,e){const t=10+n+1;if(t<e.length){const i=e[t],r=i[1].firstChild;if(null!==r)return fs(i,r)}return e[7]}function Id(n,e,t,i,r,a,o){for(;null!=t;){const s=i[t.index],l=t.type;if(o&&0===e&&(s&&tt(Oe(s),i),t.flags|=4),64!=(64&t.flags))if(8&l)Id(n,e,t.child,i,r,a,!1),yr(e,n,r,s,a);else if(32&l){const c=wd(t,i);let d;for(;d=c();)yr(e,n,r,d,a);yr(e,n,r,s,a)}else 16&l?rm(n,e,i,t,r,a):yr(e,n,r,s,a);t=o?t.projectionNext:t.next}}function Pa(n,e,t,i,r,a){Id(t,i,n.firstChild,e,r,a,!1)}function rm(n,e,t,i,r,a){const o=t[16],l=o[6].projection[i.projection];if(Array.isArray(l))for(let c=0;c<l.length;c++)yr(e,n,r,l[c],a);else Id(n,e,l,o[3],r,a,!0)}function am(n,e,t){n.setAttribute(e,"style",t)}function Ad(n,e,t){""===t?n.removeAttribute(e,"class"):n.setAttribute(e,"class",t)}class Ii{constructor(e){this.changingThisBreaksApplicationSecurity=e}toString(){return`SafeValue must use [property]=binding: ${this.changingThisBreaksApplicationSecurity} (see https://g.co/ng/security#xss)`}}function It(n){return n instanceof Ii?n.changingThisBreaksApplicationSecurity:n}const UC=/^(?:(?:https?|mailto|data|ftp|tel|file|sms):|[^&:/?#]*(?:[/?#]|$))/gi;var le=(()=>((le=le||{})[le.NONE=0]="NONE",le[le.HTML=1]="HTML",le[le.STYLE=2]="STYLE",le[le.SCRIPT=3]="SCRIPT",le[le.URL=4]="URL",le[le.RESOURCE_URL=5]="RESOURCE_URL",le))();function vr(n){const e=function Oa(){const n=w();return n&&n[12]}();return e?e.sanitize(le.URL,n)||"":function fn(n,e){const t=function NC(n){return n instanceof Ii&&n.getTypeName()||null}(n);if(null!=t&&t!==e){if("ResourceURL"===t&&"URL"===e)return!0;throw new Error(`Required a safe ${e}, got a ${t} (see https://g.co/ng/security#xss)`)}return t===e}(n,"URL")?It(n):function ms(n){return(n=String(n)).match(UC)?n:"unsafe:"+n}(z(n))}const jd=new E("ENVIRONMENT_INITIALIZER"),ym=new E("INJECTOR",-1),vm=new E("INJECTOR_DEF_TYPES");class wm{get(e,t=Ta){if(t===Ta){const i=new Error(`NullInjectorError: No provider for ${fe(e)}!`);throw i.name="NullInjectorError",i}return t}}function rx(...n){return{\u0275providers:_m(0,n)}}function _m(n,...e){const t=[],i=new Set;let r;return Rn(e,a=>{const o=a;Bd(o,t,[],i)&&(r||(r=[]),r.push(o))}),void 0!==r&&Dm(r,t),t}function Dm(n,e){for(let t=0;t<n.length;t++){const{providers:r}=n[t];Rn(r,a=>{e.push(a)})}}function Bd(n,e,t,i){if(!(n=F(n)))return!1;let r=null,a=Lp(n);const o=!a&&de(n);if(a||o){if(o&&!o.standalone)return!1;r=n}else{const l=n.ngModule;if(a=Lp(l),!a)return!1;r=l}const s=i.has(r);if(o){if(s)return!1;if(i.add(r),o.dependencies){const l="function"==typeof o.dependencies?o.dependencies():o.dependencies;for(const c of l)Bd(c,e,t,i)}}else{if(!a)return!1;{if(null!=a.imports&&!s){let c;i.add(r);try{Rn(a.imports,d=>{Bd(d,e,t,i)&&(c||(c=[]),c.push(d))})}finally{}void 0!==c&&Dm(c,e)}if(!s){const c=Ti(r)||(()=>new r);e.push({provide:r,useFactory:c,deps:se},{provide:vm,useValue:r,multi:!0},{provide:jd,useValue:()=>v(r),multi:!0})}const l=a.providers;null==l||s||Rn(l,d=>{e.push(d)})}}return r!==n&&void 0!==n.providers}const ax=he({provide:String,useValue:he});function Hd(n){return null!==n&&"object"==typeof n&&ax in n}function Ai(n){return"function"==typeof n}const zd=new E("Set Injector scope."),ys={},sx={};let Wd;function vs(){return void 0===Wd&&(Wd=new wm),Wd}class ti{}class xm extends ti{constructor(e,t,i,r){super(),this.parent=t,this.source=i,this.scopes=r,this.records=new Map,this._ngOnDestroyHooks=new Set,this._onDestroyHooks=[],this._destroyed=!1,Gd(e,o=>this.processProvider(o)),this.records.set(ym,wr(void 0,this)),r.has("environment")&&this.records.set(ti,wr(void 0,this));const a=this.records.get(zd);null!=a&&"string"==typeof a.value&&this.scopes.add(a.value),this.injectorDefTypes=new Set(this.get(vm.multi,se,O.Self))}get destroyed(){return this._destroyed}destroy(){this.assertNotDestroyed(),this._destroyed=!0;try{for(const e of this._ngOnDestroyHooks)e.ngOnDestroy();for(const e of this._onDestroyHooks)e()}finally{this.records.clear(),this._ngOnDestroyHooks.clear(),this.injectorDefTypes.clear(),this._onDestroyHooks.length=0}}onDestroy(e){this._onDestroyHooks.push(e)}runInContext(e){this.assertNotDestroyed();const t=gr(this),i=Ft(void 0);try{return e()}finally{gr(t),Ft(i)}}get(e,t=Ta,i=O.Default){this.assertNotDestroyed();const r=gr(this),a=Ft(void 0);try{if(!(i&O.SkipSelf)){let s=this.records.get(e);if(void 0===s){const l=function hx(n){return"function"==typeof n||"object"==typeof n&&n instanceof E}(e)&&zo(e);s=l&&this.injectableDefInScope(l)?wr(Vd(e),ys):null,this.records.set(e,s)}if(null!=s)return this.hydrate(e,s)}return(i&O.Self?vs():this.parent).get(e,t=i&O.Optional&&t===Ta?null:t)}catch(o){if("NullInjectorError"===o.name){if((o[ls]=o[ls]||[]).unshift(fe(e)),r)throw o;return function OS(n,e,t,i){const r=n[ls];throw e[Cg]&&r.unshift(e[Cg]),n.message=function NS(n,e,t,i=null){n=n&&"\n"===n.charAt(0)&&"\u0275"==n.charAt(1)?n.slice(2):n;let r=fe(e);if(Array.isArray(e))r=e.map(fe).join(" -> ");else if("object"==typeof e){let a=[];for(let o in e)if(e.hasOwnProperty(o)){let s=e[o];a.push(o+":"+("string"==typeof s?JSON.stringify(s):fe(s)))}r=`{${a.join(", ")}}`}return`${t}${i?"("+i+")":""}[${r}]: ${n.replace(AS,"\n  ")}`}("\n"+n.message,r,t,i),n.ngTokenPath=r,n[ls]=null,n}(o,e,"R3InjectorError",this.source)}throw o}finally{Ft(a),gr(r)}}resolveInjectorInitializers(){const e=gr(this),t=Ft(void 0);try{const i=this.get(jd.multi,se,O.Self);for(const r of i)r()}finally{gr(e),Ft(t)}}toString(){const e=[],t=this.records;for(const i of t.keys())e.push(fe(i));return`R3Injector[${e.join(", ")}]`}assertNotDestroyed(){if(this._destroyed)throw new _(205,!1)}processProvider(e){let t=Ai(e=F(e))?e:F(e&&e.provide);const i=function cx(n){return Hd(n)?wr(void 0,n.useValue):wr(function Tm(n,e,t){let i;if(Ai(n)){const r=F(n);return Ti(r)||Vd(r)}if(Hd(n))i=()=>F(n.useValue);else if(function Cm(n){return!(!n||!n.useFactory)}(n))i=()=>n.useFactory(...ud(n.deps||[]));else if(function Sm(n){return!(!n||!n.useExisting)}(n))i=()=>v(F(n.useExisting));else{const r=F(n&&(n.useClass||n.provide));if(!function dx(n){return!!n.deps}(n))return Ti(r)||Vd(r);i=()=>new r(...ud(n.deps))}return i}(n),ys)}(e);if(Ai(e)||!0!==e.multi)this.records.get(t);else{let r=this.records.get(t);r||(r=wr(void 0,ys,!0),r.factory=()=>ud(r.multi),this.records.set(t,r)),t=e,r.multi.push(e)}this.records.set(t,i)}hydrate(e,t){return t.value===ys&&(t.value=sx,t.value=t.factory()),"object"==typeof t.value&&t.value&&function ux(n){return null!==n&&"object"==typeof n&&"function"==typeof n.ngOnDestroy}(t.value)&&this._ngOnDestroyHooks.add(t.value),t.value}injectableDefInScope(e){if(!e.providedIn)return!1;const t=F(e.providedIn);return"string"==typeof t?"any"===t||this.scopes.has(t):this.injectorDefTypes.has(t)}}function Vd(n){const e=zo(n),t=null!==e?e.factory:Ti(n);if(null!==t)return t;if(n instanceof E)throw new _(204,!1);if(n instanceof Function)return function lx(n){const e=n.length;if(e>0)throw xa(e,"?"),new _(204,!1);const t=function mD(n){const e=n&&(n[Wo]||n[Op]);if(e){const t=function bD(n){if(n.hasOwnProperty("name"))return n.name;const e=(""+n).match(/^function\s*([^\s(]+)/);return null===e?"":e[1]}(n);return console.warn(`DEPRECATED: DI is instantiating a token "${t}" that inherits its @Injectable decorator but does not provide one itself.\nThis will become an error in a future version of Angular. Please add @Injectable() to the "${t}" class.`),e}return null}(n);return null!==t?()=>t.factory(n):()=>new n}(n);throw new _(204,!1)}function wr(n,e,t=!1){return{factory:n,value:e,multi:t?[]:void 0}}function fx(n){return!!n.\u0275providers}function Gd(n,e){for(const t of n)Array.isArray(t)?Gd(t,e):fx(t)?Gd(t.\u0275providers,e):e(t)}class Em{}class mx{resolveComponentFactory(e){throw function gx(n){const e=Error(`No component factory found for ${fe(n)}. Did you add it to @NgModule.entryComponents?`);return e.ngComponent=n,e}(e)}}let _r=(()=>{class n{}return n.NULL=new mx,n})();function bx(){return Dr(ze(),w())}function Dr(n,e){return new ht(Tt(n,e))}let ht=(()=>{class n{constructor(t){this.nativeElement=t}}return n.__NG_ELEMENT_ID__=bx,n})();function yx(n){return n instanceof ht?n.nativeElement:n}class Na{}let ws=(()=>{class n{}return n.__NG_ELEMENT_ID__=()=>function vx(){const n=w(),t=Et(ze().index,n);return(yt(t)?t:n)[U]}(),n})(),wx=(()=>{class n{}return n.\u0275prov=k({token:n,providedIn:"root",factory:()=>null}),n})();class Sr{constructor(e){this.full=e,this.major=e.split(".")[0],this.minor=e.split(".")[1],this.patch=e.split(".").slice(2).join(".")}}const _x=new Sr("14.3.0"),Ud={};function qd(n){return n.ngOriginalError}class ni{constructor(){this._console=console}handleError(e){const t=this._findOriginalError(e);this._console.error("ERROR",e),t&&this._console.error("ORIGINAL ERROR",t)}_findOriginalError(e){let t=e&&qd(e);for(;t&&qd(t);)t=qd(t);return t||null}}function Ln(n){return n instanceof Function?n():n}function Im(n,e,t){let i=n.length;for(;;){const r=n.indexOf(e,t);if(-1===r)return r;if(0===r||n.charCodeAt(r-1)<=32){const a=e.length;if(r+a===i||n.charCodeAt(r+a)<=32)return r}t=r+1}}const Am="ng-template";function Rx(n,e,t){let i=0;for(;i<n.length;){let r=n[i++];if(t&&"class"===r){if(r=n[i],-1!==Im(r.toLowerCase(),e,0))return!0}else if(1===r){for(;i<n.length&&"string"==typeof(r=n[i++]);)if(r.toLowerCase()===e)return!0;return!1}}return!1}function Rm(n){return 4===n.type&&n.value!==Am}function Px(n,e,t){return e===(4!==n.type||t?n.value:Am)}function Lx(n,e,t){let i=4;const r=n.attrs||[],a=function Fx(n){for(let e=0;e<n.length;e++)if(og(n[e]))return e;return n.length}(r);let o=!1;for(let s=0;s<e.length;s++){const l=e[s];if("number"!=typeof l){if(!o)if(4&i){if(i=2|1&i,""!==l&&!Px(n,l,t)||""===l&&1===e.length){if(tn(i))return!1;o=!0}}else{const c=8&i?l:e[++s];if(8&i&&null!==n.attrs){if(!Rx(n.attrs,c,t)){if(tn(i))return!1;o=!0}continue}const u=Ox(8&i?"class":l,r,Rm(n),t);if(-1===u){if(tn(i))return!1;o=!0;continue}if(""!==c){let h;h=u>a?"":r[u+1].toLowerCase();const f=8&i?h:null;if(f&&-1!==Im(f,c,0)||2&i&&c!==h){if(tn(i))return!1;o=!0}}}}else{if(!o&&!tn(i)&&!tn(l))return!1;if(o&&tn(l))continue;o=!1,i=l|1&i}}return tn(i)||o}function tn(n){return 0==(1&n)}function Ox(n,e,t,i){if(null===e)return-1;let r=0;if(i||!t){let a=!1;for(;r<e.length;){const o=e[r];if(o===n)return r;if(3===o||6===o)a=!0;else{if(1===o||2===o){let s=e[++r];for(;"string"==typeof s;)s=e[++r];continue}if(4===o)break;if(0===o){r+=4;continue}}r+=a?1:2}return-1}return function jx(n,e){let t=n.indexOf(4);if(t>-1)for(t++;t<n.length;){const i=n[t];if("number"==typeof i)return-1;if(i===e)return t;t++}return-1}(e,n)}function Pm(n,e,t=!1){for(let i=0;i<e.length;i++)if(Lx(n,e[i],t))return!0;return!1}function Bx(n,e){e:for(let t=0;t<e.length;t++){const i=e[t];if(n.length===i.length){for(let r=0;r<n.length;r++)if(n[r]!==i[r])continue e;return!0}}return!1}function Lm(n,e){return n?":not("+e.trim()+")":e}function Hx(n){let e=n[0],t=1,i=2,r="",a=!1;for(;t<n.length;){let o=n[t];if("string"==typeof o)if(2&i){const s=n[++t];r+="["+o+(s.length>0?'="'+s+'"':"")+"]"}else 8&i?r+="."+o:4&i&&(r+=" "+o);else""!==r&&!tn(o)&&(e+=Lm(a,r),r=""),i=o,a=a||!tn(i);t++}return""!==r&&(e+=Lm(a,r)),e}const W={};function ie(n){Om(ee(),w(),dt()+n,!1)}function Om(n,e,t,i){if(!i)if(3==(3&e[2])){const a=n.preOrderCheckHooks;null!==a&&Jo(e,a,t)}else{const a=n.preOrderHooks;null!==a&&Zo(e,a,0,t)}Jn(t)}function Bm(n,e=null,t=null,i){const r=Hm(n,e,t,i);return r.resolveInjectorInitializers(),r}function Hm(n,e=null,t=null,i,r=new Set){const a=[t||se,rx(n)];return i=i||("object"==typeof n?void 0:fe(n)),new xm(a,e||vs(),i||null,r)}let wt=(()=>{class n{static create(t,i){var r;if(Array.isArray(t))return Bm({name:""},i,t,"");{const a=null!==(r=t.name)&&void 0!==r?r:"";return Bm({name:a},t.parent,t.providers,a)}}}return n.THROW_IF_NOT_FOUND=Ta,n.NULL=new wm,n.\u0275prov=k({token:n,providedIn:"any",factory:()=>v(ym)}),n.__NG_ELEMENT_ID__=-1,n})();function C(n,e=O.Default){const t=w();return null===t?v(n,e):pg(ze(),t,F(n),e)}function Ds(){throw new Error("invalid")}function Ss(n,e){return n<<17|e<<2}function nn(n){return n>>17&32767}function Jd(n){return 2|n}function On(n){return(131068&n)>>2}function Zd(n,e){return-131069&n|e<<2}function eu(n){return 1|n}function ib(n,e){const t=n.contentQueries;if(null!==t)for(let i=0;i<t.length;i+=2){const r=t[i],a=t[i+1];if(-1!==a){const o=n.data[a];Xc(r),o.contentQueries(2,e[a],a)}}}function Ts(n,e,t,i,r,a,o,s,l,c,d){const u=e.blueprint.slice();return u[0]=r,u[2]=76|i,(null!==d||n&&1024&n[2])&&(u[2]|=1024),$p(u),u[3]=u[15]=n,u[8]=t,u[10]=o||n&&n[10],u[U]=s||n&&n[U],u[12]=l||n&&n[12]||null,u[9]=c||n&&n[9]||null,u[6]=a,u[20]=function ZS(){return JS++}(),u[21]=d,u[16]=2==e.type?n[16]:u,u}function Tr(n,e,t,i,r){let a=n.data[e];if(null===a)a=function lu(n,e,t,i,r){const a=Qp(),o=qc(),l=n.data[e]=function DT(n,e,t,i,r,a){return{type:t,index:i,insertBeforeIndex:null,injectorIndex:e?e.injectorIndex:-1,directiveStart:-1,directiveEnd:-1,directiveStylingLast:-1,propertyBindings:null,flags:0,providerIndexes:0,value:r,attrs:a,mergedAttrs:null,localNames:null,initialInputs:void 0,inputs:null,outputs:null,tViews:null,next:null,projectionNext:null,child:null,parent:e,projection:null,styles:null,stylesWithoutHost:null,residualStyles:void 0,classes:null,classesWithoutHost:null,residualClasses:void 0,classBindings:0,styleBindings:0}}(0,o?a:a&&a.parent,t,e,i,r);return null===n.firstChild&&(n.firstChild=l),null!==a&&(o?null==a.child&&null!==l.parent&&(a.child=l):null===a.next&&(a.next=l)),l}(n,e,t,i,r),function YD(){return B.lFrame.inI18n}()&&(a.flags|=64);else if(64&a.type){a.type=t,a.value=i,a.attrs=r;const o=function ba(){const n=B.lFrame,e=n.currentTNode;return n.isParent?e:e.parent}();a.injectorIndex=null===o?-1:o.injectorIndex}return un(a,!0),a}function Er(n,e,t,i){if(0===t)return-1;const r=e.length;for(let a=0;a<t;a++)e.push(i),n.blueprint.push(i),n.data.push(null);return r}function cu(n,e,t){Jc(e);try{const i=n.viewQuery;null!==i&&bu(1,i,t);const r=n.template;null!==r&&rb(n,e,r,1,t),n.firstCreatePass&&(n.firstCreatePass=!1),n.staticContentQueries&&ib(n,e),n.staticViewQueries&&bu(2,n.viewQuery,t);const a=n.components;null!==a&&function vT(n,e){for(let t=0;t<e.length;t++)jT(n,e[t])}(e,a)}catch(i){throw n.firstCreatePass&&(n.incompleteFirstPass=!0,n.firstCreatePass=!1),i}finally{e[2]&=-5,Zc()}}function Es(n,e,t,i){const r=e[2];if(128!=(128&r)){Jc(e);try{$p(e),function Xp(n){return B.lFrame.bindingIndex=n}(n.bindingStartIndex),null!==t&&rb(n,e,t,2,i);const o=3==(3&r);if(o){const c=n.preOrderCheckHooks;null!==c&&Jo(e,c,null)}else{const c=n.preOrderHooks;null!==c&&Zo(e,c,0,null),ed(e,0)}if(function NT(n){for(let e=_d(n);null!==e;e=Dd(e)){if(!e[2])continue;const t=e[9];for(let i=0;i<t.length;i++){const r=t[i],a=r[3];0==(512&r[2])&&$c(a,1),r[2]|=512}}}(e),function OT(n){for(let e=_d(n);null!==e;e=Dd(e))for(let t=10;t<e.length;t++){const i=e[t],r=i[1];Ko(i)&&Es(r,i,r.template,i[8])}}(e),null!==n.contentQueries&&ib(n,e),o){const c=n.contentCheckHooks;null!==c&&Jo(e,c)}else{const c=n.contentHooks;null!==c&&Zo(e,c,1),ed(e,1)}!function bT(n,e){const t=n.hostBindingOpCodes;if(null!==t)try{for(let i=0;i<t.length;i++){const r=t[i];if(r<0)Jn(~r);else{const a=r,o=t[++i],s=t[++i];QD(o,a),s(2,e[a])}}}finally{Jn(-1)}}(n,e);const s=n.components;null!==s&&function yT(n,e){for(let t=0;t<e.length;t++)FT(n,e[t])}(e,s);const l=n.viewQuery;if(null!==l&&bu(2,l,i),o){const c=n.viewCheckHooks;null!==c&&Jo(e,c)}else{const c=n.viewHooks;null!==c&&Zo(e,c,2),ed(e,2)}!0===n.firstUpdatePass&&(n.firstUpdatePass=!1),e[2]&=-41,512&e[2]&&(e[2]&=-513,$c(e[3],-1))}finally{Zc()}}}function rb(n,e,t,i,r){const a=dt(),o=2&i;try{Jn(-1),o&&e.length>22&&Om(n,e,22,!1),t(i,r)}finally{Jn(a)}}function ab(n,e,t){if(Hc(e)){const r=e.directiveEnd;for(let a=e.directiveStart;a<r;a++){const o=n.data[a];o.contentQueries&&o.contentQueries(1,t[a],a)}}}function du(n,e,t){!Yp()||(function ET(n,e,t,i){const r=t.directiveStart,a=t.directiveEnd;n.firstCreatePass||wa(t,e),tt(i,e);const o=t.initialInputs;for(let s=r;s<a;s++){const l=n.data[s],c=en(l);c&&RT(e,t,l);const d=_a(e,n,s,t);tt(d,e),null!==o&&PT(0,s-r,d,l,0,o),c&&(Et(t.index,e)[8]=d)}}(n,e,t,Tt(t,e)),128==(128&t.flags)&&function kT(n,e,t){const i=t.directiveStart,r=t.directiveEnd,a=t.index,o=function KD(){return B.lFrame.currentDirectiveIndex}();try{Jn(a);for(let s=i;s<r;s++){const l=n.data[s],c=e[s];Qc(s),(null!==l.hostBindings||0!==l.hostVars||null!==l.hostAttrs)&&hb(l,c)}}finally{Jn(-1),Qc(o)}}(n,e,t))}function uu(n,e,t=Tt){const i=e.localNames;if(null!==i){let r=e.index+1;for(let a=0;a<i.length;a+=2){const o=i[a+1],s=-1===o?t(e,n):n[o];n[r++]=s}}}function ob(n){const e=n.tView;return null===e||e.incompleteFirstPass?n.tView=hu(1,null,n.template,n.decls,n.vars,n.directiveDefs,n.pipeDefs,n.viewQuery,n.schemas,n.consts):e}function hu(n,e,t,i,r,a,o,s,l,c){const d=22+i,u=d+r,h=function wT(n,e){const t=[];for(let i=0;i<e;i++)t.push(i<n?null:W);return t}(d,u),f="function"==typeof c?c():c;return h[1]={type:n,blueprint:h,template:t,queries:null,viewQuery:s,declTNode:e,data:h.slice().fill(null,d),bindingStartIndex:d,expandoStartIndex:u,hostBindingOpCodes:null,firstCreatePass:!0,firstUpdatePass:!0,staticViewQueries:!1,staticContentQueries:!1,preOrderHooks:null,preOrderCheckHooks:null,contentHooks:null,contentCheckHooks:null,viewHooks:null,viewCheckHooks:null,destroyHooks:null,cleanup:null,contentQueries:null,components:null,directiveRegistry:"function"==typeof a?a():a,pipeRegistry:"function"==typeof o?o():o,firstChild:null,schemas:l,consts:f,incompleteFirstPass:!1}}function sb(n,e,t,i){const r=bb(e);null===t?r.push(i):(r.push(t),n.firstCreatePass&&yb(n).push(i,r.length-1))}function lb(n,e,t){for(let i in n)if(n.hasOwnProperty(i)){const r=n[i];(t=null===t?{}:t).hasOwnProperty(i)?t[i].push(e,r):t[i]=[e,r]}return t}function cb(n,e){const i=e.directiveEnd,r=n.data,a=e.attrs,o=[];let s=null,l=null;for(let c=e.directiveStart;c<i;c++){const d=r[c],u=d.inputs,h=null===a||Rm(e)?null:LT(u,a);o.push(h),s=lb(u,c,s),l=lb(d.outputs,c,l)}null!==s&&(s.hasOwnProperty("class")&&(e.flags|=16),s.hasOwnProperty("style")&&(e.flags|=32)),e.initialInputs=o,e.inputs=s,e.outputs=l}function db(n,e){const t=Et(e,n);16&t[2]||(t[2]|=32)}function fu(n,e,t,i){let r=!1;if(Yp()){const a=function MT(n,e,t){const i=n.directiveRegistry;let r=null;if(i)for(let a=0;a<i.length;a++){const o=i[a];Pm(t,o.selectors,!1)&&(r||(r=[]),rs(wa(t,e),n,o.type),en(o)?(fb(n,t),r.unshift(o)):r.push(o))}return r}(n,e,t),o=null===i?null:{"":-1};if(null!==a){r=!0,pb(t,n.data.length,a.length);for(let d=0;d<a.length;d++){const u=a[d];u.providersResolver&&u.providersResolver(u)}let s=!1,l=!1,c=Er(n,e,a.length,null);for(let d=0;d<a.length;d++){const u=a[d];t.mergedAttrs=ts(t.mergedAttrs,u.hostAttrs),gb(n,t,e,c,u),AT(c,u,o),null!==u.contentQueries&&(t.flags|=8),(null!==u.hostBindings||null!==u.hostAttrs||0!==u.hostVars)&&(t.flags|=128);const h=u.type.prototype;!s&&(h.ngOnChanges||h.ngOnInit||h.ngDoCheck)&&((n.preOrderHooks||(n.preOrderHooks=[])).push(t.index),s=!0),!l&&(h.ngOnChanges||h.ngDoCheck)&&((n.preOrderCheckHooks||(n.preOrderCheckHooks=[])).push(t.index),l=!0),c++}cb(n,t)}o&&function IT(n,e,t){if(e){const i=n.localNames=[];for(let r=0;r<e.length;r+=2){const a=t[e[r+1]];if(null==a)throw new _(-301,!1);i.push(e[r],a)}}}(t,i,o)}return t.mergedAttrs=ts(t.mergedAttrs,t.attrs),r}function ub(n,e,t,i,r,a){const o=a.hostBindings;if(o){let s=n.hostBindingOpCodes;null===s&&(s=n.hostBindingOpCodes=[]);const l=~e.index;(function TT(n){let e=n.length;for(;e>0;){const t=n[--e];if("number"==typeof t&&t<0)return t}return 0})(s)!=l&&s.push(l),s.push(i,r,o)}}function hb(n,e){null!==n.hostBindings&&n.hostBindings(1,e)}function fb(n,e){e.flags|=2,(n.components||(n.components=[])).push(e.index)}function AT(n,e,t){if(t){if(e.exportAs)for(let i=0;i<e.exportAs.length;i++)t[e.exportAs[i]]=n;en(e)&&(t[""]=n)}}function pb(n,e,t){n.flags|=1,n.directiveStart=e,n.directiveEnd=e+t,n.providerIndexes=e}function gb(n,e,t,i,r){n.data[i]=r;const a=r.factory||(r.factory=Ti(r.type)),o=new ya(a,en(r),C);n.blueprint[i]=o,t[i]=o,ub(n,e,0,i,Er(n,t,r.hostVars,W),r)}function RT(n,e,t){const i=Tt(e,n),r=ob(t),a=n[10],o=ks(n,Ts(n,r,null,t.onPush?32:16,i,e,a,a.createRenderer(i,t),null,null,null));n[e.index]=o}function pn(n,e,t,i,r,a){const o=Tt(n,e);!function pu(n,e,t,i,r,a,o){if(null==a)n.removeAttribute(e,r,t);else{const s=null==o?z(a):o(a,i||"",r);n.setAttribute(e,r,s,t)}}(e[U],o,a,n.value,t,i,r)}function PT(n,e,t,i,r,a){const o=a[e];if(null!==o){const s=i.setInput;for(let l=0;l<o.length;){const c=o[l++],d=o[l++],u=o[l++];null!==s?i.setInput(t,u,c,d):t[d]=u}}}function LT(n,e){let t=null,i=0;for(;i<e.length;){const r=e[i];if(0!==r)if(5!==r){if("number"==typeof r)break;n.hasOwnProperty(r)&&(null===t&&(t=[]),t.push(r,n[r],e[i+1])),i+=2}else i+=2;else i+=4}return t}function mb(n,e,t,i){return new Array(n,!0,!1,e,null,0,i,t,null,null)}function FT(n,e){const t=Et(e,n);if(Ko(t)){const i=t[1];48&t[2]?Es(i,t,i.template,t[8]):t[5]>0&&gu(t)}}function gu(n){for(let i=_d(n);null!==i;i=Dd(i))for(let r=10;r<i.length;r++){const a=i[r];if(Ko(a))if(512&a[2]){const o=a[1];Es(o,a,o.template,a[8])}else a[5]>0&&gu(a)}const t=n[1].components;if(null!==t)for(let i=0;i<t.length;i++){const r=Et(t[i],n);Ko(r)&&r[5]>0&&gu(r)}}function jT(n,e){const t=Et(e,n),i=t[1];(function BT(n,e){for(let t=e.length;t<n.blueprint.length;t++)e.push(n.blueprint[t])})(i,t),cu(i,t,t[8])}function ks(n,e){return n[13]?n[14][4]=e:n[13]=e,n[14]=e,e}function mu(n){for(;n;){n[2]|=32;const e=Ra(n);if(TD(n)&&!e)return n;n=e}return null}function Ms(n,e,t,i=!0){const r=e[10];r.begin&&r.begin();try{Es(n,e,n.template,t)}catch(o){throw i&&wb(e,o),o}finally{r.end&&r.end()}}function bu(n,e,t){Xc(0),e(n,t)}function bb(n){return n[7]||(n[7]=[])}function yb(n){return n.cleanup||(n.cleanup=[])}function wb(n,e){const t=n[9],i=t?t.get(ni,null):null;i&&i.handleError(e)}function yu(n,e,t,i,r){for(let a=0;a<t.length;){const o=t[a++],s=t[a++],l=e[o],c=n.data[o];null!==c.setInput?c.setInput(l,r,i,s):l[s]=r}}function Nn(n,e,t){const i=Qo(e,n);!function $g(n,e,t){n.setValue(e,t)}(n[U],i,t)}function Is(n,e,t){let i=t?n.styles:null,r=t?n.classes:null,a=0;if(null!==e)for(let o=0;o<e.length;o++){const s=e[o];"number"==typeof s?a=s:1==a?r=Ac(r,s):2==a&&(i=Ac(i,s+": "+e[++o]+";"))}t?n.styles=i:n.stylesWithoutHost=i,t?n.classes=r:n.classesWithoutHost=r}function As(n,e,t,i,r=!1){for(;null!==t;){const a=e[t.index];if(null!==a&&i.push(Oe(a)),Zt(a))for(let s=10;s<a.length;s++){const l=a[s],c=l[1].firstChild;null!==c&&As(l[1],l,c,i)}const o=t.type;if(8&o)As(n,e,t.child,i);else if(32&o){const s=wd(t,e);let l;for(;l=s();)i.push(l)}else if(16&o){const s=im(e,t);if(Array.isArray(s))i.push(...s);else{const l=Ra(e[16]);As(l[1],l,s,i,!0)}}t=r?t.projectionNext:t.next}return i}class Fa{constructor(e,t){this._lView=e,this._cdRefInjectingView=t,this._appRef=null,this._attachedToViewContainer=!1}get rootNodes(){const e=this._lView,t=e[1];return As(t,e,t.firstChild,[])}get context(){return this._lView[8]}set context(e){this._lView[8]=e}get destroyed(){return 128==(128&this._lView[2])}destroy(){if(this._appRef)this._appRef.detachView(this);else if(this._attachedToViewContainer){const e=this._lView[3];if(Zt(e)){const t=e[8],i=t?t.indexOf(this):-1;i>-1&&(xd(e,i),os(t,i))}this._attachedToViewContainer=!1}Yg(this._lView[1],this._lView)}onDestroy(e){sb(this._lView[1],this._lView,null,e)}markForCheck(){mu(this._cdRefInjectingView||this._lView)}detach(){this._lView[2]&=-65}reattach(){this._lView[2]|=64}detectChanges(){Ms(this._lView[1],this._lView,this.context)}checkNoChanges(){}attachToViewContainerRef(){if(this._appRef)throw new _(902,!1);this._attachedToViewContainer=!0}detachFromAppRef(){this._appRef=null,function mC(n,e){Pa(n,e,e[U],2,null,null)}(this._lView[1],this._lView)}attachToAppRef(e){if(this._attachedToViewContainer)throw new _(902,!1);this._appRef=e}}class HT extends Fa{constructor(e){super(e),this._view=e}detectChanges(){const e=this._view;Ms(e[1],e,e[8],!1)}checkNoChanges(){}get context(){return null}}class vu extends _r{constructor(e){super(),this.ngModule=e}resolveComponentFactory(e){const t=de(e);return new ja(t,this.ngModule)}}function _b(n){const e=[];for(let t in n)n.hasOwnProperty(t)&&e.push({propName:n[t],templateName:t});return e}class WT{constructor(e,t){this.injector=e,this.parentInjector=t}get(e,t,i){const r=this.injector.get(e,Ud,i);return r!==Ud||t===Ud?r:this.parentInjector.get(e,t,i)}}class ja extends Em{constructor(e,t){super(),this.componentDef=e,this.ngModule=t,this.componentType=e.type,this.selector=function zx(n){return n.map(Hx).join(",")}(e.selectors),this.ngContentSelectors=e.ngContentSelectors?e.ngContentSelectors:[],this.isBoundToModule=!!t}get inputs(){return _b(this.componentDef.inputs)}get outputs(){return _b(this.componentDef.outputs)}create(e,t,i,r){let a=(r=r||this.ngModule)instanceof ti?r:null==r?void 0:r.injector;a&&null!==this.componentDef.getStandaloneInjector&&(a=this.componentDef.getStandaloneInjector(a)||a);const o=a?new WT(e,a):e,s=o.get(Na,null);if(null===s)throw new _(407,!1);const l=o.get(wx,null),c=s.createRenderer(null,this.componentDef),d=this.componentDef.selectors[0][0]||"div",u=i?function _T(n,e,t){return n.selectRootElement(e,t===Xt.ShadowDom)}(c,i,this.componentDef.encapsulation):Cd(c,d,function zT(n){const e=n.toLowerCase();return"svg"===e?"svg":"math"===e?"math":null}(d)),h=this.componentDef.onPush?288:272,f=hu(0,null,null,1,0,null,null,null,null,null),p=Ts(null,f,null,h,null,null,s,c,l,o,null);let g,m;Jc(p);try{const b=function UT(n,e,t,i,r,a){const o=t[1];t[22]=n;const l=Tr(o,22,2,"#host",null),c=l.mergedAttrs=e.hostAttrs;null!==c&&(Is(l,c,!0),null!==n&&(es(r,n,c),null!==l.classes&&Ad(r,n,l.classes),null!==l.styles&&am(r,n,l.styles)));const d=i.createRenderer(n,e),u=Ts(t,ob(e),null,e.onPush?32:16,t[22],l,i,d,a||null,null,null);return o.firstCreatePass&&(rs(wa(l,t),o,e.type),fb(o,l),pb(l,t.length,1)),ks(t,u),t[22]=u}(u,this.componentDef,p,s,c);if(u)if(i)es(c,u,["ng-version",_x.full]);else{const{attrs:D,classes:y}=function Wx(n){const e=[],t=[];let i=1,r=2;for(;i<n.length;){let a=n[i];if("string"==typeof a)2===r?""!==a&&e.push(a,n[++i]):8===r&&t.push(a);else{if(!tn(r))break;r=a}i++}return{attrs:e,classes:t}}(this.componentDef.selectors[0]);D&&es(c,u,D),y&&y.length>0&&Ad(c,u,y.join(" "))}if(m=Uc(f,22),void 0!==t){const D=m.projection=[];for(let y=0;y<this.ngContentSelectors.length;y++){const S=t[y];D.push(null!=S?Array.from(S):null)}}g=function $T(n,e,t,i){const r=t[1],a=function xT(n,e,t){const i=ze();n.firstCreatePass&&(t.providersResolver&&t.providersResolver(t),gb(n,i,e,Er(n,e,1,null),t),cb(n,i));const r=_a(e,n,i.directiveStart,i);tt(r,e);const a=Tt(i,e);return a&&tt(a,e),r}(r,t,e);if(n[8]=t[8]=a,null!==i)for(const s of i)s(a,e);if(e.contentQueries){const s=ze();e.contentQueries(1,a,s.directiveStart)}const o=ze();return!r.firstCreatePass||null===e.hostBindings&&null===e.hostAttrs||(Jn(o.index),ub(t[1],o,0,o.directiveStart,o.directiveEnd,e),hb(e,a)),a}(b,this.componentDef,p,[qT]),cu(f,p,null)}finally{Zc()}return new GT(this.componentType,g,Dr(m,p),p,m)}}class GT extends class px{}{constructor(e,t,i,r,a){super(),this.location=i,this._rootLView=r,this._tNode=a,this.instance=t,this.hostView=this.changeDetectorRef=new HT(r),this.componentType=e}setInput(e,t){const i=this._tNode.inputs;let r;if(null!==i&&(r=i[e])){const a=this._rootLView;yu(a[1],a,r,e,t),db(a,this._tNode.index)}}get injector(){return new cr(this._tNode,this._rootLView)}destroy(){this.hostView.destroy()}onDestroy(e){this.hostView.onDestroy(e)}}function qT(){const n=ze();Xo(w()[1],n)}function kr(n){let e=function Db(n){return Object.getPrototypeOf(n.prototype).constructor}(n.type),t=!0;const i=[n];for(;e;){let r;if(en(n))r=e.\u0275cmp||e.\u0275dir;else{if(e.\u0275cmp)throw new _(903,!1);r=e.\u0275dir}if(r){if(t){i.push(r);const o=n;o.inputs=wu(n.inputs),o.declaredInputs=wu(n.declaredInputs),o.outputs=wu(n.outputs);const s=r.hostBindings;s&&XT(n,s);const l=r.viewQuery,c=r.contentQueries;if(l&&QT(n,l),c&&KT(n,c),Ic(n.inputs,r.inputs),Ic(n.declaredInputs,r.declaredInputs),Ic(n.outputs,r.outputs),en(r)&&r.data.animation){const d=n.data;d.animation=(d.animation||[]).concat(r.data.animation)}}const a=r.features;if(a)for(let o=0;o<a.length;o++){const s=a[o];s&&s.ngInherit&&s(n),s===kr&&(t=!1)}}e=Object.getPrototypeOf(e)}!function YT(n){let e=0,t=null;for(let i=n.length-1;i>=0;i--){const r=n[i];r.hostVars=e+=r.hostVars,r.hostAttrs=ts(r.hostAttrs,t=ts(t,r.hostAttrs))}}(i)}function wu(n){return n===er?{}:n===se?[]:n}function QT(n,e){const t=n.viewQuery;n.viewQuery=t?(i,r)=>{e(i,r),t(i,r)}:e}function KT(n,e){const t=n.contentQueries;n.contentQueries=t?(i,r,a)=>{e(i,r,a),t(i,r,a)}:e}function XT(n,e){const t=n.hostBindings;n.hostBindings=t?(i,r)=>{e(i,r),t(i,r)}:e}let Rs=null;function Ri(){if(!Rs){const n=pe.Symbol;if(n&&n.iterator)Rs=n.iterator;else{const e=Object.getOwnPropertyNames(Map.prototype);for(let t=0;t<e.length;++t){const i=e[t];"entries"!==i&&"size"!==i&&Map.prototype[i]===Map.prototype.entries&&(Rs=i)}}}return Rs}function Ba(n){return!!_u(n)&&(Array.isArray(n)||!(n instanceof Map)&&Ri()in n)}function _u(n){return null!==n&&("function"==typeof n||"object"==typeof n)}function nt(n,e,t){return!Object.is(n[e],t)&&(n[e]=t,!0)}function za(n,e,t,i){const r=w();return nt(r,or(),e)&&(ee(),pn(De(),r,n,e,t,i)),za}function Ar(n,e,t,i,r,a){const s=function Pi(n,e,t,i){const r=nt(n,e,t);return nt(n,e+1,i)||r}(n,function In(){return B.lFrame.bindingIndex}(),t,r);return An(2),s?e+z(t)+i+z(r)+a:W}function Ne(n,e,t,i,r,a,o,s){const l=w(),c=ee(),d=n+22,u=c.firstCreatePass?function aE(n,e,t,i,r,a,o,s,l){const c=e.consts,d=Tr(e,n,4,o||null,Xn(c,s));fu(e,t,d,Xn(c,l)),Xo(e,d);const u=d.tViews=hu(2,d,i,r,a,e.directiveRegistry,e.pipeRegistry,null,e.schemas,c);return null!==e.queries&&(e.queries.template(e,d),u.queries=e.queries.embeddedTView(d)),d}(d,c,l,e,t,i,r,a,o):c.data[d];un(u,!1);const h=l[U].createComment("");hs(c,l,h,u),tt(h,l),ks(l,l[d]=mb(h,l,h,u)),Yo(u)&&du(c,l,u),null!=o&&uu(l,u,s)}function re(n,e,t){const i=w();return nt(i,or(),e)&&function At(n,e,t,i,r,a,o,s){const l=Tt(e,t);let d,c=e.inputs;!s&&null!=c&&(d=c[i])?(yu(n,t,d,i,r),qo(e)&&db(t,e.index)):3&e.type&&(i=function ST(n){return"class"===n?"className":"for"===n?"htmlFor":"formaction"===n?"formAction":"innerHtml"===n?"innerHTML":"readonly"===n?"readOnly":"tabindex"===n?"tabIndex":n}(i),r=null!=o?o(r,e.value||"",i):r,a.setProperty(l,i,r))}(ee(),De(),i,n,e,i[U],t,!1),re}function Du(n,e,t,i,r){const o=r?"class":"style";yu(n,t,e.inputs[o],o,i)}function Y(n,e,t,i){const r=w(),a=ee(),o=22+n,s=r[U],l=r[o]=Cd(s,e,function rS(){return B.lFrame.currentNamespace}()),c=a.firstCreatePass?function lE(n,e,t,i,r,a,o){const s=e.consts,c=Tr(e,n,2,r,Xn(s,a));return fu(e,t,c,Xn(s,o)),null!==c.attrs&&Is(c,c.attrs,!1),null!==c.mergedAttrs&&Is(c,c.mergedAttrs,!0),null!==e.queries&&e.queries.elementStart(e,c),c}(o,a,r,0,e,t,i):a.data[o];un(c,!0);const d=c.mergedAttrs;null!==d&&es(s,l,d);const u=c.classes;null!==u&&Ad(s,l,u);const h=c.styles;return null!==h&&am(s,l,h),64!=(64&c.flags)&&hs(a,r,l,c),0===function HD(){return B.lFrame.elementDepthCount}()&&tt(l,r),function zD(){B.lFrame.elementDepthCount++}(),Yo(c)&&(du(a,r,c),ab(a,c,r)),null!==i&&uu(r,c),Y}function G(){let n=ze();qc()?Yc():(n=n.parent,un(n,!1));const e=n;!function WD(){B.lFrame.elementDepthCount--}();const t=ee();return t.firstCreatePass&&(Xo(t,n),Hc(n)&&t.queries.elementEnd(n)),null!=e.classesWithoutHost&&function cS(n){return 0!=(16&n.flags)}(e)&&Du(t,e,w(),e.classesWithoutHost,!0),null!=e.stylesWithoutHost&&function dS(n){return 0!=(32&n.flags)}(e)&&Du(t,e,w(),e.stylesWithoutHost,!1),G}function zt(n,e,t,i){return Y(n,e,t,i),G(),zt}function Wa(n,e,t){const i=w(),r=ee(),a=n+22,o=r.firstCreatePass?function cE(n,e,t,i,r){const a=e.consts,o=Xn(a,i),s=Tr(e,n,8,"ng-container",o);return null!==o&&Is(s,o,!0),fu(e,t,s,Xn(a,r)),null!==e.queries&&e.queries.elementStart(e,s),s}(a,r,i,e,t):r.data[a];un(o,!0);const s=i[a]=i[U].createComment("");return hs(r,i,s,o),tt(s,i),Yo(o)&&(du(r,i,o),ab(r,o,i)),null!=t&&uu(i,o),Wa}function Va(){let n=ze();const e=ee();return qc()?Yc():(n=n.parent,un(n,!1)),e.firstCreatePass&&(Xo(e,n),Hc(n)&&e.queries.elementEnd(n)),Va}function Ls(n){return!!n&&"function"==typeof n.then}function Pb(n){return!!n&&"function"==typeof n.subscribe}const Lb=Pb;function jr(n,e,t,i){const r=w(),a=ee(),o=ze();return function Nb(n,e,t,i,r,a,o,s){const l=Yo(i),d=n.firstCreatePass&&yb(n),u=e[8],h=bb(e);let f=!0;if(3&i.type||s){const m=Tt(i,e),b=s?s(m):m,D=h.length,y=s?j=>s(Oe(j[i.index])):i.index;let S=null;if(!s&&l&&(S=function uE(n,e,t,i){const r=n.cleanup;if(null!=r)for(let a=0;a<r.length-1;a+=2){const o=r[a];if(o===t&&r[a+1]===i){const s=e[7],l=r[a+2];return s.length>l?s[l]:null}"string"==typeof o&&(a+=2)}return null}(n,e,r,i.index)),null!==S)(S.__ngLastListenerFn__||S).__ngNextListenerFn__=a,S.__ngLastListenerFn__=a,f=!1;else{a=jb(i,e,u,a,!1);const j=t.listen(b,r,a);h.push(a,j),d&&d.push(r,y,D,D+1)}}else a=jb(i,e,u,a,!1);const p=i.outputs;let g;if(f&&null!==p&&(g=p[r])){const m=g.length;if(m)for(let b=0;b<m;b+=2){const ne=e[g[b]][g[b+1]].subscribe(a),Me=h.length;h.push(a,ne),d&&d.push(r,i.index,Me,-(Me+1))}}}(a,r,r[U],o,n,e,0,i),jr}function Fb(n,e,t,i){try{return!1!==t(i)}catch(r){return wb(n,r),!1}}function jb(n,e,t,i,r){return function a(o){if(o===Function)return i;mu(2&n.flags?Et(n.index,e):e);let l=Fb(e,0,i,o),c=a.__ngNextListenerFn__;for(;c;)l=Fb(e,0,c,o)&&l,c=c.__ngNextListenerFn__;return r&&!1===l&&(o.preventDefault(),o.returnValue=!1),l}}function Qe(n=1){return function JD(n){return(B.lFrame.contextLView=function ZD(n,e){for(;n>0;)e=e[15],n--;return e}(n,B.lFrame.contextLView))[8]}(n)}function hE(n,e){let t=null;const i=function Nx(n){const e=n.attrs;if(null!=e){const t=e.indexOf(5);if(0==(1&t))return e[t+1]}return null}(n);for(let r=0;r<e.length;r++){const a=e[r];if("*"!==a){if(null===i?Pm(n,a,!0):Bx(i,a))return r}else t=r}return t}function Su(n){const e=w()[16][6];if(!e.projection){const i=e.projection=xa(n?n.length:1,null),r=i.slice();let a=e.child;for(;null!==a;){const o=n?hE(a,n):0;null!==o&&(r[o]?r[o].projectionNext=a:i[o]=a,r[o]=a),a=a.next}}}function Os(n,e=0,t){const i=w(),r=ee(),a=Tr(r,22+n,16,null,t||null);null===a.projection&&(a.projection=e),Yc(),64!=(64&a.flags)&&function CC(n,e,t){rm(e[U],0,e,t,Qg(n,t,e),Zg(t.parent||e[6],t,e))}(r,i,a)}function Yb(n,e,t,i,r){const a=n[t+1],o=null===e;let s=i?nn(a):On(a),l=!1;for(;0!==s&&(!1===l||o);){const d=n[s+1];gE(n[s],e)&&(l=!0,n[s+1]=i?eu(d):Jd(d)),s=i?nn(d):On(d)}l&&(n[t+1]=i?Jd(a):eu(a))}function gE(n,e){return null===n||null==e||(Array.isArray(n)?n[1]:n)===e||!(!Array.isArray(n)||"string"!=typeof e)&&pr(n,e)>=0}function xu(n,e,t){return rn(n,e,t,!1),xu}function Li(n,e){return rn(n,e,null,!0),Li}function rn(n,e,t,i){const r=w(),a=ee(),o=An(2);a.firstUpdatePass&&function ny(n,e,t,i){const r=n.data;if(null===r[t+1]){const a=r[dt()],o=function ty(n,e){return e>=n.expandoStartIndex}(n,t);(function oy(n,e){return 0!=(n.flags&(e?16:32))})(a,i)&&null===e&&!o&&(e=!1),e=function CE(n,e,t,i){const r=function Kc(n){const e=B.lFrame.currentDirectiveIndex;return-1===e?null:n[e]}(n);let a=i?e.residualClasses:e.residualStyles;if(null===r)0===(i?e.classBindings:e.styleBindings)&&(t=Ga(t=Tu(null,n,e,t,i),e.attrs,i),a=null);else{const o=e.directiveStylingLast;if(-1===o||n[o]!==r)if(t=Tu(r,n,e,t,i),null===a){let l=function xE(n,e,t){const i=t?e.classBindings:e.styleBindings;if(0!==On(i))return n[nn(i)]}(n,e,i);void 0!==l&&Array.isArray(l)&&(l=Tu(null,n,e,l[1],i),l=Ga(l,e.attrs,i),function TE(n,e,t,i){n[nn(t?e.classBindings:e.styleBindings)]=i}(n,e,i,l))}else a=function EE(n,e,t){let i;const r=e.directiveEnd;for(let a=1+e.directiveStylingLast;a<r;a++)i=Ga(i,n[a].hostAttrs,t);return Ga(i,e.attrs,t)}(n,e,i)}return void 0!==a&&(i?e.residualClasses=a:e.residualStyles=a),t}(r,a,e,i),function fE(n,e,t,i,r,a){let o=a?e.classBindings:e.styleBindings,s=nn(o),l=On(o);n[i]=t;let d,c=!1;if(Array.isArray(t)){const u=t;d=u[1],(null===d||pr(u,d)>0)&&(c=!0)}else d=t;if(r)if(0!==l){const h=nn(n[s+1]);n[i+1]=Ss(h,s),0!==h&&(n[h+1]=Zd(n[h+1],i)),n[s+1]=function sT(n,e){return 131071&n|e<<17}(n[s+1],i)}else n[i+1]=Ss(s,0),0!==s&&(n[s+1]=Zd(n[s+1],i)),s=i;else n[i+1]=Ss(l,0),0===s?s=i:n[l+1]=Zd(n[l+1],i),l=i;c&&(n[i+1]=Jd(n[i+1])),Yb(n,d,i,!0),Yb(n,d,i,!1),function pE(n,e,t,i,r){const a=r?n.residualClasses:n.residualStyles;null!=a&&"string"==typeof e&&pr(a,e)>=0&&(t[i+1]=eu(t[i+1]))}(e,d,n,i,a),o=Ss(s,l),a?e.classBindings=o:e.styleBindings=o}(r,a,e,t,o,i)}}(a,n,o,i),e!==W&&nt(r,o,e)&&function ry(n,e,t,i,r,a,o,s){if(!(3&e.type))return;const l=n.data,c=l[s+1];Ns(function Qm(n){return 1==(1&n)}(c)?ay(l,e,t,r,On(c),o):void 0)||(Ns(a)||function Ym(n){return 2==(2&n)}(c)&&(a=ay(l,null,t,r,s,o)),function TC(n,e,t,i,r){if(e)r?n.addClass(t,i):n.removeClass(t,i);else{let a=-1===i.indexOf("-")?void 0:vt.DashCase;null==r?n.removeStyle(t,i,a):("string"==typeof r&&r.endsWith("!important")&&(r=r.slice(0,-10),a|=vt.Important),n.setStyle(t,i,r,a))}}(i,o,Qo(dt(),t),r,a))}(a,a.data[dt()],r,r[U],n,r[o+1]=function IE(n,e){return null==n||("string"==typeof e?n+=e:"object"==typeof n&&(n=fe(It(n)))),n}(e,t),i,o)}function Tu(n,e,t,i,r){let a=null;const o=t.directiveEnd;let s=t.directiveStylingLast;for(-1===s?s=t.directiveStart:s++;s<o&&(a=e[s],i=Ga(i,a.hostAttrs,r),a!==n);)s++;return null!==n&&(t.directiveStylingLast=s),i}function Ga(n,e,t){const i=t?1:2;let r=-1;if(null!==e)for(let a=0;a<e.length;a++){const o=e[a];"number"==typeof o?r=o:r===i&&(Array.isArray(n)||(n=void 0===n?[]:["",n]),Mt(n,o,!!t||e[++a]))}return void 0===n?null:n}function ay(n,e,t,i,r,a){const o=null===e;let s;for(;r>0;){const l=n[r],c=Array.isArray(l),d=c?l[1]:l,u=null===d;let h=t[r+1];h===W&&(h=u?se:void 0);let f=u?ld(h,i):d===i?h:void 0;if(c&&!Ns(f)&&(f=ld(l,i)),Ns(f)&&(s=f,o))return s;const p=n[r+1];r=o?nn(p):On(p)}if(null!==e){let l=a?e.residualClasses:e.residualStyles;null!=l&&(s=ld(l,i))}return s}function Ns(n){return void 0!==n}function ue(n,e=""){const t=w(),i=ee(),r=n+22,a=i.firstCreatePass?Tr(i,r,1,e,null):i.data[r],o=t[r]=function Sd(n,e){return n.createText(e)}(t[U],e);hs(i,t,o,a),un(a,!1)}function Rt(n){return Eu("",n,""),Rt}function Eu(n,e,t){const i=w(),r=function Ir(n,e,t,i){return nt(n,or(),t)?e+z(t)+i:W}(i,n,e,t);return r!==W&&Nn(i,dt(),r),Eu}function ku(n,e,t,i,r){const a=w(),o=Ar(a,n,e,t,i,r);return o!==W&&Nn(a,dt(),o),ku}const zr="en-US";let Ey=zr;class Ni{}class Zy{}class ev extends Ni{constructor(e,t){super(),this._parent=t,this._bootstrapComponents=[],this.destroyCbs=[],this.componentFactoryResolver=new vu(this);const i=Ct(e);this._bootstrapComponents=Ln(i.bootstrap),this._r3Injector=Hm(e,t,[{provide:Ni,useValue:this},{provide:_r,useValue:this.componentFactoryResolver}],fe(e),new Set(["environment"])),this._r3Injector.resolveInjectorInitializers(),this.instance=this._r3Injector.get(e)}get injector(){return this._r3Injector}destroy(){const e=this._r3Injector;!e.destroyed&&e.destroy(),this.destroyCbs.forEach(t=>t()),this.destroyCbs=null}onDestroy(e){this.destroyCbs.push(e)}}class Ou extends Zy{constructor(e){super(),this.moduleType=e}create(e){return new ev(this.moduleType,e)}}class Xk extends Ni{constructor(e,t,i){super(),this.componentFactoryResolver=new vu(this),this.instance=null;const r=new xm([...e,{provide:Ni,useValue:this},{provide:_r,useValue:this.componentFactoryResolver}],t||vs(),i,new Set(["environment"]));this.injector=r,r.resolveInjectorInitializers()}destroy(){this.injector.destroy()}onDestroy(e){this.injector.onDestroy(e)}}function zs(n,e,t=null){return new Xk(n,e,t).injector}let Jk=(()=>{class n{constructor(t){this._injector=t,this.cachedInjectors=new Map}getOrCreateStandaloneInjector(t){if(!t.standalone)return null;if(!this.cachedInjectors.has(t.id)){const i=_m(0,t.type),r=i.length>0?zs([i],this._injector,`Standalone[${t.type.name}]`):null;this.cachedInjectors.set(t.id,r)}return this.cachedInjectors.get(t.id)}ngOnDestroy(){try{for(const t of this.cachedInjectors.values())null!==t&&t.destroy()}finally{this.cachedInjectors.clear()}}}return n.\u0275prov=k({token:n,providedIn:"environment",factory:()=>new n(v(ti))}),n})();function tv(n){n.getStandaloneInjector=e=>e.get(Jk).getOrCreateStandaloneInjector(n)}function cv(n,e,t,i,r,a){const o=e+t;return nt(n,o,r)?function gn(n,e,t){return n[e]=t}(n,o+1,a?i.call(a,r):i(r)):function Ka(n,e){const t=n[e];return t===W?void 0:t}(n,o+1)}function Fu(n){return e=>{setTimeout(n,void 0,e)}}const it=class _M extends _e{constructor(e=!1){super(),this.__isAsync=e}emit(e){super.next(e)}subscribe(e,t,i){var r,a,o;let s=e,l=t||(()=>null),c=i;if(e&&"object"==typeof e){const u=e;s=null===(r=u.next)||void 0===r?void 0:r.bind(u),l=null===(a=u.error)||void 0===a?void 0:a.bind(u),c=null===(o=u.complete)||void 0===o?void 0:o.bind(u)}this.__isAsync&&(l=Fu(l),s&&(s=Fu(s)),c&&(c=Fu(c)));const d=super.subscribe({next:s,error:l,complete:c});return e instanceof Je&&e.add(d),d}};function DM(){return this._results[Ri()]()}class ju{constructor(e=!1){this._emitDistinctChangesOnly=e,this.dirty=!0,this._results=[],this._changesDetected=!1,this._changes=null,this.length=0,this.first=void 0,this.last=void 0;const t=Ri(),i=ju.prototype;i[t]||(i[t]=DM)}get changes(){return this._changes||(this._changes=new it)}get(e){return this._results[e]}map(e){return this._results.map(e)}filter(e){return this._results.filter(e)}find(e){return this._results.find(e)}reduce(e,t){return this._results.reduce(e,t)}forEach(e){this._results.forEach(e)}some(e){return this._results.some(e)}toArray(){return this._results.slice()}toString(){return this._results.toString()}reset(e,t){const i=this;i.dirty=!1;const r=kt(e);(this._changesDetected=!function _S(n,e,t){if(n.length!==e.length)return!1;for(let i=0;i<n.length;i++){let r=n[i],a=e[i];if(t&&(r=t(r),a=t(a)),a!==r)return!1}return!0}(i._results,r,t))&&(i._results=r,i.length=r.length,i.last=r[this.length-1],i.first=r[0])}notifyOnChanges(){this._changes&&(this._changesDetected||!this._emitDistinctChangesOnly)&&this._changes.emit(this)}setDirty(){this.dirty=!0}destroy(){this.changes.complete(),this.changes.unsubscribe()}}let Fn=(()=>{class n{}return n.__NG_ELEMENT_ID__=xM,n})();const SM=Fn,CM=class extends SM{constructor(e,t,i){super(),this._declarationLView=e,this._declarationTContainer=t,this.elementRef=i}createEmbeddedView(e,t){const i=this._declarationTContainer.tViews,r=Ts(this._declarationLView,i,e,16,null,i.declTNode,null,null,null,null,t||null);r[17]=this._declarationLView[this._declarationTContainer.index];const o=this._declarationLView[19];return null!==o&&(r[19]=o.createEmbeddedView(i)),cu(i,r,e),new Fa(r)}};function xM(){return Ws(ze(),w())}function Ws(n,e){return 4&n.type?new CM(e,n,Dr(n,e)):null}let Wt=(()=>{class n{}return n.__NG_ELEMENT_ID__=TM,n})();function TM(){return yv(ze(),w())}const EM=Wt,mv=class extends EM{constructor(e,t,i){super(),this._lContainer=e,this._hostTNode=t,this._hostLView=i}get element(){return Dr(this._hostTNode,this._hostLView)}get injector(){return new cr(this._hostTNode,this._hostLView)}get parentInjector(){const e=is(this._hostTNode,this._hostLView);if(cg(e)){const t=lr(e,this._hostLView),i=sr(e);return new cr(t[1].data[i+8],t)}return new cr(null,this._hostLView)}clear(){for(;this.length>0;)this.remove(this.length-1)}get(e){const t=bv(this._lContainer);return null!==t&&t[e]||null}get length(){return this._lContainer.length-10}createEmbeddedView(e,t,i){let r,a;"number"==typeof i?r=i:null!=i&&(r=i.index,a=i.injector);const o=e.createEmbeddedView(t||{},a);return this.insert(o,r),o}createComponent(e,t,i,r,a){const o=e&&!function Ca(n){return"function"==typeof n}(e);let s;if(o)s=t;else{const u=t||{};s=u.index,i=u.injector,r=u.projectableNodes,a=u.environmentInjector||u.ngModuleRef}const l=o?e:new ja(de(e)),c=i||this.parentInjector;if(!a&&null==l.ngModule){const h=(o?c:this.parentInjector).get(ti,null);h&&(a=h)}const d=l.create(c,r,void 0,a);return this.insert(d.hostView,s),d}insert(e,t){const i=e._lView,r=i[1];if(function BD(n){return Zt(n[3])}(i)){const d=this.indexOf(e);if(-1!==d)this.detach(d);else{const u=i[3],h=new mv(u,u[6],u[3]);h.detach(h.indexOf(e))}}const a=this._adjustIndex(t),o=this._lContainer;!function yC(n,e,t,i){const r=10+i,a=t.length;i>0&&(t[r-1][4]=e),i<a-10?(e[4]=t[r],wg(t,10+i,e)):(t.push(e),e[4]=null),e[3]=t;const o=e[17];null!==o&&t!==o&&function vC(n,e){const t=n[9];e[16]!==e[3][3][16]&&(n[2]=!0),null===t?n[9]=[e]:t.push(e)}(o,e);const s=e[19];null!==s&&s.insertView(n),e[2]|=64}(r,i,o,a);const s=kd(a,o),l=i[U],c=us(l,o[7]);return null!==c&&function gC(n,e,t,i,r,a){i[0]=r,i[6]=e,Pa(n,i,t,1,r,a)}(r,o[6],l,i,c,s),e.attachToViewContainerRef(),wg(Bu(o),a,e),e}move(e,t){return this.insert(e,t)}indexOf(e){const t=bv(this._lContainer);return null!==t?t.indexOf(e):-1}remove(e){const t=this._adjustIndex(e,-1),i=xd(this._lContainer,t);i&&(os(Bu(this._lContainer),t),Yg(i[1],i))}detach(e){const t=this._adjustIndex(e,-1),i=xd(this._lContainer,t);return i&&null!=os(Bu(this._lContainer),t)?new Fa(i):null}_adjustIndex(e,t=0){return null==e?this.length+t:e}};function bv(n){return n[8]}function Bu(n){return n[8]||(n[8]=[])}function yv(n,e){let t;const i=e[n.index];if(Zt(i))t=i;else{let r;if(8&n.type)r=Oe(i);else{const a=e[U];r=a.createComment("");const o=Tt(n,e);ki(a,us(a,o),r,function SC(n,e){return n.nextSibling(e)}(a,o),!1)}e[n.index]=t=mb(i,e,r,n),ks(e,t)}return new mv(t,n,e)}class Hu{constructor(e){this.queryList=e,this.matches=null}clone(){return new Hu(this.queryList)}setDirty(){this.queryList.setDirty()}}class zu{constructor(e=[]){this.queries=e}createEmbeddedView(e){const t=e.queries;if(null!==t){const i=null!==e.contentQueries?e.contentQueries[0]:t.length,r=[];for(let a=0;a<i;a++){const o=t.getByIndex(a);r.push(this.queries[o.indexInDeclarationView].clone())}return new zu(r)}return null}insertView(e){this.dirtyQueriesWithMatches(e)}detachView(e){this.dirtyQueriesWithMatches(e)}dirtyQueriesWithMatches(e){for(let t=0;t<this.queries.length;t++)null!==Sv(e,t).matches&&this.queries[t].setDirty()}}class vv{constructor(e,t,i=null){this.predicate=e,this.flags=t,this.read=i}}class Wu{constructor(e=[]){this.queries=e}elementStart(e,t){for(let i=0;i<this.queries.length;i++)this.queries[i].elementStart(e,t)}elementEnd(e){for(let t=0;t<this.queries.length;t++)this.queries[t].elementEnd(e)}embeddedTView(e){let t=null;for(let i=0;i<this.length;i++){const r=null!==t?t.length:0,a=this.getByIndex(i).embeddedTView(e,r);a&&(a.indexInDeclarationView=i,null!==t?t.push(a):t=[a])}return null!==t?new Wu(t):null}template(e,t){for(let i=0;i<this.queries.length;i++)this.queries[i].template(e,t)}getByIndex(e){return this.queries[e]}get length(){return this.queries.length}track(e){this.queries.push(e)}}class Vu{constructor(e,t=-1){this.metadata=e,this.matches=null,this.indexInDeclarationView=-1,this.crossesNgTemplate=!1,this._appliesToNextNode=!0,this._declarationNodeIndex=t}elementStart(e,t){this.isApplyingToNode(t)&&this.matchTNode(e,t)}elementEnd(e){this._declarationNodeIndex===e.index&&(this._appliesToNextNode=!1)}template(e,t){this.elementStart(e,t)}embeddedTView(e,t){return this.isApplyingToNode(e)?(this.crossesNgTemplate=!0,this.addMatch(-e.index,t),new Vu(this.metadata)):null}isApplyingToNode(e){if(this._appliesToNextNode&&1!=(1&this.metadata.flags)){const t=this._declarationNodeIndex;let i=e.parent;for(;null!==i&&8&i.type&&i.index!==t;)i=i.parent;return t===(null!==i?i.index:-1)}return this._appliesToNextNode}matchTNode(e,t){const i=this.metadata.predicate;if(Array.isArray(i))for(let r=0;r<i.length;r++){const a=i[r];this.matchTNodeWithReadOption(e,t,IM(t,a)),this.matchTNodeWithReadOption(e,t,as(t,e,a,!1,!1))}else i===Fn?4&t.type&&this.matchTNodeWithReadOption(e,t,-1):this.matchTNodeWithReadOption(e,t,as(t,e,i,!1,!1))}matchTNodeWithReadOption(e,t,i){if(null!==i){const r=this.metadata.read;if(null!==r)if(r===ht||r===Wt||r===Fn&&4&t.type)this.addMatch(t.index,-2);else{const a=as(t,e,r,!1,!1);null!==a&&this.addMatch(t.index,a)}else this.addMatch(t.index,i)}}addMatch(e,t){null===this.matches?this.matches=[e,t]:this.matches.push(e,t)}}function IM(n,e){const t=n.localNames;if(null!==t)for(let i=0;i<t.length;i+=2)if(t[i]===e)return t[i+1];return null}function RM(n,e,t,i){return-1===t?function AM(n,e){return 11&n.type?Dr(n,e):4&n.type?Ws(n,e):null}(e,n):-2===t?function PM(n,e,t){return t===ht?Dr(e,n):t===Fn?Ws(e,n):t===Wt?yv(e,n):void 0}(n,e,i):_a(n,n[1],t,e)}function wv(n,e,t,i){const r=e[19].queries[i];if(null===r.matches){const a=n.data,o=t.matches,s=[];for(let l=0;l<o.length;l+=2){const c=o[l];s.push(c<0?null:RM(e,a[c],o[l+1],t.metadata.read))}r.matches=s}return r.matches}function Gu(n,e,t,i){const r=n.queries.getByIndex(t),a=r.matches;if(null!==a){const o=wv(n,e,r,t);for(let s=0;s<a.length;s+=2){const l=a[s];if(l>0)i.push(o[s/2]);else{const c=a[s+1],d=e[-l];for(let u=10;u<d.length;u++){const h=d[u];h[17]===h[3]&&Gu(h[1],h,c,i)}if(null!==d[9]){const u=d[9];for(let h=0;h<u.length;h++){const f=u[h];Gu(f[1],f,c,i)}}}}}return i}function Wr(n){const e=w(),t=ee(),i=Zp();Xc(i+1);const r=Sv(t,i);if(n.dirty&&function jD(n){return 4==(4&n[2])}(e)===(2==(2&r.metadata.flags))){if(null===r.matches)n.reset([]);else{const a=r.crossesNgTemplate?Gu(t,e,i,[]):wv(t,e,r,i);n.reset(a,yx),n.notifyOnChanges()}return!0}return!1}function Uu(n,e,t){const i=ee();i.firstCreatePass&&(Dv(i,new vv(n,e,t),-1),2==(2&e)&&(i.staticViewQueries=!0)),_v(i,w(),e)}function Vr(){return function LM(n,e){return n[19].queries[e].queryList}(w(),Zp())}function _v(n,e,t){const i=new ju(4==(4&t));sb(n,e,i,i.destroy),null===e[19]&&(e[19]=new zu),e[19].queries.push(new Hu(i))}function Dv(n,e,t){null===n.queries&&(n.queries=new Wu),n.queries.track(new Vu(e,t))}function Sv(n,e){return n.queries.getByIndex(e)}function Us(...n){}const $s=new E("Application Initializer");let qs=(()=>{class n{constructor(t){this.appInits=t,this.resolve=Us,this.reject=Us,this.initialized=!1,this.done=!1,this.donePromise=new Promise((i,r)=>{this.resolve=i,this.reject=r})}runInitializers(){if(this.initialized)return;const t=[],i=()=>{this.done=!0,this.resolve()};if(this.appInits)for(let r=0;r<this.appInits.length;r++){const a=this.appInits[r]();if(Ls(a))t.push(a);else if(Lb(a)){const o=new Promise((s,l)=>{a.subscribe({complete:s,error:l})});t.push(o)}}Promise.all(t).then(()=>{i()}).catch(r=>{this.reject(r)}),0===t.length&&i(),this.initialized=!0}}return n.\u0275fac=function(t){return new(t||n)(v($s,8))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();const $r=new E("AppId",{providedIn:"root",factory:function Wv(){return`${Ku()}${Ku()}${Ku()}`}});function Ku(){return String.fromCharCode(97+Math.floor(25*Math.random()))}const Vv=new E("Platform Initializer"),Xu=new E("Platform ID",{providedIn:"platform",factory:()=>"unknown"}),Gv=new E("appBootstrapListener"),Fi=new E("AnimationModuleType");let nI=(()=>{class n{log(t){console.log(t)}warn(t){console.warn(t)}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"platform"}),n})();const yn=new E("LocaleId",{providedIn:"root",factory:()=>me(yn,O.Optional|O.SkipSelf)||function iI(){return"undefined"!=typeof $localize&&$localize.locale||zr}()});class aI{constructor(e,t){this.ngModuleFactory=e,this.componentFactories=t}}let Ju=(()=>{class n{compileModuleSync(t){return new Ou(t)}compileModuleAsync(t){return Promise.resolve(this.compileModuleSync(t))}compileModuleAndAllComponentsSync(t){const i=this.compileModuleSync(t),a=Ln(Ct(t).declarations).reduce((o,s)=>{const l=de(s);return l&&o.push(new ja(l)),o},[]);return new aI(i,a)}compileModuleAndAllComponentsAsync(t){return Promise.resolve(this.compileModuleAndAllComponentsSync(t))}clearCache(){}clearCacheFor(t){}getModuleId(t){}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();const lI=(()=>Promise.resolve(0))();function Zu(n){"undefined"==typeof Zone?lI.then(()=>{n&&n.apply(null,null)}):Zone.current.scheduleMicroTask("scheduleMicrotask",n)}class ae{constructor({enableLongStackTrace:e=!1,shouldCoalesceEventChangeDetection:t=!1,shouldCoalesceRunChangeDetection:i=!1}){if(this.hasPendingMacrotasks=!1,this.hasPendingMicrotasks=!1,this.isStable=!0,this.onUnstable=new it(!1),this.onMicrotaskEmpty=new it(!1),this.onStable=new it(!1),this.onError=new it(!1),"undefined"==typeof Zone)throw new _(908,!1);Zone.assertZonePatched();const r=this;if(r._nesting=0,r._outer=r._inner=Zone.current,Zone.AsyncStackTaggingZoneSpec){const a=Zone.AsyncStackTaggingZoneSpec;r._inner=r._inner.fork(new a("Angular"))}Zone.TaskTrackingZoneSpec&&(r._inner=r._inner.fork(new Zone.TaskTrackingZoneSpec)),e&&Zone.longStackTraceZoneSpec&&(r._inner=r._inner.fork(Zone.longStackTraceZoneSpec)),r.shouldCoalesceEventChangeDetection=!i&&t,r.shouldCoalesceRunChangeDetection=i,r.lastRequestAnimationFrameId=-1,r.nativeRequestAnimationFrame=function cI(){let n=pe.requestAnimationFrame,e=pe.cancelAnimationFrame;if("undefined"!=typeof Zone&&n&&e){const t=n[Zone.__symbol__("OriginalDelegate")];t&&(n=t);const i=e[Zone.__symbol__("OriginalDelegate")];i&&(e=i)}return{nativeRequestAnimationFrame:n,nativeCancelAnimationFrame:e}}().nativeRequestAnimationFrame,function hI(n){const e=()=>{!function uI(n){n.isCheckStableRunning||-1!==n.lastRequestAnimationFrameId||(n.lastRequestAnimationFrameId=n.nativeRequestAnimationFrame.call(pe,()=>{n.fakeTopEventTask||(n.fakeTopEventTask=Zone.root.scheduleEventTask("fakeTopEventTask",()=>{n.lastRequestAnimationFrameId=-1,th(n),n.isCheckStableRunning=!0,eh(n),n.isCheckStableRunning=!1},void 0,()=>{},()=>{})),n.fakeTopEventTask.invoke()}),th(n))}(n)};n._inner=n._inner.fork({name:"angular",properties:{isAngularZone:!0},onInvokeTask:(t,i,r,a,o,s)=>{try{return qv(n),t.invokeTask(r,a,o,s)}finally{(n.shouldCoalesceEventChangeDetection&&"eventTask"===a.type||n.shouldCoalesceRunChangeDetection)&&e(),Yv(n)}},onInvoke:(t,i,r,a,o,s,l)=>{try{return qv(n),t.invoke(r,a,o,s,l)}finally{n.shouldCoalesceRunChangeDetection&&e(),Yv(n)}},onHasTask:(t,i,r,a)=>{t.hasTask(r,a),i===r&&("microTask"==a.change?(n._hasPendingMicrotasks=a.microTask,th(n),eh(n)):"macroTask"==a.change&&(n.hasPendingMacrotasks=a.macroTask))},onHandleError:(t,i,r,a)=>(t.handleError(r,a),n.runOutsideAngular(()=>n.onError.emit(a)),!1)})}(r)}static isInAngularZone(){return"undefined"!=typeof Zone&&!0===Zone.current.get("isAngularZone")}static assertInAngularZone(){if(!ae.isInAngularZone())throw new _(909,!1)}static assertNotInAngularZone(){if(ae.isInAngularZone())throw new _(909,!1)}run(e,t,i){return this._inner.run(e,t,i)}runTask(e,t,i,r){const a=this._inner,o=a.scheduleEventTask("NgZoneEvent: "+r,e,dI,Us,Us);try{return a.runTask(o,t,i)}finally{a.cancelTask(o)}}runGuarded(e,t,i){return this._inner.runGuarded(e,t,i)}runOutsideAngular(e){return this._outer.run(e)}}const dI={};function eh(n){if(0==n._nesting&&!n.hasPendingMicrotasks&&!n.isStable)try{n._nesting++,n.onMicrotaskEmpty.emit(null)}finally{if(n._nesting--,!n.hasPendingMicrotasks)try{n.runOutsideAngular(()=>n.onStable.emit(null))}finally{n.isStable=!0}}}function th(n){n.hasPendingMicrotasks=!!(n._hasPendingMicrotasks||(n.shouldCoalesceEventChangeDetection||n.shouldCoalesceRunChangeDetection)&&-1!==n.lastRequestAnimationFrameId)}function qv(n){n._nesting++,n.isStable&&(n.isStable=!1,n.onUnstable.emit(null))}function Yv(n){n._nesting--,eh(n)}class fI{constructor(){this.hasPendingMicrotasks=!1,this.hasPendingMacrotasks=!1,this.isStable=!0,this.onUnstable=new it,this.onMicrotaskEmpty=new it,this.onStable=new it,this.onError=new it}run(e,t,i){return e.apply(t,i)}runGuarded(e,t,i){return e.apply(t,i)}runOutsideAngular(e){return e()}runTask(e,t,i,r){return e.apply(t,i)}}const Qv=new E(""),Ys=new E("");let Za,nh=(()=>{class n{constructor(t,i,r){this._ngZone=t,this.registry=i,this._pendingCount=0,this._isZoneStable=!0,this._didWork=!1,this._callbacks=[],this.taskTrackingZone=null,Za||(function pI(n){Za=n}(r),r.addToWindow(i)),this._watchAngularEvents(),t.run(()=>{this.taskTrackingZone="undefined"==typeof Zone?null:Zone.current.get("TaskTrackingZone")})}_watchAngularEvents(){this._ngZone.onUnstable.subscribe({next:()=>{this._didWork=!0,this._isZoneStable=!1}}),this._ngZone.runOutsideAngular(()=>{this._ngZone.onStable.subscribe({next:()=>{ae.assertNotInAngularZone(),Zu(()=>{this._isZoneStable=!0,this._runCallbacksIfReady()})}})})}increasePendingRequestCount(){return this._pendingCount+=1,this._didWork=!0,this._pendingCount}decreasePendingRequestCount(){if(this._pendingCount-=1,this._pendingCount<0)throw new Error("pending async requests below zero");return this._runCallbacksIfReady(),this._pendingCount}isStable(){return this._isZoneStable&&0===this._pendingCount&&!this._ngZone.hasPendingMacrotasks}_runCallbacksIfReady(){if(this.isStable())Zu(()=>{for(;0!==this._callbacks.length;){let t=this._callbacks.pop();clearTimeout(t.timeoutId),t.doneCb(this._didWork)}this._didWork=!1});else{let t=this.getPendingTasks();this._callbacks=this._callbacks.filter(i=>!i.updateCb||!i.updateCb(t)||(clearTimeout(i.timeoutId),!1)),this._didWork=!0}}getPendingTasks(){return this.taskTrackingZone?this.taskTrackingZone.macroTasks.map(t=>({source:t.source,creationLocation:t.creationLocation,data:t.data})):[]}addCallback(t,i,r){let a=-1;i&&i>0&&(a=setTimeout(()=>{this._callbacks=this._callbacks.filter(o=>o.timeoutId!==a),t(this._didWork,this.getPendingTasks())},i)),this._callbacks.push({doneCb:t,timeoutId:a,updateCb:r})}whenStable(t,i,r){if(r&&!this.taskTrackingZone)throw new Error('Task tracking zone is required when passing an update callback to whenStable(). Is "zone.js/plugins/task-tracking" loaded?');this.addCallback(t,i,r),this._runCallbacksIfReady()}getPendingRequestCount(){return this._pendingCount}registerApplication(t){this.registry.registerApplication(t,this)}unregisterApplication(t){this.registry.unregisterApplication(t)}findProviders(t,i,r){return[]}}return n.\u0275fac=function(t){return new(t||n)(v(ae),v(ih),v(Ys))},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})(),ih=(()=>{class n{constructor(){this._applications=new Map}registerApplication(t,i){this._applications.set(t,i)}unregisterApplication(t){this._applications.delete(t)}unregisterAllApplications(){this._applications.clear()}getTestability(t){return this._applications.get(t)||null}getAllTestabilities(){return Array.from(this._applications.values())}getAllRootElements(){return Array.from(this._applications.keys())}findTestabilityInTree(t,i=!0){var r;return null!==(r=null==Za?void 0:Za.findTestabilityInTree(this,t,i))&&void 0!==r?r:null}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"platform"}),n})(),vn=null;const Kv=new E("AllowMultipleToken"),rh=new E("PlatformDestroyListeners");class Xv{constructor(e,t){this.name=e,this.token=t}}function Zv(n,e,t=[]){const i=`Platform: ${e}`,r=new E(i);return(a=[])=>{let o=ah();if(!o||o.injector.get(Kv,!1)){const s=[...t,...a,{provide:r,useValue:!0}];n?n(s):function bI(n){if(vn&&!vn.get(Kv,!1))throw new _(400,!1);vn=n;const e=n.get(t0);(function Jv(n){const e=n.get(Vv,null);e&&e.forEach(t=>t())})(n)}(function e0(n=[],e){return wt.create({name:e,providers:[{provide:zd,useValue:"platform"},{provide:rh,useValue:new Set([()=>vn=null])},...n]})}(s,i))}return function vI(n){const e=ah();if(!e)throw new _(401,!1);return e}()}}function ah(){var n;return null!==(n=null==vn?void 0:vn.get(t0))&&void 0!==n?n:null}let t0=(()=>{class n{constructor(t){this._injector=t,this._modules=[],this._destroyListeners=[],this._destroyed=!1}bootstrapModuleFactory(t,i){const r=function r0(n,e){let t;return t="noop"===n?new fI:("zone.js"===n?void 0:n)||new ae(e),t}(null==i?void 0:i.ngZone,function n0(n){return{enableLongStackTrace:!1,shouldCoalesceEventChangeDetection:!(!n||!n.ngZoneEventCoalescing)||!1,shouldCoalesceRunChangeDetection:!(!n||!n.ngZoneRunCoalescing)||!1}}(i)),a=[{provide:ae,useValue:r}];return r.run(()=>{const o=wt.create({providers:a,parent:this.injector,name:t.moduleType.name}),s=t.create(o),l=s.injector.get(ni,null);if(!l)throw new _(402,!1);return r.runOutsideAngular(()=>{const c=r.onError.subscribe({next:d=>{l.handleError(d)}});s.onDestroy(()=>{Qs(this._modules,s),c.unsubscribe()})}),function a0(n,e,t){try{const i=t();return Ls(i)?i.catch(r=>{throw e.runOutsideAngular(()=>n.handleError(r)),r}):i}catch(i){throw e.runOutsideAngular(()=>n.handleError(i)),i}}(l,r,()=>{const c=s.injector.get(qs);return c.runInitializers(),c.donePromise.then(()=>(function ky(n){St(n,"Expected localeId to be defined"),"string"==typeof n&&(Ey=n.toLowerCase().replace(/_/g,"-"))}(s.injector.get(yn,zr)||zr),this._moduleDoBootstrap(s),s))})})}bootstrapModule(t,i=[]){const r=o0({},i);return function gI(n,e,t){const i=new Ou(t);return Promise.resolve(i)}(0,0,t).then(a=>this.bootstrapModuleFactory(a,r))}_moduleDoBootstrap(t){const i=t.injector.get(qr);if(t._bootstrapComponents.length>0)t._bootstrapComponents.forEach(r=>i.bootstrap(r));else{if(!t.instance.ngDoBootstrap)throw new _(403,!1);t.instance.ngDoBootstrap(i)}this._modules.push(t)}onDestroy(t){this._destroyListeners.push(t)}get injector(){return this._injector}destroy(){if(this._destroyed)throw new _(404,!1);this._modules.slice().forEach(i=>i.destroy()),this._destroyListeners.forEach(i=>i());const t=this._injector.get(rh,null);t&&(t.forEach(i=>i()),t.clear()),this._destroyed=!0}get destroyed(){return this._destroyed}}return n.\u0275fac=function(t){return new(t||n)(v(wt))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"platform"}),n})();function o0(n,e){return Array.isArray(e)?e.reduce(o0,n):Object.assign(Object.assign({},n),e)}let qr=(()=>{class n{constructor(t,i,r){this._zone=t,this._injector=i,this._exceptionHandler=r,this._bootstrapListeners=[],this._views=[],this._runningTick=!1,this._stable=!0,this._destroyed=!1,this._destroyListeners=[],this.componentTypes=[],this.components=[],this._onMicrotaskEmptySubscription=this._zone.onMicrotaskEmpty.subscribe({next:()=>{this._zone.run(()=>{this.tick()})}});const a=new ye(s=>{this._stable=this._zone.isStable&&!this._zone.hasPendingMacrotasks&&!this._zone.hasPendingMicrotasks,this._zone.runOutsideAngular(()=>{s.next(this._stable),s.complete()})}),o=new ye(s=>{let l;this._zone.runOutsideAngular(()=>{l=this._zone.onStable.subscribe(()=>{ae.assertNotInAngularZone(),Zu(()=>{!this._stable&&!this._zone.hasPendingMacrotasks&&!this._zone.hasPendingMicrotasks&&(this._stable=!0,s.next(!0))})})});const c=this._zone.onUnstable.subscribe(()=>{ae.assertInAngularZone(),this._stable&&(this._stable=!1,this._zone.runOutsideAngular(()=>{s.next(!1)}))});return()=>{l.unsubscribe(),c.unsubscribe()}});this.isStable=Ap(a,o.pipe(function Rp(n={}){const{connector:e=(()=>new _e),resetOnError:t=!0,resetOnComplete:i=!0,resetOnRefCountZero:r=!0}=n;return a=>{let o,s,l,c=0,d=!1,u=!1;const h=()=>{null==s||s.unsubscribe(),s=void 0},f=()=>{h(),o=l=void 0,d=u=!1},p=()=>{const g=o;f(),null==g||g.unsubscribe()};return xe((g,m)=>{c++,!u&&!d&&h();const b=l=null!=l?l:e();m.add(()=>{c--,0===c&&!u&&!d&&(s=Mc(p,r))}),b.subscribe(m),!o&&c>0&&(o=new da({next:D=>b.next(D),error:D=>{u=!0,h(),s=Mc(f,t,D),b.error(D)},complete:()=>{d=!0,h(),s=Mc(f,i),b.complete()}}),ot(g).subscribe(o))})(a)}}()))}get destroyed(){return this._destroyed}get injector(){return this._injector}bootstrap(t,i){const r=t instanceof Em;if(!this._injector.get(qs).done)throw!r&&function tr(n){const e=de(n)||st(n)||lt(n);return null!==e&&e.standalone}(t),new _(405,false);let o;o=r?t:this._injector.get(_r).resolveComponentFactory(t),this.componentTypes.push(o.componentType);const s=function mI(n){return n.isBoundToModule}(o)?void 0:this._injector.get(Ni),c=o.create(wt.NULL,[],i||o.selector,s),d=c.location.nativeElement,u=c.injector.get(Qv,null);return null==u||u.registerApplication(d),c.onDestroy(()=>{this.detachView(c.hostView),Qs(this.components,c),null==u||u.unregisterApplication(d)}),this._loadComponent(c),c}tick(){if(this._runningTick)throw new _(101,!1);try{this._runningTick=!0;for(let t of this._views)t.detectChanges()}catch(t){this._zone.runOutsideAngular(()=>this._exceptionHandler.handleError(t))}finally{this._runningTick=!1}}attachView(t){const i=t;this._views.push(i),i.attachToAppRef(this)}detachView(t){const i=t;Qs(this._views,i),i.detachFromAppRef()}_loadComponent(t){this.attachView(t.hostView),this.tick(),this.components.push(t),this._injector.get(Gv,[]).concat(this._bootstrapListeners).forEach(r=>r(t))}ngOnDestroy(){if(!this._destroyed)try{this._destroyListeners.forEach(t=>t()),this._views.slice().forEach(t=>t.destroy()),this._onMicrotaskEmptySubscription.unsubscribe()}finally{this._destroyed=!0,this._views=[],this._bootstrapListeners=[],this._destroyListeners=[]}}onDestroy(t){return this._destroyListeners.push(t),()=>Qs(this._destroyListeners,t)}destroy(){if(this._destroyed)throw new _(406,!1);const t=this._injector;t.destroy&&!t.destroyed&&t.destroy()}get viewCount(){return this._views.length}warnIfDestroyed(){}}return n.\u0275fac=function(t){return new(t||n)(v(ae),v(ti),v(ni))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();function Qs(n,e){const t=n.indexOf(e);t>-1&&n.splice(t,1)}let l0=!0,eo=(()=>{class n{}return n.__NG_ELEMENT_ID__=DI,n})();function DI(n){return function SI(n,e,t){if(qo(n)&&!t){const i=Et(n.index,e);return new Fa(i,i)}return 47&n.type?new Fa(e[16],e):null}(ze(),w(),16==(16&n))}class f0{constructor(){}supports(e){return Ba(e)}create(e){return new MI(e)}}const kI=(n,e)=>e;class MI{constructor(e){this.length=0,this._linkedRecords=null,this._unlinkedRecords=null,this._previousItHead=null,this._itHead=null,this._itTail=null,this._additionsHead=null,this._additionsTail=null,this._movesHead=null,this._movesTail=null,this._removalsHead=null,this._removalsTail=null,this._identityChangesHead=null,this._identityChangesTail=null,this._trackByFn=e||kI}forEachItem(e){let t;for(t=this._itHead;null!==t;t=t._next)e(t)}forEachOperation(e){let t=this._itHead,i=this._removalsHead,r=0,a=null;for(;t||i;){const o=!i||t&&t.currentIndex<g0(i,r,a)?t:i,s=g0(o,r,a),l=o.currentIndex;if(o===i)r--,i=i._nextRemoved;else if(t=t._next,null==o.previousIndex)r++;else{a||(a=[]);const c=s-r,d=l-r;if(c!=d){for(let h=0;h<c;h++){const f=h<a.length?a[h]:a[h]=0,p=f+h;d<=p&&p<c&&(a[h]=f+1)}a[o.previousIndex]=d-c}}s!==l&&e(o,s,l)}}forEachPreviousItem(e){let t;for(t=this._previousItHead;null!==t;t=t._nextPrevious)e(t)}forEachAddedItem(e){let t;for(t=this._additionsHead;null!==t;t=t._nextAdded)e(t)}forEachMovedItem(e){let t;for(t=this._movesHead;null!==t;t=t._nextMoved)e(t)}forEachRemovedItem(e){let t;for(t=this._removalsHead;null!==t;t=t._nextRemoved)e(t)}forEachIdentityChange(e){let t;for(t=this._identityChangesHead;null!==t;t=t._nextIdentityChange)e(t)}diff(e){if(null==e&&(e=[]),!Ba(e))throw new _(900,!1);return this.check(e)?this:null}onDestroy(){}check(e){this._reset();let r,a,o,t=this._itHead,i=!1;if(Array.isArray(e)){this.length=e.length;for(let s=0;s<this.length;s++)a=e[s],o=this._trackByFn(s,a),null!==t&&Object.is(t.trackById,o)?(i&&(t=this._verifyReinsertion(t,a,o,s)),Object.is(t.item,a)||this._addIdentityChange(t,a)):(t=this._mismatch(t,a,o,s),i=!0),t=t._next}else r=0,function nE(n,e){if(Array.isArray(n))for(let t=0;t<n.length;t++)e(n[t]);else{const t=n[Ri()]();let i;for(;!(i=t.next()).done;)e(i.value)}}(e,s=>{o=this._trackByFn(r,s),null!==t&&Object.is(t.trackById,o)?(i&&(t=this._verifyReinsertion(t,s,o,r)),Object.is(t.item,s)||this._addIdentityChange(t,s)):(t=this._mismatch(t,s,o,r),i=!0),t=t._next,r++}),this.length=r;return this._truncate(t),this.collection=e,this.isDirty}get isDirty(){return null!==this._additionsHead||null!==this._movesHead||null!==this._removalsHead||null!==this._identityChangesHead}_reset(){if(this.isDirty){let e;for(e=this._previousItHead=this._itHead;null!==e;e=e._next)e._nextPrevious=e._next;for(e=this._additionsHead;null!==e;e=e._nextAdded)e.previousIndex=e.currentIndex;for(this._additionsHead=this._additionsTail=null,e=this._movesHead;null!==e;e=e._nextMoved)e.previousIndex=e.currentIndex;this._movesHead=this._movesTail=null,this._removalsHead=this._removalsTail=null,this._identityChangesHead=this._identityChangesTail=null}}_mismatch(e,t,i,r){let a;return null===e?a=this._itTail:(a=e._prev,this._remove(e)),null!==(e=null===this._unlinkedRecords?null:this._unlinkedRecords.get(i,null))?(Object.is(e.item,t)||this._addIdentityChange(e,t),this._reinsertAfter(e,a,r)):null!==(e=null===this._linkedRecords?null:this._linkedRecords.get(i,r))?(Object.is(e.item,t)||this._addIdentityChange(e,t),this._moveAfter(e,a,r)):e=this._addAfter(new II(t,i),a,r),e}_verifyReinsertion(e,t,i,r){let a=null===this._unlinkedRecords?null:this._unlinkedRecords.get(i,null);return null!==a?e=this._reinsertAfter(a,e._prev,r):e.currentIndex!=r&&(e.currentIndex=r,this._addToMoves(e,r)),e}_truncate(e){for(;null!==e;){const t=e._next;this._addToRemovals(this._unlink(e)),e=t}null!==this._unlinkedRecords&&this._unlinkedRecords.clear(),null!==this._additionsTail&&(this._additionsTail._nextAdded=null),null!==this._movesTail&&(this._movesTail._nextMoved=null),null!==this._itTail&&(this._itTail._next=null),null!==this._removalsTail&&(this._removalsTail._nextRemoved=null),null!==this._identityChangesTail&&(this._identityChangesTail._nextIdentityChange=null)}_reinsertAfter(e,t,i){null!==this._unlinkedRecords&&this._unlinkedRecords.remove(e);const r=e._prevRemoved,a=e._nextRemoved;return null===r?this._removalsHead=a:r._nextRemoved=a,null===a?this._removalsTail=r:a._prevRemoved=r,this._insertAfter(e,t,i),this._addToMoves(e,i),e}_moveAfter(e,t,i){return this._unlink(e),this._insertAfter(e,t,i),this._addToMoves(e,i),e}_addAfter(e,t,i){return this._insertAfter(e,t,i),this._additionsTail=null===this._additionsTail?this._additionsHead=e:this._additionsTail._nextAdded=e,e}_insertAfter(e,t,i){const r=null===t?this._itHead:t._next;return e._next=r,e._prev=t,null===r?this._itTail=e:r._prev=e,null===t?this._itHead=e:t._next=e,null===this._linkedRecords&&(this._linkedRecords=new p0),this._linkedRecords.put(e),e.currentIndex=i,e}_remove(e){return this._addToRemovals(this._unlink(e))}_unlink(e){null!==this._linkedRecords&&this._linkedRecords.remove(e);const t=e._prev,i=e._next;return null===t?this._itHead=i:t._next=i,null===i?this._itTail=t:i._prev=t,e}_addToMoves(e,t){return e.previousIndex===t||(this._movesTail=null===this._movesTail?this._movesHead=e:this._movesTail._nextMoved=e),e}_addToRemovals(e){return null===this._unlinkedRecords&&(this._unlinkedRecords=new p0),this._unlinkedRecords.put(e),e.currentIndex=null,e._nextRemoved=null,null===this._removalsTail?(this._removalsTail=this._removalsHead=e,e._prevRemoved=null):(e._prevRemoved=this._removalsTail,this._removalsTail=this._removalsTail._nextRemoved=e),e}_addIdentityChange(e,t){return e.item=t,this._identityChangesTail=null===this._identityChangesTail?this._identityChangesHead=e:this._identityChangesTail._nextIdentityChange=e,e}}class II{constructor(e,t){this.item=e,this.trackById=t,this.currentIndex=null,this.previousIndex=null,this._nextPrevious=null,this._prev=null,this._next=null,this._prevDup=null,this._nextDup=null,this._prevRemoved=null,this._nextRemoved=null,this._nextAdded=null,this._nextMoved=null,this._nextIdentityChange=null}}class AI{constructor(){this._head=null,this._tail=null}add(e){null===this._head?(this._head=this._tail=e,e._nextDup=null,e._prevDup=null):(this._tail._nextDup=e,e._prevDup=this._tail,e._nextDup=null,this._tail=e)}get(e,t){let i;for(i=this._head;null!==i;i=i._nextDup)if((null===t||t<=i.currentIndex)&&Object.is(i.trackById,e))return i;return null}remove(e){const t=e._prevDup,i=e._nextDup;return null===t?this._head=i:t._nextDup=i,null===i?this._tail=t:i._prevDup=t,null===this._head}}class p0{constructor(){this.map=new Map}put(e){const t=e.trackById;let i=this.map.get(t);i||(i=new AI,this.map.set(t,i)),i.add(e)}get(e,t){const r=this.map.get(e);return r?r.get(e,t):null}remove(e){const t=e.trackById;return this.map.get(t).remove(e)&&this.map.delete(t),e}get isEmpty(){return 0===this.map.size}clear(){this.map.clear()}}function g0(n,e,t){const i=n.previousIndex;if(null===i)return i;let r=0;return t&&i<t.length&&(r=t[i]),i+e+r}class m0{constructor(){}supports(e){return e instanceof Map||_u(e)}create(){return new RI}}class RI{constructor(){this._records=new Map,this._mapHead=null,this._appendAfter=null,this._previousMapHead=null,this._changesHead=null,this._changesTail=null,this._additionsHead=null,this._additionsTail=null,this._removalsHead=null,this._removalsTail=null}get isDirty(){return null!==this._additionsHead||null!==this._changesHead||null!==this._removalsHead}forEachItem(e){let t;for(t=this._mapHead;null!==t;t=t._next)e(t)}forEachPreviousItem(e){let t;for(t=this._previousMapHead;null!==t;t=t._nextPrevious)e(t)}forEachChangedItem(e){let t;for(t=this._changesHead;null!==t;t=t._nextChanged)e(t)}forEachAddedItem(e){let t;for(t=this._additionsHead;null!==t;t=t._nextAdded)e(t)}forEachRemovedItem(e){let t;for(t=this._removalsHead;null!==t;t=t._nextRemoved)e(t)}diff(e){if(e){if(!(e instanceof Map||_u(e)))throw new _(900,!1)}else e=new Map;return this.check(e)?this:null}onDestroy(){}check(e){this._reset();let t=this._mapHead;if(this._appendAfter=null,this._forEach(e,(i,r)=>{if(t&&t.key===r)this._maybeAddToChanges(t,i),this._appendAfter=t,t=t._next;else{const a=this._getOrCreateRecordForKey(r,i);t=this._insertBeforeOrAppend(t,a)}}),t){t._prev&&(t._prev._next=null),this._removalsHead=t;for(let i=t;null!==i;i=i._nextRemoved)i===this._mapHead&&(this._mapHead=null),this._records.delete(i.key),i._nextRemoved=i._next,i.previousValue=i.currentValue,i.currentValue=null,i._prev=null,i._next=null}return this._changesTail&&(this._changesTail._nextChanged=null),this._additionsTail&&(this._additionsTail._nextAdded=null),this.isDirty}_insertBeforeOrAppend(e,t){if(e){const i=e._prev;return t._next=e,t._prev=i,e._prev=t,i&&(i._next=t),e===this._mapHead&&(this._mapHead=t),this._appendAfter=e,e}return this._appendAfter?(this._appendAfter._next=t,t._prev=this._appendAfter):this._mapHead=t,this._appendAfter=t,null}_getOrCreateRecordForKey(e,t){if(this._records.has(e)){const r=this._records.get(e);this._maybeAddToChanges(r,t);const a=r._prev,o=r._next;return a&&(a._next=o),o&&(o._prev=a),r._next=null,r._prev=null,r}const i=new PI(e);return this._records.set(e,i),i.currentValue=t,this._addToAdditions(i),i}_reset(){if(this.isDirty){let e;for(this._previousMapHead=this._mapHead,e=this._previousMapHead;null!==e;e=e._next)e._nextPrevious=e._next;for(e=this._changesHead;null!==e;e=e._nextChanged)e.previousValue=e.currentValue;for(e=this._additionsHead;null!=e;e=e._nextAdded)e.previousValue=e.currentValue;this._changesHead=this._changesTail=null,this._additionsHead=this._additionsTail=null,this._removalsHead=null}}_maybeAddToChanges(e,t){Object.is(t,e.currentValue)||(e.previousValue=e.currentValue,e.currentValue=t,this._addToChanges(e))}_addToAdditions(e){null===this._additionsHead?this._additionsHead=this._additionsTail=e:(this._additionsTail._nextAdded=e,this._additionsTail=e)}_addToChanges(e){null===this._changesHead?this._changesHead=this._changesTail=e:(this._changesTail._nextChanged=e,this._changesTail=e)}_forEach(e,t){e instanceof Map?e.forEach(t):Object.keys(e).forEach(i=>t(e[i],i))}}class PI{constructor(e){this.key=e,this.previousValue=null,this.currentValue=null,this._nextPrevious=null,this._next=null,this._prev=null,this._nextAdded=null,this._nextRemoved=null,this._nextChanged=null}}function b0(){return new Js([new f0])}let Js=(()=>{class n{constructor(t){this.factories=t}static create(t,i){if(null!=i){const r=i.factories.slice();t=t.concat(r)}return new n(t)}static extend(t){return{provide:n,useFactory:i=>n.create(t,i||b0()),deps:[[n,new mr,new ei]]}}find(t){const i=this.factories.find(r=>r.supports(t));if(null!=i)return i;throw new _(901,!1)}}return n.\u0275prov=k({token:n,providedIn:"root",factory:b0}),n})();function y0(){return new to([new m0])}let to=(()=>{class n{constructor(t){this.factories=t}static create(t,i){if(i){const r=i.factories.slice();t=t.concat(r)}return new n(t)}static extend(t){return{provide:n,useFactory:i=>n.create(t,i||y0()),deps:[[n,new mr,new ei]]}}find(t){const i=this.factories.find(r=>r.supports(t));if(i)return i;throw new _(901,!1)}}return n.\u0275prov=k({token:n,providedIn:"root",factory:y0}),n})();const NI=Zv(null,"core",[]);let FI=(()=>{class n{constructor(t){}}return n.\u0275fac=function(t){return new(t||n)(v(qr))},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({}),n})();function Yr(n){return"boolean"==typeof n?n:null!=n&&"false"!==n}let Zs=null;function ai(){return Zs}const Q=new E("DocumentToken");let dh=(()=>{class n{historyGo(t){throw new Error("Not implemented")}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275prov=k({token:n,factory:function(){return function zI(){return v(v0)}()},providedIn:"platform"}),n})();const WI=new E("Location Initialized");let v0=(()=>{class n extends dh{constructor(t){super(),this._doc=t,this._init()}_init(){this.location=window.location,this._history=window.history}getBaseHrefFromDOM(){return ai().getBaseHref(this._doc)}onPopState(t){const i=ai().getGlobalEventTarget(this._doc,"window");return i.addEventListener("popstate",t,!1),()=>i.removeEventListener("popstate",t)}onHashChange(t){const i=ai().getGlobalEventTarget(this._doc,"window");return i.addEventListener("hashchange",t,!1),()=>i.removeEventListener("hashchange",t)}get href(){return this.location.href}get protocol(){return this.location.protocol}get hostname(){return this.location.hostname}get port(){return this.location.port}get pathname(){return this.location.pathname}get search(){return this.location.search}get hash(){return this.location.hash}set pathname(t){this.location.pathname=t}pushState(t,i,r){w0()?this._history.pushState(t,i,r):this.location.hash=r}replaceState(t,i,r){w0()?this._history.replaceState(t,i,r):this.location.hash=r}forward(){this._history.forward()}back(){this._history.back()}historyGo(t=0){this._history.go(t)}getState(){return this._history.state}}return n.\u0275fac=function(t){return new(t||n)(v(Q))},n.\u0275prov=k({token:n,factory:function(){return function VI(){return new v0(v(Q))}()},providedIn:"platform"}),n})();function w0(){return!!window.history.pushState}function uh(n,e){if(0==n.length)return e;if(0==e.length)return n;let t=0;return n.endsWith("/")&&t++,e.startsWith("/")&&t++,2==t?n+e.substring(1):1==t?n+e:n+"/"+e}function _0(n){const e=n.match(/#|\?|$/),t=e&&e.index||n.length;return n.slice(0,t-("/"===n[t-1]?1:0))+n.slice(t)}function Bn(n){return n&&"?"!==n[0]?"?"+n:n}let Bi=(()=>{class n{historyGo(t){throw new Error("Not implemented")}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275prov=k({token:n,factory:function(){return me(S0)},providedIn:"root"}),n})();const D0=new E("appBaseHref");let S0=(()=>{class n extends Bi{constructor(t,i){var r,a,o;super(),this._platformLocation=t,this._removeListenerFns=[],this._baseHref=null!==(o=null!==(r=null!=i?i:this._platformLocation.getBaseHrefFromDOM())&&void 0!==r?r:null===(a=me(Q).location)||void 0===a?void 0:a.origin)&&void 0!==o?o:""}ngOnDestroy(){for(;this._removeListenerFns.length;)this._removeListenerFns.pop()()}onPopState(t){this._removeListenerFns.push(this._platformLocation.onPopState(t),this._platformLocation.onHashChange(t))}getBaseHref(){return this._baseHref}prepareExternalUrl(t){return uh(this._baseHref,t)}path(t=!1){const i=this._platformLocation.pathname+Bn(this._platformLocation.search),r=this._platformLocation.hash;return r&&t?`${i}${r}`:i}pushState(t,i,r,a){const o=this.prepareExternalUrl(r+Bn(a));this._platformLocation.pushState(t,i,o)}replaceState(t,i,r,a){const o=this.prepareExternalUrl(r+Bn(a));this._platformLocation.replaceState(t,i,o)}forward(){this._platformLocation.forward()}back(){this._platformLocation.back()}getState(){return this._platformLocation.getState()}historyGo(t=0){var i,r;null===(r=(i=this._platformLocation).historyGo)||void 0===r||r.call(i,t)}}return n.\u0275fac=function(t){return new(t||n)(v(dh),v(D0,8))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})(),GI=(()=>{class n extends Bi{constructor(t,i){super(),this._platformLocation=t,this._baseHref="",this._removeListenerFns=[],null!=i&&(this._baseHref=i)}ngOnDestroy(){for(;this._removeListenerFns.length;)this._removeListenerFns.pop()()}onPopState(t){this._removeListenerFns.push(this._platformLocation.onPopState(t),this._platformLocation.onHashChange(t))}getBaseHref(){return this._baseHref}path(t=!1){let i=this._platformLocation.hash;return null==i&&(i="#"),i.length>0?i.substring(1):i}prepareExternalUrl(t){const i=uh(this._baseHref,t);return i.length>0?"#"+i:i}pushState(t,i,r,a){let o=this.prepareExternalUrl(r+Bn(a));0==o.length&&(o=this._platformLocation.pathname),this._platformLocation.pushState(t,i,o)}replaceState(t,i,r,a){let o=this.prepareExternalUrl(r+Bn(a));0==o.length&&(o=this._platformLocation.pathname),this._platformLocation.replaceState(t,i,o)}forward(){this._platformLocation.forward()}back(){this._platformLocation.back()}getState(){return this._platformLocation.getState()}historyGo(t=0){var i,r;null===(r=(i=this._platformLocation).historyGo)||void 0===r||r.call(i,t)}}return n.\u0275fac=function(t){return new(t||n)(v(dh),v(D0,8))},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})(),el=(()=>{class n{constructor(t){this._subject=new it,this._urlChangeListeners=[],this._urlChangeSubscription=null,this._locationStrategy=t;const i=this._locationStrategy.getBaseHref();this._baseHref=_0(C0(i)),this._locationStrategy.onPopState(r=>{this._subject.emit({url:this.path(!0),pop:!0,state:r.state,type:r.type})})}ngOnDestroy(){var t;null===(t=this._urlChangeSubscription)||void 0===t||t.unsubscribe(),this._urlChangeListeners=[]}path(t=!1){return this.normalize(this._locationStrategy.path(t))}getState(){return this._locationStrategy.getState()}isCurrentPathEqualTo(t,i=""){return this.path()==this.normalize(t+Bn(i))}normalize(t){return n.stripTrailingSlash(function $I(n,e){return n&&e.startsWith(n)?e.substring(n.length):e}(this._baseHref,C0(t)))}prepareExternalUrl(t){return t&&"/"!==t[0]&&(t="/"+t),this._locationStrategy.prepareExternalUrl(t)}go(t,i="",r=null){this._locationStrategy.pushState(r,"",t,i),this._notifyUrlChangeListeners(this.prepareExternalUrl(t+Bn(i)),r)}replaceState(t,i="",r=null){this._locationStrategy.replaceState(r,"",t,i),this._notifyUrlChangeListeners(this.prepareExternalUrl(t+Bn(i)),r)}forward(){this._locationStrategy.forward()}back(){this._locationStrategy.back()}historyGo(t=0){var i,r;null===(r=(i=this._locationStrategy).historyGo)||void 0===r||r.call(i,t)}onUrlChange(t){return this._urlChangeListeners.push(t),this._urlChangeSubscription||(this._urlChangeSubscription=this.subscribe(i=>{this._notifyUrlChangeListeners(i.url,i.state)})),()=>{var i;const r=this._urlChangeListeners.indexOf(t);this._urlChangeListeners.splice(r,1),0===this._urlChangeListeners.length&&(null===(i=this._urlChangeSubscription)||void 0===i||i.unsubscribe(),this._urlChangeSubscription=null)}}_notifyUrlChangeListeners(t="",i){this._urlChangeListeners.forEach(r=>r(t,i))}subscribe(t,i,r){return this._subject.subscribe({next:t,error:i,complete:r})}}return n.normalizeQueryParams=Bn,n.joinWithSlash=uh,n.stripTrailingSlash=_0,n.\u0275fac=function(t){return new(t||n)(v(Bi))},n.\u0275prov=k({token:n,factory:function(){return function UI(){return new el(v(Bi))}()},providedIn:"root"}),n})();function C0(n){return n.replace(/\/index.html$/,"")}let P0=(()=>{class n{constructor(t,i,r,a){this._iterableDiffers=t,this._keyValueDiffers=i,this._ngEl=r,this._renderer=a,this._iterableDiffer=null,this._keyValueDiffer=null,this._initialClasses=[],this._rawClass=null}set klass(t){this._removeClasses(this._initialClasses),this._initialClasses="string"==typeof t?t.split(/\s+/):[],this._applyClasses(this._initialClasses),this._applyClasses(this._rawClass)}set ngClass(t){this._removeClasses(this._rawClass),this._applyClasses(this._initialClasses),this._iterableDiffer=null,this._keyValueDiffer=null,this._rawClass="string"==typeof t?t.split(/\s+/):t,this._rawClass&&(Ba(this._rawClass)?this._iterableDiffer=this._iterableDiffers.find(this._rawClass).create():this._keyValueDiffer=this._keyValueDiffers.find(this._rawClass).create())}ngDoCheck(){if(this._iterableDiffer){const t=this._iterableDiffer.diff(this._rawClass);t&&this._applyIterableChanges(t)}else if(this._keyValueDiffer){const t=this._keyValueDiffer.diff(this._rawClass);t&&this._applyKeyValueChanges(t)}}_applyKeyValueChanges(t){t.forEachAddedItem(i=>this._toggleClass(i.key,i.currentValue)),t.forEachChangedItem(i=>this._toggleClass(i.key,i.currentValue)),t.forEachRemovedItem(i=>{i.previousValue&&this._toggleClass(i.key,!1)})}_applyIterableChanges(t){t.forEachAddedItem(i=>{if("string"!=typeof i.item)throw new Error(`NgClass can only toggle CSS classes expressed as strings, got ${fe(i.item)}`);this._toggleClass(i.item,!0)}),t.forEachRemovedItem(i=>this._toggleClass(i.item,!1))}_applyClasses(t){t&&(Array.isArray(t)||t instanceof Set?t.forEach(i=>this._toggleClass(i,!0)):Object.keys(t).forEach(i=>this._toggleClass(i,!!t[i])))}_removeClasses(t){t&&(Array.isArray(t)||t instanceof Set?t.forEach(i=>this._toggleClass(i,!1)):Object.keys(t).forEach(i=>this._toggleClass(i,!1)))}_toggleClass(t,i){(t=t.trim())&&t.split(/\s+/g).forEach(r=>{i?this._renderer.addClass(this._ngEl.nativeElement,r):this._renderer.removeClass(this._ngEl.nativeElement,r)})}}return n.\u0275fac=function(t){return new(t||n)(C(Js),C(to),C(ht),C(ws))},n.\u0275dir=Re({type:n,selectors:[["","ngClass",""]],inputs:{klass:["class","klass"],ngClass:"ngClass"},standalone:!0}),n})();class AA{constructor(e,t,i,r){this.$implicit=e,this.ngForOf=t,this.index=i,this.count=r}get first(){return 0===this.index}get last(){return this.index===this.count-1}get even(){return this.index%2==0}get odd(){return!this.even}}let dl=(()=>{class n{constructor(t,i,r){this._viewContainer=t,this._template=i,this._differs=r,this._ngForOf=null,this._ngForOfDirty=!0,this._differ=null}set ngForOf(t){this._ngForOf=t,this._ngForOfDirty=!0}set ngForTrackBy(t){this._trackByFn=t}get ngForTrackBy(){return this._trackByFn}set ngForTemplate(t){t&&(this._template=t)}ngDoCheck(){if(this._ngForOfDirty){this._ngForOfDirty=!1;const t=this._ngForOf;!this._differ&&t&&(this._differ=this._differs.find(t).create(this.ngForTrackBy))}if(this._differ){const t=this._differ.diff(this._ngForOf);t&&this._applyChanges(t)}}_applyChanges(t){const i=this._viewContainer;t.forEachOperation((r,a,o)=>{if(null==r.previousIndex)i.createEmbeddedView(this._template,new AA(r.item,this._ngForOf,-1,-1),null===o?void 0:o);else if(null==o)i.remove(null===a?void 0:a);else if(null!==a){const s=i.get(a);i.move(s,o),N0(s,r)}});for(let r=0,a=i.length;r<a;r++){const s=i.get(r).context;s.index=r,s.count=a,s.ngForOf=this._ngForOf}t.forEachIdentityChange(r=>{N0(i.get(r.currentIndex),r)})}static ngTemplateContextGuard(t,i){return!0}}return n.\u0275fac=function(t){return new(t||n)(C(Wt),C(Fn),C(Js))},n.\u0275dir=Re({type:n,selectors:[["","ngFor","","ngForOf",""]],inputs:{ngForOf:"ngForOf",ngForTrackBy:"ngForTrackBy",ngForTemplate:"ngForTemplate"},standalone:!0}),n})();function N0(n,e){n.context.$implicit=e.item}let ro=(()=>{class n{constructor(t,i){this._viewContainer=t,this._context=new PA,this._thenTemplateRef=null,this._elseTemplateRef=null,this._thenViewRef=null,this._elseViewRef=null,this._thenTemplateRef=i}set ngIf(t){this._context.$implicit=this._context.ngIf=t,this._updateView()}set ngIfThen(t){F0("ngIfThen",t),this._thenTemplateRef=t,this._thenViewRef=null,this._updateView()}set ngIfElse(t){F0("ngIfElse",t),this._elseTemplateRef=t,this._elseViewRef=null,this._updateView()}_updateView(){this._context.$implicit?this._thenViewRef||(this._viewContainer.clear(),this._elseViewRef=null,this._thenTemplateRef&&(this._thenViewRef=this._viewContainer.createEmbeddedView(this._thenTemplateRef,this._context))):this._elseViewRef||(this._viewContainer.clear(),this._thenViewRef=null,this._elseTemplateRef&&(this._elseViewRef=this._viewContainer.createEmbeddedView(this._elseTemplateRef,this._context)))}static ngTemplateContextGuard(t,i){return!0}}return n.\u0275fac=function(t){return new(t||n)(C(Wt),C(Fn))},n.\u0275dir=Re({type:n,selectors:[["","ngIf",""]],inputs:{ngIf:"ngIf",ngIfThen:"ngIfThen",ngIfElse:"ngIfElse"},standalone:!0}),n})();class PA{constructor(){this.$implicit=null,this.ngIf=null}}function F0(n,e){if(e&&!e.createEmbeddedView)throw new Error(`${n} must be a TemplateRef, but received '${fe(e)}'.`)}class BA{createSubscription(e,t){return e.subscribe({next:t,error:i=>{throw i}})}dispose(e){e.unsubscribe()}}class HA{createSubscription(e,t){return e.then(t,i=>{throw i})}dispose(e){}}const zA=new HA,WA=new BA;let B0=(()=>{class n{constructor(t){this._latestValue=null,this._subscription=null,this._obj=null,this._strategy=null,this._ref=t}ngOnDestroy(){this._subscription&&this._dispose(),this._ref=null}transform(t){return this._obj?t!==this._obj?(this._dispose(),this.transform(t)):this._latestValue:(t&&this._subscribe(t),this._latestValue)}_subscribe(t){this._obj=t,this._strategy=this._selectStrategy(t),this._subscription=this._strategy.createSubscription(t,i=>this._updateLatestValue(t,i))}_selectStrategy(t){if(Ls(t))return zA;if(Pb(t))return WA;throw function ln(n,e){return new _(2100,!1)}()}_dispose(){this._strategy.dispose(this._subscription),this._latestValue=null,this._subscription=null,this._obj=null}_updateLatestValue(t,i){t===this._obj&&(this._latestValue=i,this._ref.markForCheck())}}return n.\u0275fac=function(t){return new(t||n)(C(eo,16))},n.\u0275pipe=bt({name:"async",type:n,pure:!1,standalone:!0}),n})(),z0=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({}),n})();const W0="browser";let cR=(()=>{class n{}return n.\u0275prov=k({token:n,providedIn:"root",factory:()=>new dR(v(Q),window)}),n})();class dR{constructor(e,t){this.document=e,this.window=t,this.offset=()=>[0,0]}setOffset(e){this.offset=Array.isArray(e)?()=>e:e}getScrollPosition(){return this.supportsScrolling()?[this.window.pageXOffset,this.window.pageYOffset]:[0,0]}scrollToPosition(e){this.supportsScrolling()&&this.window.scrollTo(e[0],e[1])}scrollToAnchor(e){if(!this.supportsScrolling())return;const t=function uR(n,e){const t=n.getElementById(e)||n.getElementsByName(e)[0];if(t)return t;if("function"==typeof n.createTreeWalker&&n.body&&(n.body.createShadowRoot||n.body.attachShadow)){const i=n.createTreeWalker(n.body,NodeFilter.SHOW_ELEMENT);let r=i.currentNode;for(;r;){const a=r.shadowRoot;if(a){const o=a.getElementById(e)||a.querySelector(`[name="${e}"]`);if(o)return o}r=i.nextNode()}}return null}(this.document,e);t&&(this.scrollToElement(t),t.focus())}setHistoryScrollRestoration(e){if(this.supportScrollRestoration()){const t=this.window.history;t&&t.scrollRestoration&&(t.scrollRestoration=e)}}scrollToElement(e){const t=e.getBoundingClientRect(),i=t.left+this.window.pageXOffset,r=t.top+this.window.pageYOffset,a=this.offset();this.window.scrollTo(i-a[0],r-a[1])}supportScrollRestoration(){try{if(!this.supportsScrolling())return!1;const e=V0(this.window.history)||V0(Object.getPrototypeOf(this.window.history));return!(!e||!e.writable&&!e.set)}catch(e){return!1}}supportsScrolling(){try{return!!this.window&&!!this.window.scrollTo&&"pageXOffset"in this.window}catch(e){return!1}}}function V0(n){return Object.getOwnPropertyDescriptor(n,"scrollRestoration")}class Eh extends class BR extends class HI{}{constructor(){super(...arguments),this.supportsDOMEvents=!0}}{static makeCurrent(){!function BI(n){Zs||(Zs=n)}(new Eh)}onAndCancel(e,t,i){return e.addEventListener(t,i,!1),()=>{e.removeEventListener(t,i,!1)}}dispatchEvent(e,t){e.dispatchEvent(t)}remove(e){e.parentNode&&e.parentNode.removeChild(e)}createElement(e,t){return(t=t||this.getDefaultDocument()).createElement(e)}createHtmlDocument(){return document.implementation.createHTMLDocument("fakeTitle")}getDefaultDocument(){return document}isElementNode(e){return e.nodeType===Node.ELEMENT_NODE}isShadowRoot(e){return e instanceof DocumentFragment}getGlobalEventTarget(e,t){return"window"===t?window:"document"===t?e:"body"===t?e.body:null}getBaseHref(e){const t=function HR(){return oo=oo||document.querySelector("base"),oo?oo.getAttribute("href"):null}();return null==t?null:function zR(n){fl=fl||document.createElement("a"),fl.setAttribute("href",n);const e=fl.pathname;return"/"===e.charAt(0)?e:`/${e}`}(t)}resetBaseElement(){oo=null}getUserAgent(){return window.navigator.userAgent}getCookie(e){return function MA(n,e){e=encodeURIComponent(e);for(const t of n.split(";")){const i=t.indexOf("="),[r,a]=-1==i?[t,""]:[t.slice(0,i),t.slice(i+1)];if(r.trim()===e)return decodeURIComponent(a)}return null}(document.cookie,e)}}let fl,oo=null;const Y0=new E("TRANSITION_ID"),VR=[{provide:$s,useFactory:function WR(n,e,t){return()=>{t.get(qs).donePromise.then(()=>{const i=ai(),r=e.querySelectorAll(`style[ng-transition="${n}"]`);for(let a=0;a<r.length;a++)i.remove(r[a])})}},deps:[Y0,Q,wt],multi:!0}];let UR=(()=>{class n{build(){return new XMLHttpRequest}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})();const pl=new E("EventManagerPlugins");let gl=(()=>{class n{constructor(t,i){this._zone=i,this._eventNameToPlugin=new Map,t.forEach(r=>r.manager=this),this._plugins=t.slice().reverse()}addEventListener(t,i,r){return this._findPluginFor(i).addEventListener(t,i,r)}addGlobalEventListener(t,i,r){return this._findPluginFor(i).addGlobalEventListener(t,i,r)}getZone(){return this._zone}_findPluginFor(t){const i=this._eventNameToPlugin.get(t);if(i)return i;const r=this._plugins;for(let a=0;a<r.length;a++){const o=r[a];if(o.supports(t))return this._eventNameToPlugin.set(t,o),o}throw new Error(`No event manager plugin found for event ${t}`)}}return n.\u0275fac=function(t){return new(t||n)(v(pl),v(ae))},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})();class Q0{constructor(e){this._doc=e}addGlobalEventListener(e,t,i){const r=ai().getGlobalEventTarget(this._doc,e);if(!r)throw new Error(`Unsupported event target ${r} for event ${t}`);return this.addEventListener(r,t,i)}}let K0=(()=>{class n{constructor(){this._stylesSet=new Set}addStyles(t){const i=new Set;t.forEach(r=>{this._stylesSet.has(r)||(this._stylesSet.add(r),i.add(r))}),this.onStylesAdded(i)}onStylesAdded(t){}getAllStyles(){return Array.from(this._stylesSet)}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})(),so=(()=>{class n extends K0{constructor(t){super(),this._doc=t,this._hostNodes=new Map,this._hostNodes.set(t.head,[])}_addStylesToHost(t,i,r){t.forEach(a=>{const o=this._doc.createElement("style");o.textContent=a,r.push(i.appendChild(o))})}addHost(t){const i=[];this._addStylesToHost(this._stylesSet,t,i),this._hostNodes.set(t,i)}removeHost(t){const i=this._hostNodes.get(t);i&&i.forEach(X0),this._hostNodes.delete(t)}onStylesAdded(t){this._hostNodes.forEach((i,r)=>{this._addStylesToHost(t,r,i)})}ngOnDestroy(){this._hostNodes.forEach(t=>t.forEach(X0))}}return n.\u0275fac=function(t){return new(t||n)(v(Q))},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})();function X0(n){ai().remove(n)}const kh={svg:"http://www.w3.org/2000/svg",xhtml:"http://www.w3.org/1999/xhtml",xlink:"http://www.w3.org/1999/xlink",xml:"http://www.w3.org/XML/1998/namespace",xmlns:"http://www.w3.org/2000/xmlns/",math:"http://www.w3.org/1998/MathML/"},Mh=/%COMP%/g;function ml(n,e,t){for(let i=0;i<e.length;i++){let r=e[i];Array.isArray(r)?ml(n,r,t):(r=r.replace(Mh,n),t.push(r))}return t}function ew(n){return e=>{if("__ngUnwrap__"===e)return n;!1===n(e)&&(e.preventDefault(),e.returnValue=!1)}}let bl=(()=>{class n{constructor(t,i,r){this.eventManager=t,this.sharedStylesHost=i,this.appId=r,this.rendererByCompId=new Map,this.defaultRenderer=new Ih(t)}createRenderer(t,i){if(!t||!i)return this.defaultRenderer;switch(i.encapsulation){case Xt.Emulated:{let r=this.rendererByCompId.get(i.id);return r||(r=new XR(this.eventManager,this.sharedStylesHost,i,this.appId),this.rendererByCompId.set(i.id,r)),r.applyToHost(t),r}case 1:case Xt.ShadowDom:return new JR(this.eventManager,this.sharedStylesHost,t,i);default:if(!this.rendererByCompId.has(i.id)){const r=ml(i.id,i.styles,[]);this.sharedStylesHost.addStyles(r),this.rendererByCompId.set(i.id,this.defaultRenderer)}return this.defaultRenderer}}begin(){}end(){}}return n.\u0275fac=function(t){return new(t||n)(v(gl),v(so),v($r))},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})();class Ih{constructor(e){this.eventManager=e,this.data=Object.create(null),this.destroyNode=null}destroy(){}createElement(e,t){return t?document.createElementNS(kh[t]||t,e):document.createElement(e)}createComment(e){return document.createComment(e)}createText(e){return document.createTextNode(e)}appendChild(e,t){(nw(e)?e.content:e).appendChild(t)}insertBefore(e,t,i){e&&(nw(e)?e.content:e).insertBefore(t,i)}removeChild(e,t){e&&e.removeChild(t)}selectRootElement(e,t){let i="string"==typeof e?document.querySelector(e):e;if(!i)throw new Error(`The selector "${e}" did not match any elements`);return t||(i.textContent=""),i}parentNode(e){return e.parentNode}nextSibling(e){return e.nextSibling}setAttribute(e,t,i,r){if(r){t=r+":"+t;const a=kh[r];a?e.setAttributeNS(a,t,i):e.setAttribute(t,i)}else e.setAttribute(t,i)}removeAttribute(e,t,i){if(i){const r=kh[i];r?e.removeAttributeNS(r,t):e.removeAttribute(`${i}:${t}`)}else e.removeAttribute(t)}addClass(e,t){e.classList.add(t)}removeClass(e,t){e.classList.remove(t)}setStyle(e,t,i,r){r&(vt.DashCase|vt.Important)?e.style.setProperty(t,i,r&vt.Important?"important":""):e.style[t]=i}removeStyle(e,t,i){i&vt.DashCase?e.style.removeProperty(t):e.style[t]=""}setProperty(e,t,i){e[t]=i}setValue(e,t){e.nodeValue=t}listen(e,t,i){return"string"==typeof e?this.eventManager.addGlobalEventListener(e,t,ew(i)):this.eventManager.addEventListener(e,t,ew(i))}}function nw(n){return"TEMPLATE"===n.tagName&&void 0!==n.content}class XR extends Ih{constructor(e,t,i,r){super(e),this.component=i;const a=ml(r+"-"+i.id,i.styles,[]);t.addStyles(a),this.contentAttr=function YR(n){return"_ngcontent-%COMP%".replace(Mh,n)}(r+"-"+i.id),this.hostAttr=function QR(n){return"_nghost-%COMP%".replace(Mh,n)}(r+"-"+i.id)}applyToHost(e){super.setAttribute(e,this.hostAttr,"")}createElement(e,t){const i=super.createElement(e,t);return super.setAttribute(i,this.contentAttr,""),i}}class JR extends Ih{constructor(e,t,i,r){super(e),this.sharedStylesHost=t,this.hostEl=i,this.shadowRoot=i.attachShadow({mode:"open"}),this.sharedStylesHost.addHost(this.shadowRoot);const a=ml(r.id,r.styles,[]);for(let o=0;o<a.length;o++){const s=document.createElement("style");s.textContent=a[o],this.shadowRoot.appendChild(s)}}nodeOrShadowRoot(e){return e===this.hostEl?this.shadowRoot:e}destroy(){this.sharedStylesHost.removeHost(this.shadowRoot)}appendChild(e,t){return super.appendChild(this.nodeOrShadowRoot(e),t)}insertBefore(e,t,i){return super.insertBefore(this.nodeOrShadowRoot(e),t,i)}removeChild(e,t){return super.removeChild(this.nodeOrShadowRoot(e),t)}parentNode(e){return this.nodeOrShadowRoot(super.parentNode(this.nodeOrShadowRoot(e)))}}let ZR=(()=>{class n extends Q0{constructor(t){super(t)}supports(t){return!0}addEventListener(t,i,r){return t.addEventListener(i,r,!1),()=>this.removeEventListener(t,i,r)}removeEventListener(t,i,r){return t.removeEventListener(i,r)}}return n.\u0275fac=function(t){return new(t||n)(v(Q))},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})();const iw=["alt","control","meta","shift"],eP={"\b":"Backspace","\t":"Tab","\x7f":"Delete","\x1b":"Escape",Del:"Delete",Esc:"Escape",Left:"ArrowLeft",Right:"ArrowRight",Up:"ArrowUp",Down:"ArrowDown",Menu:"ContextMenu",Scroll:"ScrollLock",Win:"OS"},tP={alt:n=>n.altKey,control:n=>n.ctrlKey,meta:n=>n.metaKey,shift:n=>n.shiftKey};let nP=(()=>{class n extends Q0{constructor(t){super(t)}supports(t){return null!=n.parseEventName(t)}addEventListener(t,i,r){const a=n.parseEventName(i),o=n.eventCallback(a.fullKey,r,this.manager.getZone());return this.manager.getZone().runOutsideAngular(()=>ai().onAndCancel(t,a.domEventName,o))}static parseEventName(t){const i=t.toLowerCase().split("."),r=i.shift();if(0===i.length||"keydown"!==r&&"keyup"!==r)return null;const a=n._normalizeKey(i.pop());let o="",s=i.indexOf("code");if(s>-1&&(i.splice(s,1),o="code."),iw.forEach(c=>{const d=i.indexOf(c);d>-1&&(i.splice(d,1),o+=c+".")}),o+=a,0!=i.length||0===a.length)return null;const l={};return l.domEventName=r,l.fullKey=o,l}static matchEventFullKeyCode(t,i){let r=eP[t.key]||t.key,a="";return i.indexOf("code.")>-1&&(r=t.code,a="code."),!(null==r||!r)&&(r=r.toLowerCase()," "===r?r="space":"."===r&&(r="dot"),iw.forEach(o=>{o!==r&&(0,tP[o])(t)&&(a+=o+".")}),a+=r,a===i)}static eventCallback(t,i,r){return a=>{n.matchEventFullKeyCode(a,t)&&r.runGuarded(()=>i(a))}}static _normalizeKey(t){return"esc"===t?"escape":t}}return n.\u0275fac=function(t){return new(t||n)(v(Q))},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})();const oP=Zv(NI,"browser",[{provide:Xu,useValue:W0},{provide:Vv,useValue:function iP(){Eh.makeCurrent()},multi:!0},{provide:Q,useFactory:function aP(){return function IC(n){Pd=n}(document),document},deps:[]}]),ow=new E(""),sw=[{provide:Ys,useClass:class GR{addToWindow(e){pe.getAngularTestability=(i,r=!0)=>{const a=e.findTestabilityInTree(i,r);if(null==a)throw new Error("Could not find testability for element.");return a},pe.getAllAngularTestabilities=()=>e.getAllTestabilities(),pe.getAllAngularRootElements=()=>e.getAllRootElements(),pe.frameworkStabilizers||(pe.frameworkStabilizers=[]),pe.frameworkStabilizers.push(i=>{const r=pe.getAllAngularTestabilities();let a=r.length,o=!1;const s=function(l){o=o||l,a--,0==a&&i(o)};r.forEach(function(l){l.whenStable(s)})})}findTestabilityInTree(e,t,i){if(null==t)return null;const r=e.getTestability(t);return null!=r?r:i?ai().isShadowRoot(t)?this.findTestabilityInTree(e,t.host,!0):this.findTestabilityInTree(e,t.parentElement,!0):null}},deps:[]},{provide:Qv,useClass:nh,deps:[ae,ih,Ys]},{provide:nh,useClass:nh,deps:[ae,ih,Ys]}],lw=[{provide:zd,useValue:"root"},{provide:ni,useFactory:function rP(){return new ni},deps:[]},{provide:pl,useClass:ZR,multi:!0,deps:[Q,ae,Xu]},{provide:pl,useClass:nP,multi:!0,deps:[Q]},{provide:bl,useClass:bl,deps:[gl,so,$r]},{provide:Na,useExisting:bl},{provide:K0,useExisting:so},{provide:so,useClass:so,deps:[Q]},{provide:gl,useClass:gl,deps:[pl,ae]},{provide:class hR{},useClass:UR,deps:[]},[]];let cw=(()=>{class n{constructor(t){}static withServerTransition(t){return{ngModule:n,providers:[{provide:$r,useValue:t.appId},{provide:Y0,useExisting:$r},VR]}}}return n.\u0275fac=function(t){return new(t||n)(v(ow,12))},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({providers:[...lw,...sw],imports:[z0,FI]}),n})(),dw=(()=>{class n{constructor(t){this._doc=t}getTitle(){return this._doc.title}setTitle(t){this._doc.title=t||""}}return n.\u0275fac=function(t){return new(t||n)(v(Q))},n.\u0275prov=k({token:n,factory:function(t){let i=null;return i=t?new t:function lP(){return new dw(v(Q))}(),i},providedIn:"root"}),n})();function I(...n){return He(n,ua(n))}"undefined"!=typeof window&&window;class Ut extends _e{constructor(e){super(),this._value=e}get value(){return this.getValue()}_subscribe(e){const t=super._subscribe(e);return!t.closed&&e.next(this._value),t}getValue(){const{hasError:e,thrownError:t,_value:i}=this;if(e)throw t;return this._throwIfClosed(),i}next(e){super.next(this._value=e)}}const yl=ca(n=>function(){n(this),this.name="EmptyError",this.message="no elements in sequence"}),{isArray:mP}=Array,{getPrototypeOf:bP,prototype:yP,keys:vP}=Object;const{isArray:_P}=Array;function Lh(n){return H(e=>function DP(n,e){return _P(e)?n(...e):n(e)}(n,e))}function Oh(...n){const e=ua(n),t=function Ep(n){return te(kc(n))?n.pop():void 0}(n),{args:i,keys:r}=function pw(n){if(1===n.length){const e=n[0];if(mP(e))return{args:e,keys:null};if(function wP(n){return n&&"object"==typeof n&&bP(n)===yP}(e)){const t=vP(e);return{args:t.map(i=>e[i]),keys:t}}}return{args:n,keys:null}}(n);if(0===i.length)return He([],e);const a=new ye(function SP(n,e,t=Qn){return i=>{mw(e,()=>{const{length:r}=n,a=new Array(r);let o=r,s=r;for(let l=0;l<r;l++)mw(e,()=>{const c=He(n[l],e);let d=!1;c.subscribe(ve(i,u=>{a[l]=u,d||(d=!0,s--),s||i.next(t(a.slice()))},()=>{--o||i.complete()}))},i)},i)}}(i,e,r?o=>function gw(n,e){return n.reduce((t,i,r)=>(t[i]=e[r],t),{})}(r,o):Qn));return t?a.pipe(Lh(t)):a}function mw(n,e,t){n?Tn(t,n,e):e()}function vl(...n){return function CP(){return Zi(1)}()(He(n,ua(n)))}function bw(n){return new ye(e=>{ot(n()).subscribe(e)})}function Qr(n,e){const t=te(n)?n:()=>n,i=r=>r.error(t());return new ye(e?r=>e.schedule(i,0,r):i)}function Nh(){return xe((n,e)=>{let t=null;n._refCount++;const i=ve(e,void 0,void 0,void 0,()=>{if(!n||n._refCount<=0||0<--n._refCount)return void(t=null);const r=n._connection,a=t;t=null,r&&(!a||r===a)&&r.unsubscribe(),e.unsubscribe()});n.subscribe(i),i.closed||(t=n.connect())})}class yw extends ye{constructor(e,t){super(),this.source=e,this.subjectFactory=t,this._subject=null,this._refCount=0,this._connection=null,fp(e)&&(this.lift=e.lift)}_subscribe(e){return this.getSubject().subscribe(e)}getSubject(){const e=this._subject;return(!e||e.isStopped)&&(this._subject=this.subjectFactory()),this._subject}_teardown(){this._refCount=0;const{_connection:e}=this;this._subject=this._connection=null,null==e||e.unsubscribe()}connect(){let e=this._connection;if(!e){e=this._connection=new Je;const t=this.getSubject();e.add(this.source.subscribe(ve(t,void 0,()=>{this._teardown(),t.complete()},i=>{this._teardown(),t.error(i)},()=>this._teardown()))),e.closed&&(this._connection=null,e=Je.EMPTY)}return e}refCount(){return Nh()(this)}}function wn(n,e){return xe((t,i)=>{let r=null,a=0,o=!1;const s=()=>o&&!r&&i.complete();t.subscribe(ve(i,l=>{null==r||r.unsubscribe();let c=0;const d=a++;ot(n(l,d)).subscribe(r=ve(i,u=>i.next(e?e(l,u,d,c++):u),()=>{r=null,s()}))},()=>{o=!0,s()}))})}function si(n){return n<=0?()=>En:xe((e,t)=>{let i=0;e.subscribe(ve(t,r=>{++i<=n&&(t.next(r),n<=i&&t.complete())}))})}function vw(...n){const e=ua(n);return xe((t,i)=>{(e?vl(n,t,e):vl(n,t)).subscribe(i)})}function cn(n,e){return xe((t,i)=>{let r=0;t.subscribe(ve(i,a=>n.call(e,a,r++)&&i.next(a)))})}function wl(n){return xe((e,t)=>{let i=!1;e.subscribe(ve(t,r=>{i=!0,t.next(r)},()=>{i||t.next(n),t.complete()}))})}function ww(n=xP){return xe((e,t)=>{let i=!1;e.subscribe(ve(t,r=>{i=!0,t.next(r)},()=>i?t.complete():t.error(n())))})}function xP(){return new yl}function li(n,e){const t=arguments.length>=2;return i=>i.pipe(n?cn((r,a)=>n(r,a,i)):Qn,si(1),t?wl(e):ww(()=>new yl))}function ci(n,e){return te(e)?$e(n,e,1):$e(n,1)}function Ue(n,e,t){const i=te(n)||e||t?{next:n,error:e,complete:t}:n;return i?xe((r,a)=>{var o;null===(o=i.subscribe)||void 0===o||o.call(i);let s=!0;r.subscribe(ve(a,l=>{var c;null===(c=i.next)||void 0===c||c.call(i,l),a.next(l)},()=>{var l;s=!1,null===(l=i.complete)||void 0===l||l.call(i),a.complete()},l=>{var c;s=!1,null===(c=i.error)||void 0===c||c.call(i,l),a.error(l)},()=>{var l,c;s&&(null===(l=i.unsubscribe)||void 0===l||l.call(i)),null===(c=i.finalize)||void 0===c||c.call(i)}))}):Qn}function zn(n){return xe((e,t)=>{let a,i=null,r=!1;i=e.subscribe(ve(t,void 0,void 0,o=>{a=ot(n(o,zn(n)(e))),i?(i.unsubscribe(),i=null,a.subscribe(t)):r=!0})),r&&(i.unsubscribe(),i=null,a.subscribe(t))})}function TP(n,e,t,i,r){return(a,o)=>{let s=t,l=e,c=0;a.subscribe(ve(o,d=>{const u=c++;l=s?n(l,d,u):(s=!0,d),i&&o.next(l)},r&&(()=>{s&&o.next(l),o.complete()})))}}function _w(n,e){return xe(TP(n,e,arguments.length>=2,!0))}function Fh(n){return n<=0?()=>En:xe((e,t)=>{let i=[];e.subscribe(ve(t,r=>{i.push(r),n<i.length&&i.shift()},()=>{for(const r of i)t.next(r);t.complete()},void 0,()=>{i=null}))})}function Dw(n,e){const t=arguments.length>=2;return i=>i.pipe(n?cn((r,a)=>n(r,a,i)):Qn,Fh(1),t?wl(e):ww(()=>new yl))}function _l(n){return xe((e,t)=>{try{e.subscribe(t)}finally{t.add(n)}})}const q="primary",lo=Symbol("RouteTitle");class MP{constructor(e){this.params=e||{}}has(e){return Object.prototype.hasOwnProperty.call(this.params,e)}get(e){if(this.has(e)){const t=this.params[e];return Array.isArray(t)?t[0]:t}return null}getAll(e){if(this.has(e)){const t=this.params[e];return Array.isArray(t)?t:[t]}return[]}get keys(){return Object.keys(this.params)}}function Kr(n){return new MP(n)}function IP(n,e,t){const i=t.path.split("/");if(i.length>n.length||"full"===t.pathMatch&&(e.hasChildren()||i.length<n.length))return null;const r={};for(let a=0;a<i.length;a++){const o=i[a],s=n[a];if(o.startsWith(":"))r[o.substring(1)]=s;else if(o!==s.path)return null}return{consumed:n.slice(0,i.length),posParams:r}}function _n(n,e){const t=n?Object.keys(n):void 0,i=e?Object.keys(e):void 0;if(!t||!i||t.length!=i.length)return!1;let r;for(let a=0;a<t.length;a++)if(r=t[a],!Sw(n[r],e[r]))return!1;return!0}function Sw(n,e){if(Array.isArray(n)&&Array.isArray(e)){if(n.length!==e.length)return!1;const t=[...n].sort(),i=[...e].sort();return t.every((r,a)=>i[a]===r)}return n===e}function Cw(n){return Array.prototype.concat.apply([],n)}function xw(n){return n.length>0?n[n.length-1]:null}function Ke(n,e){for(const t in n)n.hasOwnProperty(t)&&e(n[t],t)}function di(n){return Lb(n)?n:Ls(n)?He(Promise.resolve(n)):I(n)}const PP={exact:function kw(n,e,t){if(!zi(n.segments,e.segments)||!Dl(n.segments,e.segments,t)||n.numberOfChildren!==e.numberOfChildren)return!1;for(const i in e.children)if(!n.children[i]||!kw(n.children[i],e.children[i],t))return!1;return!0},subset:Mw},Tw={exact:function LP(n,e){return _n(n,e)},subset:function OP(n,e){return Object.keys(e).length<=Object.keys(n).length&&Object.keys(e).every(t=>Sw(n[t],e[t]))},ignored:()=>!0};function Ew(n,e,t){return PP[t.paths](n.root,e.root,t.matrixParams)&&Tw[t.queryParams](n.queryParams,e.queryParams)&&!("exact"===t.fragment&&n.fragment!==e.fragment)}function Mw(n,e,t){return Iw(n,e,e.segments,t)}function Iw(n,e,t,i){if(n.segments.length>t.length){const r=n.segments.slice(0,t.length);return!(!zi(r,t)||e.hasChildren()||!Dl(r,t,i))}if(n.segments.length===t.length){if(!zi(n.segments,t)||!Dl(n.segments,t,i))return!1;for(const r in e.children)if(!n.children[r]||!Mw(n.children[r],e.children[r],i))return!1;return!0}{const r=t.slice(0,n.segments.length),a=t.slice(n.segments.length);return!!(zi(n.segments,r)&&Dl(n.segments,r,i)&&n.children[q])&&Iw(n.children[q],e,a,i)}}function Dl(n,e,t){return e.every((i,r)=>Tw[t](n[r].parameters,i.parameters))}class Hi{constructor(e,t,i){this.root=e,this.queryParams=t,this.fragment=i}get queryParamMap(){return this._queryParamMap||(this._queryParamMap=Kr(this.queryParams)),this._queryParamMap}toString(){return jP.serialize(this)}}class K{constructor(e,t){this.segments=e,this.children=t,this.parent=null,Ke(t,(i,r)=>i.parent=this)}hasChildren(){return this.numberOfChildren>0}get numberOfChildren(){return Object.keys(this.children).length}toString(){return Sl(this)}}class co{constructor(e,t){this.path=e,this.parameters=t}get parameterMap(){return this._parameterMap||(this._parameterMap=Kr(this.parameters)),this._parameterMap}toString(){return Lw(this)}}function zi(n,e){return n.length===e.length&&n.every((t,i)=>t.path===e[i].path)}let Aw=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275prov=k({token:n,factory:function(){return new Bh},providedIn:"root"}),n})();class Bh{parse(e){const t=new qP(e);return new Hi(t.parseRootSegment(),t.parseQueryParams(),t.parseFragment())}serialize(e){const t=`/${uo(e.root,!0)}`,i=function zP(n){const e=Object.keys(n).map(t=>{const i=n[t];return Array.isArray(i)?i.map(r=>`${Cl(t)}=${Cl(r)}`).join("&"):`${Cl(t)}=${Cl(i)}`}).filter(t=>!!t);return e.length?`?${e.join("&")}`:""}(e.queryParams);return`${t}${i}${"string"==typeof e.fragment?`#${function BP(n){return encodeURI(n)}(e.fragment)}`:""}`}}const jP=new Bh;function Sl(n){return n.segments.map(e=>Lw(e)).join("/")}function uo(n,e){if(!n.hasChildren())return Sl(n);if(e){const t=n.children[q]?uo(n.children[q],!1):"",i=[];return Ke(n.children,(r,a)=>{a!==q&&i.push(`${a}:${uo(r,!1)}`)}),i.length>0?`${t}(${i.join("//")})`:t}{const t=function FP(n,e){let t=[];return Ke(n.children,(i,r)=>{r===q&&(t=t.concat(e(i,r)))}),Ke(n.children,(i,r)=>{r!==q&&(t=t.concat(e(i,r)))}),t}(n,(i,r)=>r===q?[uo(n.children[q],!1)]:[`${r}:${uo(i,!1)}`]);return 1===Object.keys(n.children).length&&null!=n.children[q]?`${Sl(n)}/${t[0]}`:`${Sl(n)}/(${t.join("//")})`}}function Rw(n){return encodeURIComponent(n).replace(/%40/g,"@").replace(/%3A/gi,":").replace(/%24/g,"$").replace(/%2C/gi,",")}function Cl(n){return Rw(n).replace(/%3B/gi,";")}function Hh(n){return Rw(n).replace(/\(/g,"%28").replace(/\)/g,"%29").replace(/%26/gi,"&")}function xl(n){return decodeURIComponent(n)}function Pw(n){return xl(n.replace(/\+/g,"%20"))}function Lw(n){return`${Hh(n.path)}${function HP(n){return Object.keys(n).map(e=>`;${Hh(e)}=${Hh(n[e])}`).join("")}(n.parameters)}`}const WP=/^[^\/()?;=#]+/;function Tl(n){const e=n.match(WP);return e?e[0]:""}const VP=/^[^=?&#]+/,UP=/^[^&#]+/;class qP{constructor(e){this.url=e,this.remaining=e}parseRootSegment(){return this.consumeOptional("/"),""===this.remaining||this.peekStartsWith("?")||this.peekStartsWith("#")?new K([],{}):new K([],this.parseChildren())}parseQueryParams(){const e={};if(this.consumeOptional("?"))do{this.parseQueryParam(e)}while(this.consumeOptional("&"));return e}parseFragment(){return this.consumeOptional("#")?decodeURIComponent(this.remaining):null}parseChildren(){if(""===this.remaining)return{};this.consumeOptional("/");const e=[];for(this.peekStartsWith("(")||e.push(this.parseSegment());this.peekStartsWith("/")&&!this.peekStartsWith("//")&&!this.peekStartsWith("/(");)this.capture("/"),e.push(this.parseSegment());let t={};this.peekStartsWith("/(")&&(this.capture("/"),t=this.parseParens(!0));let i={};return this.peekStartsWith("(")&&(i=this.parseParens(!1)),(e.length>0||Object.keys(t).length>0)&&(i[q]=new K(e,t)),i}parseSegment(){const e=Tl(this.remaining);if(""===e&&this.peekStartsWith(";"))throw new _(4009,!1);return this.capture(e),new co(xl(e),this.parseMatrixParams())}parseMatrixParams(){const e={};for(;this.consumeOptional(";");)this.parseParam(e);return e}parseParam(e){const t=Tl(this.remaining);if(!t)return;this.capture(t);let i="";if(this.consumeOptional("=")){const r=Tl(this.remaining);r&&(i=r,this.capture(i))}e[xl(t)]=xl(i)}parseQueryParam(e){const t=function GP(n){const e=n.match(VP);return e?e[0]:""}(this.remaining);if(!t)return;this.capture(t);let i="";if(this.consumeOptional("=")){const o=function $P(n){const e=n.match(UP);return e?e[0]:""}(this.remaining);o&&(i=o,this.capture(i))}const r=Pw(t),a=Pw(i);if(e.hasOwnProperty(r)){let o=e[r];Array.isArray(o)||(o=[o],e[r]=o),o.push(a)}else e[r]=a}parseParens(e){const t={};for(this.capture("(");!this.consumeOptional(")")&&this.remaining.length>0;){const i=Tl(this.remaining),r=this.remaining[i.length];if("/"!==r&&")"!==r&&";"!==r)throw new _(4010,!1);let a;i.indexOf(":")>-1?(a=i.slice(0,i.indexOf(":")),this.capture(a),this.capture(":")):e&&(a=q);const o=this.parseChildren();t[a]=1===Object.keys(o).length?o[q]:new K([],o),this.consumeOptional("//")}return t}peekStartsWith(e){return this.remaining.startsWith(e)}consumeOptional(e){return!!this.peekStartsWith(e)&&(this.remaining=this.remaining.substring(e.length),!0)}capture(e){if(!this.consumeOptional(e))throw new _(4011,!1)}}function zh(n){return n.segments.length>0?new K([],{[q]:n}):n}function El(n){const e={};for(const i of Object.keys(n.children)){const a=El(n.children[i]);(a.segments.length>0||a.hasChildren())&&(e[i]=a)}return function YP(n){if(1===n.numberOfChildren&&n.children[q]){const e=n.children[q];return new K(n.segments.concat(e.segments),e.children)}return n}(new K(n.segments,e))}function Wi(n){return n instanceof Hi}function XP(n,e,t,i,r){var a;if(0===t.length)return Xr(e.root,e.root,e.root,i,r);const s=function Fw(n){if("string"==typeof n[0]&&1===n.length&&"/"===n[0])return new Nw(!0,0,n);let e=0,t=!1;const i=n.reduce((r,a,o)=>{if("object"==typeof a&&null!=a){if(a.outlets){const s={};return Ke(a.outlets,(l,c)=>{s[c]="string"==typeof l?l.split("/"):l}),[...r,{outlets:s}]}if(a.segmentPath)return[...r,a.segmentPath]}return"string"!=typeof a?[...r,a]:0===o?(a.split("/").forEach((s,l)=>{0==l&&"."===s||(0==l&&""===s?t=!0:".."===s?e++:""!=s&&r.push(s))}),r):[...r,a]},[]);return new Nw(t,e,i)}(t);return s.toRoot()?Xr(e.root,e.root,new K([],{}),i,r):function l(d){var u;const h=function ZP(n,e,t,i){if(n.isAbsolute)return new Jr(e.root,!0,0);if(-1===i)return new Jr(t,t===e.root,0);return function jw(n,e,t){let i=n,r=e,a=t;for(;a>r;){if(a-=r,i=i.parent,!i)throw new _(4005,!1);r=i.segments.length}return new Jr(i,!1,r-a)}(t,i+(ho(n.commands[0])?0:1),n.numberOfDoubleDots)}(s,e,null===(u=n.snapshot)||void 0===u?void 0:u._urlSegment,d),f=h.processChildren?po(h.segmentGroup,h.index,s.commands):Vh(h.segmentGroup,h.index,s.commands);return Xr(e.root,h.segmentGroup,f,i,r)}(null===(a=n.snapshot)||void 0===a?void 0:a._lastPathIndex)}function ho(n){return"object"==typeof n&&null!=n&&!n.outlets&&!n.segmentPath}function fo(n){return"object"==typeof n&&null!=n&&n.outlets}function Xr(n,e,t,i,r){let o,a={};i&&Ke(i,(l,c)=>{a[c]=Array.isArray(l)?l.map(d=>`${d}`):`${l}`}),o=n===e?t:Ow(n,e,t);const s=zh(El(o));return new Hi(s,a,r)}function Ow(n,e,t){const i={};return Ke(n.children,(r,a)=>{i[a]=r===e?t:Ow(r,e,t)}),new K(n.segments,i)}class Nw{constructor(e,t,i){if(this.isAbsolute=e,this.numberOfDoubleDots=t,this.commands=i,e&&i.length>0&&ho(i[0]))throw new _(4003,!1);const r=i.find(fo);if(r&&r!==xw(i))throw new _(4004,!1)}toRoot(){return this.isAbsolute&&1===this.commands.length&&"/"==this.commands[0]}}class Jr{constructor(e,t,i){this.segmentGroup=e,this.processChildren=t,this.index=i}}function Vh(n,e,t){if(n||(n=new K([],{})),0===n.segments.length&&n.hasChildren())return po(n,e,t);const i=function t3(n,e,t){let i=0,r=e;const a={match:!1,pathIndex:0,commandIndex:0};for(;r<n.segments.length;){if(i>=t.length)return a;const o=n.segments[r],s=t[i];if(fo(s))break;const l=`${s}`,c=i<t.length-1?t[i+1]:null;if(r>0&&void 0===l)break;if(l&&c&&"object"==typeof c&&void 0===c.outlets){if(!Hw(l,c,o))return a;i+=2}else{if(!Hw(l,{},o))return a;i++}r++}return{match:!0,pathIndex:r,commandIndex:i}}(n,e,t),r=t.slice(i.commandIndex);if(i.match&&i.pathIndex<n.segments.length){const a=new K(n.segments.slice(0,i.pathIndex),{});return a.children[q]=new K(n.segments.slice(i.pathIndex),n.children),po(a,0,r)}return i.match&&0===r.length?new K(n.segments,{}):i.match&&!n.hasChildren()?Gh(n,e,t):i.match?po(n,0,r):Gh(n,e,t)}function po(n,e,t){if(0===t.length)return new K(n.segments,{});{const i=function e3(n){return fo(n[0])?n[0].outlets:{[q]:n}}(t),r={};return Ke(i,(a,o)=>{"string"==typeof a&&(a=[a]),null!==a&&(r[o]=Vh(n.children[o],e,a))}),Ke(n.children,(a,o)=>{void 0===i[o]&&(r[o]=a)}),new K(n.segments,r)}}function Gh(n,e,t){const i=n.segments.slice(0,e);let r=0;for(;r<t.length;){const a=t[r];if(fo(a)){const l=n3(a.outlets);return new K(i,l)}if(0===r&&ho(t[0])){i.push(new co(n.segments[e].path,Bw(t[0]))),r++;continue}const o=fo(a)?a.outlets[q]:`${a}`,s=r<t.length-1?t[r+1]:null;o&&s&&ho(s)?(i.push(new co(o,Bw(s))),r+=2):(i.push(new co(o,{})),r++)}return new K(i,{})}function n3(n){const e={};return Ke(n,(t,i)=>{"string"==typeof t&&(t=[t]),null!==t&&(e[i]=Gh(new K([],{}),0,t))}),e}function Bw(n){const e={};return Ke(n,(t,i)=>e[i]=`${t}`),e}function Hw(n,e,t){return n==t.path&&_n(e,t.parameters)}class Wn{constructor(e,t){this.id=e,this.url=t}}class Uh extends Wn{constructor(e,t,i="imperative",r=null){super(e,t),this.type=0,this.navigationTrigger=i,this.restoredState=r}toString(){return`NavigationStart(id: ${this.id}, url: '${this.url}')`}}class Vi extends Wn{constructor(e,t,i){super(e,t),this.urlAfterRedirects=i,this.type=1}toString(){return`NavigationEnd(id: ${this.id}, url: '${this.url}', urlAfterRedirects: '${this.urlAfterRedirects}')`}}class kl extends Wn{constructor(e,t,i,r){super(e,t),this.reason=i,this.code=r,this.type=2}toString(){return`NavigationCancel(id: ${this.id}, url: '${this.url}')`}}class zw extends Wn{constructor(e,t,i,r){super(e,t),this.error=i,this.target=r,this.type=3}toString(){return`NavigationError(id: ${this.id}, url: '${this.url}', error: ${this.error})`}}class r3 extends Wn{constructor(e,t,i,r){super(e,t),this.urlAfterRedirects=i,this.state=r,this.type=4}toString(){return`RoutesRecognized(id: ${this.id}, url: '${this.url}', urlAfterRedirects: '${this.urlAfterRedirects}', state: ${this.state})`}}class a3 extends Wn{constructor(e,t,i,r){super(e,t),this.urlAfterRedirects=i,this.state=r,this.type=7}toString(){return`GuardsCheckStart(id: ${this.id}, url: '${this.url}', urlAfterRedirects: '${this.urlAfterRedirects}', state: ${this.state})`}}class o3 extends Wn{constructor(e,t,i,r,a){super(e,t),this.urlAfterRedirects=i,this.state=r,this.shouldActivate=a,this.type=8}toString(){return`GuardsCheckEnd(id: ${this.id}, url: '${this.url}', urlAfterRedirects: '${this.urlAfterRedirects}', state: ${this.state}, shouldActivate: ${this.shouldActivate})`}}class s3 extends Wn{constructor(e,t,i,r){super(e,t),this.urlAfterRedirects=i,this.state=r,this.type=5}toString(){return`ResolveStart(id: ${this.id}, url: '${this.url}', urlAfterRedirects: '${this.urlAfterRedirects}', state: ${this.state})`}}class l3 extends Wn{constructor(e,t,i,r){super(e,t),this.urlAfterRedirects=i,this.state=r,this.type=6}toString(){return`ResolveEnd(id: ${this.id}, url: '${this.url}', urlAfterRedirects: '${this.urlAfterRedirects}', state: ${this.state})`}}class c3{constructor(e){this.route=e,this.type=9}toString(){return`RouteConfigLoadStart(path: ${this.route.path})`}}class d3{constructor(e){this.route=e,this.type=10}toString(){return`RouteConfigLoadEnd(path: ${this.route.path})`}}class u3{constructor(e){this.snapshot=e,this.type=11}toString(){return`ChildActivationStart(path: '${this.snapshot.routeConfig&&this.snapshot.routeConfig.path||""}')`}}class h3{constructor(e){this.snapshot=e,this.type=12}toString(){return`ChildActivationEnd(path: '${this.snapshot.routeConfig&&this.snapshot.routeConfig.path||""}')`}}class f3{constructor(e){this.snapshot=e,this.type=13}toString(){return`ActivationStart(path: '${this.snapshot.routeConfig&&this.snapshot.routeConfig.path||""}')`}}class p3{constructor(e){this.snapshot=e,this.type=14}toString(){return`ActivationEnd(path: '${this.snapshot.routeConfig&&this.snapshot.routeConfig.path||""}')`}}class Ww{constructor(e,t,i){this.routerEvent=e,this.position=t,this.anchor=i,this.type=15}toString(){return`Scroll(anchor: '${this.anchor}', position: '${this.position?`${this.position[0]}, ${this.position[1]}`:null}')`}}class Vw{constructor(e){this._root=e}get root(){return this._root.value}parent(e){const t=this.pathFromRoot(e);return t.length>1?t[t.length-2]:null}children(e){const t=$h(e,this._root);return t?t.children.map(i=>i.value):[]}firstChild(e){const t=$h(e,this._root);return t&&t.children.length>0?t.children[0].value:null}siblings(e){const t=qh(e,this._root);return t.length<2?[]:t[t.length-2].children.map(r=>r.value).filter(r=>r!==e)}pathFromRoot(e){return qh(e,this._root).map(t=>t.value)}}function $h(n,e){if(n===e.value)return e;for(const t of e.children){const i=$h(n,t);if(i)return i}return null}function qh(n,e){if(n===e.value)return[e];for(const t of e.children){const i=qh(n,t);if(i.length)return i.unshift(e),i}return[]}class Vn{constructor(e,t){this.value=e,this.children=t}toString(){return`TreeNode(${this.value})`}}function Zr(n){const e={};return n&&n.children.forEach(t=>e[t.value.outlet]=t),e}class Gw extends Vw{constructor(e,t){super(e),this.snapshot=t,Yh(this,e)}toString(){return this.snapshot.toString()}}function Uw(n,e){const t=function m3(n,e){const o=new Ml([],{},{},"",{},q,e,null,n.root,-1,{});return new qw("",new Vn(o,[]))}(n,e),i=new Ut([new co("",{})]),r=new Ut({}),a=new Ut({}),o=new Ut({}),s=new Ut(""),l=new ui(i,r,o,s,a,q,e,t.root);return l.snapshot=t.root,new Gw(new Vn(l,[]),t)}class ui{constructor(e,t,i,r,a,o,s,l){var c,d;this.url=e,this.params=t,this.queryParams=i,this.fragment=r,this.data=a,this.outlet=o,this.component=s,this.title=null!==(d=null===(c=this.data)||void 0===c?void 0:c.pipe(H(u=>u[lo])))&&void 0!==d?d:I(void 0),this._futureSnapshot=l}get routeConfig(){return this._futureSnapshot.routeConfig}get root(){return this._routerState.root}get parent(){return this._routerState.parent(this)}get firstChild(){return this._routerState.firstChild(this)}get children(){return this._routerState.children(this)}get pathFromRoot(){return this._routerState.pathFromRoot(this)}get paramMap(){return this._paramMap||(this._paramMap=this.params.pipe(H(e=>Kr(e)))),this._paramMap}get queryParamMap(){return this._queryParamMap||(this._queryParamMap=this.queryParams.pipe(H(e=>Kr(e)))),this._queryParamMap}toString(){return this.snapshot?this.snapshot.toString():`Future(${this._futureSnapshot})`}}function $w(n,e="emptyOnly"){const t=n.pathFromRoot;let i=0;if("always"!==e)for(i=t.length-1;i>=1;){const r=t[i],a=t[i-1];if(r.routeConfig&&""===r.routeConfig.path)i--;else{if(a.component)break;i--}}return function b3(n){return n.reduce((e,t)=>{var i;return{params:Object.assign(Object.assign({},e.params),t.params),data:Object.assign(Object.assign({},e.data),t.data),resolve:Object.assign(Object.assign(Object.assign(Object.assign({},t.data),e.resolve),null===(i=t.routeConfig)||void 0===i?void 0:i.data),t._resolvedData)}},{params:{},data:{},resolve:{}})}(t.slice(i))}class Ml{constructor(e,t,i,r,a,o,s,l,c,d,u,h){var f;this.url=e,this.params=t,this.queryParams=i,this.fragment=r,this.data=a,this.outlet=o,this.component=s,this.title=null===(f=this.data)||void 0===f?void 0:f[lo],this.routeConfig=l,this._urlSegment=c,this._lastPathIndex=d,this._correctedLastPathIndex=null!=h?h:d,this._resolve=u}get root(){return this._routerState.root}get parent(){return this._routerState.parent(this)}get firstChild(){return this._routerState.firstChild(this)}get children(){return this._routerState.children(this)}get pathFromRoot(){return this._routerState.pathFromRoot(this)}get paramMap(){return this._paramMap||(this._paramMap=Kr(this.params)),this._paramMap}get queryParamMap(){return this._queryParamMap||(this._queryParamMap=Kr(this.queryParams)),this._queryParamMap}toString(){return`Route(url:'${this.url.map(i=>i.toString()).join("/")}', path:'${this.routeConfig?this.routeConfig.path:""}')`}}class qw extends Vw{constructor(e,t){super(t),this.url=e,Yh(this,t)}toString(){return Yw(this._root)}}function Yh(n,e){e.value._routerState=n,e.children.forEach(t=>Yh(n,t))}function Yw(n){const e=n.children.length>0?` { ${n.children.map(Yw).join(", ")} } `:"";return`${n.value}${e}`}function Qh(n){if(n.snapshot){const e=n.snapshot,t=n._futureSnapshot;n.snapshot=t,_n(e.queryParams,t.queryParams)||n.queryParams.next(t.queryParams),e.fragment!==t.fragment&&n.fragment.next(t.fragment),_n(e.params,t.params)||n.params.next(t.params),function AP(n,e){if(n.length!==e.length)return!1;for(let t=0;t<n.length;++t)if(!_n(n[t],e[t]))return!1;return!0}(e.url,t.url)||n.url.next(t.url),_n(e.data,t.data)||n.data.next(t.data)}else n.snapshot=n._futureSnapshot,n.data.next(n._futureSnapshot.data)}function Kh(n,e){const t=_n(n.params,e.params)&&function NP(n,e){return zi(n,e)&&n.every((t,i)=>_n(t.parameters,e[i].parameters))}(n.url,e.url);return t&&!(!n.parent!=!e.parent)&&(!n.parent||Kh(n.parent,e.parent))}function go(n,e,t){if(t&&n.shouldReuseRoute(e.value,t.value.snapshot)){const i=t.value;i._futureSnapshot=e.value;const r=function v3(n,e,t){return e.children.map(i=>{for(const r of t.children)if(n.shouldReuseRoute(i.value,r.value.snapshot))return go(n,i,r);return go(n,i)})}(n,e,t);return new Vn(i,r)}{if(n.shouldAttach(e.value)){const a=n.retrieve(e.value);if(null!==a){const o=a.route;return o.value._futureSnapshot=e.value,o.children=e.children.map(s=>go(n,s)),o}}const i=function w3(n){return new ui(new Ut(n.url),new Ut(n.params),new Ut(n.queryParams),new Ut(n.fragment),new Ut(n.data),n.outlet,n.component,n)}(e.value),r=e.children.map(a=>go(n,a));return new Vn(i,r)}}const Xh="ngNavigationCancelingError";function Qw(n,e){const{redirectTo:t,navigationBehaviorOptions:i}=Wi(e)?{redirectTo:e,navigationBehaviorOptions:void 0}:e,r=Kw(!1,0,e);return r.url=t,r.navigationBehaviorOptions=i,r}function Kw(n,e,t){const i=new Error("NavigationCancelingError: "+(n||""));return i[Xh]=!0,i.cancellationCode=e,t&&(i.url=t),i}function Xw(n){return Jw(n)&&Wi(n.url)}function Jw(n){return n&&n[Xh]}class _3{constructor(){this.outlet=null,this.route=null,this.resolver=null,this.injector=null,this.children=new mo,this.attachRef=null}}let mo=(()=>{class n{constructor(){this.contexts=new Map}onChildOutletCreated(t,i){const r=this.getOrCreateContext(t);r.outlet=i,this.contexts.set(t,r)}onChildOutletDestroyed(t){const i=this.getContext(t);i&&(i.outlet=null,i.attachRef=null)}onOutletDeactivated(){const t=this.contexts;return this.contexts=new Map,t}onOutletReAttached(t){this.contexts=t}getOrCreateContext(t){let i=this.getContext(t);return i||(i=new _3,this.contexts.set(t,i)),i}getContext(t){return this.contexts.get(t)||null}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();const Il=!1;let Jh=(()=>{class n{constructor(t,i,r,a,o){this.parentContexts=t,this.location=i,this.changeDetector=a,this.environmentInjector=o,this.activated=null,this._activatedRoute=null,this.activateEvents=new it,this.deactivateEvents=new it,this.attachEvents=new it,this.detachEvents=new it,this.name=r||q,t.onChildOutletCreated(this.name,this)}ngOnDestroy(){var t;(null===(t=this.parentContexts.getContext(this.name))||void 0===t?void 0:t.outlet)===this&&this.parentContexts.onChildOutletDestroyed(this.name)}ngOnInit(){if(!this.activated){const t=this.parentContexts.getContext(this.name);t&&t.route&&(t.attachRef?this.attach(t.attachRef,t.route):this.activateWith(t.route,t.injector))}}get isActivated(){return!!this.activated}get component(){if(!this.activated)throw new _(4012,Il);return this.activated.instance}get activatedRoute(){if(!this.activated)throw new _(4012,Il);return this._activatedRoute}get activatedRouteData(){return this._activatedRoute?this._activatedRoute.snapshot.data:{}}detach(){if(!this.activated)throw new _(4012,Il);this.location.detach();const t=this.activated;return this.activated=null,this._activatedRoute=null,this.detachEvents.emit(t.instance),t}attach(t,i){this.activated=t,this._activatedRoute=i,this.location.insert(t.hostView),this.attachEvents.emit(t.instance)}deactivate(){if(this.activated){const t=this.component;this.activated.destroy(),this.activated=null,this._activatedRoute=null,this.deactivateEvents.emit(t)}}activateWith(t,i){if(this.isActivated)throw new _(4013,Il);this._activatedRoute=t;const r=this.location,o=t._futureSnapshot.component,s=this.parentContexts.getOrCreateContext(this.name).children,l=new D3(t,s,r.injector);if(i&&function S3(n){return!!n.resolveComponentFactory}(i)){const c=i.resolveComponentFactory(o);this.activated=r.createComponent(c,r.length,l)}else this.activated=r.createComponent(o,{index:r.length,injector:l,environmentInjector:null!=i?i:this.environmentInjector});this.changeDetector.markForCheck(),this.activateEvents.emit(this.activated.instance)}}return n.\u0275fac=function(t){return new(t||n)(C(mo),C(Wt),Da("name"),C(eo),C(ti))},n.\u0275dir=Re({type:n,selectors:[["router-outlet"]],outputs:{activateEvents:"activate",deactivateEvents:"deactivate",attachEvents:"attach",detachEvents:"detach"},exportAs:["outlet"],standalone:!0}),n})();class D3{constructor(e,t,i){this.route=e,this.childContexts=t,this.parent=i}get(e,t){return e===ui?this.route:e===mo?this.childContexts:this.parent.get(e,t)}}let Zh=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275cmp=mt({type:n,selectors:[["ng-component"]],standalone:!0,features:[tv],decls:1,vars:0,template:function(t,i){1&t&&zt(0,"router-outlet")},dependencies:[Jh],encapsulation:2}),n})();function Zw(n,e){var t;return n.providers&&!n._injector&&(n._injector=zs(n.providers,e,`Route: ${n.path}`)),null!==(t=n._injector)&&void 0!==t?t:e}function tf(n){const e=n.children&&n.children.map(tf),t=e?Object.assign(Object.assign({},n),{children:e}):Object.assign({},n);return!t.component&&!t.loadComponent&&(e||t.loadChildren)&&t.outlet&&t.outlet!==q&&(t.component=Zh),t}function $t(n){return n.outlet||q}function e1(n,e){const t=n.filter(i=>$t(i)===e);return t.push(...n.filter(i=>$t(i)!==e)),t}function bo(n){var e;if(!n)return null;if(null!==(e=n.routeConfig)&&void 0!==e&&e._injector)return n.routeConfig._injector;for(let t=n.parent;t;t=t.parent){const i=t.routeConfig;if(null!=i&&i._loadedInjector)return i._loadedInjector;if(null!=i&&i._injector)return i._injector}return null}class k3{constructor(e,t,i,r){this.routeReuseStrategy=e,this.futureState=t,this.currState=i,this.forwardEvent=r}activate(e){const t=this.futureState._root,i=this.currState?this.currState._root:null;this.deactivateChildRoutes(t,i,e),Qh(this.futureState.root),this.activateChildRoutes(t,i,e)}deactivateChildRoutes(e,t,i){const r=Zr(t);e.children.forEach(a=>{const o=a.value.outlet;this.deactivateRoutes(a,r[o],i),delete r[o]}),Ke(r,(a,o)=>{this.deactivateRouteAndItsChildren(a,i)})}deactivateRoutes(e,t,i){const r=e.value,a=t?t.value:null;if(r===a)if(r.component){const o=i.getContext(r.outlet);o&&this.deactivateChildRoutes(e,t,o.children)}else this.deactivateChildRoutes(e,t,i);else a&&this.deactivateRouteAndItsChildren(t,i)}deactivateRouteAndItsChildren(e,t){e.value.component&&this.routeReuseStrategy.shouldDetach(e.value.snapshot)?this.detachAndStoreRouteSubtree(e,t):this.deactivateRouteAndOutlet(e,t)}detachAndStoreRouteSubtree(e,t){const i=t.getContext(e.value.outlet),r=i&&e.value.component?i.children:t,a=Zr(e);for(const o of Object.keys(a))this.deactivateRouteAndItsChildren(a[o],r);if(i&&i.outlet){const o=i.outlet.detach(),s=i.children.onOutletDeactivated();this.routeReuseStrategy.store(e.value.snapshot,{componentRef:o,route:e,contexts:s})}}deactivateRouteAndOutlet(e,t){const i=t.getContext(e.value.outlet),r=i&&e.value.component?i.children:t,a=Zr(e);for(const o of Object.keys(a))this.deactivateRouteAndItsChildren(a[o],r);i&&i.outlet&&(i.outlet.deactivate(),i.children.onOutletDeactivated(),i.attachRef=null,i.resolver=null,i.route=null)}activateChildRoutes(e,t,i){const r=Zr(t);e.children.forEach(a=>{this.activateRoutes(a,r[a.value.outlet],i),this.forwardEvent(new p3(a.value.snapshot))}),e.children.length&&this.forwardEvent(new h3(e.value.snapshot))}activateRoutes(e,t,i){var r;const a=e.value,o=t?t.value:null;if(Qh(a),a===o)if(a.component){const s=i.getOrCreateContext(a.outlet);this.activateChildRoutes(e,t,s.children)}else this.activateChildRoutes(e,t,i);else if(a.component){const s=i.getOrCreateContext(a.outlet);if(this.routeReuseStrategy.shouldAttach(a.snapshot)){const l=this.routeReuseStrategy.retrieve(a.snapshot);this.routeReuseStrategy.store(a.snapshot,null),s.children.onOutletReAttached(l.contexts),s.attachRef=l.componentRef,s.route=l.route.value,s.outlet&&s.outlet.attach(l.componentRef,l.route.value),Qh(l.route.value),this.activateChildRoutes(e,null,s.children)}else{const l=bo(a.snapshot),c=null!==(r=null==l?void 0:l.get(_r))&&void 0!==r?r:null;s.attachRef=null,s.route=a,s.resolver=c,s.injector=l,s.outlet&&s.outlet.activateWith(a,s.injector),this.activateChildRoutes(e,null,s.children)}}else this.activateChildRoutes(e,null,i)}}class t1{constructor(e){this.path=e,this.route=this.path[this.path.length-1]}}class Al{constructor(e,t){this.component=e,this.route=t}}function M3(n,e,t){const i=n._root;return yo(i,e?e._root:null,t,[i.value])}function ea(n,e){const t=Symbol(),i=e.get(n,t);return i===t?"function"!=typeof n||function gD(n){return null!==zo(n)}(n)?e.get(n):n:i}function yo(n,e,t,i,r={canDeactivateChecks:[],canActivateChecks:[]}){const a=Zr(e);return n.children.forEach(o=>{(function A3(n,e,t,i,r={canDeactivateChecks:[],canActivateChecks:[]}){const a=n.value,o=e?e.value:null,s=t?t.getContext(n.value.outlet):null;if(o&&a.routeConfig===o.routeConfig){const l=function R3(n,e,t){if("function"==typeof t)return t(n,e);switch(t){case"pathParamsChange":return!zi(n.url,e.url);case"pathParamsOrQueryParamsChange":return!zi(n.url,e.url)||!_n(n.queryParams,e.queryParams);case"always":return!0;case"paramsOrQueryParamsChange":return!Kh(n,e)||!_n(n.queryParams,e.queryParams);default:return!Kh(n,e)}}(o,a,a.routeConfig.runGuardsAndResolvers);l?r.canActivateChecks.push(new t1(i)):(a.data=o.data,a._resolvedData=o._resolvedData),yo(n,e,a.component?s?s.children:null:t,i,r),l&&s&&s.outlet&&s.outlet.isActivated&&r.canDeactivateChecks.push(new Al(s.outlet.component,o))}else o&&vo(e,s,r),r.canActivateChecks.push(new t1(i)),yo(n,null,a.component?s?s.children:null:t,i,r)})(o,a[o.value.outlet],t,i.concat([o.value]),r),delete a[o.value.outlet]}),Ke(a,(o,s)=>vo(o,t.getContext(s),r)),r}function vo(n,e,t){const i=Zr(n),r=n.value;Ke(i,(a,o)=>{vo(a,r.component?e?e.children.getContext(o):null:e,t)}),t.canDeactivateChecks.push(new Al(r.component&&e&&e.outlet&&e.outlet.isActivated?e.outlet.component:null,r))}function wo(n){return"function"==typeof n}function nf(n){return n instanceof yl||"EmptyError"===(null==n?void 0:n.name)}const Rl=Symbol("INITIAL_VALUE");function ta(){return wn(n=>Oh(n.map(e=>e.pipe(si(1),vw(Rl)))).pipe(H(e=>{for(const t of e)if(!0!==t){if(t===Rl)return Rl;if(!1===t||t instanceof Hi)return t}return!0}),cn(e=>e!==Rl),si(1)))}function n1(n){return function F2(...n){return dp(n)}(Ue(e=>{if(Wi(e))throw Qw(0,e)}),H(e=>!0===e))}const rf={matched:!1,consumedSegments:[],remainingSegments:[],parameters:{},positionalParamSegments:{}};function r1(n,e,t,i,r){const a=af(n,e,t);return a.matched?function Q3(n,e,t,i){const r=e.canMatch;return r&&0!==r.length?I(r.map(o=>{const s=ea(o,n);return di(function j3(n){return n&&wo(n.canMatch)}(s)?s.canMatch(e,t):n.runInContext(()=>s(e,t)))})).pipe(ta(),n1()):I(!0)}(i=Zw(e,i),e,t).pipe(H(o=>!0===o?a:Object.assign({},rf))):I(a)}function af(n,e,t){var i;if(""===e.path)return"full"===e.pathMatch&&(n.hasChildren()||t.length>0)?Object.assign({},rf):{matched:!0,consumedSegments:[],remainingSegments:t,parameters:{},positionalParamSegments:{}};const a=(e.matcher||IP)(t,n,e);if(!a)return Object.assign({},rf);const o={};Ke(a.posParams,(l,c)=>{o[c]=l.path});const s=a.consumed.length>0?Object.assign(Object.assign({},o),a.consumed[a.consumed.length-1].parameters):o;return{matched:!0,consumedSegments:a.consumed,remainingSegments:t.slice(a.consumed.length),parameters:s,positionalParamSegments:null!==(i=a.posParams)&&void 0!==i?i:{}}}function Pl(n,e,t,i,r="corrected"){if(t.length>0&&function J3(n,e,t){return t.some(i=>Ll(n,e,i)&&$t(i)!==q)}(n,t,i)){const o=new K(e,function X3(n,e,t,i){const r={};r[q]=i,i._sourceSegment=n,i._segmentIndexShift=e.length;for(const a of t)if(""===a.path&&$t(a)!==q){const o=new K([],{});o._sourceSegment=n,o._segmentIndexShift=e.length,r[$t(a)]=o}return r}(n,e,i,new K(t,n.children)));return o._sourceSegment=n,o._segmentIndexShift=e.length,{segmentGroup:o,slicedSegments:[]}}if(0===t.length&&function Z3(n,e,t){return t.some(i=>Ll(n,e,i))}(n,t,i)){const o=new K(n.segments,function K3(n,e,t,i,r,a){const o={};for(const s of i)if(Ll(n,t,s)&&!r[$t(s)]){const l=new K([],{});l._sourceSegment=n,l._segmentIndexShift="legacy"===a?n.segments.length:e.length,o[$t(s)]=l}return Object.assign(Object.assign({},r),o)}(n,e,t,i,n.children,r));return o._sourceSegment=n,o._segmentIndexShift=e.length,{segmentGroup:o,slicedSegments:t}}const a=new K(n.segments,n.children);return a._sourceSegment=n,a._segmentIndexShift=e.length,{segmentGroup:a,slicedSegments:t}}function Ll(n,e,t){return(!(n.hasChildren()||e.length>0)||"full"!==t.pathMatch)&&""===t.path}function a1(n,e,t,i){return!!($t(n)===i||i!==q&&Ll(e,t,n))&&("**"===n.path||af(e,n,t).matched)}function o1(n,e,t){return 0===e.length&&!n.children[t]}const Ol=!1;class Nl{constructor(e){this.segmentGroup=e||null}}class s1{constructor(e){this.urlTree=e}}function _o(n){return Qr(new Nl(n))}function l1(n){return Qr(new s1(n))}class i8{constructor(e,t,i,r,a){this.injector=e,this.configLoader=t,this.urlSerializer=i,this.urlTree=r,this.config=a,this.allowRedirects=!0}apply(){const e=Pl(this.urlTree.root,[],[],this.config).segmentGroup,t=new K(e.segments,e.children);return this.expandSegmentGroup(this.injector,this.config,t,q).pipe(H(a=>this.createUrlTree(El(a),this.urlTree.queryParams,this.urlTree.fragment))).pipe(zn(a=>{if(a instanceof s1)return this.allowRedirects=!1,this.match(a.urlTree);throw a instanceof Nl?this.noMatchError(a):a}))}match(e){return this.expandSegmentGroup(this.injector,this.config,e.root,q).pipe(H(r=>this.createUrlTree(El(r),e.queryParams,e.fragment))).pipe(zn(r=>{throw r instanceof Nl?this.noMatchError(r):r}))}noMatchError(e){return new _(4002,Ol)}createUrlTree(e,t,i){const r=zh(e);return new Hi(r,t,i)}expandSegmentGroup(e,t,i,r){return 0===i.segments.length&&i.hasChildren()?this.expandChildren(e,t,i).pipe(H(a=>new K([],a))):this.expandSegment(e,i,t,i.segments,r,!0)}expandChildren(e,t,i){const r=[];for(const a of Object.keys(i.children))"primary"===a?r.unshift(a):r.push(a);return He(r).pipe(ci(a=>{const o=i.children[a],s=e1(t,a);return this.expandSegmentGroup(e,s,o,a).pipe(H(l=>({segment:l,outlet:a})))}),_w((a,o)=>(a[o.outlet]=o.segment,a),{}),Dw())}expandSegment(e,t,i,r,a,o){return He(i).pipe(ci(s=>this.expandSegmentAgainstRoute(e,t,i,s,r,a,o).pipe(zn(c=>{if(c instanceof Nl)return I(null);throw c}))),li(s=>!!s),zn((s,l)=>{if(nf(s))return o1(t,r,a)?I(new K([],{})):_o(t);throw s}))}expandSegmentAgainstRoute(e,t,i,r,a,o,s){return a1(r,t,a,o)?void 0===r.redirectTo?this.matchSegmentAgainstRoute(e,t,r,a,o):s&&this.allowRedirects?this.expandSegmentAgainstRouteUsingRedirect(e,t,i,r,a,o):_o(t):_o(t)}expandSegmentAgainstRouteUsingRedirect(e,t,i,r,a,o){return"**"===r.path?this.expandWildCardWithParamsAgainstRouteUsingRedirect(e,i,r,o):this.expandRegularSegmentAgainstRouteUsingRedirect(e,t,i,r,a,o)}expandWildCardWithParamsAgainstRouteUsingRedirect(e,t,i,r){const a=this.applyRedirectCommands([],i.redirectTo,{});return i.redirectTo.startsWith("/")?l1(a):this.lineralizeSegments(i,a).pipe($e(o=>{const s=new K(o,{});return this.expandSegment(e,s,t,o,r,!1)}))}expandRegularSegmentAgainstRouteUsingRedirect(e,t,i,r,a,o){const{matched:s,consumedSegments:l,remainingSegments:c,positionalParamSegments:d}=af(t,r,a);if(!s)return _o(t);const u=this.applyRedirectCommands(l,r.redirectTo,d);return r.redirectTo.startsWith("/")?l1(u):this.lineralizeSegments(r,u).pipe($e(h=>this.expandSegment(e,t,i,h.concat(c),o,!1)))}matchSegmentAgainstRoute(e,t,i,r,a){return"**"===i.path?(e=Zw(i,e),i.loadChildren?(i._loadedRoutes?I({routes:i._loadedRoutes,injector:i._loadedInjector}):this.configLoader.loadChildren(e,i)).pipe(H(s=>(i._loadedRoutes=s.routes,i._loadedInjector=s.injector,new K(r,{})))):I(new K(r,{}))):r1(t,i,r,e).pipe(wn(({matched:o,consumedSegments:s,remainingSegments:l})=>{var c;return o?(e=null!==(c=i._injector)&&void 0!==c?c:e,this.getChildConfig(e,i,r).pipe($e(u=>{var h;const f=null!==(h=u.injector)&&void 0!==h?h:e,p=u.routes,{segmentGroup:g,slicedSegments:m}=Pl(t,s,l,p),b=new K(g.segments,g.children);if(0===m.length&&b.hasChildren())return this.expandChildren(f,p,b).pipe(H(j=>new K(s,j)));if(0===p.length&&0===m.length)return I(new K(s,{}));const D=$t(i)===a;return this.expandSegment(f,b,p,m,D?q:a,!0).pipe(H(S=>new K(s.concat(S.segments),S.children)))}))):_o(t)}))}getChildConfig(e,t,i){return t.children?I({routes:t.children,injector:e}):t.loadChildren?void 0!==t._loadedRoutes?I({routes:t._loadedRoutes,injector:t._loadedInjector}):function Y3(n,e,t,i){const r=e.canLoad;return void 0===r||0===r.length?I(!0):I(r.map(o=>{const s=ea(o,n);return di(function L3(n){return n&&wo(n.canLoad)}(s)?s.canLoad(e,t):n.runInContext(()=>s(e,t)))})).pipe(ta(),n1())}(e,t,i).pipe($e(r=>r?this.configLoader.loadChildren(e,t).pipe(Ue(a=>{t._loadedRoutes=a.routes,t._loadedInjector=a.injector})):function t8(n){return Qr(Kw(Ol,3))}())):I({routes:[],injector:e})}lineralizeSegments(e,t){let i=[],r=t.root;for(;;){if(i=i.concat(r.segments),0===r.numberOfChildren)return I(i);if(r.numberOfChildren>1||!r.children[q])return Qr(new _(4e3,Ol));r=r.children[q]}}applyRedirectCommands(e,t,i){return this.applyRedirectCreateUrlTree(t,this.urlSerializer.parse(t),e,i)}applyRedirectCreateUrlTree(e,t,i,r){const a=this.createSegmentGroup(e,t.root,i,r);return new Hi(a,this.createQueryParams(t.queryParams,this.urlTree.queryParams),t.fragment)}createQueryParams(e,t){const i={};return Ke(e,(r,a)=>{if("string"==typeof r&&r.startsWith(":")){const s=r.substring(1);i[a]=t[s]}else i[a]=r}),i}createSegmentGroup(e,t,i,r){const a=this.createSegments(e,t.segments,i,r);let o={};return Ke(t.children,(s,l)=>{o[l]=this.createSegmentGroup(e,s,i,r)}),new K(a,o)}createSegments(e,t,i,r){return t.map(a=>a.path.startsWith(":")?this.findPosParam(e,a,r):this.findOrReturn(a,i))}findPosParam(e,t,i){const r=i[t.path.substring(1)];if(!r)throw new _(4001,Ol);return r}findOrReturn(e,t){let i=0;for(const r of t){if(r.path===e.path)return t.splice(i),r;i++}return e}}class a8{}class l8{constructor(e,t,i,r,a,o,s,l){this.injector=e,this.rootComponentType=t,this.config=i,this.urlTree=r,this.url=a,this.paramsInheritanceStrategy=o,this.relativeLinkResolution=s,this.urlSerializer=l}recognize(){const e=Pl(this.urlTree.root,[],[],this.config.filter(t=>void 0===t.redirectTo),this.relativeLinkResolution).segmentGroup;return this.processSegmentGroup(this.injector,this.config,e,q).pipe(H(t=>{if(null===t)return null;const i=new Ml([],Object.freeze({}),Object.freeze(Object.assign({},this.urlTree.queryParams)),this.urlTree.fragment,{},q,this.rootComponentType,null,this.urlTree.root,-1,{}),r=new Vn(i,t),a=new qw(this.url,r);return this.inheritParamsAndData(a._root),a}))}inheritParamsAndData(e){const t=e.value,i=$w(t,this.paramsInheritanceStrategy);t.params=Object.freeze(i.params),t.data=Object.freeze(i.data),e.children.forEach(r=>this.inheritParamsAndData(r))}processSegmentGroup(e,t,i,r){return 0===i.segments.length&&i.hasChildren()?this.processChildren(e,t,i):this.processSegment(e,t,i,i.segments,r)}processChildren(e,t,i){return He(Object.keys(i.children)).pipe(ci(r=>{const a=i.children[r],o=e1(t,r);return this.processSegmentGroup(e,o,a,r)}),_w((r,a)=>r&&a?(r.push(...a),r):null),function EP(n,e=!1){return xe((t,i)=>{let r=0;t.subscribe(ve(i,a=>{const o=n(a,r++);(o||e)&&i.next(a),!o&&i.complete()}))})}(r=>null!==r),wl(null),Dw(),H(r=>{if(null===r)return null;const a=c1(r);return function c8(n){n.sort((e,t)=>e.value.outlet===q?-1:t.value.outlet===q?1:e.value.outlet.localeCompare(t.value.outlet))}(a),a}))}processSegment(e,t,i,r,a){return He(t).pipe(ci(o=>{var s;return this.processSegmentAgainstRoute(null!==(s=o._injector)&&void 0!==s?s:e,o,i,r,a)}),li(o=>!!o),zn(o=>{if(nf(o))return o1(i,r,a)?I([]):I(null);throw o}))}processSegmentAgainstRoute(e,t,i,r,a){var o,s;if(t.redirectTo||!a1(t,i,r,a))return I(null);let l;if("**"===t.path){const c=r.length>0?xw(r).parameters:{},d=u1(i)+r.length;l=I({snapshot:new Ml(r,c,Object.freeze(Object.assign({},this.urlTree.queryParams)),this.urlTree.fragment,f1(t),$t(t),null!==(s=null!==(o=t.component)&&void 0!==o?o:t._loadedComponent)&&void 0!==s?s:null,t,d1(i),d,p1(t),d),consumedSegments:[],remainingSegments:[]})}else l=r1(i,t,r,e).pipe(H(({matched:c,consumedSegments:d,remainingSegments:u,parameters:h})=>{var f,p;if(!c)return null;const g=u1(i)+d.length;return{snapshot:new Ml(d,h,Object.freeze(Object.assign({},this.urlTree.queryParams)),this.urlTree.fragment,f1(t),$t(t),null!==(p=null!==(f=t.component)&&void 0!==f?f:t._loadedComponent)&&void 0!==p?p:null,t,d1(i),g,p1(t),g),consumedSegments:d,remainingSegments:u}}));return l.pipe(wn(c=>{var d,u;if(null===c)return I(null);const{snapshot:h,consumedSegments:f,remainingSegments:p}=c;e=null!==(d=t._injector)&&void 0!==d?d:e;const g=null!==(u=t._loadedInjector)&&void 0!==u?u:e,m=function d8(n){return n.children?n.children:n.loadChildren?n._loadedRoutes:[]}(t),{segmentGroup:b,slicedSegments:D}=Pl(i,f,p,m.filter(S=>void 0===S.redirectTo),this.relativeLinkResolution);if(0===D.length&&b.hasChildren())return this.processChildren(g,m,b).pipe(H(S=>null===S?null:[new Vn(h,S)]));if(0===m.length&&0===D.length)return I([new Vn(h,[])]);const y=$t(t)===a;return this.processSegment(g,m,b,D,y?q:a).pipe(H(S=>null===S?null:[new Vn(h,S)]))}))}}function u8(n){const e=n.value.routeConfig;return e&&""===e.path&&void 0===e.redirectTo}function c1(n){const e=[],t=new Set;for(const i of n){if(!u8(i)){e.push(i);continue}const r=e.find(a=>i.value.routeConfig===a.value.routeConfig);void 0!==r?(r.children.push(...i.children),t.add(r)):e.push(i)}for(const i of t){const r=c1(i.children);e.push(new Vn(i.value,r))}return e.filter(i=>!t.has(i))}function d1(n){let e=n;for(;e._sourceSegment;)e=e._sourceSegment;return e}function u1(n){var e,t;let i=n,r=null!==(e=i._segmentIndexShift)&&void 0!==e?e:0;for(;i._sourceSegment;)i=i._sourceSegment,r+=null!==(t=i._segmentIndexShift)&&void 0!==t?t:0;return r-1}function f1(n){return n.data||{}}function p1(n){return n.resolve||{}}function g1(n){return"string"==typeof n.title||null===n.title}function sf(n){return wn(e=>{const t=n(e);return t?He(t).pipe(H(()=>e)):I(e)})}let m1=(()=>{class n{buildTitle(t){var i;let r,a=t.root;for(;void 0!==a;)r=null!==(i=this.getResolvedTitleForRoute(a))&&void 0!==i?i:r,a=a.children.find(o=>o.outlet===q);return r}getResolvedTitleForRoute(t){return t.data[lo]}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275prov=k({token:n,factory:function(){return me(b1)},providedIn:"root"}),n})(),b1=(()=>{class n extends m1{constructor(t){super(),this.title=t}updateTitle(t){const i=this.buildTitle(t);void 0!==i&&this.title.setTitle(i)}}return n.\u0275fac=function(t){return new(t||n)(v(dw))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();class v8{}class _8 extends class w8{shouldDetach(e){return!1}store(e,t){}shouldAttach(e){return!1}retrieve(e){return null}shouldReuseRoute(e,t){return e.routeConfig===t.routeConfig}}{}const jl=new E("",{providedIn:"root",factory:()=>({})}),lf=new E("ROUTES");let cf=(()=>{class n{constructor(t,i){this.injector=t,this.compiler=i,this.componentLoaders=new WeakMap,this.childrenLoaders=new WeakMap}loadComponent(t){if(this.componentLoaders.get(t))return this.componentLoaders.get(t);if(t._loadedComponent)return I(t._loadedComponent);this.onLoadStartListener&&this.onLoadStartListener(t);const i=di(t.loadComponent()).pipe(Ue(a=>{this.onLoadEndListener&&this.onLoadEndListener(t),t._loadedComponent=a}),_l(()=>{this.componentLoaders.delete(t)})),r=new yw(i,()=>new _e).pipe(Nh());return this.componentLoaders.set(t,r),r}loadChildren(t,i){if(this.childrenLoaders.get(i))return this.childrenLoaders.get(i);if(i._loadedRoutes)return I({routes:i._loadedRoutes,injector:i._loadedInjector});this.onLoadStartListener&&this.onLoadStartListener(i);const a=this.loadModuleFactoryOrRoutes(i.loadChildren).pipe(H(s=>{this.onLoadEndListener&&this.onLoadEndListener(i);let l,c,d=!1;Array.isArray(s)?c=s:(l=s.create(t).injector,c=Cw(l.get(lf,[],O.Self|O.Optional)));return{routes:c.map(tf),injector:l}}),_l(()=>{this.childrenLoaders.delete(i)})),o=new yw(a,()=>new _e).pipe(Nh());return this.childrenLoaders.set(i,o),o}loadModuleFactoryOrRoutes(t){return di(t()).pipe($e(i=>i instanceof Zy||Array.isArray(i)?I(i):He(this.compiler.compileModuleAsync(i))))}}return n.\u0275fac=function(t){return new(t||n)(v(wt),v(Ju))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();class S8{}class C8{shouldProcessUrl(e){return!0}extract(e){return e}merge(e,t){return e}}function x8(n){throw n}function T8(n,e,t){return e.parse("/")}const E8={paths:"exact",fragment:"ignored",matrixParams:"ignored",queryParams:"exact"},k8={paths:"subset",fragment:"ignored",matrixParams:"ignored",queryParams:"subset"};function v1(){var n,e;const t=me(Aw),i=me(mo),r=me(el),a=me(wt),o=me(Ju),s=null!==(n=me(lf,{optional:!0}))&&void 0!==n?n:[],l=null!==(e=me(jl,{optional:!0}))&&void 0!==e?e:{},c=me(b1),d=me(m1,{optional:!0}),u=me(S8,{optional:!0}),h=me(v8,{optional:!0}),f=new Ye(null,t,i,r,a,o,Cw(s));return u&&(f.urlHandlingStrategy=u),h&&(f.routeReuseStrategy=h),f.titleStrategy=null!=d?d:c,function M8(n,e){n.errorHandler&&(e.errorHandler=n.errorHandler),n.malformedUriErrorHandler&&(e.malformedUriErrorHandler=n.malformedUriErrorHandler),n.onSameUrlNavigation&&(e.onSameUrlNavigation=n.onSameUrlNavigation),n.paramsInheritanceStrategy&&(e.paramsInheritanceStrategy=n.paramsInheritanceStrategy),n.relativeLinkResolution&&(e.relativeLinkResolution=n.relativeLinkResolution),n.urlUpdateStrategy&&(e.urlUpdateStrategy=n.urlUpdateStrategy),n.canceledNavigationResolution&&(e.canceledNavigationResolution=n.canceledNavigationResolution)}(l,f),f}let Ye=(()=>{class n{constructor(t,i,r,a,o,s,l){this.rootComponentType=t,this.urlSerializer=i,this.rootContexts=r,this.location=a,this.config=l,this.lastSuccessfulNavigation=null,this.currentNavigation=null,this.disposed=!1,this.navigationId=0,this.currentPageId=0,this.isNgZoneEnabled=!1,this.events=new _e,this.errorHandler=x8,this.malformedUriErrorHandler=T8,this.navigated=!1,this.lastSuccessfulId=-1,this.afterPreactivation=()=>I(void 0),this.urlHandlingStrategy=new C8,this.routeReuseStrategy=new _8,this.onSameUrlNavigation="ignore",this.paramsInheritanceStrategy="emptyOnly",this.urlUpdateStrategy="deferred",this.relativeLinkResolution="corrected",this.canceledNavigationResolution="replace",this.configLoader=o.get(cf),this.configLoader.onLoadEndListener=h=>this.triggerEvent(new d3(h)),this.configLoader.onLoadStartListener=h=>this.triggerEvent(new c3(h)),this.ngModule=o.get(Ni),this.console=o.get(nI);const u=o.get(ae);this.isNgZoneEnabled=u instanceof ae&&ae.isInAngularZone(),this.resetConfig(l),this.currentUrlTree=function RP(){return new Hi(new K([],{}),{},null)}(),this.rawUrlTree=this.currentUrlTree,this.browserUrlTree=this.currentUrlTree,this.routerState=Uw(this.currentUrlTree,this.rootComponentType),this.transitions=new Ut({id:0,targetPageId:0,currentUrlTree:this.currentUrlTree,currentRawUrl:this.currentUrlTree,extractedUrl:this.urlHandlingStrategy.extract(this.currentUrlTree),urlAfterRedirects:this.urlHandlingStrategy.extract(this.currentUrlTree),rawUrl:this.currentUrlTree,extras:{},resolve:null,reject:null,promise:Promise.resolve(!0),source:"imperative",restoredState:null,currentSnapshot:this.routerState.snapshot,targetSnapshot:null,currentRouterState:this.routerState,targetRouterState:null,guards:{canActivateChecks:[],canDeactivateChecks:[]},guardsResult:null}),this.navigations=this.setupNavigations(this.transitions),this.processNavigations()}get browserPageId(){var t;return null===(t=this.location.getState())||void 0===t?void 0:t.\u0275routerPageId}setupNavigations(t){const i=this.events;return t.pipe(cn(r=>0!==r.id),H(r=>Object.assign(Object.assign({},r),{extractedUrl:this.urlHandlingStrategy.extract(r.rawUrl)})),wn(r=>{let a=!1,o=!1;return I(r).pipe(Ue(s=>{this.currentNavigation={id:s.id,initialUrl:s.rawUrl,extractedUrl:s.extractedUrl,trigger:s.source,extras:s.extras,previousNavigation:this.lastSuccessfulNavigation?Object.assign(Object.assign({},this.lastSuccessfulNavigation),{previousNavigation:null}):null}}),wn(s=>{const l=this.browserUrlTree.toString(),c=!this.navigated||s.extractedUrl.toString()!==l||l!==this.currentUrlTree.toString();if(("reload"===this.onSameUrlNavigation||c)&&this.urlHandlingStrategy.shouldProcessUrl(s.rawUrl))return w1(s.source)&&(this.browserUrlTree=s.extractedUrl),I(s).pipe(wn(u=>{const h=this.transitions.getValue();return i.next(new Uh(u.id,this.serializeUrl(u.extractedUrl),u.source,u.restoredState)),h!==this.transitions.getValue()?En:Promise.resolve(u)}),function r8(n,e,t,i){return wn(r=>function n8(n,e,t,i,r){return new i8(n,e,t,i,r).apply()}(n,e,t,r.extractedUrl,i).pipe(H(a=>Object.assign(Object.assign({},r),{urlAfterRedirects:a}))))}(this.ngModule.injector,this.configLoader,this.urlSerializer,this.config),Ue(u=>{this.currentNavigation=Object.assign(Object.assign({},this.currentNavigation),{finalUrl:u.urlAfterRedirects}),r.urlAfterRedirects=u.urlAfterRedirects}),function f8(n,e,t,i,r,a){return $e(o=>function s8(n,e,t,i,r,a,o="emptyOnly",s="legacy"){return new l8(n,e,t,i,r,o,s,a).recognize().pipe(wn(l=>null===l?function o8(n){return new ye(e=>e.error(n))}(new a8):I(l)))}(n,e,t,o.urlAfterRedirects,i.serialize(o.urlAfterRedirects),i,r,a).pipe(H(s=>Object.assign(Object.assign({},o),{targetSnapshot:s}))))}(this.ngModule.injector,this.rootComponentType,this.config,this.urlSerializer,this.paramsInheritanceStrategy,this.relativeLinkResolution),Ue(u=>{if(r.targetSnapshot=u.targetSnapshot,"eager"===this.urlUpdateStrategy){if(!u.extras.skipLocationChange){const f=this.urlHandlingStrategy.merge(u.urlAfterRedirects,u.rawUrl);this.setBrowserUrl(f,u)}this.browserUrlTree=u.urlAfterRedirects}const h=new r3(u.id,this.serializeUrl(u.extractedUrl),this.serializeUrl(u.urlAfterRedirects),u.targetSnapshot);i.next(h)}));if(c&&this.rawUrlTree&&this.urlHandlingStrategy.shouldProcessUrl(this.rawUrlTree)){const{id:h,extractedUrl:f,source:p,restoredState:g,extras:m}=s,b=new Uh(h,this.serializeUrl(f),p,g);i.next(b);const D=Uw(f,this.rootComponentType).snapshot;return I(r=Object.assign(Object.assign({},s),{targetSnapshot:D,urlAfterRedirects:f,extras:Object.assign(Object.assign({},m),{skipLocationChange:!1,replaceUrl:!1})}))}return this.rawUrlTree=s.rawUrl,s.resolve(null),En}),Ue(s=>{const l=new a3(s.id,this.serializeUrl(s.extractedUrl),this.serializeUrl(s.urlAfterRedirects),s.targetSnapshot);this.triggerEvent(l)}),H(s=>r=Object.assign(Object.assign({},s),{guards:M3(s.targetSnapshot,s.currentSnapshot,this.rootContexts)})),function H3(n,e){return $e(t=>{const{targetSnapshot:i,currentSnapshot:r,guards:{canActivateChecks:a,canDeactivateChecks:o}}=t;return 0===o.length&&0===a.length?I(Object.assign(Object.assign({},t),{guardsResult:!0})):function z3(n,e,t,i){return He(n).pipe($e(r=>function q3(n,e,t,i,r){const a=e&&e.routeConfig?e.routeConfig.canDeactivate:null;return a&&0!==a.length?I(a.map(s=>{var l;const c=null!==(l=bo(e))&&void 0!==l?l:r,d=ea(s,c);return di(function F3(n){return n&&wo(n.canDeactivate)}(d)?d.canDeactivate(n,e,t,i):c.runInContext(()=>d(n,e,t,i))).pipe(li())})).pipe(ta()):I(!0)}(r.component,r.route,t,e,i)),li(r=>!0!==r,!0))}(o,i,r,n).pipe($e(s=>s&&function P3(n){return"boolean"==typeof n}(s)?function W3(n,e,t,i){return He(e).pipe(ci(r=>vl(function G3(n,e){return null!==n&&e&&e(new u3(n)),I(!0)}(r.route.parent,i),function V3(n,e){return null!==n&&e&&e(new f3(n)),I(!0)}(r.route,i),function $3(n,e,t){const i=e[e.length-1],a=e.slice(0,e.length-1).reverse().map(o=>function I3(n){const e=n.routeConfig?n.routeConfig.canActivateChild:null;return e&&0!==e.length?{node:n,guards:e}:null}(o)).filter(o=>null!==o).map(o=>bw(()=>I(o.guards.map(l=>{var c;const d=null!==(c=bo(o.node))&&void 0!==c?c:t,u=ea(l,d);return di(function N3(n){return n&&wo(n.canActivateChild)}(u)?u.canActivateChild(i,n):d.runInContext(()=>u(i,n))).pipe(li())})).pipe(ta())));return I(a).pipe(ta())}(n,r.path,t),function U3(n,e,t){const i=e.routeConfig?e.routeConfig.canActivate:null;if(!i||0===i.length)return I(!0);const r=i.map(a=>bw(()=>{var o;const s=null!==(o=bo(e))&&void 0!==o?o:t,l=ea(a,s);return di(function O3(n){return n&&wo(n.canActivate)}(l)?l.canActivate(e,n):s.runInContext(()=>l(e,n))).pipe(li())}));return I(r).pipe(ta())}(n,r.route,t))),li(r=>!0!==r,!0))}(i,a,n,e):I(s)),H(s=>Object.assign(Object.assign({},t),{guardsResult:s})))})}(this.ngModule.injector,s=>this.triggerEvent(s)),Ue(s=>{if(r.guardsResult=s.guardsResult,Wi(s.guardsResult))throw Qw(0,s.guardsResult);const l=new o3(s.id,this.serializeUrl(s.extractedUrl),this.serializeUrl(s.urlAfterRedirects),s.targetSnapshot,!!s.guardsResult);this.triggerEvent(l)}),cn(s=>!!s.guardsResult||(this.restoreHistory(s),this.cancelNavigationTransition(s,"",3),!1)),sf(s=>{if(s.guards.canActivateChecks.length)return I(s).pipe(Ue(l=>{const c=new s3(l.id,this.serializeUrl(l.extractedUrl),this.serializeUrl(l.urlAfterRedirects),l.targetSnapshot);this.triggerEvent(c)}),wn(l=>{let c=!1;return I(l).pipe(function p8(n,e){return $e(t=>{const{targetSnapshot:i,guards:{canActivateChecks:r}}=t;if(!r.length)return I(t);let a=0;return He(r).pipe(ci(o=>function g8(n,e,t,i){const r=n.routeConfig,a=n._resolve;return void 0!==(null==r?void 0:r.title)&&!g1(r)&&(a[lo]=r.title),function m8(n,e,t,i){const r=function b8(n){return[...Object.keys(n),...Object.getOwnPropertySymbols(n)]}(n);if(0===r.length)return I({});const a={};return He(r).pipe($e(o=>function y8(n,e,t,i){var r;const a=null!==(r=bo(e))&&void 0!==r?r:i,o=ea(n,a);return di(o.resolve?o.resolve(e,t):a.runInContext(()=>o(e,t)))}(n[o],e,t,i).pipe(li(),Ue(s=>{a[o]=s}))),Fh(1),function kP(n){return H(()=>n)}(a),zn(o=>nf(o)?En:Qr(o)))}(a,n,e,i).pipe(H(o=>(n._resolvedData=o,n.data=$w(n,t).resolve,r&&g1(r)&&(n.data[lo]=r.title),null)))}(o.route,i,n,e)),Ue(()=>a++),Fh(1),$e(o=>a===r.length?I(t):En))})}(this.paramsInheritanceStrategy,this.ngModule.injector),Ue({next:()=>c=!0,complete:()=>{c||(this.restoreHistory(l),this.cancelNavigationTransition(l,"",2))}}))}),Ue(l=>{const c=new l3(l.id,this.serializeUrl(l.extractedUrl),this.serializeUrl(l.urlAfterRedirects),l.targetSnapshot);this.triggerEvent(c)}))}),sf(s=>{const l=c=>{var d;const u=[];(null===(d=c.routeConfig)||void 0===d?void 0:d.loadComponent)&&!c.routeConfig._loadedComponent&&u.push(this.configLoader.loadComponent(c.routeConfig).pipe(Ue(h=>{c.component=h}),H(()=>{})));for(const h of c.children)u.push(...l(h));return u};return Oh(l(s.targetSnapshot.root)).pipe(wl(),si(1))}),sf(()=>this.afterPreactivation()),H(s=>{const l=function y3(n,e,t){const i=go(n,e._root,t?t._root:void 0);return new Gw(i,e)}(this.routeReuseStrategy,s.targetSnapshot,s.currentRouterState);return r=Object.assign(Object.assign({},s),{targetRouterState:l})}),Ue(s=>{this.currentUrlTree=s.urlAfterRedirects,this.rawUrlTree=this.urlHandlingStrategy.merge(s.urlAfterRedirects,s.rawUrl),this.routerState=s.targetRouterState,"deferred"===this.urlUpdateStrategy&&(s.extras.skipLocationChange||this.setBrowserUrl(this.rawUrlTree,s),this.browserUrlTree=s.urlAfterRedirects)}),((n,e,t)=>H(i=>(new k3(e,i.targetRouterState,i.currentRouterState,t).activate(n),i)))(this.rootContexts,this.routeReuseStrategy,s=>this.triggerEvent(s)),Ue({next(){a=!0},complete(){a=!0}}),_l(()=>{var s;a||o||this.cancelNavigationTransition(r,"",1),(null===(s=this.currentNavigation)||void 0===s?void 0:s.id)===r.id&&(this.currentNavigation=null)}),zn(s=>{var l;if(o=!0,Jw(s)){Xw(s)||(this.navigated=!0,this.restoreHistory(r,!0));const c=new kl(r.id,this.serializeUrl(r.extractedUrl),s.message,s.cancellationCode);if(i.next(c),Xw(s)){const d=this.urlHandlingStrategy.merge(s.url,this.rawUrlTree),u={skipLocationChange:r.extras.skipLocationChange,replaceUrl:"eager"===this.urlUpdateStrategy||w1(r.source)};this.scheduleNavigation(d,"imperative",null,u,{resolve:r.resolve,reject:r.reject,promise:r.promise})}else r.resolve(!1)}else{this.restoreHistory(r,!0);const c=new zw(r.id,this.serializeUrl(r.extractedUrl),s,null!==(l=r.targetSnapshot)&&void 0!==l?l:void 0);i.next(c);try{r.resolve(this.errorHandler(s))}catch(d){r.reject(d)}}return En}))}))}resetRootComponentType(t){this.rootComponentType=t,this.routerState.root.component=this.rootComponentType}setTransition(t){this.transitions.next(Object.assign(Object.assign({},this.transitions.value),t))}initialNavigation(){this.setUpLocationChangeListener(),0===this.navigationId&&this.navigateByUrl(this.location.path(!0),{replaceUrl:!0})}setUpLocationChangeListener(){this.locationSubscription||(this.locationSubscription=this.location.subscribe(t=>{const i="popstate"===t.type?"popstate":"hashchange";"popstate"===i&&setTimeout(()=>{var r;const a={replaceUrl:!0},o=null!==(r=t.state)&&void 0!==r&&r.navigationId?t.state:null;if(o){const l=Object.assign({},o);delete l.navigationId,delete l.\u0275routerPageId,0!==Object.keys(l).length&&(a.state=l)}const s=this.parseUrl(t.url);this.scheduleNavigation(s,i,o,a)},0)}))}get url(){return this.serializeUrl(this.currentUrlTree)}getCurrentNavigation(){return this.currentNavigation}triggerEvent(t){this.events.next(t)}resetConfig(t){this.config=t.map(tf),this.navigated=!1,this.lastSuccessfulId=-1}ngOnDestroy(){this.dispose()}dispose(){this.transitions.complete(),this.locationSubscription&&(this.locationSubscription.unsubscribe(),this.locationSubscription=void 0),this.disposed=!0}createUrlTree(t,i={}){const{relativeTo:r,queryParams:a,fragment:o,queryParamsHandling:s,preserveFragment:l}=i,c=r||this.routerState.root,d=l?this.currentUrlTree.fragment:o;let u=null;switch(s){case"merge":u=Object.assign(Object.assign({},this.currentUrlTree.queryParams),a);break;case"preserve":u=this.currentUrlTree.queryParams;break;default:u=a||null}return null!==u&&(u=this.removeEmptyProps(u)),XP(c,this.currentUrlTree,t,u,null!=d?d:null)}navigateByUrl(t,i={skipLocationChange:!1}){const r=Wi(t)?t:this.parseUrl(t),a=this.urlHandlingStrategy.merge(r,this.rawUrlTree);return this.scheduleNavigation(a,"imperative",null,i)}navigate(t,i={skipLocationChange:!1}){return function I8(n){for(let e=0;e<n.length;e++){if(null==n[e])throw new _(4008,false)}}(t),this.navigateByUrl(this.createUrlTree(t,i),i)}serializeUrl(t){return this.urlSerializer.serialize(t)}parseUrl(t){let i;try{i=this.urlSerializer.parse(t)}catch(r){i=this.malformedUriErrorHandler(r,this.urlSerializer,t)}return i}isActive(t,i){let r;if(r=!0===i?Object.assign({},E8):!1===i?Object.assign({},k8):i,Wi(t))return Ew(this.currentUrlTree,t,r);const a=this.parseUrl(t);return Ew(this.currentUrlTree,a,r)}removeEmptyProps(t){return Object.keys(t).reduce((i,r)=>{const a=t[r];return null!=a&&(i[r]=a),i},{})}processNavigations(){this.navigations.subscribe(t=>{var i;this.navigated=!0,this.lastSuccessfulId=t.id,this.currentPageId=t.targetPageId,this.events.next(new Vi(t.id,this.serializeUrl(t.extractedUrl),this.serializeUrl(this.currentUrlTree))),this.lastSuccessfulNavigation=this.currentNavigation,null===(i=this.titleStrategy)||void 0===i||i.updateTitle(this.routerState.snapshot),t.resolve(!0)},t=>{this.console.warn(`Unhandled Navigation Error: ${t}`)})}scheduleNavigation(t,i,r,a,o){var s,l;if(this.disposed)return Promise.resolve(!1);let c,d,u;o?(c=o.resolve,d=o.reject,u=o.promise):u=new Promise((p,g)=>{c=p,d=g});const h=++this.navigationId;let f;return"computed"===this.canceledNavigationResolution?(0===this.currentPageId&&(r=this.location.getState()),f=r&&r.\u0275routerPageId?r.\u0275routerPageId:a.replaceUrl||a.skipLocationChange?null!==(s=this.browserPageId)&&void 0!==s?s:0:(null!==(l=this.browserPageId)&&void 0!==l?l:0)+1):f=0,this.setTransition({id:h,targetPageId:f,source:i,restoredState:r,currentUrlTree:this.currentUrlTree,currentRawUrl:this.rawUrlTree,rawUrl:t,extras:a,resolve:c,reject:d,promise:u,currentSnapshot:this.routerState.snapshot,currentRouterState:this.routerState}),u.catch(p=>Promise.reject(p))}setBrowserUrl(t,i){const r=this.urlSerializer.serialize(t),a=Object.assign(Object.assign({},i.extras.state),this.generateNgRouterState(i.id,i.targetPageId));this.location.isCurrentPathEqualTo(r)||i.extras.replaceUrl?this.location.replaceState(r,"",a):this.location.go(r,"",a)}restoreHistory(t,i=!1){var r,a;if("computed"===this.canceledNavigationResolution){const o=this.currentPageId-t.targetPageId;"popstate"!==t.source&&"eager"!==this.urlUpdateStrategy&&this.currentUrlTree!==(null===(r=this.currentNavigation)||void 0===r?void 0:r.finalUrl)||0===o?this.currentUrlTree===(null===(a=this.currentNavigation)||void 0===a?void 0:a.finalUrl)&&0===o&&(this.resetState(t),this.browserUrlTree=t.currentUrlTree,this.resetUrlToCurrentUrlTree()):this.location.historyGo(o)}else"replace"===this.canceledNavigationResolution&&(i&&this.resetState(t),this.resetUrlToCurrentUrlTree())}resetState(t){this.routerState=t.currentRouterState,this.currentUrlTree=t.currentUrlTree,this.rawUrlTree=this.urlHandlingStrategy.merge(this.currentUrlTree,t.rawUrl)}resetUrlToCurrentUrlTree(){this.location.replaceState(this.urlSerializer.serialize(this.rawUrlTree),"",this.generateNgRouterState(this.lastSuccessfulId,this.currentPageId))}cancelNavigationTransition(t,i,r){const a=new kl(t.id,this.serializeUrl(t.extractedUrl),i,r);this.triggerEvent(a),t.resolve(!1)}generateNgRouterState(t,i){return"computed"===this.canceledNavigationResolution?{navigationId:t,\u0275routerPageId:i}:{navigationId:t}}}return n.\u0275fac=function(t){Ds()},n.\u0275prov=k({token:n,factory:function(){return v1()},providedIn:"root"}),n})();function w1(n){return"imperative"!==n}let Do=(()=>{class n{constructor(t,i,r,a,o){this.router=t,this.route=i,this.tabIndexAttribute=r,this.renderer=a,this.el=o,this._preserveFragment=!1,this._skipLocationChange=!1,this._replaceUrl=!1,this.commands=null,this.onChanges=new _e,this.setTabIndexIfNotOnNativeEl("0")}set preserveFragment(t){this._preserveFragment=Yr(t)}get preserveFragment(){return this._preserveFragment}set skipLocationChange(t){this._skipLocationChange=Yr(t)}get skipLocationChange(){return this._skipLocationChange}set replaceUrl(t){this._replaceUrl=Yr(t)}get replaceUrl(){return this._replaceUrl}setTabIndexIfNotOnNativeEl(t){if(null!=this.tabIndexAttribute)return;const i=this.renderer,r=this.el.nativeElement;null!==t?i.setAttribute(r,"tabindex",t):i.removeAttribute(r,"tabindex")}ngOnChanges(t){this.onChanges.next(this)}set routerLink(t){null!=t?(this.commands=Array.isArray(t)?t:[t],this.setTabIndexIfNotOnNativeEl("0")):(this.commands=null,this.setTabIndexIfNotOnNativeEl(null))}onClick(){return null===this.urlTree||this.router.navigateByUrl(this.urlTree,{skipLocationChange:this.skipLocationChange,replaceUrl:this.replaceUrl,state:this.state}),!0}get urlTree(){return null===this.commands?null:this.router.createUrlTree(this.commands,{relativeTo:void 0!==this.relativeTo?this.relativeTo:this.route,queryParams:this.queryParams,fragment:this.fragment,queryParamsHandling:this.queryParamsHandling,preserveFragment:this.preserveFragment})}}return n.\u0275fac=function(t){return new(t||n)(C(Ye),C(ui),Da("tabindex"),C(ws),C(ht))},n.\u0275dir=Re({type:n,selectors:[["","routerLink","",5,"a",5,"area"]],hostBindings:function(t,i){1&t&&jr("click",function(){return i.onClick()})},inputs:{queryParams:"queryParams",fragment:"fragment",queryParamsHandling:"queryParamsHandling",state:"state",relativeTo:"relativeTo",preserveFragment:"preserveFragment",skipLocationChange:"skipLocationChange",replaceUrl:"replaceUrl",routerLink:"routerLink"},standalone:!0,features:[Ei]}),n})(),Hl=(()=>{class n{constructor(t,i,r){this.router=t,this.route=i,this.locationStrategy=r,this._preserveFragment=!1,this._skipLocationChange=!1,this._replaceUrl=!1,this.commands=null,this.href=null,this.onChanges=new _e,this.subscription=t.events.subscribe(a=>{a instanceof Vi&&this.updateTargetUrlAndHref()})}set preserveFragment(t){this._preserveFragment=Yr(t)}get preserveFragment(){return this._preserveFragment}set skipLocationChange(t){this._skipLocationChange=Yr(t)}get skipLocationChange(){return this._skipLocationChange}set replaceUrl(t){this._replaceUrl=Yr(t)}get replaceUrl(){return this._replaceUrl}set routerLink(t){this.commands=null!=t?Array.isArray(t)?t:[t]:null}ngOnChanges(t){this.updateTargetUrlAndHref(),this.onChanges.next(this)}ngOnDestroy(){this.subscription.unsubscribe()}onClick(t,i,r,a,o){return!!(0!==t||i||r||a||o||"string"==typeof this.target&&"_self"!=this.target||null===this.urlTree)||(this.router.navigateByUrl(this.urlTree,{skipLocationChange:this.skipLocationChange,replaceUrl:this.replaceUrl,state:this.state}),!1)}updateTargetUrlAndHref(){this.href=null!==this.urlTree?this.locationStrategy.prepareExternalUrl(this.router.serializeUrl(this.urlTree)):null}get urlTree(){return null===this.commands?null:this.router.createUrlTree(this.commands,{relativeTo:void 0!==this.relativeTo?this.relativeTo:this.route,queryParams:this.queryParams,fragment:this.fragment,queryParamsHandling:this.queryParamsHandling,preserveFragment:this.preserveFragment})}}return n.\u0275fac=function(t){return new(t||n)(C(Ye),C(ui),C(Bi))},n.\u0275dir=Re({type:n,selectors:[["a","routerLink",""],["area","routerLink",""]],hostVars:2,hostBindings:function(t,i){1&t&&jr("click",function(a){return i.onClick(a.button,a.ctrlKey,a.shiftKey,a.altKey,a.metaKey)}),2&t&&za("target",i.target)("href",i.href,vr)},inputs:{target:"target",queryParams:"queryParams",fragment:"fragment",queryParamsHandling:"queryParamsHandling",state:"state",relativeTo:"relativeTo",preserveFragment:"preserveFragment",skipLocationChange:"skipLocationChange",replaceUrl:"replaceUrl",routerLink:"routerLink"},standalone:!0,features:[Ei]}),n})();class _1{}let P8=(()=>{class n{constructor(t,i,r,a,o){this.router=t,this.injector=r,this.preloadingStrategy=a,this.loader=o}setUpPreloading(){this.subscription=this.router.events.pipe(cn(t=>t instanceof Vi),ci(()=>this.preload())).subscribe(()=>{})}preload(){return this.processRoutes(this.injector,this.router.config)}ngOnDestroy(){this.subscription&&this.subscription.unsubscribe()}processRoutes(t,i){var r,a,o;const s=[];for(const l of i){l.providers&&!l._injector&&(l._injector=zs(l.providers,t,`Route: ${l.path}`));const c=null!==(r=l._injector)&&void 0!==r?r:t,d=null!==(a=l._loadedInjector)&&void 0!==a?a:c;l.loadChildren&&!l._loadedRoutes&&void 0===l.canLoad||l.loadComponent&&!l._loadedComponent?s.push(this.preloadConfig(c,l)):(l.children||l._loadedRoutes)&&s.push(this.processRoutes(d,null!==(o=l.children)&&void 0!==o?o:l._loadedRoutes))}return He(s).pipe(Zi())}preloadConfig(t,i){return this.preloadingStrategy.preload(i,()=>{let r;r=i.loadChildren&&void 0===i.canLoad?this.loader.loadChildren(t,i):I(null);const a=r.pipe($e(o=>{var s;return null===o?I(void 0):(i._loadedRoutes=o.routes,i._loadedInjector=o.injector,this.processRoutes(null!==(s=o.injector)&&void 0!==s?s:t,o.routes))}));return i.loadComponent&&!i._loadedComponent?He([a,this.loader.loadComponent(i)]).pipe(Zi()):a})}}return n.\u0275fac=function(t){return new(t||n)(v(Ye),v(Ju),v(ti),v(_1),v(cf))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();const df=new E("");let D1=(()=>{class n{constructor(t,i,r={}){this.router=t,this.viewportScroller=i,this.options=r,this.lastId=0,this.lastSource="imperative",this.restoredId=0,this.store={},r.scrollPositionRestoration=r.scrollPositionRestoration||"disabled",r.anchorScrolling=r.anchorScrolling||"disabled"}init(){"disabled"!==this.options.scrollPositionRestoration&&this.viewportScroller.setHistoryScrollRestoration("manual"),this.routerEventsSubscription=this.createScrollEvents(),this.scrollEventsSubscription=this.consumeScrollEvents()}createScrollEvents(){return this.router.events.subscribe(t=>{t instanceof Uh?(this.store[this.lastId]=this.viewportScroller.getScrollPosition(),this.lastSource=t.navigationTrigger,this.restoredId=t.restoredState?t.restoredState.navigationId:0):t instanceof Vi&&(this.lastId=t.id,this.scheduleScrollEvent(t,this.router.parseUrl(t.urlAfterRedirects).fragment))})}consumeScrollEvents(){return this.router.events.subscribe(t=>{t instanceof Ww&&(t.position?"top"===this.options.scrollPositionRestoration?this.viewportScroller.scrollToPosition([0,0]):"enabled"===this.options.scrollPositionRestoration&&this.viewportScroller.scrollToPosition(t.position):t.anchor&&"enabled"===this.options.anchorScrolling?this.viewportScroller.scrollToAnchor(t.anchor):"disabled"!==this.options.scrollPositionRestoration&&this.viewportScroller.scrollToPosition([0,0]))})}scheduleScrollEvent(t,i){this.router.triggerEvent(new Ww(t,"popstate"===this.lastSource?this.store[this.restoredId]:null,i))}ngOnDestroy(){this.routerEventsSubscription&&this.routerEventsSubscription.unsubscribe(),this.scrollEventsSubscription&&this.scrollEventsSubscription.unsubscribe()}}return n.\u0275fac=function(t){Ds()},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})();function na(n,e){return{\u0275kind:n,\u0275providers:e}}function uf(n){return[{provide:lf,multi:!0,useValue:n}]}function C1(){const n=me(wt);return e=>{var t,i;const r=n.get(qr);if(e!==r.components[0])return;const a=n.get(Ye),o=n.get(x1);1===n.get(hf)&&a.initialNavigation(),null===(t=n.get(T1,null,O.Optional))||void 0===t||t.setUpPreloading(),null===(i=n.get(df,null,O.Optional))||void 0===i||i.init(),a.resetRootComponentType(r.componentTypes[0]),o.closed||(o.next(),o.unsubscribe())}}const x1=new E("",{factory:()=>new _e}),hf=new E("",{providedIn:"root",factory:()=>1});const T1=new E("");function F8(n){return na(0,[{provide:T1,useExisting:P8},{provide:_1,useExisting:n}])}const E1=new E("ROUTER_FORROOT_GUARD"),j8=[el,{provide:Aw,useClass:Bh},{provide:Ye,useFactory:v1},mo,{provide:ui,useFactory:function S1(n){return n.routerState.root},deps:[Ye]},cf];function B8(){return new Xv("Router",Ye)}let k1=(()=>{class n{constructor(t){}static forRoot(t,i){return{ngModule:n,providers:[j8,[],uf(t),{provide:E1,useFactory:V8,deps:[[Ye,new ei,new mr]]},{provide:jl,useValue:i||{}},null!=i&&i.useHash?{provide:Bi,useClass:GI}:{provide:Bi,useClass:S0},{provide:df,useFactory:()=>{const n=me(Ye),e=me(cR),t=me(jl);return t.scrollOffset&&e.setOffset(t.scrollOffset),new D1(n,e,t)}},null!=i&&i.preloadingStrategy?F8(i.preloadingStrategy).\u0275providers:[],{provide:Xv,multi:!0,useFactory:B8},null!=i&&i.initialNavigation?G8(i):[],[{provide:M1,useFactory:C1},{provide:Gv,multi:!0,useExisting:M1}]]}}static forChild(t){return{ngModule:n,providers:[uf(t)]}}}return n.\u0275fac=function(t){return new(t||n)(v(E1,8))},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({imports:[Zh]}),n})();function V8(n){return"guarded"}function G8(n){return["disabled"===n.initialNavigation?na(3,[{provide:$s,multi:!0,useFactory:()=>{const e=me(Ye);return()=>{e.setUpLocationChangeListener()}}},{provide:hf,useValue:2}]).\u0275providers:[],"enabledBlocking"===n.initialNavigation?na(2,[{provide:hf,useValue:0},{provide:$s,multi:!0,deps:[wt],useFactory:e=>{const t=e.get(WI,Promise.resolve());let i=!1;return()=>t.then(()=>new Promise(a=>{const o=e.get(Ye),s=e.get(x1);(function r(a){e.get(Ye).events.pipe(cn(s=>s instanceof Vi||s instanceof kl||s instanceof zw),H(s=>s instanceof Vi||s instanceof kl&&(0===s.code||1===s.code)&&null),cn(s=>null!==s),si(1)).subscribe(()=>{a()})})(()=>{a(!0),i=!0}),o.afterPreactivation=()=>(a(!0),i||s.closed?I(void 0):s),o.initialNavigation()}))}}]).\u0275providers:[]]}const M1=new E("");let $8=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275cmp=mt({type:n,selectors:[["app-main"]],decls:21,vars:0,consts:[["src","assets/lab/main.jpg","width","100%","alt","Lab image 2017"]],template:function(t,i){1&t&&(Y(0,"p"),ue(1," Natural language processing allows computers to analyze and \u201cunderstand\u201d human language, by combining advanced computer science algorithms with linguistic knowledge.\n"),G(),Y(2,"p"),ue(3," In our lab, we work on problems at the forefront of the worldwide academic community. For that we use cutting-edge methods in Machine Learning, Deep Learning and Artificial Intelligence.\n"),G(),Y(4,"p"),ue(5,"Our research interests include, among others:"),G(),Y(6,"ul")(7,"li"),ue(8," Information extraction and aggregation"),G(),Y(9,"li"),ue(10," Bias detection and mitigation"),G(),Y(11,"li"),ue(12," Textual entailment"),G(),Y(13,"li"),ue(14," Model interpretation"),G(),Y(15,"li"),ue(16," NLP pipeline for Hebrew"),G()(),Y(17,"p"),ue(18," We collaborate with leading labs all over the world. We believe in open research, and make our papers, tools, models, and algorithms publicly available to facilitate future research.\n"),G(),Y(19,"p"),zt(20,"img",0),G())},styles:["[_nghost-%COMP%]{margin:40px 0}"]}),n})();const I1=JSON.parse('[{"id":"562b4688ff0cd423fb3263ab0c87f78e.html","title":"The MRL 2022 Shared Task on Multilingual Clause-level Morphology","url":"https://hal.inria.fr/hal-03878174/","authors":["Omer Goldman","Francesco Tinner","Hila Gonen","Benjamin Muller","Victoria Basmov","Shadrack Kirimi","Lydia Nishimwe","Beno\\\\xc3\\\\xaet Sagot","Djam\\\\xc3\\\\xa9 Seddah","Reut Tsarfaty","Duygu Ataman"],"date":"2022/12/08","abstract":""},{"id":"15bba53641bfbcc9b0ffa4d1b5d305f9.html","title":"Draw Me a Flower: Processing and Grounding Abstraction in Natural Language","url":"https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00522/114048","authors":["Royi Lachmy","Valentina Pyatkin","Avshalom Manevich","Reut Tsarfaty"],"date":"2022/11/28","journal":"Transactions of the Association for Computational Linguistics","abstract":"Abstraction is a core tenet of human cognition and communication. When composing natural language instructions, humans naturally evoke abstraction to convey complex procedures in an efficient and concise way. Yet, interpreting and grounding abstraction expressed in NL has not yet been systematically studied in NLP, with no accepted benchmarks specifically eliciting abstraction in NL. In this work, we set the foundation for a systematic study of processing and grounding abstraction in NLP. First, we deliver a novel abstraction elicitation method and present <span class=\\"gs_fscp\\">Hexagons</span>, a 2D instruction-following game. Using <span class=\\"gs_fscp\\">Hexagons</span> we collected over 4k naturally occurring visually-grounded instructions rich with diverse types of abstractions. From these data, we derive an <i>instruction-to-execution</i> task and assess different types of neural models. Our results show that contemporary models and modeling practices are\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"be49a8643342011222c53fd2eb551b99.html","title":"Large Pre-Trained Models with Extra-Large Vocabularies: A Contrastive Analysis of Hebrew BERT Models and a New One to Outperform Them All","url":"https://arxiv.org/abs/2211.15199","authors":["Eylon Guetta","Avi Shmidman","Shaltiel Shmidman","Cheyn Shmuel Shmidman","Joshua Guedalia","Moshe Koppel","Dan Bareket","Amit Seker","Reut Tsarfaty"],"date":"2022/11/28","journal":"arXiv preprint arXiv:2211.15199","abstract":""},{"id":"0e0a62946133b07050d498e9154a29ab.html","title":"Ham2Pose: Animating Sign Language Notation into Pose Sequences","url":"https://arxiv.org/abs/2211.13613","authors":["Rotem Shalev-Arkushin","Amit Moryossef","Ohad Fried"],"date":"2022/11/24","journal":"arXiv preprint arXiv:2211.13613","abstract":""},{"id":"acc88982fe6e9c1e84152c3887d7dab7.html","title":"Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs","url":"https://arxiv.org/abs/2211.07950","authors":["Kyle Richardson","Ronen Tamari","Oren Sultan","Reut Tsarfaty","Dafna Shahaf","Ashish Sabharwal"],"date":"2022/11/15","journal":"arXiv preprint arXiv:2211.07950","abstract":""},{"id":"effe9c45a7607e83818374acfe3da42a.html","title":"Prompting Language Models for Linguistic Structure","url":"https://arxiv.org/abs/2211.07830","authors":["Terra Blevins","Hila Gonen","Luke Zettlemoyer"],"date":"2022/11/15","journal":"arXiv preprint arXiv:2211.07830","abstract":""},{"id":"d359b6a9541adc91ea278f25b2109d77.html","title":"Just-DREAM-about-it: Figurative Language Understanding with DREAM-FLUTE","url":"https://arxiv.org/abs/2210.16407","authors":["Yuling Gu","Yao Fu","Valentina Pyatkin","Ian Magnusson","Bhavana Dalvi Mishra","Peter Clark"],"date":"2022/10/28","journal":"arXiv preprint arXiv:2210.16407","abstract":""},{"id":"68c740649f21202e5aad2517c809ff11.html","title":"Controlled Text Reduction","url":"https://arxiv.org/abs/2210.13449","authors":["Aviv Slobodkin","Paul Roit","Eran Hirsch","Ori Ernst","Ido Dagan"],"date":"2022/10/24","journal":"arXiv preprint arXiv:2210.13449","abstract":""},{"id":"24bebc80901f6da4901b4b5f52da8d42.html","title":"How\\" Multi\\" is Multi-Document Summarization?","url":"https://arxiv.org/abs/2210.12688","authors":["Ruben Wolhandler","Arie Cattan","Ori Ernst","Ido Dagan"],"date":"2022/10/23","journal":"arXiv preprint arXiv:2210.12688","abstract":""},{"id":"53079a45d32a1539c8c17095b16f39fc.html","title":"Lexical Generalization Improves with Larger Models and Longer Training","url":"https://arxiv.org/abs/2210.12673","authors":["Elron Bandel","Yanai Elazar"],"date":"2022/10/23","journal":"arXiv preprint arXiv:2210.12673","abstract":""},{"id":"feede2b6c8a51f7905dd448894832bc6.html","title":"Cross-document Event Coreference Search: Task, Dataset and Modeling","url":"https://arxiv.org/abs/2210.12654","authors":["Alon Eirew","Avi Caciularu","Ido Dagan"],"date":"2022/10/23","journal":"arXiv preprint arXiv:2210.12654","abstract":""},{"id":"3e5e534a2ac1c832daa67caa53e21f48.html","title":"DALLE-2 is Seeing Double: Flaws in Word-to-Concept Mapping in Text2Image Models","url":"https://arxiv.org/abs/2210.10606","authors":["Royi Rassin","Shauli Ravfogel","Yoav Goldberg"],"date":"2022/10/19","journal":"arXiv preprint arXiv:2210.10606","abstract":""},{"id":"bd484eaaae104c11af625cbe10948472.html","title":"Linear Guardedness and its Implications","url":"https://arxiv.org/abs/2210.10012","authors":["Shauli Ravfogel","Yoav Goldberg","Ryan Cotterell"],"date":"2022/10/18","journal":"arXiv preprint arXiv:2210.10012","abstract":""},{"id":"6b0fc2da358ddf11512486a542cb3c47.html","title":"Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting","url":"https://arxiv.org/abs/2210.05404","authors":["Zifan Jiang","Amit Moryossef","Mathias M\\\\xc3\\\\xbcller","Sarah Ebling"],"date":"2022/10/11","journal":"arXiv preprint arXiv:2210.05404","abstract":""},{"id":"f466e84301a795fa91f187d9200893af.html","title":"State-of-the-art generalisation research in NLP: a taxonomy and review","url":"https://arxiv.org/abs/2210.03050","authors":["Dieuwke Hupkes","Mario Giulianelli","Verna Dankers","Mikel Artetxe","Yanai Elazar","Tiago Pimentel","Christos Christodoulopoulos","Karim Lasri","Naomi Saphra","Arabella Sinclair","Dennis Ulmer","Florian Schottmann","Khuyagbaatar Batsuren","Kaiser Sun","Koustuv Sinha","Leila Khalatbari","Rita Frieske","Ryan Cotterell","Zhijing Jin"],"date":"2022/10/06","journal":"arXiv preprint arXiv:2210.03050","abstract":""},{"id":"285d6190d13a0c4e8c25942354010d6b.html","title":"Measuring Causal Effects of Data Statistics on Language Model\\\\\'sFactual\\\\\'Predictions","url":"https://arxiv.org/abs/2207.14251","authors":["Yanai Elazar","Nora Kassner","Shauli Ravfogel","Amir Feder","Abhilasha Ravichander","Marius Mosbach","Yonatan Belinkov","Hinrich Sch\\\\xc3\\\\xbctze","Yoav Goldberg"],"date":"2022/07/28","journal":"arXiv preprint arXiv:2207.14251","abstract":""},{"id":"9036af0188a7901150f8a5a4780ed987.html","title":"Text-based NP enrichment","url":"https://arxiv.org/abs/2109.12085","authors":["Yanai Elazar","Yoav Goldberg","Reut Tsarfaty"],"date":"2022/07/27","journal":"Transactions of the Association for Computational Linguistics","abstract":""},{"id":"4b88001487482391a022f0a711d85faf.html","title":"Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)","url":"https://aclanthology.org/2022.gebnlp-1.0.pdf","authors":["Christian Hardmeier","Christine Basta","Marta R Costa-juss\\\\xc3\\\\xa0","Gabriel Stanovsky","Hila Gonen"],"date":"2022/07","abstract":"This volume contains the proceedings of the Fourth Workshop on Gender Bias in Natural Language Processing, held in conjunction with the 2022 Conference of the North American Chapter of the Association for Computational Linguistics\\\\xe2\\\\x80\\\\x93Human Language Technologies (NAACL-HLT2022). This year, the organization committee changed membership: Kellie Webster made way for Christine Basta and Gabriel Stanovsky. Kellie has been one of the main reasons for the success of this workshop and we would like to thank her for her valuable and enthusiastic contribution to this workshop. We are glad to welcome our two co-organizers and look forward to sharing their insights and expertise."},{"id":"5234f3f42255750dcf489493b6000787.html","title":"Proceedings of the Second Workshop on Understanding Implicit and Underspecified Language","url":"https://aclanthology.org/2022.unimplicit-1.0.pdf","authors":["Valentina Pyatkin","Daniel Fried","Talita Anthonio"],"date":"2022/07","abstract":"Welcome to UnImplicit: The Second Workshop on Understanding Implicit and Underspecified Language. The focus of this workshop is on implicit and underspecified phenomena in language, which pose serious challenges to standard natural language processing models as they often require incorporating greater context, using symbolic inference and common-sense reasoning, or more generally, going beyond strictly lexical and compositional meaning constructs. This challenge spans all phases of the NLP model\\\\xe2\\\\x80\\\\x99s life cycle: from collecting and annotating relevant data, through devising computational methods for modeling such phenomena, to evaluating and designing proper evaluation metrics. In this workshop, our goal is to bring together theoreticians and practitioners from the entire NLP cycle, from annotation and benchmarking to modeling and applications, and to provide an umbrella for the development\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"71d0c481f90dff0e3eac89bbcc0cc881.html","title":"Long Context Question Answering via Supervised Contrastive Learning","url":"https://aclanthology.org/2022.naacl-main.207/","authors":["Avi Caciularu","Ido Dagan","Jacob Goldberger","Arman Cohan"],"date":"2022/07","abstract":"Long-context question answering (QA) tasks require reasoning over a long document or multiple documents. Addressing these tasks often benefits from identifying a set of evidence spans (eg, sentences), which provide supporting evidence for answering the question. In this work, we propose a novel method for equipping long-context QA models with an additional sequence-level objective for better identification of the supporting evidence. We achieve this via an additional contrastive supervision signal in finetuning, where the model is encouraged to explicitly discriminate supporting evidence sentences from negative ones by maximizing question-evidence similarity. The proposed additional loss exhibits consistent improvements on three different strong long-context transformer models, across two challenging question answering benchmarks\\\\xe2\\\\x80\\\\x93HotpotQA and QAsper."},{"id":"b5db2126f65faa374dac8947aa7f3c60.html","title":"Proposition-level clustering for multi-document summarization","url":"https://aclanthology.org/2022.naacl-main.128/","authors":["Ori Ernst","Avi Caciularu","Ori Shapira","Ramakanth Pasunuru","Mohit Bansal","Jacob Goldberger","Ido Dagan"],"date":"2022/07","abstract":"Text clustering methods were traditionally incorporated into multi-document summarization (MDS) as a means for coping with considerable information repetition. Particularly, clusters were leveraged to indicate information saliency as well as to avoid redundancy. Such prior methods focused on clustering sentences, even though closely related sentences usually contain also non-aligned parts. In this work, we revisit the clustering approach, grouping together sub-sentential propositions, aiming at more precise information alignment. Specifically, our method detects salient propositions, clusters them into paraphrastic clusters, and generates a representative sentence for each cluster via text fusion. Our summarization method improves over the previous state-of-the-art MDS method in the DUC 2004 and TAC 2011 datasets, both in automatic ROUGE scores and human preference."},{"id":"eda6f8a4db1ecf9b29ccb0cfe3c3ee10.html","title":"Interactive Query-Assisted Summarization via Deep Reinforcement Learning","url":"https://aclanthology.org/2022.naacl-main.184/","authors":["Ori Shapira","Ramakanth Pasunuru","Mohit Bansal","Ido Dagan","Yael Amsterdamer"],"date":"2022/07","abstract":"Interactive summarization is a task that facilitates user-guided exploration of information within a document set. While one would like to employ state of the art neural models to improve the quality of interactive summarization, many such technologies cannot ingest the full document set or cannot operate at sufficient speed for interactivity. To that end, we propose two novel deep reinforcement learning models for the task that address, respectively, the subtask of summarizing salient information that adheres to user queries, and the subtask of listing suggested queries to assist users throughout their exploration. In particular, our models allow encoding the interactive session state and history to refrain from redundancy. Together, these models compose a state of the art solution that addresses all of the task requirements. We compare our solution to a recent interactive summarization system, and show through an experimental study involving real users that our models are able to improve informativeness while preserving positive user experience."},{"id":"15a1a635ff8ceb74efd1d7185d875103.html","title":"Linear adversarial concept erasure","url":"https://proceedings.mlr.press/v162/ravfogel22a.html","authors":["Shauli Ravfogel","Michael Twiton","Yoav Goldberg","Ryan D Cotterell"],"date":"2022/06/28","abstract":"Modern neural models trained on textual data rely on pre-trained representations that emerge without direct supervision. As these representations are increasingly being used in real-world applications, the inability to control their content becomes an increasingly important problem. In this work, we formulate the problem of identifying a linear subspace that corresponds to a given concept, and removing it from the representation. We formulate this problem as a constrained, linear minimax game, and show that existing solutions are generally not optimal for this task. We derive a closed-form solution for certain objectives, and propose a convex relaxation that works well for others. When evaluated in the context of binary gender removal, the method recovers a low-dimensional subspace whose removal mitigates bias by intrinsic and extrinsic evaluation. Surprisingly, we show that the method\\\\xe2\\\\x80\\\\x94despite being linear\\\\xe2\\\\x80\\\\x94is highly expressive, effectively mitigating bias in the output layers of deep, nonlinear classifiers while maintaining tractability and interpretability."},{"id":"5897f9ffd4cb2255aba034eff1044889.html","title":"Design choices in crowdsourcing discourse relation annotations: The effect of worker selection and training","url":"https://aclanthology.org/2022.lrec-1.231/","authors":["Merel Scholman","Valentina Pyatkin","Frances Yung","Ido Dagan","Reut Tsarfaty","Vera Demberg"],"date":"2022/06","abstract":"Obtaining linguistic annotation from novice crowdworkers is far from trivial. A case in point is the annotation of discourse relations, which is a complicated task. Recent methods have obtained promising results by extracting relation labels from either discourse connectives (DCs) or question-answer (QA) pairs that participants provide. The current contribution studies the effect of worker selection and training on the agreement on implicit relation labels between workers and gold labels, for both the DC and the QA method. In Study 1, workers were not specifically selected or trained, and the results show that there is much room for improvement. Study 2 shows that a combination of selection and training does lead to improved results, but the method is cost-and time-intensive. Study 3 shows that a selection-only approach is a viable alternative; it results in annotations of comparable quality compared to annotations from trained participants. The results generalized over both the DC and QA method and therefore indicate that a selection-only approach could also be effective for other crowdsourced discourse annotation tasks."},{"id":"a5231e38695c3329b3a89354659ee661.html","title":"LingMess: Linguistically Informed Multi Expert Scorers for Coreference Resolution","url":"https://arxiv.org/abs/2205.12644","authors":["Shon Otmazgin","Arie Cattan","Yoav Goldberg"],"date":"2022/05/25","journal":"arXiv preprint arXiv:2205.12644","abstract":""},{"id":"f05fef7aa7bbf99e439040a67ed5d69b.html","title":"Analyzing the Mono-and Cross-Lingual Pretraining Dynamics of Multilingual Language Models","url":"https://arxiv.org/abs/2205.11758","authors":["Terra Blevins","Hila Gonen","Luke Zettlemoyer"],"date":"2022/05/24","journal":"arXiv preprint arXiv:2205.11758","abstract":""},{"id":"a0163628cd6cf9d59fcf9f8f5ab4abf7.html","title":"QASem Parsing: Text-to-text Modeling of QA-based Semantics","url":"https://arxiv.org/abs/2205.11413","authors":["Ayal Klein","Eran Hirsch","Ron Eliav","Valentina Pyatkin","Avi Caciularu","Ido Dagan"],"date":"2022/05/23","journal":"arXiv preprint arXiv:2205.11413","abstract":""},{"id":"fd8b47dc17a4c1bfa00a6890f1b108fc.html","title":"UniMorph 4.0: Universal Morphology","url":"https://arxiv.org/abs/2205.03608","authors":["Khuyagbaatar Batsuren","Omer Goldman","Salam Khalifa","Nizar Habash","Witold Kiera\\\\xc5\\\\x9b","G\\\\xc3\\\\xa1bor Bella","Brian Leonard","Garrett Nicolai","Kyle Gorman","Yustinus Ghanggo Ate","Maria Ryskina","Sabrina J Mielke","Elena Budianskaya","Charbel El-Khaissi","Tiago Pimentel","Michael Gasser","William Lane","Mohit Raj","Matt Coler","Jaime Rafael Montoya Samame","Delio Siticonatzi Camaiteri","Esa\\\\xc3\\\\xba Zumaeta Rojas","Didier L\\\\xc3\\\\xb3pez Francis","Arturo Oncevay","Juan L\\\\xc3\\\\xb3pez Bautista","Gema Celeste Silva Villegas","Lucas Torroba Hennigen","Adam Ek","David Guriel","Peter Dirix","Jean-Philippe Bernardy","Andrey Scherbakov","Aziyana Bayyr-ool","Antonios Anastasopoulos","Roberto Zariquiey","Karina Sheifer","Sofya Ganieva","Hilaria Cruz","Ritv\\\\xc3\\\\xa1n Karah\\\\xc3\\\\xb3\\\\xc7\\\\xa7a","Stella Markantonatou","George Pavlidis","Matvey Plugaryov","Elena Klyachko","Ali Salehi","Candy Angulo","Jatayu Baxi","Andrew Krizhanovsky","Natalia Krizhanovskaya","Elizabeth Salesky","Clara Vania","Sardana Ivanova","Jennifer White","Rowan Hall Maudslay","Josef Valvoda","Ran Zmigrod","Paula Czarnowska","Irene Nikkarinen","Aelita Salchak","Brijesh Bhatt","Christopher Straughn","Zoey Liu","Jonathan North Washington","Yuval Pinter","Duygu Ataman","Marcin Wolinski","Totok Suhardijanto","Anna Yablonskaya","Niklas Stoehr","Hossep Dolatian","Zahroh Nuriah","Shyam Ratan","Francis M Tyers","Edoardo M Ponti","Grant Aiton","Aryaman Arora","Richard J Hatcher","Ritesh Kumar","Jeremiah Young","Daria Rodionova","Anastasia Yemelina","Taras Andrushko","Igor Marchenko","Polina Mashkovtseva","Alexandra Serova","Emily Prud\\\\\'hommeaux","Maria Nepomniashchaya","Fausto Giunchiglia","Eleanor Chodroff","Mans Hulden","Miikka Silfverberg","Arya D McCarthy","David Yarowsky","Ryan Cotterell","Reut Tsarfaty","Ekaterina Vylomova"],"date":"2022/05/07","journal":"arXiv preprint arXiv:2205.03608","abstract":""},{"id":"a6f6ebb11f5edc2cd063d3ed87a07ba7.html","title":"Opinion-based Relational Pivoting for Cross-domain Aspect Term Extraction","url":"https://aclanthology.org/2022.wassa-1.11/","authors":["Ayal Klein","Oren Pereg","Daniel Korat","Vasudev Lal","Moshe Wasserblat","Ido Dagan"],"date":"2022/05","abstract":"Domain adaptation methods often exploit domain-transferable input features, aka pivots. The task of Aspect and Opinion Term Extraction presents a special challenge for domain transfer: while opinion terms largely transfer across domains, aspects change drastically from one domain to another (eg from restaurants to laptops). In this paper, we investigate and establish empirically a prior conjecture, which suggests that the linguistic relations connecting opinion terms to their aspects transfer well across domains and therefore can be leveraged for cross-domain aspect term extraction. We present several analyses supporting this conjecture, via experiments with four linguistic dependency formalisms to represent relation patterns. Subsequently, we present an aspect term extraction method that drives models to consider opinion\\\\xe2\\\\x80\\\\x93aspect relations via explicit multitask objectives. This method provides significant performance gains, even on top of a prior state-of-the-art linguistically-informed model, which are shown in analysis to stem from the relational pivoting signal."},{"id":"b097615026657653cad9c221088c4cae.html","title":"AlephBERT: Language Model Pre-training and Evaluation from Sub-Word to Sentence Level","url":"https://aclanthology.org/2022.acl-long.4/","authors":["Amit Seker","Elron Bandel","Dan Bareket","Idan Brusilovsky","Refael Greenfeld","Reut Tsarfaty"],"date":"2022/05","abstract":"Large Pre-trained Language Models (PLMs) have become ubiquitous in the development of language understanding technology and lie at the heart of many artificial intelligence advances. While advances reported for English using PLMs are unprecedented, reported advances using PLMs for Hebrew are few and far between. The problem is twofold. First, so far, Hebrew resources for training large language models are not of the same magnitude as their English counterparts. Second, most benchmarks available to evaluate progress in Hebrew NLP require morphological boundaries which are not available in the output of standard PLMs. In this work we remedy both aspects. We present AlephBERT, a large PLM for Modern Hebrew, trained on larger vocabulary and a larger dataset than any Hebrew PLM before. Moreover, we introduce a novel neural architecture that recovers the morphological segments encoded in contextualized embedding vectors. Based on this new morphological component we offer an evaluation suite consisting of multiple tasks and benchmarks that cover sentence-level, word-level and sub-word level analyses. On all tasks, AlephBERT obtains state-of-the-art results beyond contemporary Hebrew baselines. We make our AlephBERT model, the morphological extraction model, and the Hebrew evaluation suite publicly available, for evaluating future Hebrew PLMs."},{"id":"d2ef03bede16313466a240cd92c525b2.html","title":"LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models","url":"https://arxiv.org/abs/2204.12130","authors":["Mor Geva","Avi Caciularu","Guy Dar","Paul Roit","Shoval Sadde","Micah Shlain","Bar Tamir","Yoav Goldberg"],"date":"2022/04/26","journal":"arXiv preprint arXiv:2204.12130","abstract":""},{"id":"1bac1fda4d4a9a1f4dfb3e6afc98a500.html","title":"Analyzing Gender Representation in Multilingual Models","url":"https://arxiv.org/abs/2204.09168","authors":["Hila Gonen","Shauli Ravfogel","Yoav Goldberg"],"date":"2022/04/20","journal":"arXiv preprint arXiv:2204.09168","abstract":""},{"id":"4afcacbe373c4e7b395ff70d35364a9d.html","title":"Breaking Character: Are Subwords Good Enough for MRLs After All?","url":"https://arxiv.org/abs/2204.04748","authors":["Omri Keren","Tal Avinari","Reut Tsarfaty","Omer Levy"],"date":"2022/04/10","journal":"arXiv preprint arXiv:2204.04748","abstract":""},{"id":"67f0cbaa061b4c230d304c87b2fbbf0d.html","title":"Neural Token Segmentation for High Token-Internal Complexity","url":"https://arxiv.org/abs/2203.10845","authors":["Idan Brusilovsky","Reut Tsarfaty"],"date":"2022/03/21","journal":"arXiv preprint arXiv:2203.10845","abstract":""},{"id":"7a3f9aa301fbd747e72a6d69f973b077.html","title":"Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a Georgian Case Study","url":"https://arxiv.org/abs/2203.08527","authors":["David Guriel","Omer Goldman","Reut Tsarfaty"],"date":"2022/03/16","journal":"arXiv preprint arXiv:2203.08527","abstract":""},{"id":"b0826628f0a31049131b031fd12a56f7.html","title":"CIKQA: Learning Commonsense Inference with a Unified Knowledge-in-the-loop QA Paradigm","url":"https://openreview.net/forum?id=S6Pl8ztg_b5","authors":["Hongming Zhang","Yintong Huo","Yanai Elazar","Yangqiu Song","Yoav Goldberg","Dan Roth"],"date":"2022/03/10","abstract":"Recently, the community has achieved substantial progress on many commonsense reasoning benchmarks. However, it is still unclear what was learned from the training process: the knowledge, how to do inference, or both? We argue that due to the large scale of commonsense knowledge, it is infeasible to annotate a large enough training set for each task to cover all commonsense for learning. Thus we should separate the commonsense knowledge acquisition and inference over commonsense knowledge as two separate tasks. In this work, we focus on investigating models\\\\\' commonsense inference capabilities from two perspectives:(1) Whether models can know if the knowledge they have is enough to solve the task;(2) Whether models can learn commonsense inference capabilities, that generalize across commonsense tasks. We first align commonsense tasks with relevant knowledge from commonsense knowledge bases and ask humans to annotate whether the knowledge is enough or not. Then, we convert different commonsense tasks into a unified question answering format to evaluate models\\\\\' generalization capabilities. We name the benchmark as Commonsense Inference with knowledge-in-the-loop Question Answering (CIKQA)."},{"id":"08e537ec47a88484fb9aa35cde607d0d.html","title":"Infrastructure for rapid open knowledge network development","url":"https://onlinelibrary.wiley.com/doi/abs/10.1002/aaai.12038","authors":["Michael Cafarella","Michael Anderson","Iz Beltagy","Arie Cattan","Sarah Chasins","Ido Dagan","Doug Downey","Oren Etzioni","Sergey Feldman","Tian Gao","Tom Hope","Kexin Huang","Sophie Johnson","Daniel King","Kyle Lo","Yuze Lou","Matthew Shapiro","Dinghao Shen","Shivashankar Subramanian","Lucy Lu Wang","Yuning Wang","Yitong Wang","Daniel S Weld","Jenny Vo\\\\xe2\\\\x80\\\\x90Phamhi","Anna Zeng","Jiayun Zou"],"date":"2022/03","journal":"AI Magazine","abstract":"The past decade has witnessed a growth in the use of knowledge graph technologies for advanced data search, data integration, and query\\\\xe2\\\\x80\\\\x90answering applications. The leading example of a public, general\\\\xe2\\\\x80\\\\x90purpose open knowledge network (<i>aka</i> knowledge graph) is Wikidata, which has demonstrated remarkable advances in quality and coverage over this time. Proprietary knowledge graphs drive some of the leading applications of the day including, for example, Google Search, Alexa, Siri, and Cortana. Open Knowledge Networks are exciting: they promise the power of structured database\\\\xe2\\\\x80\\\\x90like queries with the potential for the wide coverage that is today only provided by the Web. With the current state of the art, building, using, and scaling large knowledge networks can still be frustratingly slow. This article describes a National Science Foundation Convergence Accelerator project to build a set of Knowledge\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"b3ecaca059c96da37ff79e9f0f0a3f43.html","title":"Morphology Without Borders: Clause-Level Morphological Annotation","url":"https://arxiv.org/abs/2202.12832","authors":["Omer Goldman","Reut Tsarfaty"],"date":"2022/02/25","journal":"arXiv preprint arXiv:2202.12832","abstract":""},{"id":"19f9fd3f9103cf3458009cf8e778d87f.html","title":"Sex and gender bias in natural language processing","url":"https://www.sciencedirect.com/science/article/pii/B9780128213926000091","authors":["Davide Cirillo","Hila Gonen","Enrico Santus","Alfonso Valencia","Marta R Costa-juss\\\\xc3\\\\xa0","Marta Villegas"],"date":"2022/01/1","abstract":"<div><div class=\\"gsh_csp\\">Natural language processing (NLP) is increasingly applied to a broad range of sensitive tasks, such as human resources, biomedicine, and healthcare. Accordingly, a growing body of research is investigating the impact of sex and gender bias in the models and the data on which such models are trained. As NLP systems become more pervasive in our societies, the vulnerability to sex and gender bias may cause the perpetuation of prejudice and discriminatory decisions. To address this challenge, a widespread awareness of bias needs to be created in the NLP community and more robust learning algorithms and fair solutions are required for the development and evaluation of NLP methods. In this chapter, we survey the state-of-the-art NLP models and some popular applications to biomedicine and health, with special emphasis on chatbots for mental health. Moreover, we discuss sources and implications of bias\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"f24c16c830cabafb87ac197c2aeab083.html","title":"\\" Um... it\\\\\'s really difficult to... um.. speak fluently\\": Neural tracking of spontaneous speech","url":"https://www.biorxiv.org/content/10.1101/2022.09.20.508802.abstract","authors":["Galit Agmon","Manuela Jaeger","Reut Tsarfaty","Martin G Bleichner","Elana Zion Golumbic"],"date":"2022/01/1","journal":"bioRxiv","abstract":"Spontaneous real-life speech is imperfect in many ways. It contains disfluencies and ill-formed utterances that the brain needs to contend with in order to extract meaning out of speech. Here, we studied how the neural speech-tracking response is affected by three specific factors that are prevalent in spontaneous colloquial speech: (1) the presence of non-lexical fillers, (2) the need to detect syntactic boundaries in disfluent speech and (3) the effort involved in processing syntactically complex phrases. Neural activity (EEG) was recorded from individuals as they listened to an unscripted, spontaneous narrative, which was analyzed in a time-resolved fashion to identify fillers, detect syntactic boundaries and assess the syntactic complexity of different phrases. When considering these factors in the speech-tracking analysis, we found that it was affected by all of them. The most consistent effect, observed for all three factors, was modulation of a centro-frontal negative response that peaked around 350 ms, highly resembling the well-known N400 ERP response linked to various aspects of lexical access and semantic processing. This response was observed for lexical words but not for fillers, was larger for opening vs. closing words of a clause and was enhanced in response to high-complexity phrases. These findings broaden ongoing efforts to understand neural processing of speech under increasingly realistic conditions. They highlight the importance of considering the imperfect nature of real-life spoken language, linking past research on linguistically well-formed and meticulously controlled speech to the type of speech that the brain actually deals\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"487ee42eaf144108997648125d4f3ea8.html","title":"Morphology Without Borders: Clause-Level Morphology","url":"http://114.215.220.151:8000/20221020/Morphology%20Without%20Borders-%20Clause-Level%20Morphology.pdf","authors":["Omer Goldman","Reut Tsarfaty"],"date":"2022","journal":"Transactions of the Association for Computational Linguistics","abstract":"Morphological tasks use large multi-lingual datasets that organize words into inflection tables, which then serve as training and evaluation data for various tasks. However, a closer inspection of these data reveals profound cross-linguistic inconsistencies, that arise from the lack of a clear linguistic and operational definition of what is a word, and that severely impair the universality of the derived tasks. To overcome this deficiency, we propose to view morphology as a clauselevel phenomenon, rather than word-level. It is anchored in a fixed yet inclusive set of features, that encapsulates all functions realized in a saturated clause. We deliver MIGHTYMORPH, a novel dataset for clause-level morphology covering 4 typologically-different languages: English, German, Turkish and Hebrew. We use this dataset to derive 3 clause-level morphological tasks: inflection, reinflection and analysis. Our experiments show that the clause-level tasks are substantially harder than the respective word-level tasks, while having comparable complexity across languages. Furthermore, redefining morphology to the clause-level provides a neat interface with contextualized language models (LMs) and allows assessing the morphological knowledge encoded in these models and their usability for morphological tasks. Taken together, this work opens up new horizons in the study of computational morphology, leaving ample space for studying neural morphology cross-linguistically."},{"id":"a347f34c1e96e44a01bf3aaa346178fe.html","title":"Annotation Scheme for Aggregated Argumentation in Online Educational Forums","url":"https://www.researchgate.net/profile/Noa-Brandel/publication/363039612_Annotation_Scheme_for_Aggregated_Argumentation_in_Online_Educational_Forums/links/632b2a520a708521500f281c/Annotation-Scheme-for-Aggregated-Argumentation-in-Online-Educational-Forums.pdf","authors":["Noa Brandel","Royi Lachmy","Noa Yomtovyan","Reut Tsarfaty","Baruch Schwarz"],"date":"2022","abstract":"We present an ongoing work of collecting small-group online discussions written in Hebrew. Discussants deal with contentious topics, while complying with educational ground rules. Our growing repository comprises 152 discussions containing over 3k turns. We propose an annotation procedure for turning such unstructured data into structured argument data in three stages:(i) segmenting to discourse and e-talk units,(ii) classifying e-talk units, and (iii) identifying and classifying relations between e-talk units. We applied the scheme to a sample. The results indicate the feasibility of the scheme as well as the abundance of natural argumentation and the intensity of interaction in the data."},{"id":"e0cb3ea66ec2bbd28f82a7f14c474c39.html","title":"McPhraSy: Multi context phrase similarity and clustering","url":"https://www.amazon.science/publications/mcphrasy-multi-context-phrase-similarity-and-clustering","authors":["Amir DN Cohen","Hila Gonen","Ori Shapira","Ran Levy","Yoav Goldberg"],"date":"2022","abstract":"Phrase similarity is a key component of many NLP applications. Current phrase similarity methods focus on embedding the phrase itself and use the phrase context only during training of the pretrained model. To better leverage the information in the context, we propose McPhraSy (Multi-context Phrase Similarity), a novel algorithm for estimating the similarity of phrases based on multiple contexts. At inference time, McPhraSy represents each phrase by considering multiple contexts in which it appears and computes the similarity of two phrases by aggregating the pairwise similarities between the contexts of the phrases. Incorporating context during inference enables McPhraSy to outperform current state-of-theart models on two phrase similarity datasets by up to 13.3%. Finally, we also present a new downstream task that relies on phrase similarity\\\\xe2\\\\x80\\\\x93keyphrase clustering\\\\xe2\\\\x80\\\\x93and create a new benchmark for it in the product reviews domain. We show that McPhraSy surpasses all other baselines for this task."},{"id":"b3b3e87a2b04e58a2c135454d7f9ea9d.html","title":"A proposition-level clustering approach for multi-document summarization","url":"https://arxiv.org/abs/2112.08770","authors":["Ori Ernst","Avi Caciularu","Ori Shapira","Ramakanth Pasunuru","Mohit Bansal","Jacob Goldberger","Ido Dagan"],"date":"2021/12/16","journal":"arXiv preprint arXiv:2112.08770","abstract":""},{"id":"b50aec380dd86ee681952fccde2b69d4.html","title":"Utilizing Evidence Spans via Sequence-Level Contrastive Learning for Long-Context Question Answering","url":"https://arxiv.org/abs/2112.08777","authors":["Avi Caciularu","Ido Dagan","Jacob Goldberger","Arman Cohan"],"date":"2021/12/16","journal":"arXiv preprint arXiv:2112.08777","abstract":""},{"id":"dd4123c3c445f65b2b0474821d00efbe.html","title":"Erratum: Measuring and Improving Consistency in Pretrained Language Models","url":"https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_x_00455/108602","authors":["Yanai Elazar","Nora Kassner","Shauli Ravfogel","Abhilasha Ravichander","Eduard Hovy","Hinrich Sch\\\\xc3\\\\xbctze","Yoav Goldberg"],"date":"2021/12/06","journal":"Transactions of the Association for Computational Linguistics","abstract":"During production of this paper, an error was introduced to the formula on the bottom of the right column of page 1020. In the last two terms of the formula, the n and m subscripts were swapped. The correct formula is:Lc=\\\\xe2\\\\x88\\\\x91n=1k\\\\xe2\\\\x88\\\\x91m=n+1kDKL(Qnri\\\\xe2\\\\x88\\\\xa5Qmri)+DKL(Qmri\\\\xe2\\\\x88\\\\xa5Qnri)The paper has been updated."},{"id":"7ce905c5f5921dd3b7f37343a5f84f12.html","title":"Proceedings of the Fifth Workshop on Universal Dependencies (UDW, SyntaxFest 2021)","url":"https://aclanthology.org/2021.udw-1.0.pdf","authors":["Miryam de Lhoneux","Reut Tsarfaty"],"date":"2021/12","abstract":"Universal Dependencies (UD) is a framework for cross-linguistically consistent treebank annotation that has so far been applied to over 100 languages (http://universaldependencies. org/). The framework is aiming to capture similarities as well as idiosyncrasies among typologically different languages (eg, morphologically rich languages, pro-drop languages, and languages featuring clitic doubling). The goal in developing UD was not only to support comparative evaluation and cross-lingual learning but also to facilitate multilingual natural language processing and enable comparative linguistic studies."},{"id":"2492b5eb66c9992f66e20deadbf55cc2.html","title":"Dyna-bAbI: unlocking bAbI\\\\\'s potential with dynamic synthetic benchmarking","url":"https://arxiv.org/abs/2112.00086","authors":["Ronen Tamari","Kyle Richardson","Aviad Sar-Shalom","Noam Kahlon","Nelson Liu","Reut Tsarfaty","Dafna Shahaf"],"date":"2021/11/30","journal":"arXiv preprint arXiv:2112.00086","abstract":"99% accuracy), neither approach succeeded in the compositional generalization setting, indicating the limitations of the original training data. We explored ways to augment the original data, and found that though diversifying training data was far more useful than simply increasing dataset size, it was still insufficient for driving robust compositional generalization (with <70% accuracy for complex compositions). Our results underscore the importance of highly controllable task generators for creating robust NLU systems through a virtuous cycle of model and data development."},{"id":"3a7395970dce2ba0258d886c2876fd66.html","title":"Well-defined morphology is sentence-level morphology","url":"https://aclanthology.org/2021.mrl-1.23/","authors":["Omer Goldman","Reut Tsarfaty"],"date":"2021/11","abstract":"Morphological tasks have gained decent popularity within the NLP community in the recent years, with large multi-lingual datasets providing morphological analysis of words, either in or out of context. However, the lack of a clear linguistic definition for words destines the annotative work to be incomplete and mired in inconsistencies, especially cross-linguistically. In this work we expand morphological inflection of words to inflection of sentences to provide true universality disconnected from orthographic traditions of white-space usage. To allow annotation for sentence-inflection we define a morphological annotation scheme by a fixed set of inflectional features. We present a small cross-linguistic dataset including semi-manually generated simple sentences in 4 typologically diverse languages annotated according to our suggested scheme, and show that the task of reinflection gets substantially more difficult but that the change of scope from words to well-defined sentences allows interface with contextualized language models."},{"id":"49dd869f1f5e47d5009735cf40bec935.html","title":"Extending Multi-Text Sentence Fusion Resources via Pyramid Annotations","url":"https://arxiv.org/abs/2110.04517","authors":["Daniela Brook Weiss","Paul Roit","Ori Ernst","Ido Dagan"],"date":"2021/10/09","journal":"arXiv preprint arXiv:2110.04517","abstract":""},{"id":"3ee304f6496e818f9d02eba9239e8e7b.html","title":"Multi-Document Keyphrase Extraction: A Literature Review and the First Dataset","url":"https://arxiv.org/abs/2110.01073","authors":["Ori Shapira","Ramakanth Pasunuru","Ido Dagan","Yael Amsterdamer"],"date":"2021/10/03","abstract":""},{"id":"c1d628449bb8c857b2abd704c469605e.html","title":"ifacetsum: Coreference-based interactive faceted summarization for multi-document exploration","url":"https://arxiv.org/abs/2109.11621","authors":["Eran Hirsch","Alon Eirew","Ori Shapira","Avi Caciularu","Arie Cattan","Ori Ernst","Ramakanth Pasunuru","Hadar Ronen","Mohit Bansal","Ido Dagan"],"date":"2021/09/23","journal":"arXiv preprint arXiv:2109.11621","abstract":""},{"id":"38493c710fe9f3d5aa21b13ef47170d7.html","title":"Asking it all: Generating contextualized questions for any semantic role","url":"https://arxiv.org/abs/2109.04832","authors":["Valentina Pyatkin","Paul Roit","Julian Michael","Reut Tsarfaty","Yoav Goldberg","Ido Dagan"],"date":"2021/09/10","journal":"arXiv preprint arXiv:2109.04832","abstract":""},{"id":"007cbefebd815eb34519ac816809ef99.html","title":"QA-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions","url":"https://ui.adsabs.harvard.edu/abs/2021arXiv210912655B/abstract","authors":["Daniela Brook Weiss","Paul Roit","Ayal Klein","Ori Ernst","Ido Dagan"],"date":"2021/09","journal":"arXiv e-prints","abstract":"Multi-text applications, such as multi-document summarization, are typically required to model redundancies across related texts. Current methods confronting consolidation struggle to fuse overlapping information. In order to explicitly represent content overlap, we propose to align predicate-argument relations across texts, providing a potential scaffold for information consolidation. We go beyond clustering coreferring mentions, and instead model overlap with respect to redundancy at a propositional level, rather than merely detecting shared referents. Our setting exploits QA-SRL, utilizing question-answer pairs to capture predicate-argument relations, facilitating laymen annotation of cross-text alignments. We employ crowd-workers for constructing a dataset of QA-based alignments, and present a baseline QA alignment model trained over our dataset. Analyses show that our new task is semantically challenging\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"a2e8249360d6096eaf3c7fbee82a93c4.html","title":"(Un) solving Morphological Inflection: Lemma Overlap Artificially Inflates Models\\\\\' Performance","url":"https://arxiv.org/abs/2108.05682","authors":["Omer Goldman","David Guriel","Reut Tsarfaty"],"date":"2021/08/12","journal":"arXiv preprint arXiv:2108.05682","abstract":""},{"id":"347b12d8a25711df92a879220358e3e2.html","title":"Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021)","url":"https://aclanthology.org/2021.iwpt-1.0.pdf","authors":["Stephan Oepen","Kenji Sagae","Reut Tsarfaty","Gosse Bouma","Djam\\\\xc3\\\\xa9 Seddah","Daniel Zeman"],"date":"2021/08","abstract":"Welcome to the 17th International Conference on Parsing Technologies (IWPT 2021), which this year (for only the second time since 2007) is co-located with the Annual Meeting of the Association for Computational Linguistics and of the Asian Federation of Natural Language Processing (ACL-IJCNLP). The IWPT meeting series, hosted by the ACL Special Interest Group in Natural Language Parsing (SIGPARSE), has been held biennualy since its inaugual meeting in 1989 in Pittsburgh, PA (USA)."},{"id":"43278519e32daa2ce64d5794bdd069de.html","title":"Teach the Rules, Provide the Facts: Targeted Relational-knowledge Enhancement for Textual Inference","url":"https://aclanthology.org/2021.starsem-1.8/","authors":["Ohad Rozen","Shmuel Amar","Vered Shwartz","Ido Dagan"],"date":"2021/08","abstract":"We present InferBert, a method to enhance transformer-based inference models with relevant relational knowledge. Our approach facilitates learning generic inference patterns requiring relational knowledge (eg inferences related to hypernymy) during training, while injecting on-demand the relevant relational facts (eg pangolin is an animal) at test time. We apply InferBERT to the NLI task over a diverse set of inference types (hypernymy, location, color, and country of origin), for which we collected challenge datasets. In this setting, InferBert succeeds to learn general inference patterns, from a relatively small number of training instances, while not hurting performance on the original NLI data and substantially outperforming prior knowledge enhancement models on the challenge data. It further applies its inferences successfully at test time to previously unobserved entities. InferBert is computationally more efficient than most prior methods, in terms of number of parameters, memory consumption and training time."},{"id":"48f5dee23c8f0c76c59e6914411c14da.html","title":"Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)","url":"https://aclanthology.org/2021.nlp4prog-1.0.pdf","authors":["Royi Lachmy","Ziyu Yao","Greg Durrett","Milos Gligoric","Junyi Jessy Li","Ray Mooney","Graham Neubig","Yu Su","Huan Sun","Reut Tsarfaty"],"date":"2021/08","abstract":"The proliferation of programming-related platforms such as GitHub and Stack Overflow has led to large amounts of rich, open-source data consisting of programs associated with natural language, such as natural language questions and answers with code snippets, open-source repositories with natural language comments, and communications between software developers. At the same time, deep learning based techniques have shown promising performance for modeling both natural language and computer programs. Driven by these revolutions on data and models, recent years have witnessed a major resurgence of using NLP techniques to assist programming (NLP4Prog)."},{"id":"50129dee9aadb1f75379e5032e2324a6.html","title":"Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language","url":"https://aclanthology.org/2021.unimplicit-1.0.pdf","authors":["Michael Roth","Reut Tsarfaty","Yoav Goldberg"],"date":"2021/08","abstract":"Welcome to UnImplicit: The First Workshop on Understanding Implicit and Underspecified Language. The focus of this workshop is on implicit and underspecified phenomena in language, which pose serious challenges to standard natural language processing models as they often require incorporating greater context, using symbolic inference and common-sense reasoning, or more generally, going beyond strictly lexical and compositional meaning constructs. This challenge spans all phases of the NLP model\\\\xe2\\\\x80\\\\x99s life cycle: from collecting and annotating relevant data, through devising computational methods for modelling such phenomena, to evaluating and designing proper evaluation metrics."},{"id":"ac29c9dac3204f92c7b6844c0424cae7.html","title":"Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing","url":"https://aclanthology.org/2021.gebnlp-1.0.pdf","authors":["Marta Costa-jussa","Hila Gonen","Christian Hardmeier","Kellie Webster"],"date":"2021/08","abstract":"This volume contains the proceedings of the Third Workshop on Gender Bias in Natural Language Processing held in conjunction with the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021). This year, the organisation committee changed membership: Will Radford made way to Hila Gonen. We thank Will greatly for his valuable and enthusiastic contributions to this workshop, and offer a warm welcome to Hila, whose expertise and insight we are all excited to learn from."},{"id":"f704b5a3aa08a7c8e4c3c9e70d6b32e5.html","title":"On the Power of Saturated Transformers: A View from Circuit Complexity","url":"https://arxiv.org/abs/2106.16213","authors":["William Merrill","Yoav Goldberg","Roy Schwartz","Noah A Smith"],"date":"2021/06/30","journal":"arXiv preprint arXiv:2106.16213","abstract":"<g transform=\\"matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 10.50000)\\"><path transform=\\"scale(0.48828, -0.48828)\\" d=\\"M 623 -45 Q 469 -45 350 25 T 166 221 T 102 500 Q 102 723 228 944 T 562 1304 T 995 1444 Q 1112 1444 1209 1402 T 1373 1285 T 1476 1111 T 1513 893 Q 1513 724 1441 556 T 1241 251 T 955 34 T 623 -45 Z M 639 16 Q 788 16 917 110 T 1137 354 T 1276 668 T 1325 975 Q 1325 1086 1287 1178 T 1170 1327 T 981 1384 Q 873 1384 771 1332 T 586 1194 Q 501 1107 435 978 T 335 708 T 301 442 Q 301 268 386 142 T 639 16 Z \\"></path><path transform=\\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 790.55902, 0.00000)\\" d=\\"M 635 -508 Q 521 -418 438 -302 T 303 -53 T 225 223 T 199 512 Q 199 659 225 803 T 304 1080 T 441 1329 T 635 1532 Q 635 1536 645 1536 H 664 Q 670 1536 675 1530 T 680 1518 Q 680 1509 676 1505 Q 576 1407 509 1295 T 402 1056 T 344 794 T 326 512 Q 326 -139 674 -477 Q 680 -483 680 -494 Q 680 -499 674 -506 T 664 -512 H 645 Q 635 -512 635 -508 Z \\"></path><g transform=\\"translate(1179.44897, 0.00000)\\"><path transform=\\"scale(0.48828, -0.48828)\\" d=\\"M 63 0 V 72 Q 133 72 178 83 T 223 137 V 1212 Q 223 1267 206 1291 T 159 1321 T 63 1327 V 1399 L 367 1421 V 137 Q 367 94 412 83 T 526 72 V 0 H 63 Z \\"></path><path transform=\\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 277.78000, 0.00000)\\" d=\\"M 512 -23 Q 389 -23 284 39 T 118 207 T 57 436 Q 57 530 90 617 T 186 772 T 332 879 T 512 918 Q 638 918 741 851 T 905 673 T 965 436 Q 965 313 904 207 T 738 39 T 512 -23 Z M 512 37 Q 676 37 731 156 T 786 459 Q 786 562 775 629 T 727 752 Q 704 786 668 811 T 593 850 T 512 864 Q 448 864 390 835 T 295 752 Q 257 694 246 624 T 236 459 Q 236 344 256 252 T 336 99 T 512 37 Z \\"></path><path transform=\\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 777.78003, 0.00000)\\" d=\\"M 57 -160 Q 57 -87 110 -33 T 236 45 Q 195 76 173 123 T 152 223 Q 152 319 213 393 Q 119 485 119 604 Q 119 668 146 724 T 223 821 T 332 883 T 455 905 Q 577 905 674 834 Q 716 879 773 903 T 893 928 Q 937 928 965 896 T 993 821 Q 993 796 974 777 T 930 758 Q 904 758 885 777 T 866 821 Q 866 859 891 874 Q 785 874 709 801 Q 746 764 768 710 T 791 604 Q 791 517 743 447 T 616 339 T 455 301 Q 343 301 250 362 Q 221 322 221 272 Q 221 218 256 177 T 346 137 H 514 Q 636 137 734 115 T 898 27 T 965 -160 Q 965 -250 889 -310 T 707 -396 T 512 -422 Q 421 -422 315 -396 T 133 -310 T 57 -160 Z M 172 -160 Q 172 -229 228 -276 T 363 -345 T 512 -367 Q 581 -367 660 -345 T 794 -276 T 850 -160 Q 850 -53 752 -22 T 514 10 H 346 Q 299 10 259 -13 T 196 -76 T 172 -160 Z M 455 356 Q 629 356 629 604 Q 629 711 592 780 T 455 850 Q 355 850 318 780 T 281 604 Q 281 536 295 481 T 347 391 T 455 356 Z \\"></path></g><path transform=\\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 2637.78418, 0.00000)\\" d=\\"M 158 35 Q 158 47 160 53 L 313 664 Q 328 721 328 764 Q 328 852 268 852 Q 204 852 173 775 T 113 582 Q 113 576 107 572 T 96 569 H 72 Q 65 569 60 576 T 55 590 Q 77 679 97 741 T 161 854 T 270 905 Q 347 905 406 856 T 465 733 Q 526 813 608 859 T 782 905 Q 855 905 908 880 T 990 804 T 1020 684 Q 1020 600 982 481 T 889 215 Q 860 148 860 92 Q 860 31 907 31 Q 987 31 1040 117 T 1116 301 Q 1120 313 1133 313 H 1157 Q 1165 313 1170 308 T 1176 295 Q 1176 293 1174 289 Q 1146 173 1076 75 T 903 -23 Q 831 -23 780 26 T 729 147 Q 729 185 745 227 Q 771 294 804 387 T 859 565 T 881 715 Q 881 772 857 812 T 778 852 Q 703 852 640 819 T 530 731 T 444 602 L 305 45 Q 298 17 273 -3 T 219 -23 Q 194 -23 176 -7 T 158 35 Z \\"></path><path transform=\\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 3238.00415, 0.00000)\\" d=\\"M 133 -512 Q 115 -512 115 -494 Q 115 -485 119 -481 Q 469 -139 469 512 Q 469 1163 123 1501 Q 115 1506 115 1518 Q 115 1525 120 1530 T 133 1536 H 152 Q 158 1536 162 1532 Q 309 1416 407 1250 T 550 896 T 596 512 Q 596 367 571 226 T 493 -51 T 358 -303 T 162 -508 Q 158 -512 152 -512 H 133 Z \\"></path></g></svg>, which implies saturated transformers can be simulated by log-depth circuits. Thus, the jump from hard to saturated attention can be understood as increasing the transformer\\\\\'s effective circuit depth by a factor of <svg class=\\"gs_fsvg\\" aria-label=\\"O(\\\\\\\\log n)\\" width=\\"51px\\" height=\\"14px\\" style=\\"vertical-align:-4px;\\"><g transform=\\"matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 10.50000)\\"><path transform=\\"scale(0.48828, -0.48828)\\" d=\\"M 623 -45 Q 469 -45 350 25 T 166 221 T 102 500 Q 102 723 228 944 T 562 1304 T 995 1444 Q 1112 1444 1209 1402 T 1373 1285 T 1476 1111 T 1513 893 Q 1513 724 1441 556 T 1241 251 T 955 34 T 623 -45 Z M 639 16 Q 788 16 917 110 T 1137 354 T 1276 668 T 1325 975 Q 1325 1086 1287 1178 T 1170 1327 T 981 1384 Q 873 1384 771 1332 T 586 1194 Q 501 1107 435 978 T 335 708 T 301 442 Q 301 268 386 142 T 639 16 Z \\"></path><path transform=\\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 790.55902, 0.00000)\\" d=\\"M 635 -508 Q 521 -418 438 -302 T 303 -53 T 225 223 T 199 512 Q 199 659 225 803 T 304 1080 T 441 1329 T 635 1532 Q 635 1536 645 1536 H 664 Q 670 1536 675 1530 T 680 1518 Q 680 1509 676 1505 Q 576 1407 509 1295 T 402 1056 T 344 794 T 326 512 Q 326 -139 674 -477 Q 680 -483 680 -494 Q 680 -499 674 -506 T 664 -512 H 645 Q 635 -512 635 -508 Z \\"></path><g transform=\\"translate(1179.44897, 0.00000)\\"><path transform=\\"scale(0.48828, -0.48828)\\" d=\\"M 63 0 V 72 Q 133 72 178 83 T 223 137 V 1212 Q 223 1267 206 1291 T 159 1321 T 63 1327 V 1399 L 367 1421 V 137 Q 367 94 412 83 T 526 72 V 0 H 63 Z \\"></path><path transform=\\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 277.78000, 0.00000)\\" d=\\"M 512 -23 Q 389 -23 284 39 T 118 207 T 57 436 Q 57 530 90 617 T 186 772 T 332 879 T 512 918 Q 638 918 741 851 T 905 673 T 965 436 Q 965 313 904 207 T 738 39 T 512 -23 Z M 512 37 Q 676 37 731 156 T 786 459 Q 786 562 775 629 T 727 752 Q 704 786 668 811 T 593 850 T 512 864 Q 448 864 390 835 T 295 752 Q 257 694 246 624 T 236 459 Q 236 344 256 252 T 336 99 T 512 37 Z \\"></path><path transform=\\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 777.78003, 0.00000)\\" d=\\"M 57 -160 Q 57 -87 110 -33 T 236 45 Q 195 76 173 123 T 152 223 Q 152 319 213 393 Q 119 485 119 604 Q 119 668 146 724 T 223 821 T 332 883 T 455 905 Q 577 905 674 834 Q 716 879 773 903 T 893 928 Q 937 928 965 896 T 993 821 Q 993 796 974 777 T 930 758 Q 904 758 885 777 T 866 821 Q 866 859 891 874 Q 785 874 709 801 Q 746 764 768 710 T 791 604 Q 791 517 743 447 T 616 339 T 455 301 Q 343 301 250 362 Q 221 322 221 272 Q 221 218 256 177 T 346 137 H 514 Q 636 137 734 115 T 898 27 T 965 -160 Q 965 -250 889 -310 T 707 -396 T 512 -422 Q 421 -422 315 -396 T 133 -310 T 57 -160 Z M 172 -160 Q 172 -229 228 -276 T 363 -345 T 512 -367 Q 581 -367 660 -345 T 794 -276 T 850 -160 Q 850 -53 752 -22 T 514 10 H 346 Q 299 10 259 -13 T 196 -76 T 172 -160 Z M 455 356 Q 629 356 629 604 Q 629 711 592 780 T 455 850 Q 355 850 318 780 T 281 604 Q 281 536 295 481 T 347 391 T 455 356 Z \\"></path></g><path transform=\\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 2637.78418, 0.00000)\\" d=\\"M 158 35 Q 158 47 160 53 L 313 664 Q 328 721 328 764 Q 328 852 268 852 Q 204 852 173 775 T 113 582 Q 113 576 107 572 T 96 569 H 72 Q 65 569 60 576 T 55 590 Q 77 679 97 741 T 161 854 T 270 905 Q 347 905 406 856 T 465 733 Q 526 813 608 859 T 782 905 Q 855 905 908 880 T 990 804 T 1020 684 Q 1020 600 982 481 T 889 215 Q 860 148 860 92 Q 860 31 907 31 Q 987 31 1040 117 T 1116 301 Q 1120 313 1133 313 H 1157 Q 1165 313 1170 308 T 1176 295 Q 1176 293 1174 289 Q 1146 173 1076 75 T 903 -23 Q 831 -23 780 26 T 729 147 Q 729 185 745 227 Q 771 294 804 387 T 859 565 T 881 715 Q 881 772 857 812 T 778 852 Q 703 852 640 819 T 530 731 T 444 602 L 305 45 Q 298 17 273 -3 T 219 -23 Q 194 -23 176 -7 T 158 35 Z \\"></path><path transform=\\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 3238.00415, 0.00000)\\" d=\\"M 133 -512 Q 115 -512 115 -494 Q 115 -485 119 -481 Q 469 -139 469 512 Q 469 1163 123 1501 Q 115 1506 115 1518 Q 115 1525 120 1530 T 133 1536 H 152 Q 158 1536 162 1532 Q 309 1416 407 1250 T 550 896 T 596 512 Q 596 367 571 226 T 493 -51 T 358 -303 T 162 -508 Q 158 -512 152 -512 H 133 Z \\"></path></g></svg>."},{"id":"7c64af89057b7dbfdf7564be51f23c47.html","title":"Draw me a flower: Grounding formal abstract structures stated in informal natural language","url":"https://arxiv.org/abs/2106.14321","authors":["Royi Lachmy","Valentina Pyatkin","Reut Tsarfaty"],"date":"2021/06/27","journal":"arXiv preprint arXiv:2106.14321","abstract":""},{"id":"363866b024b00107a22948adb2c7cf1f.html","title":"The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing","url":"https://arxiv.org/abs/2106.08037","authors":["Valentina Pyatkin","Shoval Sadde","Aynat Rubinstein","Paul Portner","Reut Tsarfaty"],"date":"2021/06/15","journal":"arXiv preprint arXiv:2106.08037","abstract":""},{"id":"0d94aa6a4e12227644e9be0ad18ec389.html","title":"Thinking Like Transformers","url":"http://proceedings.mlr.press/v139/weiss21a.html","authors":["Gail Weiss","Yoav Goldberg","Eran Yahav"],"date":"2021/06/13","journal":"arXiv preprint arXiv:2106.06981","abstract":"What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder\\\\xe2\\\\x80\\\\x94attention and feed-forward computation\\\\xe2\\\\x80\\\\x94into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works."},{"id":"9ce1183f30e173e5ae1d293a931aaf20.html","title":"Neural Extractive Search","url":"https://arxiv.org/abs/2106.04612","authors":["Shauli Ravfogel","Hillel Taub-Tabib","Yoav Goldberg"],"date":"2021/06/08","journal":"arXiv preprint arXiv:2106.04612","abstract":""},{"id":"b647f413c9121f00882ad2bad4c0598f.html","title":"Realistic evaluation principles for cross-document coreference resolution","url":"https://arxiv.org/abs/2106.04192","authors":["Arie Cattan","Alon Eirew","Gabriel Stanovsky","Mandar Joshi","Ido Dagan"],"date":"2021/06/08","journal":"arXiv preprint arXiv:2106.04192","abstract":""},{"id":"e06752572bd6369ff3c3bd44e3b93ce9.html","title":"CommonsenseQA 2.0: Exposing the Limits of AI through Gamification","url":"https://arxiv.org/abs/2201.05320","authors":["Alon Talmor","Ori Yoran","Ronan Le Bras","Chandra Bhagavatula","Yoav Goldberg","Yejin Choi","Jonathan Berant"],"date":"2021/06/07","abstract":""},{"id":"0a7b8b079aedc1abd1b4ec3b25c1b386.html","title":"Denoising word embeddings by averaging in a shared space","url":"https://arxiv.org/abs/2106.02954","authors":["Avi Caciularu","Ido Dagan","Jacob Goldberger"],"date":"2021/06/05","journal":"arXiv preprint arXiv:2106.02954","abstract":""},{"id":"a8dcfd514063bca825ea5e630341d9e0.html","title":"Cross-document coreference resolution over predicted mentions","url":"https://arxiv.org/abs/2106.01210","authors":["Arie Cattan","Alon Eirew","Gabriel Stanovsky","Mandar Joshi","Ido Dagan"],"date":"2021/06/02","journal":"arXiv preprint arXiv:2106.01210","abstract":""},{"id":"2b0c8b1eaf4252067f57b709adabc8d7.html","title":"Extending multi-document summarization evaluation to the interactive setting","url":"https://aclanthology.org/2021.naacl-main.54/?utm_campaign=%E6%AF%8E%E9%80%B1%20NLP%20%E8%AB%96%E6%96%87&utm_medium=email&utm_source=Revue%20newsletter","authors":["Ori Shapira","Ramakanth Pasunuru","Hadar Ronen","Mohit Bansal","Yael Amsterdamer","Ido Dagan"],"date":"2021/06","abstract":"Allowing users to interact with multi-document summarizers is a promising direction towards improving and customizing summary results. Different ideas for interactive summarization have been proposed in previous work but these solutions are highly divergent and incomparable. In this paper, we develop an end-to-end evaluation framework for interactive summarization, focusing on expansion-based interaction, which considers the accumulating information along a user session. Our framework includes a procedure of collecting real user sessions, as well as evaluation measures relying on summarization standards, but adapted to reflect interaction. All of our solutions and resources are available publicly as a benchmark, allowing comparison of future developments in interactive summarization, and spurring progress in its methodological evaluation. We demonstrate the use of our framework by evaluating and comparing baseline implementations that we developed for this purpose, which will serve as part of our benchmark. Our extensive experimentation and analysis motivate the proposed evaluation framework design and support its viability."},{"id":"6e725ce480416aca11940f2eb741c882.html","title":"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models","url":"https://arxiv.org/abs/2106.10199","authors":["Elad Ben Zaken","Shauli Ravfogel","Yoav Goldberg"],"date":"2021/06","journal":"arXiv e-prints","abstract":""},{"id":"785098357ec34f544e1e9502c83d6c41.html","title":"Hebrew Psychological Lexicons","url":"https://aclanthology.org/2021.clpsych-1.6/","authors":["Natalie Shapira","Dana Atzil-Slonim","Daniel Juravski","Moran Baruch","Dana Stolowicz-Melman","Adar Paz","Tal Alfi-Yogev","Roy Azoulay","Adi Singer","Maayan Revivo","Chen Dahbash","Limor Dayan","Tamar Naim","Lidar Gez","Boaz Yanai","Adva Maman","Adam Nadaf","Elinor Sarfati","Amna Baloum","Tal Naor","Ephraim Mosenkis","Badreya Sarsour","Jany Gelfand Morgenshteyn","Yarden Elias","Liat Braun","Moria Rubin","Matan Kenigsbuch","Noa Bergwerk","Noam Yosef","Sivan Peled","Coral Avigdor","Rahav Obercyger","Rachel Mann","Tomer Alper","Inbal Beka","Ori Shapira","Yoav Goldberg"],"date":"2021/06","abstract":"We introduce a large set of Hebrew lexicons pertaining to psychological aspects. These lexicons are useful for various psychology applications such as detecting emotional state, well being, relationship quality in conversation, identifying topics (eg, family, work) and many more. We discuss the challenges in creating and validating lexicons in a new language, and highlight our methodological considerations in the data-driven lexicon construction process. Most of the lexicons are publicly available, which will facilitate further research on Hebrew clinical psychology text analysis. The lexicons were developed through data driven means, and verified by domain experts, clinical psychologists and psychology students, in a process of reconciliation with three judges. Development and verification relied on a dataset of a total of 872 psychotherapy session transcripts. We describe the construction process of each collection, the final resource and initial results of research studies employing this resource."},{"id":"6d3dfed90fb29dfb47edc38fa73c3f71.html","title":"Cofga: A Dataset for Fine Grained Classification of Objects from Aerial Imagery","url":"https://arxiv.org/abs/2105.12786","authors":["Eran Dahan","Tzvi Diskin","Amit Amram","Amit Moryossef","Omer Koren"],"date":"2021/05/26","journal":"arXiv preprint arXiv:2105.12786","abstract":""},{"id":"0e2afeb4afe5f29ea22f588e5276188a.html","title":"Data Augmentation for Sign Language Gloss Translation","url":"https://arxiv.org/abs/2105.07476","authors":["Amit Moryossef","Kayo Yin","Graham Neubig","Yoav Goldberg"],"date":"2021/05/16","journal":"arXiv preprint arXiv:2105.07476","abstract":""},{"id":"cbbad3071050f42e9a8bf15f12bfad46.html","title":"Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction","url":"https://arxiv.org/abs/2105.06965","authors":["Shauli Ravfogel","Grusha Prasad","Tal Linzen","Yoav Goldberg"],"date":"2021/05/14","journal":"arXiv preprint arXiv:2105.06965","abstract":""},{"id":"9776b95b42a6943012b1c0b7c30e660e.html","title":"Including Signed Languages in Natural Language Processing","url":"https://arxiv.org/abs/2105.05222","authors":["Kayo Yin","Amit Moryossef","Julie Hochgesang","Yoav Goldberg","Malihe Alikhani"],"date":"2021/05/11","journal":"arXiv preprint arXiv:2105.05222","abstract":""},{"id":"33efd6ff4fe5d274a664b9ac02a131fe.html","title":"Provable Limitations of Acquiring Meaning from Ungrounded Form: What will Future Language Models Understand?","url":"https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00412/107385","authors":["William Merrill","Yoav Goldberg","Roy Schwartz","Noah A Smith"],"date":"2021/04/22","journal":"arXiv preprint arXiv:2104.10809","abstract":"Language models trained on billions of tokens have recently led to unprecedented results on many NLP tasks. This success raises the question of whether, in principle, a system can ever \\\\xe2\\\\x80\\\\x9cunderstand\\\\xe2\\\\x80\\\\x9d raw text without access to some form of grounding. We formally investigate the abilities of ungrounded systems to acquire meaning. Our analysis focuses on the role of \\\\xe2\\\\x80\\\\x9cassertions\\\\xe2\\\\x80\\\\x9d: textual contexts that provide indirect clues about the underlying semantics. We study whether assertions enable a system to emulate representations preserving semantic relations like equivalence. We find that assertions enable semantic emulation of languages that satisfy a strong notion of semantic transparency. However, for classes of languages where the same expression can take different values in different contexts, we show that emulation can become uncomputable. Finally, we discuss differences between our formal model and\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"d3f3e4eba4adbed7c2e26d1e1e9a0916.html","title":"Identifying helpful sentences in product reviews","url":"https://arxiv.org/abs/2104.09792","authors":["Iftah Gamzu","Hila Gonen","Gilad Kutiel","Ran Levy","Eugene Agichtein"],"date":"2021/04/20","journal":"arXiv preprint arXiv:2104.09792","abstract":""},{"id":"7b7e03bd4311a50328c104c48ff76a1d.html","title":"Scico: Hierarchical cross-document coreference for scientific concepts","url":"https://arxiv.org/abs/2104.08809","authors":["Arie Cattan","Sophie Johnson","Daniel Weld","Ido Dagan","Iz Beltagy","Doug Downey","Tom Hope"],"date":"2021/04/18","journal":"arXiv preprint arXiv:2104.08809","abstract":""},{"id":"bfa433d1388b8e5fbfe7ab80a88ce245.html","title":"Revisiting Few-shot Relation Classification: Evaluation Data and Classification Schemes","url":"https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00392/106791","authors":["Ofer Sabo","Yanai Elazar","Yoav Goldberg","Ido Dagan"],"date":"2021/04/17","journal":"arXiv preprint arXiv:2104.08481","abstract":"We explore few-shot learning (FSL) for relation classification (RC). Focusing on the realistic scenario of FSL, in which a test instance might not belong to any of the target categories (none-of-the-above, [NOTA]), we first revisit the recent popular dataset structure for FSL, pointing out its unrealistic data distribution. To remedy this, we propose a novel methodology for deriving more realistic few-shot test data from available datasets for supervised RC, and apply it to the TACRED dataset. This yields a new challenging benchmark for FSL-RC, on which state of the art models show poor performance. Next, we analyze classification schemes within the popular embedding-based nearest-neighbor approach for FSL, with respect to constraints they impose on the embedding space. Triggered by this analysis, we propose a novel classification scheme in which the NOTA category is represented as learned vectors, shown\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"9e2d3b89b49b6c47ed2913e83d252685.html","title":"Minimal Supervision for Morphological Inflection","url":"https://arxiv.org/abs/2104.08512","authors":["Omer Goldman","Reut Tsarfaty"],"date":"2021/04/17","journal":"arXiv preprint arXiv:2104.08512","abstract":""},{"id":"0b2418527e22a049b8f7e785f51a6750.html","title":"Back to square one: Artifact detection, training and commonsense disentanglement in the winograd schema","url":"https://arxiv.org/abs/2104.08161","authors":["Yanai Elazar","Hongming Zhang","Yoav Goldberg","Dan Roth"],"date":"2021/04/16","journal":"arXiv preprint arXiv:2104.08161","abstract":""},{"id":"932dc4ef13d1f54c7e303124485b8111.html","title":"Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?","url":"https://arxiv.org/abs/2104.07762","authors":["Eric Lehman","Sarthak Jain","Karl Pichotta","Yoav Goldberg","Byron C Wallace"],"date":"2021/04/15","journal":"arXiv preprint arXiv:2104.07762","abstract":""},{"id":"a0caa53d022de1939381b6cb7002d06d.html","title":"WEC: Deriving a large-scale cross-document event coreference dataset from Wikipedia","url":"https://arxiv.org/abs/2104.05022","authors":["Alon Eirew","Arie Cattan","Ido Dagan"],"date":"2021/04/11","journal":"arXiv preprint arXiv:2104.05022","abstract":""},{"id":"e7c2f659b544b8495a55283f8179f234.html","title":"AlephBERT: A Hebrew large pre-trained language model to start-off your Hebrew NLP application with","url":"https://arxiv.org/abs/2104.04052","authors":["Amit Seker","Elron Bandel","Dan Bareket","Idan Brusilovsky","Refael Shaked Greenfeld","Reut Tsarfaty"],"date":"2021/04/08","journal":"arXiv preprint arXiv:2104.04052","abstract":""},{"id":"b1a6865fcb55d610d11692f6f10d6604.html","title":"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume","url":"https://aclanthology.org/2021.eacl-main.0.pdf","authors":["Paola Merlo","J\\\\xc3\\\\xb6rg Tiedemann","Reut Tsarfaty"],"date":"2021/04","abstract":"Welcome to EACL 2021, the 16th conference of the European Chapter of the Association for Computational Linguistics! This year\\\\xe2\\\\x80\\\\x99s conference is held from the 21st to the 23rd of April, 2021. While we were planning to hold the conference in Kyiv, due to the current COVID situation the conference is held entirely online. EACL 2021 is also an anchor conference to several workshops and tutorials, that are held on April 19th and 20th, also online."},{"id":"45be7452f3099134506a504603c4549a.html","title":"Using topic models to identify clients\\\\xe2\\\\x80\\\\x99 functioning levels and alliance ruptures in psychotherapy.","url":"https://psycnet.apa.org/psycarticles/2021-27454-001.pdf","authors":["Dana Atzil-Slonim","Daniel Juravski","Eran Bar-Kalifa","Eva Gilboa-Schechtman","Rivka Tuval-Mashiach","Natalie Shapira","Yoav Goldberg"],"date":"2021/03/18","journal":"Psychotherapy","abstract":"Computerized natural language processing techniques can analyze psychotherapy sessions as texts, thus generating information about the therapy process and outcome and supporting the scaling-up of psychotherapy research. We used topic modeling to identify topics discussed in psychotherapy sessions and explored (a) which topics best identified clients\\\\xe2\\\\x80\\\\x99 functioning and alliance ruptures and (b) whether changes in these topics were associated with changes in outcome. Transcripts of 873 sessions from 58 clients treated by 52 therapists were analyzed. Before each session, clients self-reported functioning and symptom level. After each session, therapists reported the extent of alliance rupture. Latent Dirichlet allocation was used to extract latent topics from psychotherapy textual data. Then a sparse multinomial logistic regression model was used to predict which topics best identified clients\\\\xe2\\\\x80\\\\x99 functioning levels\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"72e4ff79b23db72f79ba470bfd32b78d.html","title":"Contrastive explanations for model interpretability","url":"https://arxiv.org/abs/2103.01378","authors":["Alon Jacovi","Swabha Swayamdipta","Shauli Ravfogel","Yanai Elazar","Yejin Choi","Yoav Goldberg"],"date":"2021/03/02","journal":"arXiv preprint arXiv:2103.01378","abstract":""},{"id":"746b675e268561d51ea26c624e73f530.html","title":"Bootstrapping Relation Extractors using Syntactic Search by Examples","url":"https://arxiv.org/abs/2102.05007","authors":["Matan Eyal","Asaf Amrami","Hillel Taub-Tabib","Yoav Goldberg"],"date":"2021/02/09","journal":"arXiv preprint arXiv:2102.05007","abstract":""},{"id":"178ffbed70bef3156cfbfab4fcaf2be5.html","title":"Measuring and improving consistency in pretrained language models","url":"https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00410/107384","authors":["Yanai Elazar","Nora Kassner","Shauli Ravfogel","Abhilasha Ravichander","Eduard Hovy","Hinrich Sch\\\\xc3\\\\xbctze","Yoav Goldberg"],"date":"2021/02/01","journal":"Transactions of the Association for Computational Linguistics","abstract":" <i>Consistency</i> of a model\\\\xe2\\\\x80\\\\x94that is, the invariance of its behavior under meaning-preserving alternations in its input\\\\xe2\\\\x80\\\\x94is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create <span class=\\"gs_fscp\\">ParaRel</span>\\\\xf0\\\\x9f\\\\xa4\\\\x98, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using <span class=\\"gs_fscp\\">ParaRel</span>\\\\xf0\\\\x9f\\\\xa4\\\\x98, we show that the consistency of all PLMs we experiment with is poor\\\\xe2\\\\x80\\\\x94 though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness."},{"id":"201f91fe3499a61ad07cb3a943497d9c.html","title":"Amnesic probing: Behavioral explanation with amnesic counterfactuals","url":"https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00359/98091","authors":["Yanai Elazar","Shauli Ravfogel","Alon Jacovi","Yoav Goldberg"],"date":"2021/02/01","journal":"Transactions of the Association for Computational Linguistics","abstract":"A growing body of work makes use of <i>probing</i> in order to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results, and offer an alternative method that focuses on how the information is being used, rather than on what information is encoded. Our method, <i>Amnesic Probing</i>, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention that removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, for example, is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"ebdddbe36b3556558219dffcd777a23b.html","title":"CD2CR: Co-reference Resolution Across Documents and Domains","url":"https://arxiv.org/abs/2101.12637","authors":["James Ravenscroft","Arie Cattan","Amanda Clare","Ido Dagan","Maria Liakata"],"date":"2021/01/29","journal":"arXiv preprint arXiv:2101.12637","abstract":"<g transform=\\"matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 11.58769)\\"><g><path transform=\\"matrix(0.34180, 0.00000, 0.00000, -0.34180, 0.00000, -362.89200)\\" d=\\"M 129 0 V 59 Q 129 68 137 76 L 502 432 Q 523 454 535 466 T 571 502 Q 692 623 759 727 T 827 956 Q 827 1024 804 1083 T 739 1187 T 641 1254 T 516 1278 Q 425 1278 349 1231 T 236 1104 H 242 Q 291 1104 322 1071 T 354 991 Q 354 945 322 912 T 242 879 Q 195 879 162 912 T 129 991 Q 129 1101 190 1185 T 349 1314 T 551 1360 Q 675 1360 786 1313 T 965 1174 T 1034 956 Q 1034 865 993 786 T 892 648 T 726 505 T 588 395 L 338 182 H 526 Q 603 182 687 182 T 836 185 T 905 193 Q 923 212 934 261 T 956 381 H 1034 L 973 0 H 129 Z \\"></path></g></g></svg>CR). The task aims to identify links between entities across heterogeneous document types. We show that in this cross-domain, cross-document setting, existing CDCR models do not perform well and we provide a baseline model that outperforms current state-of-the-art CDCR models on CD<svg class=\\"gs_fsvg\\" aria-label=\\"^2\\" width=\\"6px\\" height=\\"12px\\" style=\\"vertical-align:0px;\\"><g transform=\\"matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 11.58769)\\"><g><path transform=\\"matrix(0.34180, 0.00000, 0.00000, -0.34180, 0.00000, -362.89200)\\" d=\\"M 129 0 V 59 Q 129 68 137 76 L 502 432 Q 523 454 535 466 T 571 502 Q 692 623 759 727 T 827 956 Q 827 1024 804 1083 T 739 1187 T 641 1254 T 516 1278 Q 425 1278 349 1231 T 236 1104 H 242 Q 291 1104 322 1071 T 354 991 Q 354 945 322 912 T 242 879 Q 195 879 162 912 T 129 991 Q 129 1101 190 1185 T 349 1314 T 551 1360 Q 675 1360 786 1313 T 965 1174 T 1034 956 Q 1034 865 993 786 T 892 648 T 726 505 T 588 395 L 338 182 H 526 Q 603 182 687 182 T 836 185 T 905 193 Q 923 212 934 261 T 956 381 H 1034 L 973 0 H 129 Z \\"></path></g></g></svg>CR. Our data set, annotation tool and guidelines as well as our model for cross-document cross-domain co-reference are all supplied as open access open source resources."},{"id":"fae2d35b880158058f87ad7c0f722685.html","title":"First align, then predict: Understanding the cross-lingual ability of multilingual BERT","url":"https://arxiv.org/abs/2101.11109","authors":["Benjamin Muller","Yanai Elazar","Beno\\\\xc3\\\\xaet Sagot","Djam\\\\xc3\\\\xa9 Seddah"],"date":"2021/01/26","journal":"arXiv preprint arXiv:2101.11109","abstract":""},{"id":"283d2642fca0a572a0de586578f8a6fd.html","title":"Message from the Organizers","url":"https://cris.bgu.ac.il/en/publications/message-from-the-organizers","authors":["Royi Lachmy","Ziyu Yao","Greg Durrett","Milos Gligoric","Junyi Jessy Li","Ray Mooney","Graham Neubig","Yu Su","Huan Sun","Reut Tsarfaty"],"date":"2021/01/1","journal":"NLP4Prog 2021-1st Workshop on Natural Language Processing for Programming, Proceedings of the Workshop","abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_oci_field\\">Scholar articles"},{"id":"2fb9378a0d421d61a6453176e0688652.html","title":"Cross-document language modeling","url":"https://researchain.net/archives/pdf/Cross-Document-Language-Modeling-1914484","authors":["Avi Caciularu","Arman Cohan","Iz Beltagy","Matthew E Peters","Arie Cattan","Ido Dagan"],"date":"2021/01/1","journal":"arXiv preprint arXiv:2101.00406","abstract":"We introduce a new pretraining approach for language models that are geared to support multi-document NLP tasks. Our cross-document language model (CD-LM) improves masked language modeling for these tasks with two key ideas. First, we pretrain with multiple related documents in a single input, via cross-document masking, which encourages the model to learn cross-document and long-range relationships. Second, extending the recent Longformer model, we pretrain with long contexts of several thousand tokens and introduce a new attention pattern that uses sequence-level global attention to predict masked tokens, while retaining the familiar local attention elsewhere. We show that our CD-LM sets new state-of-the-art results for several multi-text tasks, including cross-document event and entity coreference resolution, paper citation recommendation, and documents plagiarism detection, while using a significantly reduced number of training parameters relative to prior works."},{"id":"2dcb02939a37220b93cc2a4413a22261.html","title":"CDLM: Cross-document language modeling","url":"https://arxiv.org/abs/2101.00406","authors":["Avi Caciularu","Arman Cohan","Iz Beltagy","Matthew E Peters","Arie Cattan","Ido Dagan"],"date":"2021/01/02","journal":"arXiv preprint arXiv:2101.00406","abstract":""},{"id":"3243498d1f2a20bd0486dedf38d54d8f.html","title":"Sign language processing","url":"https://scholar.google.com/scholar?cluster=14499252734653308117&hl=en&oi=scholarr","authors":["Amit Moryossef","Yoav Goldberg"],"date":"2021"},{"id":"b37a5995d1ef6b3f70a35c608456b596.html","title":"Evaluating the immediate applicability of pose estimation for sign language recognition","url":"https://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/html/Moryossef_Evaluating_the_Immediate_Applicability_of_Pose_Estimation_for_Sign_Language_CVPRW_2021_paper.html","authors":["Amit Moryossef","Ioannis Tsochantaridis","Joe Dinn","Necati Cihan Camgoz","Richard Bowden","Tao Jiang","Annette Rios","Mathias Muller","Sarah Ebling"],"date":"2021","abstract":"Signed languages are visual languages produced by the movement of the hands, face, and body. In this paper, we evaluate representations based on skeleton poses, as these are explainable, person-independent, privacy-preserving, low-dimensional representations. Basically, skeletal representations generalize over an individual\\\\\'s appearance and background, allowing us to focus on the recognition of motion. But how much information is lost by the skeletal representation? We perform two independent studies using two state-of-the-art pose estimation systems. We analyze the applicability of the pose estimation systems to sign language recognition by evaluating the failure cases of the recognition models. Importantly, this allows us to characterize the current limitations of skeletal pose estimation approaches in sign language recognition."},{"id":"fe447599c5140bd32bd799e3a6d85b15.html","title":"Sign language datasets","url":"https://scholar.google.com/scholar?cluster=16152962152402983930&hl=en&oi=scholarr","authors":["Amit Moryossef"],"date":"2021"},{"id":"ae51f679c646689f0a72f4f382ae911b.html","title":"Facts2Story: Controlling Text Generation by Key Facts","url":"https://arxiv.org/abs/2012.04332","authors":["Eyal Orbach","Yoav Goldberg"],"date":"2020/12/08","journal":"arXiv preprint arXiv:2012.04332","abstract":""},{"id":"04cc422d2348b6e2ada8c4bf388fd8fc.html","title":"QANom: Question-Answer driven SRL for Nominalizations","url":"https://www.aclweb.org/anthology/2020.coling-main.274/","authors":["Ayal Klein","Jonathan Mamou","Valentina Pyatkin","Daniela Stepanov","Hangfeng He","Dan Roth","Luke Zettlemoyer","Ido Dagan"],"date":"2020/12","abstract":"We propose a new semantic scheme for capturing predicate-argument relations for nominalizations, termed QANom. This scheme extends the QA-SRL formalism (He et al., 2015), modeling the relations between nominalizations and their arguments via natural language question-answer pairs. We construct the first QANom dataset using controlled crowdsourcing, analyze its quality and compare it to expertly annotated nominal-SRL annotations, as well as to other QA-driven annotations. In addition, we train a baseline QANom parser for identifying nominalizations and labeling their arguments with question-answer pairs. Finally, we demonstrate the extrinsic utility of our annotations for downstream tasks using both indirect supervision and zero-shot settings."},{"id":"3bca8464c18af9b71026cd27b9169dd2.html","title":"oLMpics-on what language model pre-training captures","url":"https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00342/96476","authors":["Alon Talmor","Yanai Elazar","Yoav Goldberg","Jonathan Berant"],"date":"2020/12","journal":"Transactions of the Association for Computational Linguistics","abstract":"Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"5aa70928d6a64b5b6b50f6129740a50c.html","title":"Compressing Pre-trained Language Models by Matrix Decomposition","url":"https://aclanthology.org/2020.aacl-main.88/?ref=https://githubhelp.com","authors":["Matan Ben Noach","Yoav Goldberg"],"date":"2020/12","abstract":"Large pre-trained language models reach state-of-the-art results on many different NLP tasks when fine-tuned individually; They also come with a significant memory and computational requirements, calling for methods to reduce model sizes (green AI). We propose a two-stage model-compression method to reduce a model\\\\xe2\\\\x80\\\\x99s inference time cost. We first decompose the matrices in the model into smaller matrices and then perform feature distillation on the internal representation to recover from the decomposition. This approach has the benefit of reducing the number of parameters while preserving much of the information within the model. We experimented on BERT-base model with the GLUE benchmark dataset and show that we can reduce the number of parameters by a factor of 0.4 x, and increase inference speed by a factor of 1.45 x, while maintaining a minimal loss in metric performance."},{"id":"baa39734569d30938c4ae6393939daf2.html","title":"Proceedings of Knowledgeable NLP: the First Workshop on Integrating Structured Knowledge and Neural Networks for NLP","url":"https://www.aclweb.org/anthology/2020.knlp-1.0.pdf","authors":["Oren Sar Shalom","Alexander Panchenko","Cicero dos Santos","Varvara Logacheva","Alessandro Moschitti","Ido Dagan"],"date":"2020/12","abstract":"Welcome to the First Knowledgeable NLP Workshop\u2014a workshop on the use of structured resources in NLP and on development of such resources."},{"id":"43bfecf1bea02dd552b5184fa834a3f6.html","title":"Within-Between Lexical Relation Classification Using Path-based and Distributional Data","url":"https://www.aclweb.org/anthology/2020.emnlp-main.284.pdf","authors":["Oren Barkan","Avi Caciularu","Ido Dagan"],"date":"2020/11","abstract":"We propose the novel Within-Between Relation model for recognizing lexical-semantic relations between words. Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation. We show that the proposed model is competitive and outperforms other baselines, across various benchmarks."},{"id":"7bb57596abec821b5ea839bbb016f495.html","title":"A Pointer Network Architecture for Joint Morphological Segmentation and Tagging","url":"https://www.aclweb.org/anthology/2020.findings-emnlp.391/","authors":["Amit Seker","Reut Tsarfaty"],"date":"2020/11","abstract":"Morphologically Rich Languages (MRLs) such as Arabic, Hebrew and Turkish often require Morphological Disambiguation (MD), ie, the prediction of morphological decomposition of tokens into morphemes, early in the pipeline. Neural MD may be addressed as a simple pipeline, where segmentation is followed by sequence tagging, or as an end-to-end model, predicting morphemes from raw tokens. Both approaches are sub-optimal; the former is heavily prone to error propagation, and the latter does not enjoy explicit access to the basic processing units called morphemes. This paper offers MD architecture that combines the symbolic knowledge of morphemes with the learning capacity of neural end-to-end modeling. We propose a new, general and easy-to-implement Pointer Network model where the input is a morphological lattice and the output is a sequence of indices pointing at a single disambiguated path of morphemes. We demonstrate the efficacy of the model on segmentation and tagging, for Hebrew and Turkish texts, based on their respective Universal Dependencies (UD) treebanks. Our experiments show that with complete lattices, our model outperforms all shared-task results on segmenting and tagging these languages. On the SPMRL treebank, our model outperforms all previously reported results for Hebrew MD in realistic scenarios."},{"id":"a945bc98cd547309b7286bfb6bd93b06.html","title":"Evaluating models\u2019 local decision boundaries via contrast sets","url":"https://www.aclweb.org/anthology/2020.findings-emnlp.117/","authors":["Matt Gardner","Yoav Artzi","Victoria Basmov","Jonathan Berant","Ben Bogin","Sihao Chen","Pradeep Dasigi","Dheeru Dua","Yanai Elazar","Ananth Gottumukkala","Nitish Gupta","Hannaneh Hajishirzi","Gabriel Ilharco","Daniel Khashabi","Kevin Lin","Jiangming Liu","Nelson F Liu","Phoebe Mulcaire","Qiang Ning","Sameer Singh","Noah A Smith","Sanjay Subramanian","Reut Tsarfaty","Eric Wallace","Ally Zhang","Ben Zhou"],"date":"2020/11","abstract":"Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (eg, annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the abilities a dataset is intended to test. We propose a more rigorous annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model\u2019s decision boundary, which can be used to more accurately evaluate a model\u2019s true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (eg, DROP reading comprehension, UD parsing, and IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets\u2014up to 25% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes."},{"id":"8a74baffbe6a6590985c779149df421f.html","title":"Pick a Fight or Bite your Tongue: Investigation of Gender Differences in Idiomatic Language Usage","url":"https://arxiv.org/abs/2011.00335","authors":["Ella Rabinovich","Hila Gonen","Suzanne Stevenson"],"date":"2020/10/31","journal":"arXiv preprint arXiv:2011.00335","abstract":"A large body of research on gender-linked language has established foundations regarding cross-gender differences in lexical, emotional, and topical preferences, along with their sociological underpinnings. We compile a novel, large and diverse corpus of spontaneous linguistic productions annotated with speakers\' gender, and perform a first large-scale empirical study of distinctions in the usage of\\\\\\\\textit {figurative language} between male and female authors. Our analyses suggest that (1) idiomatic choices reflect gender-specific lexical and semantic preferences in general language,(2) men\'s and women\'s idiomatic usages express higher emotion than their literal language, with detectable, albeit more subtle, differences between male and female authors along the dimension of dominance compared to similar distinctions in their literal utterances, and (3) contextual analysis of idiomatic expressions reveals considerable differences, reflecting subtle divergences in usage environments, shaped by cross-gender communication styles and semantic biases."},{"id":"5d930d821acc2f3e0c2f15b1ddda2560.html","title":"Synthesizing control for a system with black box environment, based on deep learning","url":"https://link.springer.com/chapter/10.1007/978-3-030-61470-6_27","authors":["Simon Iosti","Doron Peled","Khen Aharon","Saddek Bensalem","Yoav Goldberg"],"date":"2020/10/20","abstract":"We study the synthesis of control for a system that interacts with a black-box environment, based on deep learning. The goal is to minimize the number of interaction failures. The current state of the environment is unavailable to the controller, hence its operation depends on a limited view of the history. We suggest a reinforcement learning framework of training a Recurrent Neural Network (RNN) to control such a system. We experiment with various parameters: loss function, exploration/exploitation ratio, and size of lookahead. We designed examples that capture various potential control difficulties. We present experiments performed with the toolkit DyNet."},{"id":"4cc6f5cbe445d170e3d2000a281055c5.html","title":"Parameter Norm Growth During Training of Transformers","url":"https://openreview.net/forum?id=tC0xwrmOKXE","authors":["William Merrill","Vivek Ramanujan","Yoav Goldberg","Roy Schwartz","Noah Smith"],"date":"2020/10/19","journal":"arXiv preprint arXiv:2010.09697","abstract":"The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically some variant of gradient descent (GD). To better understand this bias, we study the tendency of transformer parameters to grow in magnitude during training. We find, both theoretically and empirically, that, in certain contexts, GD increases the parameter <svg class=\\"gs_fsvg\\" aria-label=\\" L_2 \\" width=\\"16px\\" height=\\"12px\\" style=\\"vertical-align:-2px;\\"><g transform=\\"matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 9.56200)\\"><g><path transform=\\"scale(0.48828, -0.48828)\\" d=\\"M 96 0 Q 76 0 76 27 Q 77 32 80 44 T 88 64 T 102 72 Q 227 72 276 86 Q 303 95 315 141 L 596 1266 Q 600 1286 600 1294 Q 600 1316 575 1319 Q 537 1327 428 1327 Q 408 1327 408 1354 Q 415 1380 419 1389 T 442 1399 H 1047 Q 1065 1399 1065 1372 Q 1062 1351 1058 1339 T 1038 1327 Q 887 1327 836 1317 Q 786 1308 774 1257 L 494 133 Q 485 108 485 88 Q 485 72 555 72 H 745 Q 906 72 1006 136 T 1154 280 T 1234 447 T 1282 537 H 1300 Q 1321 537 1321 510 L 1139 14 Q 1136 0 1120 0 H 96 Z \\"></path><g transform=\\"translate(680.56000, 150.00000)\\"><path transform=\\"scale(0.34180, -0.34180)\\" d=\\"M 129 0 V 59 Q 129 68 137 76 L 502 432 Q 523 454 535 466 T 571 502 Q 692 623 759 727 T 827 956 Q 827 1024 804 1083 T 739 1187 T 641 1254 T 516 1278 Q 425 1278 349 1231 T 236 1104 H 242 Q 291 1104 322 1071 T 354 991 Q 354 945 322 912 T 242 879 Q 195 879 162 912 T 129 991 Q 129 1101 190 1185 T 349 1314 T 551 1360 Q 675 1360 786 1313 T 965 1174 T 1034 956 Q 1034 865 993 786 T 892 648 T 726 505 T 588 395 L 338 182 H 526 Q 603 182 687 182 T 836 185 T 905 193 Q 923 212 934 261 T 956 381 H 1034 L 973 0 H 129 Z \\"></path></g></g></g></svg> norm up to a threshold that itself increases with training-set accuracy. This means increasing training accuracy over time enables the norm to increase. Empirically, we show that the norm grows continuously over pretraining for T5 (Raffel et al., 2019). We show that pretrained T5 approximates a semi-discretized network with saturated activation functions. Such\\" saturated\\" networks are known to have a reduced capacity compared to the original network family that can be\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"09223d33ce40b8a410e85ecc82357aa5.html","title":"It\\\\\'s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT","url":"https://arxiv.org/abs/2010.08275","authors":["Hila Gonen","Shauli Ravfogel","Yanai Elazar","Yoav Goldberg"],"date":"2020/10/16","journal":"arXiv preprint arXiv:2010.08275","abstract":""},{"id":"24057e3f0ce9d6750cfe9e1c9972fa96.html","title":"It\'s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT","url":"https://arxiv.org/abs/2010.08275","authors":["Hila Gonen","Shauli Ravfogel","Yanai Elazar","Yoav Goldberg"],"date":"2020/10/16","journal":"arXiv preprint arXiv:2010.08275","abstract":"Recent works have demonstrated that multilingual BERT (mBERT) learns rich cross-lingual representations, that allow for transfer across languages. We study the word-level translation information embedded in mBERT and present two simple methods that expose remarkable translation capabilities with no fine-tuning. The results suggest that most of this information is encoded in a non-linear way, while some of it can also be recovered with purely linear tools. As part of our analysis, we test the hypothesis that mBERT learns representations which contain both a language-encoding component and an abstract, cross-lingual component, and explicitly identify an empirical language-identity subspace within mBERT representations."},{"id":"f7838865887b58e5ad3bdce9bec47041.html","title":"Formalizing trust in artificial intelligence: Prerequisites, causes and goals of human trust in ai","url":"https://dl.acm.org/doi/abs/10.1145/3442188.3445923","authors":["Alon Jacovi","Ana Marasovi\\\\xc4\\\\x87","Tim Miller","Yoav Goldberg"],"date":"2020/10/15","journal":"arXiv preprint arXiv:2010.07487","abstract":"Trust is a central component of the interaction between people and AI, in that\\\\\'incorrect\\\\\'levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (ie, trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model\\\\\'s decisions. We incorporate a formalization of\\\\\'contractual trust\\\\\', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of\\\\\'trustworthiness\\\\\'(that detaches from the notion of trustworthiness in\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"2e35c73abcbc16d7b18e37ecf1f05ae5.html","title":"The Extraordinary Failure of Complement Coercion Crowdsourcing","url":"https://arxiv.org/abs/2010.05971","authors":["Yanai Elazar","Victoria Basmov","Shauli Ravfogel","Yoav Goldberg","Reut Tsarfaty"],"date":"2020/10/12","journal":"arXiv preprint arXiv:2010.05971","abstract":""},{"id":"296d6857607d59e36261a63cb19a04a1.html","title":"Do Language Embeddings Capture Scales?","url":"https://arxiv.org/abs/2010.05345","authors":["Xikun Zhang","Deepak Ramachandran","Ian Tenney","Yanai Elazar","Dan Roth"],"date":"2020/10/11","journal":"arXiv preprint arXiv:2010.05345","abstract":"Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense, and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance and show that a simple method of canonicalizing numbers can have a significant effect on the results."},{"id":"3c6fe40f8dff5eb5938d3be27f38f38d.html","title":"Unsupervised Distillation of Syntactic Information from Contextualized Word Representations","url":"https://arxiv.org/abs/2010.05265","authors":["Shauli Ravfogel","Yanai Elazar","Jacob Goldberger","Yoav Goldberg"],"date":"2020/10/11","journal":"arXiv preprint arXiv:2010.05265","abstract":""},{"id":"b70600ec9ca969016a4d1130dd06f9f4.html","title":"Relation Classification as Two-way Span-Prediction","url":"https://arxiv.org/abs/2010.04829","authors":["Amir DN Cohen","Shachar Rosenman","Yoav Goldberg"],"date":"2020/10/09","journal":"arXiv preprint arXiv:2010.04829","abstract":""},{"id":"bc01c05573b4d670884fea3fa3c8f7b8.html","title":"Relation Extraction as Two-way Span-Prediction","url":"https://scholar.google.com/scholar?cluster=5663042029809355594&hl=en&oi=scholarr","authors":["Amir DN Cohen","Shachar Rosenman","Yoav Goldberg"],"date":"2020/10/09","journal":"arXiv preprint arXiv:2010.04829"},{"id":"29f4ce691e8c93728c2f69abb7fa0032.html","title":"Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data","url":"https://arxiv.org/abs/2010.03656","authors":["Shachar Rosenman","Alon Jacovi","Yoav Goldberg"],"date":"2020/10/07","journal":"arXiv preprint arXiv:2010.03656","abstract":""},{"id":"e5620947b08d5bbc26cdf5e014b90bfa.html","title":"ZEST: Zero-shot Learning from Text Descriptions using Textual Similarity and Visual Summarization","url":"https://arxiv.org/abs/2010.03276","authors":["Tzuf Paz-Argaman","Yuval Atzmon","Gal Chechik","Reut Tsarfaty"],"date":"2020/10/07","journal":"arXiv preprint arXiv:2010.03276","abstract":"We study the problem of recognizing visual entities from the textual descriptions of their classes. Specifically, given birds\' images with free-text descriptions of their species, we learn to classify images of previously-unseen species based on specie descriptions. This setup has been studied in the vision community under the name zero-shot learning from text, focusing on learning to transfer knowledge about visual aspects of birds from seen classes to previously-unseen ones. Here, we suggest focusing on the textual description and distilling from the description the most relevant information to effectively match visual features to the parts of the text that discuss them. Specifically,(1) we propose to leverage the similarity between species, reflected in the similarity between text descriptions of the species.(2) we derive visual summaries of the texts, ie, extractive summaries that focus on the visual features that tend to be reflected in images. We propose a simple attention-based model augmented with the similarity and visual summaries components. Our empirical results consistently and significantly outperform the state-of-the-art on the largest benchmarks for text-based zero-shot learning, illustrating the critical importance of texts for zero-shot image-recognition."},{"id":"0a80c11eac766e0300fbfb6452e4fb29.html","title":"QADiscourse--Discourse Relations as QA Pairs: Representation, Crowdsourcing and Baselines","url":"https://arxiv.org/abs/2010.02815","authors":["Valentina Pyatkin","Ayal Klein","Reut Tsarfaty","Ido Dagan"],"date":"2020/10/06","journal":"arXiv preprint arXiv:2010.02815","abstract":"Discourse relations describe how two propositions relate to one another, and identifying them automatically is an integral part of natural language understanding. However, annotating discourse relations typically requires expert annotators. Recently, different semantic aspects of a sentence have been represented and crowd-sourced via question-and-answer (QA) pairs. This paper proposes a novel representation of discourse relations as QA pairs, which in turn allows us to crowd-source wide-coverage data annotated with discourse relations, via an intuitively appealing interface for composing such questions and answers. Based on our proposed representation, we collect a novel and wide-coverage QADiscourse dataset, and present baseline algorithms for predicting QADiscourse relations."},{"id":"647b497b8b48502dd206e44bbf6df621.html","title":"A Novel Challenge Set for Hebrew Morphological Disambiguation and Diacritics Restoration","url":"https://arxiv.org/abs/2010.02864","authors":["Avi Shmidman","Joshua Guedalia","Shaltiel Shmidman","Moshe Koppel","Reut Tsarfaty"],"date":"2020/10/06","journal":"arXiv preprint arXiv:2010.02864","abstract":"One of the primary tasks of morphological parsers is the disambiguation of homographs. Particularly difficult are cases of unbalanced ambiguity, where one of the possible analyses is far more frequent than the others. In such cases, there may not exist sufficient examples of the minority analyses in order to properly evaluate performance, nor to train effective classifiers. In this paper we address the issue of unbalanced morphological ambiguities in Hebrew. We offer a challenge set for Hebrew homographs--the first of its kind--containing substantial attestation of each analysis of 21 Hebrew homographs. We show that the current SOTA of Hebrew disambiguation performs poorly on cases of unbalanced ambiguity. Leveraging our new dataset, we achieve a new state-of-the-art for all 21 words, improving the overall average F1 score from 0.67 to 0.95. Our resulting annotated datasets are made publicly available for further research."},{"id":"b548ab2f9114e739be9915864000faef.html","title":"CoRefi: A Crowd Sourcing Suite for Coreference Annotation","url":"https://arxiv.org/abs/2010.02588","authors":["Aaron Bornstein","Arie Cattan","Ido Dagan"],"date":"2020/10/06","journal":"arXiv preprint arXiv:2010.02588","abstract":"Coreference annotation is an important, yet expensive and time consuming, task, which often involved expert annotators trained on complex decision guidelines. To enable cheaper and more efficient annotation, we present CoRefi, a web-based coreference annotation suite, oriented for crowdsourcing. Beyond the core coreference annotation tool, CoRefi provides guided onboarding for the task as well as a novel algorithm for a reviewing phase. CoRefi is open source and directly embeds into any website, including popular crowdsourcing platforms."},{"id":"df9b9193f90805ff84cf6810a2309028.html","title":"Streamlining Cross-Document Coreference Resolution: Evaluation and Modeling","url":"https://arxiv.org/abs/2009.11032","authors":["Arie Cattan","Alon Eirew","Gabriel Stanovsky","Mandar Joshi","Ido Dagan"],"date":"2020/09/23","journal":"arXiv preprint arXiv:2009.11032","abstract":"Recent evaluation protocols for Cross-document (CD) coreference resolution have often been inconsistent or lenient, leading to incomparable results across works and overestimation of performance. To facilitate proper future research on this task, our primary contribution is proposing a pragmatic evaluation methodology which assumes access to only raw text--rather than assuming gold mentions, disregards singleton prediction, and addresses typical targeted settings in CD coreference resolution. Aiming to set baseline results for future research that would follow our evaluation methodology, we build the first end-to-end model for this task. Our model adapts and extends recent neural models for within-document coreference resolution to address the CD coreference setting, which outperforms state-of-the-art results by a significant margin."},{"id":"95ba82370273c07dd23f85b17086bfcf.html","title":"Evaluating Interactive Summarization: an Expansion-Based Framework","url":"https://arxiv.org/abs/2009.08380","authors":["Ori Shapira","Ramakanth Pasunuru","Hadar Ronen","Mohit Bansal","Yael Amsterdamer","Ido Dagan"],"date":"2020/09/17","journal":"arXiv preprint arXiv:2009.08380","abstract":"Allowing users to interact with multi-document summarizers is a promising direction towards improving and customizing summary results. Different ideas for interactive summarization have been proposed in previous work but these solutions are highly divergent and incomparable. In this paper, we develop an end-to-end evaluation framework for expansion-based interactive summarization, which considers the accumulating information along an interactive session. Our framework includes a procedure of collecting real user sessions and evaluation measures relying on standards, but adapted to reflect interaction. All of our solutions are intended to be released publicly as a benchmark, allowing comparison of future developments in interactive summarization. We demonstrate the use of our framework by evaluating and comparing baseline implementations that we developed for this purpose, which will serve as part of our benchmark. Our extensive experimentation and analysis of these systems motivate our design choices and support the viability of our framework."},{"id":"5ecec18d709a671785d1bd5d60c3e90a.html","title":"Summary-Source Proposition-level Alignment: Task, Datasets and Supervised Baseline","url":"https://arxiv.org/abs/2009.00590","authors":["Ori Ernst","Ori Shapira","Ramakanth Pasunuru","Michael Lepioshkin","Jacob Goldberger","Mohit Bansal","Ido Dagan"],"date":"2020/09/01","journal":"arXiv preprint arXiv:2009.00590","abstract":""},{"id":"bf9d3edd679f721762eedb5c287ddd27.html","title":"SuperPAL: Supervised Proposition ALignment for Multi-Document Summarization and Derivative Sub-Tasks","url":"https://arxiv.org/abs/2009.00590","authors":["Ori Ernst","Ori Shapira","Ramakanth Pasunuru","Michael Lepioshkin","Jacob Goldberger","Mohit Bansal","Ido Dagan"],"date":"2020/09/01","journal":"arXiv preprint arXiv:2009.00590","abstract":"Multi-document summarization (MDS) is a challenging task, often decomposed to subtasks of salience and redundancy detection, followed by generation. While alignment of spans between reference summaries and source documents has been leveraged for training component tasks, the underlying alignment step was never independently addressed or evaluated. We advocate developing high quality source-reference alignment algorithms, that can be applied to recent large-scale datasets to obtain useful\\" silver\\", ie approximate, training data. As a first step, we present an annotation methodology by which we create gold standard development and test sets for summary-source alignment, and suggest its utility for tuning and evaluating effective alignment algorithms, as well as for properly evaluating MDS subtasks. Second, we introduce a new large-scale alignment dataset for training, with which an automatic alignment model was trained. This aligner achieves higher coherency with the reference summary than previous aligners used for summarization, and gets significantly higher ROUGE results when replacing a simpler aligner in a competitive summarization model. Finally, we release three additional datasets (for salience, clustering and generation), naturally derived from our alignment datasets. Furthermore, these datasets can be derived from any summarization dataset automatically after extracting alignments with our trained aligner. Hence, they can be utilized for training summarization sub-tasks."},{"id":"cf94cb07d6e817ef9d291fab3a6dee9f.html","title":"Real-Time Sign Language Detection using Human Pose Estimation","url":"https://arxiv.org/abs/2008.04637","authors":["Amit Moryossef","Ioannis Tsochantaridis","Roee Aharoni","Sarah Ebling","Srini Narayanan"],"date":"2020/08/11","journal":"arXiv preprint arXiv:2008.04637","abstract":"We propose a lightweight real-time sign language detection model, as we identify the need for such a case in videoconferencing. We extract optical flow features based on human pose estimation and, using a linear classifier, show these features are meaningful with an accuracy of 80%, evaluated on the DGS Corpus. Using a recurrent model directly on the input, we see improvements of up to 91% accuracy, while still working under 4ms. We describe a demo application to sign language detection in the browser in order to demonstrate its usage possibility in videoconferencing applications."},{"id":"8c72dfb64ae830082220fcf2685db851.html","title":"Neural Modeling for Named Entities and Morphology (NEMO^ 2)","url":"https://arxiv.org/abs/2007.15620","authors":["Dan Bareket","Reut Tsarfaty"],"date":"2020/07/30","journal":"arXiv preprint arXiv:2007.15620","abstract":"Named Entity Recognition (NER) is a fundamental NLP task, commonly formulated as classification over a sequence of tokens. Morphologically-Rich Languages (MRLs) pose a challenge to this basic formulation, as the boundaries of Named Entities do not coincide with token boundaries, rather, they respect morphological boundaries. To address NER in MRLs we then need to answer two fundamental modeling questions:(i) What should be the basic units to be identified and labeled, are they token-based or morpheme-based? and (ii) How can morphological units be encoded and accurately obtained in realistic (non-gold) scenarios? We empirically investigate these questions on a novel parallel NER benchmark we deliver, with parallel token-level and morpheme-level NER annotations for Modern Hebrew, a morphologically complex language. Our results show that explicitly modeling morphological boundaries consistently leads to improved NER performance, and that a novel hybrid architecture that we propose, in which NER precedes and prunes the morphological decomposition (MD) space, greatly outperforms the standard pipeline approach, on both Hebrew NER and Hebrew MD in realistic scenarios."},{"id":"2b63509b1e903de9fdcaa95e8717c75f.html","title":"Controlled crowdsourcing for high-quality qa-srl annotation","url":"https://www.aclweb.org/anthology/2020.acl-main.626/","authors":["Paul Roit","Ayal Klein","Daniela Stepanov","Jonathan Mamou","Julian Michael","Gabriel Stanovsky","Luke Zettlemoyer","Ido Dagan"],"date":"2020/07","abstract":"Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations."},{"id":"56a17c934c9913ea211b87c4554c713f.html","title":"Simple, interpretable and stable method for detecting words with usage change across corpora","url":"https://www.aclweb.org/anthology/2020.acl-main.51/","authors":["Hila Gonen","Ganesh Jawahar","Djam\\\\xe9 Seddah","Yoav Goldberg"],"date":"2020/07","abstract":"The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science. This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large. However, these methods often require extensive filtering of the vocabulary to perform well, and-as we show in this work-result in unstable, and hence less reliable, results. We propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word. The method is simple, interpretable and stable. We demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew)."},{"id":"d50379d5a343086a976fe55113b3e813.html","title":"Getting the## life out of living: How Adequate Are Word-Pieces for Modelling Complex Morphology?","url":"https://www.aclweb.org/anthology/2020.sigmorphon-1.24/","authors":["Stav Klein","Reut Tsarfaty"],"date":"2020/07","abstract":"This work investigates the most basic units that underlie contextualized word embeddings, such as BERT\u2014the so-called word pieces. In Morphologically-Rich Languages (MRLs) which exhibit morphological fusion and non-concatenative morphology, the different units of meaning within a word may be fused, intertwined, and cannot be separated linearly. Therefore, when using word-pieces in MRLs, we must consider that:(1) a linear segmentation into sub-word units might not capture the full morphological complexity of words; and (2) representations that leave morphological knowledge on sub-word units inaccessible might negatively affect performance. Here we empirically examine the capacity of word-pieces to capture morphology by investigating the task of multi-tagging in Modern Hebrew, as a proxy to evaluate the underlying segmentation. Our results show that, while models trained to predict multi-tags for complete words outperform models tuned to predict the distinct tags of WPs, we can improve the WPs tag prediction by purposefully constraining the word-pieces to reflect their internal functions. We suggest that linguistically-informed word-pieces schemes, that make the morphological structure explicit, might boost performance for MRLs."},{"id":"dc88c54aeab604100e7559b33f271c40.html","title":"Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies","url":"https://www.aclweb.org/anthology/2020.iwpt-1.0.pdf","authors":["Gosse Bouma","Yuji Matsumoto","Stephan Oepen","Kenji Sagae","Djam\\\\xe9 Seddah","Weiwei Sun","Anders S\\\\xf8gaard","Reut Tsarfaty","Daniel Zeman"],"date":"2020/07","abstract":"Welcome to the 16th International Conference on Parsing Technologies (IWPT 2020), which this year (for the first time since 2007) is co-located with the Annual Meeting of the Association for Computational Linguistics (ACL). The IWPT meeting series, hosted by the ACL Special Interest Group in Natural Language Parsing (SIGPARSE), has been held biennualy since its inaugual meeting in 1989 in Pittsburgh, PA (USA)."},{"id":"924a8e57c665a50f0077592663e2c351.html","title":"Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge","url":"https://proceedings.neurips.cc/paper/2020/hash/e992111e4ab9985366e806733383bd8c-Abstract.html","authors":["Alon Talmor","Oyvind Tafjord","Peter Clark","Yoav Goldberg","Jonathan Berant"],"date":"2020/06/11","journal":"arXiv preprint arXiv:2006.06609","abstract":"To what extent can a neural network systematically reason over symbolic facts? Evidence suggests that large pre-trained language models (LMs) acquire some reasoning capacity, but this ability is difficult to control. Recently, it has been shown that Transformer-based models succeed in consistent reasoning over explicit symbolic facts, under a\\" closed-world\\" assumption. However, in an open-domain setup, it is desirable to tap into the vast reservoir of implicit knowledge already encoded in the parameters of pre-trained LMs. In this work, we provide a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements."},{"id":"f4c5b35dd26192e486d3f9564f208dca.html","title":"Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge","url":"https://ask.qcloudimg.com/draft/7305468/icu1dcp3g6.pdf","authors":["Alon Talmor","Oyvind Tafjord","Peter Clark","Yoav Goldberg","Jonathan Berant"],"date":"2020/06/11","journal":"arXiv preprint arXiv:2006.06609","abstract":"To what extent can a neural network systematically reason over symbolic facts? Evidence suggests that large pre-trained language models (LMs) acquire some reasoning capacity, but this ability is difficult to control. Recently, it has been shown that Transformer-based models succeed in consistent reasoning over explicit symbolic facts, under a \\\\xe2\\\\x80\\\\x9cclosed-world\\" assumption. However, in an open-domain setup, it is desirable to tap into the vast reservoir of implicit knowledge already encoded in the parameters of pre-trained LMs. In this work, we provide a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements. To do this, we describe a procedure for automatically generating datasets that teach a model new reasoning skills, and demonstrate that models learn to effectively perform inference which involves implicit taxonomic and world knowledge, chaining and counting. Finally, we show that \\\\xe2\\\\x80\\\\x9cteaching\\\\xe2\\\\x80\\\\x9d the models to reason generalizes beyond the training distribution: they successfully compose the usage of multiple reasoning skills in single examples. Our work paves a path towards open-domain systems that constantly improve by interacting with users who can instantly correct a model by adding simple natural language statements."},{"id":"45cf634f3e54e44da76f934ce4e997a9.html","title":"Interactive Extractive Search over Biomedical Corpora","url":"https://arxiv.org/abs/2006.04148","authors":["Hillel Taub-Tabib","Micah Shlain","Shoval Sadde","Dan Lahav","Matan Eyal","Yaara Cohen","Yoav Goldberg"],"date":"2020/06/07","journal":"arXiv preprint arXiv:2006.04148","abstract":""},{"id":"aa9e042eb5957bd21a033323b9e18f32.html","title":"Aligning Faithful Interpretations with their Social Attribution","url":"https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00367/98620","authors":["Alon Jacovi","Yoav Goldberg"],"date":"2020/06/01","journal":"arXiv preprint arXiv:2006.01067","abstract":"We find that the requirement of model interpretations to be faithful is vague and incomplete. With interpretation by textual highlights as a case study, we present several failure cases. Borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and the attribution of human behavior to the interpretation (social attribution). We reformulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of aligned faithfulness: faithful causal chains that are aligned with their expected social behavior. The two steps of causal attribution and social attribution together complete the process of explaining behavior. With this formalization, we characterize various failures of misaligned faithful highlight interpretations, and propose an alternative causal chain to remedy the issues. Finally, we implement highlight\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"aad146d057684ab1c6bdf12c25e9cb2d.html","title":"When Bert Forgets How To POS: Amnesic Probing of Linguistic Properties and MLM Predictions","url":"https://openreview.net/forum?id=28q3WsfS1M","authors":["Yanai Elazar","Shauli Ravfogel","Alon Jacovi","Yoav Goldberg"],"date":"2020/06/01","journal":"arXiv preprint arXiv:2006.00995","abstract":"A growing body of work makes use of probing to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results and offer an alternative method that focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention that removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, eg is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing\xa0\\\\xe2\\\\x80\\\\xa6"},{"id":"054e0d1487e040372d182330fcfa888a.html","title":"Nakdan: Professional Hebrew Diacritizer","url":"https://arxiv.org/abs/2005.03312","authors":["Avi Shmidman","Shaltiel Shmidman","Moshe Koppel","Yoav Goldberg"],"date":"2020/05/07","journal":"arXiv preprint arXiv:2005.03312","abstract":"We present a system for automatic diacritization of Hebrew text. The system combines modern neural models with carefully curated declarative linguistic knowledge and comprehensive manually constructed tables and dictionaries. Besides providing state of the art diacritization accuracy, the system also supports an interface for manual editing and correction of the automatic output, and has several features which make it particularly useful for preparation of scientific editions of Hebrew texts. The system supports Modern Hebrew, Rabbinic Hebrew and Poetic Hebrew. The system is freely accessible for all use at this http URL."},{"id":"16bc27f00280565f5a64660d8445cfc4.html","title":"pyBART: Evidence-based Syntactic Transformations for IE","url":"https://arxiv.org/abs/2005.01306","authors":["Aryeh Tiktinsky","Yoav Goldberg","Reut Tsarfaty"],"date":"2020/05/04","journal":"arXiv preprint arXiv:2005.01306","abstract":"Syntactic dependencies can be predicted with high accuracy, and are useful for both machine-learned and pattern-based information extraction tasks. However, their utility can be improved. These syntactic dependencies are designed to accurately reflect syntactic relations, and they do not make semantic relations explicit. Therefore, these representations lack many explicit connections between content words, that would be useful for downstream applications. Proposals like English Enhanced UD improve the situation by extending universal dependency trees with additional explicit arcs. However, they are not available to Python users, and are also limited in coverage. We introduce a broad-coverage, data-driven and linguistically sound set of transformations, that makes event-structure and many lexical relations explicit. We present pyBART, an easy-to-use open-source Python library for converting English UD trees either to Enhanced UD graphs or to our representation. The library can work as a standalone package or be integrated within a spaCy NLP pipeline. When evaluated in a pattern-based relation extraction scenario, our representation results in higher extraction scores than Enhanced UD, while requiring fewer patterns."},{"id":"8e99762e3f8772b3890597c1905240fa.html","title":"From SPMRL to NMRL: What Did We Learn (and Unlearn) in a Decade of Parsing Morphologically-Rich Languages (MRLs)?","url":"https://arxiv.org/abs/2005.01330","authors":["Reut Tsarfaty","Dan Bareket","Stav Klein","Amit Seker"],"date":"2020/05/04","journal":"arXiv preprint arXiv:2005.01330","abstract":"It has been exactly a decade since the first establishment of SPMRL, a research initiative unifying multiple research efforts to address the peculiar challenges of Statistical Parsing for Morphologically-Rich Languages (MRLs). Here we reflect on parsing MRLs in that decade, highlight the solutions and lessons learned for the architectural, modeling and lexical challenges in the pre-neural era, and argue that similar challenges re-emerge in neural architectures for MRLs. We then aim to offer a climax, suggesting that incorporating symbolic ideas proposed in SPMRL terms into nowadays neural architectures has the potential to push NLP for MRLs to a new level. We sketch strategies for designing Neural Models for MRLs (NMRL), and showcase preliminary support for these strategies via investigating the task of multi-tagging in Hebrew, a morphologically-rich, high-fusion, language"},{"id":"42a8fbdc06fe2db6b1152a37e72a2b42.html","title":"A Two-Stage Masked LM Method for Term Set Expansion","url":"https://arxiv.org/abs/2005.01063","authors":["Guy Kushilevitz","Shaul Markovitch","Yoav Goldberg"],"date":"2020/05/03","journal":"arXiv preprint arXiv:2005.01063","abstract":"We tackle the task of Term Set Expansion (TSE): given a small seed set of example terms from a semantic class, finding more members of that class. The task is of great practical utility, and also of theoretical utility as it requires generalization from few examples. Previous approaches to the TSE task can be characterized as either distributional or pattern-based. We harness the power of neural masked language models (MLM) and propose a novel TSE algorithm, which combines the pattern-based and distributional approaches. Due to the small size of the seed set, fine-tuning methods are not effective, calling for more creative use of the MLM. The gist of the idea is to use the MLM to first mine for informative patterns with respect to the seed set, and then to obtain more members of the seed class by generalizing these patterns. Our method outperforms state-of-the-art TSE algorithms. Implementation is available at: this https URL guykush/TermSetExpansion-MPB/"},{"id":"76a64892a4005093844b85277551debe.html","title":"Using Computerized Text Analysis to Examine Associations Between Linguistic Features and Clients\u2019 Distress during Psychotherapy","url":"https://www.researchgate.net/profile/Natalie_Shapira/publication/340595443_Running_head_ASSOCIATIONS_BETWEEN_LINGUISTIC_FEATURES_AND_CLIENTS\'_DISTRESS_Using_Computerized_Text_Analysis_to_Examine_Associations_Between_Linguistic_Features_and_Clients\'_Distress_during_Psychother/links/5e9391ad299bf13079945b1b/Running-head-ASSOCIATIONS-BETWEEN-LINGUISTIC-FEATURES-AND-CLIENTS-DISTRESS-Using-Computerized-Text-Analysis-to-Examine-Associations-Between-Linguistic-Features-and-Clients-Distress-during-Psychotherap.pdf","authors":["Natalie Shapira","Gal Lazarus","Yoav Goldberg","Eva Gilboa-Schechtman","Rivka Tuval-Mashiach","Daniel Juravski","Dana Atzil-Slonim"],"date":"2020/04/30","journal":"Journal of counseling psychology","abstract":"Raw linguistic data within psychotherapy sessions may provide important information about clients\u2019 progress and well-being. In the current study, computerized text analytic techniques were applied to examine whether linguistic features were associated with clients\u2019 experiences of distress within and between clients and whether changes in linguistic features were associated with changes in treatment outcome. Method: Transcripts of 729 psychotherapy sessions from 58 clients treated by 52 therapists were analyzed. Prior to each session, clients reported their distress level. Linguistic features were extracted automatically by employing natural language parser for first-person singular identification and using positive and negative emotion words lexicon. The association between linguistic features and levels of distress was examined using multilevel models. Results: At the within-client level, fewer first-person singular words, fewer negative emotional words and more positive emotional words were associated with lower distress in the same session; and fewer negative emotion words were associated with lower next session distress (rather small f2 effect sizes\u20130.011< f2< 0.022). At the between-client level, only first session use of positive emotion words was associated with first session distress (medium \u03b7 \u{1d45d}"},{"id":"e0508c6ecb0e6ba4737c1945ea39b6eb.html","title":"Paraphrasing vs Coreferring: Two Sides of the Same Coin","url":"https://arxiv.org/abs/2004.14979","authors":["Yehudit Meged","Avi Caciularu","Vered Shwartz","Ido Dagan"],"date":"2020/04/30","journal":"arXiv preprint arXiv:2004.14979","abstract":"We study the potential synergy between two different NLP tasks, both confronting lexical variability: identifying predicate paraphrases and event coreference resolution. First, we used annotations from an event coreference dataset as distant supervision to re-score heuristically-extracted predicate paraphrases. The new scoring gained more than 18 points in average precision upon their ranking by the original scoring method. Then, we used the same re-ranking features as additional inputs to a state-of-the-art event coreference resolution model, which yielded modest but consistent improvements to the model\'s performance. The results suggest a promising direction to leverage data and models for each of the tasks to the benefit of the other."},{"id":"50d0507519953ad00a0e1b09f6704372.html","title":"Automatically identifying gender issues in machine translation using perturbations","url":"https://arxiv.org/abs/2004.14065","authors":["Hila Gonen","Kellie Webster"],"date":"2020/04/29","journal":"arXiv preprint arXiv:2004.14065","abstract":"The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. Where previous studies have identified concerns using manually-curated synthetic examples, we develop a novel technique to leverage real world data to explore challenges for deployed systems. We use our new method to compile an evaluation benchmark spanning examples relating to four languages from three language families, which we will publicly release to facilitate research. The examples in our benchmark expose the ways in which gender is represented in a model and the unintended consequences these gendered representations can have in downstream applications."},{"id":"2c98f7f8aa544de4350e63137bd25daa.html","title":"A Formal Hierarchy of RNN Architectures","url":"https://arxiv.org/abs/2004.08500","authors":["William Merrill","Gail Weiss","Yoav Goldberg","Roy Schwartz","Noah A Smith","Eran Yahav"],"date":"2020/04/18","journal":"arXiv preprint arXiv:2004.08500","abstract":"We develop a formal hierarchy of the expressive capacity of RNN architectures. The hierarchy is based on two formal properties: space complexity, which measures the RNN\'s memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine. We place several RNN variants within this hierarchy. For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016). We also show how these models\' expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions. Our results build on the theory of\\" saturated\\" RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy. Experimental findings from training unsaturated networks on formal languages support this conjecture."},{"id":"fadef3f241d93ffa3f43a829ed6e41dd.html","title":"Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection","url":"https://arxiv.org/abs/2004.07667","authors":["Shauli Ravfogel","Yanai Elazar","Hila Gonen","Michael Twiton","Yoav Goldberg"],"date":"2020/04/16","journal":"arXiv preprint arXiv:2004.07667","abstract":"The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for general scenarios, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification."},{"id":"4e77a783843bb5b3bc66278713ebf353.html","title":"Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?","url":"https://arxiv.org/abs/2004.03685","authors":["Alon Jacovi","Yoav Goldberg"],"date":"2020/04/07","journal":"arXiv preprint arXiv:2004.03685","abstract":"With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is\\" defined\\" by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility."},{"id":"0bb9e8b4b855e9f78cee11d8cde41f16.html","title":"Evaluating nlp models via contrast sets","url":"https://arxiv.org/abs/2004.02709","authors":["Matt Gardner","Yoav Artzi","Victoria Basmova","Jonathan Berant","Ben Bogin","Sihao Chen","Pradeep Dasigi","Dheeru Dua","Yanai Elazar","Ananth Gottumukkala","Nitish Gupta","Hanna Hajishirzi","Gabriel Ilharco","Daniel Khashabi","Kevin Lin","Jiangming Liu","Nelson F Liu","Phoebe Mulcaire","Qiang Ning","Sameer Singh","Noah A Smith","Sanjay Subramanian","Reut Tsarfaty","Eric Wallace","Ally Zhang","Ben Zhou"],"date":"2020/04/06","journal":"arXiv preprint arXiv:2004.02709","abstract":"Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (eg, annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture a dataset\'s intended capabilities. We propose a new annotation paradigm for NLP that helps to close systematic gaps in the test data. In particular, after a dataset is constructed, we recommend that the dataset authors manually perturb the test instances in small but meaningful ways that (typically) change the gold label, creating contrast sets. Contrast sets provide a local view of a model\'s decision boundary, which can be used to more accurately evaluate a model\'s true linguistic capabilities. We demonstrate the efficacy of contrast sets by creating them for 10 diverse NLP datasets (eg, DROP reading comprehension, UD parsing, IMDb sentiment analysis). Although our contrast sets are not explicitly adversarial, model performance is significantly lower on them than on the original test sets---up to 25\\\\\\\\% in some cases. We release our contrast sets as new evaluation benchmarks and encourage future dataset construction efforts to follow similar annotation processes."},{"id":"58d850863bebd8065e4845e69d73dde0.html","title":"Unsupervised Domain Clusters in Pretrained Language Models","url":"https://arxiv.org/abs/2004.02105","authors":["Roee Aharoni","Yoav Goldberg"],"date":"2020/04/05","journal":"arXiv preprint arXiv:2004.02105","abstract":"The notion of\\" in-domain data\\" in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision--suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and by precision and recall of sentence selection with respect to an oracle."},{"id":"da0f74b9a22cffb207103c24ea3e52c4.html","title":"Ecological semantics: Programming environments for situated language understanding","url":"https://arxiv.org/abs/2003.04567","authors":["Ronen Tamari","Gabriel Stanovsky","Dafna Shahaf","Reut Tsarfaty"],"date":"2020/03/10","journal":"arXiv preprint arXiv:2003.04567","abstract":"Large-scale natural language understanding (NLU) systems have made impressive progress: they can be applied flexibly across a variety of tasks, and employ minimal structural assumptions. However, extensive empirical research has shown this to be a double-edged sword, coming at the cost of shallow understanding: inferior generalization, grounding and explainability. Grounded language learning approaches offer the promise of deeper understanding by situating learning in richer, more structured training environments, but are limited in scale to relatively narrow, predefined domains. How might we enjoy the best of both worlds: grounded, general NLU? Following extensive contemporary cognitive science, we propose treating environments as``first-class citizens\'\'in semantic representations, worthy of research and development in their own right. Importantly, models should also be partners in the creation and configuration of environments, rather than just actors within them, as in existing approaches. To do so, we argue that models must begin to understand and program in the language of affordances (which define possible actions in a given situation) both for online, situated discourse comprehension, as well as large-scale, offline common-sense knowledge mining. To this end we propose an environment-oriented ecological semantics, outlining theoretical and practical approaches towards implementation. We further provide actual demonstrations building upon interactive fiction programming languages."},{"id":"04b91de6664f1f296c8f05fa8f302f8d.html","title":"Break It Down: A Question Understanding Benchmark","url":"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00309","authors":["Tomer Wolfson","Mor Geva","Ankit Gupta","Matt Gardner","Yoav Goldberg","Daniel Deutch","Jonathan Berant"],"date":"2020/01/31","journal":"arXiv preprint arXiv:2001.11770","abstract":"Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality QDMRs can be annotated at scale, and release the B<span class=\\"gs_fscp\\">reak</span> dataset, containing over 83K pairs of questions and their QDMRs. We demonstrate the utility of QDMR by showing that (a) it can be used to improve open-domain question answering on the H<span class=\\"gs_fscp\\">otpot</span>QA dataset, (b) it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use B<span class=\\"gs_fscp\\">reak</span> to train a sequence-to-sequence model with copying that parses questions\\\\xa0\u2026"},{"id":"43eee1744919b42a55fbe144aced91e1.html","title":"Football Coreference Corpus","url":"https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2305","authors":["Michael Bugert","Nils Reimers","Shany Barhom","Ido Dagan","Iryna Gurevych"],"date":"2020","abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Scholar articles"},{"id":"8481db4744fb1cd07fab447138ae6500.html","title":"Breaking the Subtopic Barrier in Cross-Document Event Coreference Resolution.","url":"https://public.ukp.informatik.tu-darmstadt.de/UKP_Webpage/publications/2020/2020_Text2Story_MB_cross-doc-event-coref-corpus.pdf","authors":["Michael Bugert","Nils Reimers","Shany Barhom","Ido Dagan","Iryna Gurevych"],"date":"2020","abstract":"Cross-document event coreference resolution (CDCR) is the task of detecting and clustering mentions of events across a set of documents. A major bottleneck in CDCR is a lack of appropriate datasets, which stems from the difficulty of annotating data for this task. We present the first scalable approach for annotating cross-subtopic event coreference links, a highly valuable but rarely occurring type of cross-document link. The annotation of these links requires combing through hundreds of documents\u2013an endeavor for which conventional token-level annotation schemes with trained expert annotators are too expensive. We instead propose crowdsourcing annotation on sentence level to achieve scalability. We apply our approach to create the Football Coreference Corpus (FCC), a corpus of 451 sports news reports, while reaching high agreement between NLP experts and crowd annotators in the process. 1"},{"id":"c3c1bbc6432c3376ce1b6fc142dccf39.html","title":"Automatically Identifying Gender Bias in Machine Translation using Perturbations","url":"https://research.google/pubs/pub50758/","authors":["Kellie Webster","Hila Gonen"],"date":"2020","abstract":"Gender bias has been shown to affect many tasks applications in NLU. In the setting of machine translation (MT), research has primarily focused on measuring bias via synthetic datasets. We present an automatic method for identifying gender biases in MT using a novel-application of BERT-generated sentence perturbations. Using this method, we compile a dataset to serve as a benchmark for evaluating gender bias in MT across a diverse range of languages. Our dataset further serves to highlight the limitations of the current task definition which requires a single translation be produced, even in the presence of underspecified input."},{"id":"d828d94f31730df2d1897a07d61ad383.html","title":"oLMpics--On what Language Model Pre-training Captures","url":"https://arxiv.org/abs/1912.13283","authors":["Alon Talmor","Yanai Elazar","Yoav Goldberg","Jonathan Berant"],"date":"2019/12/31","journal":"arXiv preprint arXiv:1912.13283","abstract":"Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that:(a) different LMs exhibit qualitatively different reasoning abilities, eg, RoBERTa succeeds in reasoning tasks where BERT fails completely;(b) LMs do not reason in an abstract manner and are context-dependent, eg, while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages;(c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training."},{"id":"4da6c6af35892e52ef86367f268c3152.html","title":"A \u2018wind of change\u2019\u2014shaping public opinion of the Arab Spring using metaphors","url":"https://academic.oup.com/dsh/article-abstract/34/Supplement_1/i142/5151186","authors":["Alexandra N\\\\xfa\\\\xf1ez","Malte Gerloff","Erik-L\\\\xe2n Do Dinh","Andrea Rapp","Petra Gehring","Iryna Gurevych"],"date":"2019/12/01","journal":"Digital Scholarship in the Humanities","abstract":"Newspapers create publicity, draw attention to topics, and try to gain thematic acceptance from the reader. To achieve this, they use linguistic strategies and select culturally and historically evolved encyclopedic knowledge sources. In our pilot study we explore the presentation of the events in the Middle East\u2013North African region between December 2010 and November 2011 that were soon metaphorically framed as the Arab Spring. To this end, we use a text corpus consisting of 300 opinion pieces from five national German newspapers. To get access to the conceptual knowledge structure and the linguistic strategies, we combine text mining methods and cognitive linguistics. We focus on conceptual metaphors (Lakoff and Johnson, ) and their binary source\u2013target structure, where the source domain reveals the underlying conceptual knowledge structures of the speaker. This research focus is justified by the\\\\xa0\u2026"},{"id":"6a045d0c1a0eae7c992683b5a08f3264.html","title":"Crowdsourcing a High-Quality Gold Standard for QA-SRL","url":"https://arxiv.org/abs/1911.03243","authors":["Paul Roit","Ayal Klein","Daniela Stepanov","Jonathan Mamou","Julian Michael","Gabriel Stanovsky","Luke Zettlemoyer","Ido Dagan"],"date":"2019/11/08","journal":"arXiv preprint arXiv:1911.03243","abstract":"Question-answer driven Semantic Role Labeling (QA-SRL) has been proposed as an attractive open and natural form of SRL, easily crowdsourceable for new corpora. Recently, a large-scale QA-SRL corpus and a trained parser were released, accompanied by a densely annotated dataset for evaluation. Trying to replicate the QA-SRL annotation and evaluation scheme for new texts, we observed that the resulting annotations were lacking in quality and coverage, particularly insufficient for creating gold standards for evaluation. In this paper, we present an improved QA-SRL annotation protocol, involving crowd-worker selection and training, followed by data consolidation. Applying this process, we release a new gold evaluation dataset for QA-SRL, yielding more consistent annotations and greater coverage. We believe that our new annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations."},{"id":"784e78a0d59c9e671d19aec600f2ee31.html","title":"Adversarial removal of demographic attributes revisited","url":"https://www.aclweb.org/anthology/D19-1662/","authors":["Maria Barrett","Yova Kementchedjhieva","Yanai Elazar","Desmond Elliott","Anders S\\\\xf8gaard"],"date":"2019/11","abstract":"Elazar and Goldberg (2018) showed that protected attributes can be extracted from the representations of a debiased neural network for mention detection at above-chance levels, by evaluating a diagnostic classifier on a held-out subsample of the data it was trained on. We revisit their experiments and conduct a series of follow-up experiments showing that, in fact, the diagnostic classifier generalizes poorly to both new in-domain samples and new domains, indicating that it relies on correlations specific to their particular data sample. We further show that a diagnostic classifier trained on the biased baseline neural network also does not generalize to new samples. In other words, the biases detected in Elazar and Goldberg (2018) seem restricted to their particular data sample, and would therefore not bias the decisions of the model on new samples, whether in-domain or out-of-domain. In light of this, we discuss better methodologies for detecting bias in our models."},{"id":"263037f69d75071945e89d9b52f82b10.html","title":"How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?","url":"https://arxiv.org/abs/1910.14161","authors":["Hila Gonen","Yova Kementchedjhieva","Yoav Goldberg"],"date":"2019/10/30","journal":"arXiv preprint arXiv:1910.14161","abstract":"Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun\'s gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While\\" embedding debiasing\\" methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words\' context when training word embeddings is effective in removing it. Fixing the grammatical gender bias yields a positive effect on the quality of the resulting word embeddings, both in monolingual and cross-lingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results."},{"id":"fb5f4fa95da6b8f18626f152b1fc4f9c.html","title":"Scalable Evaluation and Improvement of Document Set Expansion via Neural Positive-Unlabeled Learning","url":"https://arxiv.org/abs/1910.13339","authors":["Alon Jacovi","Gang Niu","Yoav Goldberg","Masashi Sugiyama"],"date":"2019/10/29","journal":"arXiv preprint arXiv:1910.13339","abstract":"We consider the situation in which a user has collected a small set of documents on a cohesive topic, and they want to retrieve additional documents on this topic from a large collection. Information Retrieval (IR) solutions treat the document set as a query, and look for similar documents in the collection. We propose to extend the IR approach by treating the problem as an instance of positive-unlabeled (PU) learning---ie, learning binary classifiers from only positive and unlabeled data, where the positive data corresponds to the query documents, and the unlabeled data is the results returned by the IR engine. Utilizing PU learning for text with big neural networks is a largely unexplored field. We discuss various challenges in applying PU learning to the setting, including an unknown class prior, extremely imbalanced data and large-scale accurate evaluation of models, and we propose solutions and empirically validate them. We demonstrate the effectiveness of the method using a series of experiments of retrieving PubMed abstracts adhering to fine-grained topics. We demonstrate improvements over the base IR solution and other baselines. Implementation is available at this https URL."},{"id":"03f9cb53e42324e6874234c6b69cbe8d.html","title":"Diversify your datasets: Analyzing generalization via controlled variance in adversarial datasets","url":"https://arxiv.org/abs/1910.09302","authors":["Ohad Rozen","Vered Shwartz","Roee Aharoni","Ido Dagan"],"date":"2019/10/21","journal":"arXiv preprint arXiv:1910.09302","abstract":"Phenomenon-specific\\" adversarial\\" datasets have been recently designed to perform targeted stress-tests for particular inference types. Recent work (Liu et al., 2019a) proposed that such datasets can be utilized for training NLI and other types of models, often allowing to learn the phenomenon in focus and improve on the challenge dataset, indicating a\\" blind spot\\" in the original training data. Yet, although a model can improve in such a training process, it might still be vulnerable to other challenge datasets targeting the same phenomenon but drawn from a different distribution, such as having a different syntactic complexity level. In this work, we extend this method to drive conclusions about a model\'s ability to learn and generalize a target phenomenon rather than to\\" learn\\" a dataset, by controlling additional aspects in the adversarial datasets. We demonstrate our approach on two inference phenomena-dative alternation and numerical reasoning, elaborating, and in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements."},{"id":"64c5c06ccc0e49fd24c945545e4c96a2.html","title":"At Your Fingertips: Automatic Piano Fingering Detection","url":"https://openreview.net/forum?id=H1MOqeHYvB","authors":["Amit Moryossef","Yanai Elazar","Yoav Goldberg"],"date":"2019/09/25","abstract":"Automatic Piano Fingering is a hard task which computers can learn using data. As data collection is hard and expensive, we propose to automate this process by automatically extracting fingerings from public videos and MIDI files, using computer-vision techniques. Running this process on 90 videos results in the largest dataset for piano fingering with more than 150K notes. We show that when running a previously proposed model for automatic piano fingering on our dataset and then fine-tuning it on manually labeled piano fingering data, we achieve state-of-the-art results. In addition to the fingering extraction method, we also introduce a novel method for transferring deep-learning computer-vision models to work on out-of-domain data, by fine-tuning it on out-of-domain augmentation proposed by a Generative Adversarial Network (GAN). For demonstration, we anonymously release a visualization of the output of our process for a single video on https://youtu. be/Gfs1UWQhr5Q"},{"id":"a3392624bc8c74db00cd94100720ff31.html","title":"A Simple Geometric Proof for the Benefit of Depth in ReLU Networks","url":"https://arxiv.org/abs/2101.07126","authors":["Asaf Amrami","Yoav Goldberg"],"date":"2019/09/25","abstract":"<g transform=\\"matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 6.18800)\\"><path transform=\\"scale(0.48828, -0.48828)\\" d=\\"M 158 35 Q 158 47 160 53 L 313 664 Q 328 721 328 764 Q 328 852 268 852 Q 204 852 173 775 T 113 582 Q 113 576 107 572 T 96 569 H 72 Q 65 569 60 576 T 55 590 Q 77 679 97 741 T 161 854 T 270 905 Q 347 905 406 856 T 465 733 Q 526 813 608 859 T 782 905 Q 879 905 949 855 T 1020 715 Q 1083 804 1167 854 T 1352 905 Q 1458 905 1522 847 T 1587 684 Q 1587 600 1549 481 T 1456 215 Q 1427 144 1427 92 Q 1427 31 1475 31 Q 1555 31 1608 117 T 1683 301 Q 1689 313 1700 313 H 1724 Q 1732 313 1737 307 T 1743 295 Q 1743 293 1741 289 Q 1713 173 1643 75 T 1470 -23 Q 1398 -23 1347 26 T 1296 147 Q 1296 183 1313 227 Q 1371 381 1409 502 T 1448 715 Q 1448 772 1425 812 T 1348 852 Q 1236 852 1154 783 T 1012 602 Q 1008 582 1006 571 L 874 45 Q 867 17 842 -3 T 788 -23 Q 764 -23 745 -7 T 727 35 Q 727 47 729 53 L 860 575 Q 881 659 881 715 Q 881 772 857 812 T 778 852 Q 703 852 640 819 T 530 731 T 444 602 L 305 45 Q 298 17 273 -3 T 219 -23 Q 194 -23 176 -7 T 158 35 Z \\"></path></g></svg> such that (a) for any fixed depth rectified network there exist an <svg class=\\"gs_fsvg\\" aria-label=\\"m\\" width=\\"12px\\" height=\\"6px\\" style=\\"vertical-align:0px;\\"><g transform=\\"matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 6.18800)\\"><path transform=\\"scale(0.48828, -0.48828)\\" d=\\"M 158 35 Q 158 47 160 53 L 313 664 Q 328 721 328 764 Q 328 852 268 852 Q 204 852 173 775 T 113 582 Q 113 576 107 572 T 96 569 H 72 Q 65 569 60 576 T 55 590 Q 77 679 97 741 T 161 854 T 270 905 Q 347 905 406 856 T 465 733 Q 526 813 608 859 T 782 905 Q 879 905 949 855 T 1020 715 Q 1083 804 1167 854 T 1352 905 Q 1458 905 1522 847 T 1587 684 Q 1587 600 1549 481 T 1456 215 Q 1427 144 1427 92 Q 1427 31 1475 31 Q 1555 31 1608 117 T 1683 301 Q 1689 313 1700 313 H 1724 Q 1732 313 1737 307 T 1743 295 Q 1743 293 1741 289 Q 1713 173 1643 75 T 1470 -23 Q 1398 -23 1347 26 T 1296 147 Q 1296 183 1313 227 Q 1371 381 1409 502 T 1448 715 Q 1448 772 1425 812 T 1348 852 Q 1236 852 1154 783 T 1012 602 Q 1008 582 1006 571 L 874 45 Q 867 17 842 -3 T 788 -23 Q 764 -23 745 -7 T 727 35 Q 727 47 729 53 L 860 575 Q 881 659 881 715 Q 881 772 857 812 T 778 852 Q 703 852 640 819 T 530 731 T 444 602 L 305 45 Q 298 17 273 -3 T 219 -23 Q 194 -23 176 -7 T 158 35 Z \\"></path></g></svg> above which classifying problem <svg class=\\"gs_fsvg\\" aria-label=\\"m\\" width=\\"12px\\" height=\\"6px\\" style=\\"vertical-align:0px;\\"><g transform=\\"matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 6.18800)\\"><path transform=\\"scale(0.48828, -0.48828)\\" d=\\"M 158 35 Q 158 47 160 53 L 313 664 Q 328 721 328 764 Q 328 852 268 852 Q 204 852 173 775 T 113 582 Q 113 576 107 572 T 96 569 H 72 Q 65 569 60 576 T 55 590 Q 77 679 97 741 T 161 854 T 270 905 Q 347 905 406 856 T 465 733 Q 526 813 608 859 T 782 905 Q 879 905 949 855 T 1020 715 Q 1083 804 1167 854 T 1352 905 Q 1458 905 1522 847 T 1587 684 Q 1587 600 1549 481 T 1456 215 Q 1427 144 1427 92 Q 1427 31 1475 31 Q 1555 31 1608 117 T 1683 301 Q 1689 313 1700 313 H 1724 Q 1732 313 1737 307 T 1743 295 Q 1743 293 1741 289 Q 1713 173 1643 75 T 1470 -23 Q 1398 -23 1347 26 T 1296 147 Q 1296 183 1313 227 Q 1371 381 1409 502 T 1448 715 Q 1448 772 1425 812 T 1348 852 Q 1236 852 1154 783 T 1012 602 Q 1008 582 1006 571 L 874 45 Q 867 17 842 -3 T 788 -23 Q 764 -23 745 -7 T 727 35 Q 727 47 729 53 L 860 575 Q 881 659 881 715 Q 881 772 857 812 T 778 852 Q 703 852 640 819 T 530 731 T 444 602 L 305 45 Q 298 17 273 -3 T 219 -23 Q 194 -23 176 -7 T 158 35 Z \\"></path></g></svg> correctly requires exponential number of parameters (in <svg class=\\"gs_fsvg\\" aria-label=\\"m\\" width=\\"12px\\" height=\\"6px\\" style=\\"vertical-align:0px;\\"><g transform=\\"matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 6.18800)\\"><path transform=\\"scale(0.48828, -0.48828)\\" d=\\"M 158 35 Q 158 47 160 53 L 313 664 Q 328 721 328 764 Q 328 852 268 852 Q 204 852 173 775 T 113 582 Q 113 576 107 572 T 96 569 H 72 Q 65 569 60 576 T 55 590 Q 77 679 97 741 T 161 854 T 270 905 Q 347 905 406 856 T 465 733 Q 526 813 608 859 T 782 905 Q 879 905 949 855 T 1020 715 Q 1083 804 1167 854 T 1352 905 Q 1458 905 1522 847 T 1587 684 Q 1587 600 1549 481 T 1456 215 Q 1427 144 1427 92 Q 1427 31 1475 31 Q 1555 31 1608 117 T 1683 301 Q 1689 313 1700 313 H 1724 Q 1732 313 1737 307 T 1743 295 Q 1743 293 1741 289 Q 1713 173 1643 75 T 1470 -23 Q 1398 -23 1347 26 T 1296 147 Q 1296 183 1313 227 Q 1371 381 1409 502 T 1448 715 Q 1448 772 1425 812 T 1348 852 Q 1236 852 1154 783 T 1012 602 Q 1008 582 1006 571 L 874 45 Q 867 17 842 -3 T 788 -23 Q 764 -23 745 -7 T 727 35 Q 727 47 729 53 L 860 575 Q 881 659 881 715 Q 881 772 857 812 T 778 852 Q 703 852 640 819 T 530 731 T 444 602 L 305 45 Q 298 17 273 -3 T 219 -23 Q 194 -23 176 -7 T 158 35 Z \\"></path></g></svg>); and (b) for any problem in the sequence, we present a concrete neural network with linear depth (in <svg class=\\"gs_fsvg\\" aria-label=\\"m\\" width=\\"12px\\" height=\\"6px\\" style=\\"vertical-align:0px;\\"><g transform=\\"matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 6.18800)\\"><path transform=\\"scale(0.48828, -0.48828)\\" d=\\"M 158 35 Q 158 47 160 53 L 313 664 Q 328 721 328 764 Q 328 852 268 852 Q 204 852 173 775 T 113 582 Q 113 576 107 572 T 96 569 H 72 Q 65 569 60 576 T 55 590 Q 77 679 97 741 T 161 854 T 270 905 Q 347 905 406 856 T 465 733 Q 526 813 608 859 T 782 905 Q 879 905 949 855 T 1020 715 Q 1083 804 1167 854 T 1352 905 Q 1458 905 1522 847 T 1587 684 Q 1587 600 1549 481 T 1456 215 Q 1427 144 1427 92 Q 1427 31 1475 31 Q 1555 31 1608 117 T 1683 301 Q 1689 313 1700 313 H 1724 Q 1732 313 1737 307 T 1743 295 Q 1743 293 1741 289 Q 1713 173 1643 75 T 1470 -23 Q 1398 -23 1347 26 T 1296 147 Q 1296 183 1313 227 Q 1371 381 1409 502 T 1448 715 Q 1448 772 1425 812 T 1348 852 Q 1236 852 1154 783 T 1012 602 Q 1008 582 1006 571 L 874 45 Q 867 17 842 -3 T 788 -23 Q 764 -23 745 -7 T 727 35 Q 727 47 729 53 L 860 575 Q 881 659 881 715 Q 881 772 857 812 T 778 852 Q 703 852 640 819 T 530 731 T 444 602 L 305 45 Q 298 17 273 -3 T 219 -23 Q 194 -23 176 -7 T 158 35 Z \\"></path></g></svg>) and small constant width (<svg class=\\"gs_fsvg\\" aria-label=\\"\\\\\\\\leq 4\\" width=\\"22px\\" height=\\"11px\\" style=\\"vertical-align:-2px;\\"><g transform=\\"matrix(0.01400, 0.00000, 0.00000, 0.01400, 0.00000, 9.32400)\\"><path transform=\\"scale(0.48828, -0.48828)\\" d=\\"M 209 -281 Q 192 -281 181 -268 T 170 -240 Q 170 -223 181 -211 T 209 -199 H 1384 Q 1400 -199 1410 -211 T 1421 -240 Q 1421 -255 1410 -268 T 1384 -281 H 209 Z M 190 676 Q 170 682 170 711 Q 170 737 197 748 L 1366 1300 Q 1370 1303 1380 1303 Q 1397 1303 1409 1291 T 1421 1262 Q 1421 1235 1397 1225 L 307 711 L 1403 193 Q 1421 183 1421 158 Q 1421 140 1409 128 T 1380 117 Q 1370 117 1366 121 L 190 676 Z \\"></path><path transform=\\"matrix(0.48828, 0.00000, 0.00000, -0.48828, 1055.55859, 0.00000)\\" d=\\"M 57 338 V 410 L 690 1354 Q 697 1364 711 1364 H 741 Q 764 1364 764 1341 V 410 H 965 V 338 H 764 V 137 Q 764 95 824 83 T 963 72 V 0 H 399 V 72 Q 478 72 538 83 T 598 137 V 338 H 57 Z M 125 410 H 610 V 1135 L 125 410 Z \\"></path></g></svg>) that classifies the problem with zero error. The constructive proof is based on geometric arguments and a space folding construction. While stronger bounds and results exist, our proof uses substantially simpler tools and techniques, and should be accessible to undergraduate students in computer science and people with similar backgrounds."},{"id":"896651d38fd730ffe6d0414f27c6941f.html","title":"Improving Quality and Efficiency in Plan-based Neural Data-to-Text Generation","url":"https://arxiv.org/abs/1909.09986","authors":["Amit Moryossef","Ido Dagan","Yoav Goldberg"],"date":"2019/09/22","journal":"arXiv preprint arXiv:1909.09986","abstract":"We follow the step-by-step approach to neural data-to-text generation we proposed in Moryossef et al (2019), in which the generation process is divided into a text-planning stage followed by a plan-realization stage. We suggest four extensions to that framework:(1) we introduce a trainable neural planning component that can generate effective plans several orders of magnitude faster than the original planner;(2) we incorporate typing hints that improve the model\'s ability to deal with unseen relations and entities;(3) we introduce a verification-by-reranking stage that substantially improves the faithfulness of the resulting texts;(4) we incorporate a simple but effective referring expression generation module. These extensions result in a generation process that is faster, more fluent, and more accurate."},{"id":"4ef4be07b008dea6b3fc7d7cb6ae7e9f.html","title":"RUN through the Streets: A New Dataset and Baseline Models for Realistic Urban Navigation","url":"https://arxiv.org/abs/1909.08970","authors":["Tzuf Paz-Argaman","Reut Tsarfaty"],"date":"2019/09/19","journal":"arXiv preprint arXiv:1909.08970","abstract":"Following navigation instructions in natural language requires a composition of language, action, and knowledge of the environment. Knowledge of the environment may be provided via visual sensors or as a symbolic world representation referred to as a map. Here we introduce the Realistic Urban Navigation (RUN) task, aimed at interpreting navigation instructions based on a real, dense, urban map. Using Amazon Mechanical Turk, we collected a dataset of 2515 instructions aligned with actual routes over three regions of Manhattan. We propose a strong baseline for the task and empirically investigate which aspects of the neural architecture are important for the RUN success. Our results empirically show that entity abstraction, attention over words and worlds, and a constantly updating world-state, significantly contribute to task accuracy."},{"id":"bf21b5720767961287bb8b67ad3301ee.html","title":"ABSApp: A Portable Weakly-Supervised Aspect-Based Sentiment Extraction System","url":"https://arxiv.org/abs/1909.05608","authors":["Oren Pereg","Daniel Korat","Moshe Wasserblat","Jonathan Mamou","Ido Dagan"],"date":"2019/09/12","journal":"arXiv preprint arXiv:1909.05608","abstract":"We present ABSApp, a portable system for weakly-supervised aspect-based sentiment extraction. The system is interpretable and user friendly and does not require labeled training data, hence can be rapidly and cost-effectively used across different domains in applied setups. The system flow includes three stages: First, it generates domain-specific aspect and opinion lexicons based on an unlabeled dataset; second, it enables the user to view and edit those lexicons (weak supervision); and finally, it enables the user to select an unlabeled target dataset from the same domain, classify it, and generate an aspect-based sentiment report. ABSApp has been successfully used in a number of real-life use cases, among them movie review analysis and convention impact analysis."},{"id":"4b4181671ce83398a7f55a4680e0b73d.html","title":"Better rewards yield better summaries: Learning to summarise without references","url":"https://arxiv.org/abs/1909.01214","authors":["Florian B\\\\xf6hm","Yang Gao","Christian M Meyer","Ori Shapira","Ido Dagan","Iryna Gurevych"],"date":"2019/09/03","journal":"arXiv preprint arXiv:1909.01214","abstract":"Reinforcement Learning (RL) based document summarisation systems yield state-of-the-art performance in terms of ROUGE scores, because they directly use ROUGE as the rewards during training. However, summaries with high ROUGE scores often receive low human judgement. To find a better reward function that can guide RL to generate human-appealing summaries, we learn a reward function from human ratings on 2,500 summaries. Our reward function only takes the document and system summary as input. Hence, once trained, it can be used to train RL-based summarisation systems without using any reference summaries. We show that our learned rewards have significantly higher correlation with human ratings than previous approaches. Human evaluation experiments show that, compared to the state-of-the-art supervised-learning systems and ROUGE-as-rewards RL summarisation systems, the RL systems using our learned rewards during training generate summarieswith higher human ratings. The learned reward function and our source code are available at this https URL."},{"id":"0f67078a7fa90dab89060b3f9a142e1a.html","title":"It\'s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution","url":"https://arxiv.org/abs/1909.00871","authors":["Rowan Hall Maudslay","Hila Gonen","Ryan Cotterell","Simone Teufel"],"date":"2019/09/02","journal":"arXiv preprint arXiv:1909.00871","abstract":"This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, eg by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation."},{"id":"b386ece2b32d5beeed6398418315c4aa.html","title":"Transfer Learning Between Related Tasks Using Expected Label Proportions","url":"https://arxiv.org/abs/1909.00430","authors":["Matan Ben Noach","Yoav Goldberg"],"date":"2019/09/01","journal":"arXiv preprint arXiv:1909.00430","abstract":"Deep learning systems thrive on abundance of labeled training data but such data is not always available, calling for alternative methods of supervision. One such method is expectation regularization (XR)(Mann and McCallum, 2007), where models are trained based on expected label proportions. We propose a novel application of the XR framework for transfer learning between related tasks, where knowing the labels of task A provides an estimation of the label proportion of task B. We then use a model trained for A to label a large corpus, and use this corpus with an XR loss to train a model for task B. To make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure. We demonstrate the approach on the task of Aspect-based Sentiment classification, where we effectively use a sentence-level sentiment predictor to train accurate aspect-based predictor. The method improves upon fully supervised neural system trained on aspect-level data, and is also cumulative with LM-based pretraining, as we demonstrate by improving a BERT-based Aspect-based Sentiment model."},{"id":"4fd6c8ebae72a77724f222df08702d3e.html","title":"Where\u2019s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution","url":"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00280","authors":["Yanai Elazar","Yoav Goldberg"],"date":"2019/09","journal":"Transactions of the Association for Computational Linguistics","abstract":"We provide the first computational treatment of fused-heads constructions (FHs), focusing on the numeric fused-heads (NFHs). FHs constructions are noun phrases in which the head noun is missing and is said to be \u201cfused\u201d with its dependent modifier. This missing information is implicit and is important for sentence understanding. The missing references are easily filled in by humans but pose a challenge for computational models. We formulate the handling of FHs as a two stages process: <b>Identification</b> of the FH construction and <b>resolution</b> of the missing head. We explore the NFH phenomena in large corpora of English text and create (1) a data set and a highly accurate method for NFH identification; (2) a 10k examples (1 M tokens) crowd-sourced data set of NFH resolution; and (3) a neural baseline for the NFH resolution task. We release our code and data set, to foster further research into this challenging problem."},{"id":"d68114a3942accb301199ffb621d81f7.html","title":"Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets","url":"https://arxiv.org/abs/1908.07898","authors":["Mor Geva","Yoav Goldberg","Jonathan Berant"],"date":"2019/08/21","journal":"arXiv preprint arXiv:1908.07898","abstract":"Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only a few workers generate the majority of examples raises concerns about data diversity, especially when workers freely generate sentences. In this paper, we perform a series of experiments showing these concerns are evident in three recent NLP datasets. We show that model performance improves when training with annotator identifiers as features, and that models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators."},{"id":"0d9e0c3a4009ad25415a00aaf86c9bc3.html","title":"What\'s Wrong with Hebrew NLP? And How to Make it Right","url":"https://arxiv.org/abs/1908.05453","authors":["Reut Tsarfaty","Amit Seker","Shoval Sadde","Stav Klein"],"date":"2019/08/15","journal":"arXiv preprint arXiv:1908.05453","abstract":"For languages with simple morphology, such as English, automatic annotation pipelines such as spaCy or Stanford\'s CoreNLP successfully serve projects in academia and the industry. For many morphologically-rich languages (MRLs), similar pipelines show sub-optimal performance that limits their applicability for text analysis in research and the industry. The sub-optimal performance is mainly due to errors in early morphological disambiguation decisions, which cannot be recovered later in the pipeline, yielding incoherent annotations on the whole. In this paper we describe the design and use of the Onlp suite, a joint morpho-syntactic parsing framework for processing Modern Hebrew texts. The joint inference over morphology and syntax substantially limits error propagation, and leads to high accuracy. Onlp provides rich and expressive output which already serves diverse academic and commercial needs. Its accompanying online demo further serves educational activities, introducing Hebrew NLP intricacies to researchers and non-researchers alike."},{"id":"c858c3fddfb64f2188393d19833f6a7f.html","title":"Ab Antiquo: Proto-language Reconstruction with RNNs","url":"https://arxiv.org/abs/1908.02477","authors":["Carlo Meloni","Shauli Ravfogel","Yoav Goldberg"],"date":"2019/08/07","journal":"arXiv preprint arXiv:1908.02477","abstract":"Historical linguists have identified regularities in the process of historic sound change. The comparative method utilizes those regularities to reconstruct proto-words based on observed forms in daughter languages. Can this process be efficiently automated? We address the task of proto-word reconstruction, in which the model is exposed to cognates in contemporary daughter languages, and has to predict the proto word in the ancestor language. We provide a novel dataset for this task, encompassing over 8,000 comparative entries, and show that neural sequence models outperform conventional methods applied to this task so far. Error analysis reveals a variability in the ability of neural model to capture different phonological changes, correlating with the complexity of the changes. Analysis of learned embeddings reveals the models learn phonologically meaningful generalizations, corresponding to well-attested phonological shifts documented by historical linguistics."},{"id":"f200125df0d5a46f2f1c3d7c0e96a5fa.html","title":"What does BERT learn about the structure of language?","url":"https://hal.inria.fr/hal-02131630/","authors":["Ganesh Jawahar","Beno\\\\xeet Sagot","Djam\\\\xe9 Seddah"],"date":"2019/07/28","abstract":""},{"id":"84a65845f14b2a202ff72265c59174ce.html","title":"Ranking generated summaries by correctness: An interesting but challenging application for natural language inference","url":"https://www.aclweb.org/anthology/P19-1213.pdf","authors":["Tobias Falke","Leonardo FR Ribeiro","Prasetya Ajie Utama","Ido Dagan","Iryna Gurevych"],"date":"2019/07","abstract":"While recent progress on abstractive summarization has led to remarkably fluent summaries, factual errors in generated summaries still severely limit their use in practice. In this paper, we evaluate summaries produced by state-of-the-art models via crowdsourcing and show that such errors occur frequently, in particular with more abstractive models. We study whether textual entailment predictions can be used to detect such errors and if they can be reduced by reranking alternative predicted summaries. That leads to an interesting downstream application for entailment models. In our experiments, we find that out-of-the-box entailment models trained on NLI datasets do not yet offer the desired performance for the downstream task and we therefore release our annotations as additional test data for future extrinsic evaluations of NLI."},{"id":"d34aabfcb9890451cda4c042b75065d6.html","title":"Still a pain in the neck: Evaluating text representations on lexical composition","url":"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00277","authors":["Vered Shwartz","Ido Dagan"],"date":"2019/07","journal":"Transactions of the Association for Computational Linguistics","abstract":"Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddings, more so on detecting meaning shift than in recovering implicit information, in which their performance is still far from that of humans. Our evaluation suite, consisting of six tasks related to lexical composition effects, can serve future research aiming to improve representations."},{"id":"4573b1bb55f722c509ebdddd68f84e83.html","title":"The Third Workshop on Evaluating Vector Space Representations for NLP","url":"http://scholar.google.com/scholar?cluster=1412601863258781221&hl=en&oi=scholarr","authors":["Anna Rogers","Aleksandr Drozd","Anna Rumshisky","Yoav Goldberg"],"date":"2019/06/6"},{"id":"226911a28e76772b06c8dee5272f0e4c.html","title":"Correctness of Generated Summaries","url":"https://tudatalib.ulb.tu-darmstadt.de/bitstream/handle/tudatalib/2002/summary-correctness-v1.0.zip?sequence=3","authors":["Tobias Falke","Leonardo Ribeiro","Prasetya Utama","Ido Dagan","Iryna Gurevych"],"date":"2019/06/04","abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Scholar articles"},{"id":"41d697ece5f80944681d464b0ec384ff.html","title":"How large are lions? inducing distributions over quantitative attributes","url":"https://arxiv.org/abs/1906.01327","authors":["Yanai Elazar","Abhijit Mahabal","Deepak Ramachandran","Tania Bedrax-Weiss","Dan Roth"],"date":"2019/06/04","journal":"arXiv preprint arXiv:1906.01327","abstract":""},{"id":"ee7286d8aa413f845f44a085df34a041.html","title":"Revisiting joint modeling of cross-document entity and event coreference resolution","url":"https://arxiv.org/abs/1906.01753","authors":["Shany Barhom","Vered Shwartz","Alon Eirew","Michael Bugert","Nils Reimers","Ido Dagan"],"date":"2019/06/04","journal":"arXiv preprint arXiv:1906.01753","abstract":"Recognizing coreferring events and entities across multiple texts is crucial for many NLP applications. Despite the task\'s importance, research focus was given mostly to within-document entity coreference, with rather little attention to the other variants. We propose a neural architecture for cross-document coreference resolution. Inspired by Lee et al (2012), we jointly model entity and event coreference. We represent an event (entity) mention using its lexical span, surrounding context, and relation to entity (event) mentions via predicate-arguments structures. Our model outperforms the previous state-of-the-art event coreference model on ECB+, while providing the first entity coreference results on this corpus. Our analysis confirms that all our representation elements, including the mention span itself, its context, and the relation to other mentions contribute to the model\'s success."},{"id":"14e6329bb8df6dae4c048e8bf218008c.html","title":"How to Compare Summarizers without Target Length? Pitfalls, Solutions and Re-Examination of the Neural Summarization Literature","url":"https://www.aclweb.org/anthology/W19-2303.pdf","authors":["Simeng Sun","Ori Shapira","Ido Dagan","Ani Nenkova"],"date":"2019/06","abstract":"We show that plain ROUGE F1 scores are not ideal for comparing current neural systems which on average produce different lengths. This is due to a non-linear pattern between ROUGE F1 and summary length. To alleviate the effect of length during evaluation, we have proposed a new method which normalizes the ROUGE F1 scores of a system by that of a random system with same average output length. A pilot human evaluation has shown that humans prefer short summaries in terms of the verbosity of a summary but overall consider longer summaries to be of higher quality. While human evaluations are more expensive in time and resources, it is clear that normalization, such as the one we proposed for automatic evaluation, will make human evaluations more meaningful."},{"id":"f48990b837664226748e76b8ee8fd43d.html","title":"Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP","url":"https://www.aclweb.org/anthology/W19-2000.pdf","authors":["Anna Rogers","Aleksandr Drozd","Anna Rumshisky","Yoav Goldberg"],"date":"2019/06","abstract":"The RepEval series of workshops started in the midst of a boom of word embeddings with the goals of promoting new benchmarks for vector space meaning representations, highlighting the issues with existing benchmarks and improving on them. In addition to proposals for new evaluation tasks, it has played an important role by providing an outlet for critical analysis, negative results, and methodological caveats (reproducibility, parameters impact, the issue of attribution of results to the representation or the whole system, dataset structure/balance/representativeness)."},{"id":"23502dbfa5756b8e2fa4a8893e5a167d.html","title":"Towards better substitution-based word sense induction","url":"https://arxiv.org/abs/1905.12598","authors":["Asaf Amrami","Yoav Goldberg"],"date":"2019/05/29","journal":"arXiv preprint arXiv:1905.12598","abstract":"Word sense induction (WSI) is the task of unsupervised clustering of word usages within a sentence to distinguish senses. Recent work obtain strong results by clustering lexical substitutes derived from pre-trained RNN language models (ELMo). Adapting the method to BERT improves the scores even further. We extend the previous method to support a dynamic rather than a fixed number of clusters as supported by other prominent methods, and propose a method for interpreting the resulting clusters by associating them with their most informative substitutes. We then perform extensive error analysis revealing the remaining sources of errors in the WSI task."},{"id":"96d4e11bd406cd57634d5bb4902af4e1.html","title":"Towards Neural Decompilation","url":"https://arxiv.org/abs/1905.08325","authors":["Omer Katz","Yuval Olshaker","Yoav Goldberg","Eran Yahav"],"date":"2019/05/20","journal":"arXiv preprint arXiv:1905.08325","abstract":"We address the problem of automatic decompilation, converting a program in low-level representation back to a higher-level human-readable programming language. The problem of decompilation is extremely important for security researchers. Finding vulnerabilities and understanding how malware operates is much easier when done over source code."},{"id":"db68a6d5b6f35d4b3a018682a454cbcc.html","title":"An Algorithmic Scheme for Statistical Thesaurus Construction in a Morphologically Rich Language","url":"https://www.tandfonline.com/doi/abs/10.1080/08839514.2019.1583447","authors":["Chaya Liebeskind","Ido Dagan","Jonathan Schler"],"date":"2019/05/12","journal":"Applied Artificial Intelligence","abstract":"Corpus-based automatic thesaurus construction uses linguistic methods, such as Part-of-Speech taggers and parsers, which often perform poorly on MRLs. Therefore, in this paper, we focused on the complex task of adapting corpus-based thesaurus construction methods for MRLs. We investigated two statistical approaches for thesaurus construction; a) a first-order co-occurrence-based approach and b) a second-order distributional-based approach. We explored alternative levels of morphological term representations complemented by grouping the morphological variants. We then introduced and adopted a generic algorithmic scheme for thesaurus construction in MRLs for both first-order and second-order approaches. Our scheme investigated alternative representation levels and offered alternative configurations. We demonstrated the empirical benefits of our methodology for a diachronic Hebrew thesaurus\\\\xa0\u2026"},{"id":"f866f5c1e22bf4bd50535b47c393809a.html","title":"Multi-Context Term Embeddings: the Use Case of Corpus-based Term Set Expansion","url":"https://arxiv.org/abs/1904.02496","authors":["Jonathan Mamou","Oren Pereg","Moshe Wasserblat","Ido Dagan"],"date":"2019/04/4","journal":"arXiv preprint arXiv:1904.02496","abstract":"In this paper, we present a novel algorithm that combines multi-context term embeddings using a neural classifier and we test this approach on the use case of corpus-based term set expansion. In addition, we present a novel and unique dataset for intrinsic evaluation of corpus-based term set expansion algorithms. We show that, over this dataset, our algorithm provides up to 5 mean average precision points over the best baseline."},{"id":"43b946ed6bf2282cd14efa6e71306c63.html","title":"Crowdsourcing lightweight pyramids for manual summary evaluation","url":"https://arxiv.org/abs/1904.05929","authors":["Ori Shapira","David Gabay","Yang Gao","Hadar Ronen","Ramakanth Pasunuru","Mohit Bansal","Yael Amsterdamer","Ido Dagan"],"date":"2019/04/11","journal":"arXiv preprint arXiv:1904.05929","abstract":"Conducting a manual evaluation is considered an essential part of summary evaluation methodology. Traditionally, the Pyramid protocol, which exhaustively compares system summaries to references, has been perceived as very reliable, providing objective scores. Yet, due to the high cost of the Pyramid method and the required expertise, researchers resorted to cheaper and less thorough manual evaluation methods, such as Responsiveness and pairwise comparison, attainable via crowdsourcing. We revisit the Pyramid approach, proposing a lightweight sampling-based version that is crowdsourcable. We analyze the performance of our method in comparison to original expert-based Pyramid evaluations, showing higher correlation relative to the common Responsiveness method. We release our crowdsourced Summary-Content-Units, along with all crowdsourcing scripts, for future evaluations."},{"id":"e2d027023e8f368c15d1af98fc6a2914.html","title":"Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation","url":"https://arxiv.org/abs/1904.03396","authors":["Amit Moryossef","Yoav Goldberg","Ido Dagan"],"date":"2019/04/06","journal":"arXiv preprint arXiv:1904.03396","abstract":"Data-to-text generation can be conceptually divided into two parts: ordering and structuring the information (planning), and generating fluent language describing the information (realization). Modern neural generation systems conflate these two steps into a single end-to-end differentiable system. We propose to split the generation process into a symbolic text-planning stage that is faithful to the input, followed by a neural generation stage that focuses only on realization. For training a plan-to-text generator, we present a method for matching reference texts to their corresponding text plans. For inference time, we describe a method for selecting high-quality text plans for new inputs. We implement and evaluate our approach on the WebNLG benchmark. Our results demonstrate that decoupling text planning from neural realization indeed improves the system\'s reliability and adequacy while maintaining fluent output. We observe improvements both in BLEU scores and in manual evaluations. Another benefit of our approach is the ability to output diverse realizations of the same input, paving the way to explicit control over the generated text structure."},{"id":"bd364e33367bfcd004f3b50c935009c6.html","title":"Studying the Inductive Biases of RNNs with Synthetic Variations of Natural Languages","url":"https://arxiv.org/abs/1903.06400","authors":["Shauli Ravfogel","Yoav Goldberg","Tal Linzen"],"date":"2019/03/15","journal":"arXiv preprint arXiv:1903.06400","abstract":"How do typological properties such as word order and morphological case marking affect the ability of neural sequence models to acquire the syntax of a language? Cross-linguistic comparisons of RNNs\' syntactic performance (eg, on subject-verb agreement prediction) are complicated by the fact that any two languages differ in multiple typological properties, as well as by differences in training corpus. We propose a paradigm that addresses these issues: we create synthetic versions of English, which differ from English in one or more typological parameters, and generate corpora for those languages based on a parsed English corpus. We report a series of experiments in which RNNs were trained to predict agreement features for verbs in each of those synthetic languages. Among other findings,(1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias;(2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order."},{"id":"74aa80c1682049171796211bfe7fbe99.html","title":"Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them","url":"https://arxiv.org/abs/1903.03862","authors":["Hila Gonen","Yoav Goldberg"],"date":"2019/03/09","journal":"arXiv preprint arXiv:1903.03862","abstract":"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between\\" gender-neutralized\\" words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling."},{"id":"7bf79175af1d91fa9fbf5f1fc8319727.html","title":"Filling Gender & Number Gaps in Neural Machine Translation with Black-box Context Injection","url":"https://arxiv.org/abs/1903.03467","authors":["Amit Moryossef","Roee Aharoni","Yoav Goldberg"],"date":"2019/03/08","journal":"arXiv preprint arXiv:1903.03467","abstract":"When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must\\" guess\\" this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our method on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the translation accuracy in up to 2.3 BLEU on a female-speaker test set for a state-of-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method."},{"id":"278a876e8b0d9d6723ea8649eeb15ac5.html","title":"Aligning Vector-spaces with Noisy Supervised Lexicons","url":"https://ui.adsabs.harvard.edu/abs/2019arXiv190310238Y/abstract","authors":["Noa Yehezkel Lubin","Jacob Goldberger","Yoav Goldberg"],"date":"2019/03","journal":"arXiv preprint arXiv:1903.10238","abstract":"The problem of learning to translate between two vector spaces given a set of aligned points arises in several application areas of NLP. Current solutions assume that the lexicon which defines the alignment pairs is noise-free. We consider the case where the set of aligned points is allowed to contain an amount of noise, in the form of incorrect lexicon pairs and show that this arises in practice by analyzing the edited dictionaries after the cleaning process. We demonstrate that such noise substantially degrades the accuracy of the learned translation when using current methods. We propose a model that accounts for noisy pairs. This is achieved by introducing a generative model with a compatible iterative EM algorithm. The algorithm jointly learns the noise level in the lexicon, finds the set of noisy pairs, and learns the mapping between the spaces. We demonstrate the effectiveness of our proposed algorithm on two\\\\xa0\u2026"},{"id":"317b41cbe0cb126aa0df1625eec201b5.html","title":"Joint Transition-Based Models for Morpho-Syntactic Parsing: Parsing Strategies for MRLs and a Case Study from Modern Hebrew","url":"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00253","authors":["Amir More","Amit Seker","Victoria Basmova","Reut Tsarfaty"],"date":"2019/03","journal":"Transactions of the Association for Computational Linguistics","abstract":"In standard NLP pipelines, <i>morphological analysis and disambiguation</i> (MA&D) precedes syntactic and semantic downstream tasks. However, for languages with complex and ambiguous word-internal structure, known as <i>morphologically rich languages</i> (MRLs), it has been hypothesized that syntactic context may be crucial for accurate MA&D, and vice versa. In this work we empirically confirm this hypothesis for Modern Hebrew, an MRL with complex morphology and severe word-level ambiguity, in a novel transition-based framework. Specifically, we propose a joint morphosyntactic transition-based framework which formally unifies two distinct transition systems, morphological and syntactic, into a single transition-based system with joint training and joint inference. We empirically show that MA&D results obtained in the joint settings outperform MA&D results obtained by the respective standalone components, and\\\\xa0\u2026"},{"id":"56d1abcb5505668aed5c19ad4b6c641e.html","title":"ABI Neural Ensemble Model for Gender Prediction Adapt Bar-Ilan Submission for the CLIN29 Shared Task on Gender Prediction","url":"https://arxiv.org/abs/1902.08856","authors":["Eva Vanmassenhove","Amit Moryossef","Alberto Poncelas","Andy Way","Dimitar Shterionov"],"date":"2019/02/23","journal":"arXiv preprint arXiv:1902.08856","abstract":"We present our system for the CLIN29 shared task on cross-genre gender detection for Dutch. We experimented with a multitude of neural models (CNN, RNN, LSTM, etc.), more\\" traditional\\" models (SVM, RF, LogReg, etc.), different feature sets as well as data pre-processing. The final results suggested that using tokenized, non-lowercased data works best for most of the neural models, while a combination of word clusters, character trigrams and word lists showed to be most beneficial for the majority of the more\\" traditional\\"(that is, non-neural) models, beating features used in previous tasks such as n-grams, character n-grams, part-of-speech tags and combinations thereof. In contradiction with the results described in previous comparable shared tasks, our neural models performed better than our best traditional approaches with our best feature set-up. Our final model consisted of a weighted ensemble model combining the top 25 models. Our final model won both the in-domain gender prediction task and the cross-genre challenge, achieving an average accuracy of 64.93% on the in-domain gender prediction task, and 56.26% on cross-genre gender prediction."},{"id":"ceff2a8eeb94f245d531abfad9d17307.html","title":"Mining fall-related information in clinical notes: Comparison of rule-based and novel word embedding-based machine learning approaches","url":"https://www.sciencedirect.com/science/article/pii/S1532046419300218","authors":["Maxim Topaz","Ludmila Murga","Katherine M Gaddis","Margaret V McDonald","Ofrit Bar-Bachar","Yoav Goldberg","Kathryn H Bowles"],"date":"2019/02/01","journal":"Journal of biomedical informatics","abstract":"<div><h3 class=\\"gsh_h3\\">Background</h3><div class=\\"gsh_csp\\">Natural language processing (NLP) of health-related data is still an expertise demanding, and resource expensive process. We created a novel, open source rapid clinical text mining system called NimbleMiner. NimbleMiner combines several machine learning techniques (word embedding models and positive only labels learning) to facilitate the process in which a human rapidly performs text mining of clinical narratives, while being aided by the machine learning components."},{"id":"3058c018f635f683f547cb086e5f4fbe.html","title":"Assessing BERT\'s Syntactic Abilities","url":"https://arxiv.org/abs/1901.05287","authors":["Yoav Goldberg"],"date":"2019/01/16","journal":"arXiv preprint arXiv:1901.05287","abstract":"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli;(2)\\" coloreless green ideas\\" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases."},{"id":"25180d5fb1f2927e5d9e08240924e48b.html","title":"A little is enough: Circumventing defenses for distributed learning","url":"http://papers.nips.cc/paper/9069-a-little-is-enough-circumventing-defenses-for-distributed-learning","authors":["Gilad Baruch","Moran Baruch","Yoav Goldberg"],"date":"2019","abstract":"Distributed learning is central for large-scale training of deep-learning models. However, it is exposed to a security threat in which Byzantine participants can interrupt or control the learning process. Previous attack models assume that the rogue participants (a) are omniscient (know the data of all other participants), and (b) introduce large changes to the parameters. Accordingly, most defense mechanisms make a similar assumption and attempt to use statistically robust methods to identify and discard values whose reported gradients are far from the population mean. We observe that if the empirical variance between the gradients of workers is high enough, an attacker could take advantage of this and launch a non-omniscient attack that operates within the population variance. We show that the variance is indeed high enough even for simple datasets such as MNIST, allowing an attack that is not only undetected by existing defenses, but also uses their power against them, causing those defense mechanisms to consistently select the byzantine workers while discarding legitimate ones. We demonstrate our attack method works not only for preventing convergence but also for repurposing of the model behavior (``backdooring\'\'). We show that less than 25\\\\\\\\% of colluding workers are sufficient to degrade the accuracy of models trained on MNIST, CIFAR10 and CIFAR100 by 50\\\\\\\\%, as well as to introduce backdoors without hurting the accuracy for MNIST and CIFAR10 datasets, but with a degradation for CIFAR100."},{"id":"3c6e8ef4cca51a2dd2cad1aa3220a590.html","title":"Revisiting the Binary Linearization Technique for Surface Realization","url":"https://www.aclweb.org/anthology/W19-8635/","authors":["Yevgeniy Puzikov","Claire Gardent","Ido Dagan","Iryna Gurevych"],"date":"2019","abstract":"End-to-end neural approaches have achieved state-of-the-art performance in many natural language processing (NLP) tasks. Yet, they often lack transparency of the underlying decision-making process, hindering error analysis and certain model improvements. In this work, we revisit the binary linearization approach to surface realization, which exhibits more interpretable behavior, but was falling short in terms of prediction accuracy. We show how enriching the training data to better capture word order constraints almost doubles the performance of the system. We further demonstrate that encoding both local and global prediction contexts yields another considerable performance boost. With the proposed modifications, the system which ranked low in the latest shared task on multilingual surface realization now achieves best results in five out of ten languages, while being on par with the state-of-the-art approaches in others."},{"id":"44b5d9f49183cc1779dfabe38bdfc32c.html","title":"Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. arXiv 2019","url":"https://scholar.google.com/scholar?cluster=14750699376859225063&hl=en&oi=scholarr","authors":["Hila Gonen","Yoav Goldberg"],"date":"2019","journal":"arXiv preprint arXiv:1903.03862"},{"id":"81bbb1dd394f1acc04c3b582e917c0a6.html","title":"Learning Deterministic Weighted Automata with Queries and Counterexamples","url":"http://papers.nips.cc/paper/9062-learning-deterministic-weighted-automata-with-queries-and-counterexamples","authors":["Gail Weiss","Yoav Goldberg","Eran Yahav"],"date":"2019","abstract":"We present an algorithm for reconstruction of a probabilistic deterministic finite automaton (PDFA) from a given black-box language model, such as a recurrent neural network (RNN). The algorithm is a variant of the exact-learning algorithm L*, adapted to work in a probabilistic setting under noise. The key insight of the adaptation is the use of conditional probabilities when making observations on the model, and the introduction of a variation tolerance when comparing observations. When applied to RNNs, our algorithm returns models with better or equal word error rate (WER) and normalised distributed cumulative gain (NDCG) than achieved by n-gram or weighted finite automata (WFA) approximations of the same networks. The PDFAs capture a richer class of languages than n-grams, and are guaranteed to be stochastic and deterministic--unlike the WFAs."},{"id":"1907d804fe50c7d2f11c6e988d247513.html","title":"The Hebrew Universal Dependency Treebank: Past Present and Future","url":"https://www.aclweb.org/anthology/W18-6016.pdf","authors":["Shoval Sade","Amit Seker","Reut Tsarfaty"],"date":"2018/11","abstract":"The Hebrew treebank (HTB), consisting of 6221 morpho-syntactically annotated newspaper sentences, has been the only resource for training and validating Hebrew statistical parsers for almost two decades now. During these decades, the HTB has gone through a trajectory of automatic and semi-automatic conversions, until arriving at its current UDv2 form. In this work we set out to manually validate the UDv2 version and, accordingly, we apply scheme changes to bring the UD HTB into the same theoretical ground as the rest of UD. Our experimental results show that improving the linguistic coherence and internal consistency of the UD HTB has indeed led to improved syntactic parsing performance. At the same time, there is more to be done at the points of intersection with other linguistic processing layers, in particular, at the interface of UD with external morphological and lexical resources."},{"id":"31c02ce282cb3ec2b7076642c3f6726f.html","title":"Language Modeling for Code-Switching: Evaluation, Integration of Monolingual Data, and Discriminative Training","url":"https://arxiv.org/abs/1810.11895","authors":["Hila Gonen","Yoav Goldberg"],"date":"2018/10/28","journal":"arXiv preprint arXiv:1810.11895","abstract":"We focus on the problem of language modeling for code-switched language, in the context of automatic speech recognition (ASR). Language modeling for code-switched language is challenging for (at least) three reasons:(1) lack of available large-scale code-switched data for training;(2) lack of a replicable evaluation setup that is ASR directed yet isolates language modeling performance from the other intricacies of the ASR system; and (3) the reliance on generative modeling. We tackle these three issues: we propose an ASR-motivated evaluation setup which is decoupled from an ASR system and the choice of vocabulary, and provide an evaluation dataset for English-Spanish code-switching. This setup lends itself to a discriminative training approach, which we demonstrate to work better than generative language modeling. Finally, we explore a variety of training protocols and verify the effectiveness of training with large amounts of monolingual data followed by fine-tuning with small amounts of code-switched data, for both the generative and discriminative cases."},{"id":"ffaa011ac86911750b469598abb105f2.html","title":"Universal Morpho-syntactic Parsing and the Contribution of Lexica: Analyzing the ONLP Lab Submission to the CoNLL 2018 Shared Task","url":"https://www.aclweb.org/anthology/K18-2021.pdf","authors":["Amit Seker","Amir More","Reut Tsarfaty"],"date":"2018/10","abstract":"We present the contribution of the ONLP lab at the Open University of Israel to the UD shared task on multilingual parsing from raw text to Universal Dependencies. Our contribution is based on a transition-based parser called \u2018yap\u2013yet another parser\u2019, which includes a standalone morphological model, a standalone dependency model, and a joint morphosyntactic model. In the task we used yap\u2018s standalone dependency parser to parse input morphologically disambiguated by UDPipe, and obtained the official score of 58.35 LAS. In our follow up investigation we use yap to show how the incorporation of morphological and lexical resources may improve the performance of end-to-end raw-to-dependencies parsing in the case of a morphologically-rich and low-resource language, Modern Hebrew. Our results on Hebrew underscore the importance of CoNLL-UL, a UD-compatible standard for accessing external lexical resources, for enhancing end-to-end UD parsing, in particular for morphologically rich and low-resource languages. We thus encourage the community to create, convert, or make available more such lexica in future tasks."},{"id":"98768bc2d9680b9a2eda708a61cd1836.html","title":"Understanding Convolutional Neural Networks for Text Classification","url":"https://arxiv.org/abs/1809.08037","authors":["Alon Jacovi","Oren Sar Shalom","Yoav Goldberg"],"date":"2018/09/21","journal":"arXiv preprint arXiv:1809.08037","abstract":"We present an analysis into the inner workings of Convolutional Neural Networks (CNNs) for processing text. CNNs used for computer vision can be interpreted by projecting filters into image space, but for discrete sequence inputs CNNs remain a mystery. We aim to understand the method by which the networks process and classify text. We examine common hypotheses to this problem: that filters, accompanied by global max-pooling, serve as ngram detectors. We show that filters may capture several different semantic classes of ngrams by using different activation patterns, and that global max-pooling induces behavior which separates important ngrams from the rest. Finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and NLP) and prediction interpretability (explaining predictions). Code implementation is available online at this http URL."},{"id":"d4b2f4dd816732023c5204ba61be1589.html","title":"Can LSTM Learn to Capture Agreement? The Case of Basque","url":"https://arxiv.org/abs/1809.04022","authors":["Shauli Ravfogel","Francis M Tyers","Yoav Goldberg"],"date":"2018/09/11","journal":"arXiv preprint arXiv:1809.04022","abstract":"Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks. The sequential nature of these models raises the questions: to what extent can these models implicitly learn hierarchical structures typical to human language, and what kind of grammatical phenomena can they acquire?"},{"id":"36cb5b73a18551d63bd9b14578539cd5.html","title":"Word Sense Induction with Neural biLM and Symmetric Patterns","url":"https://arxiv.org/abs/1808.08518","authors":["Asaf Amrami","Yoav Goldberg"],"date":"2018/08/26","journal":"arXiv preprint arXiv:1808.08518","abstract":"An established method for Word Sense Induction (WSI) uses a language model to predict probable substitutes for target words, and induces senses by clustering these resulting substitute vectors."},{"id":"41ed0b3746a53e81f3263f99b6fffd20.html","title":"Adversarial Removal of Demographic Attributes from Text Data","url":"https://arxiv.org/abs/1808.06640","authors":["Yanai Elazar","Yoav Goldberg"],"date":"2018/08/20","journal":"arXiv preprint arXiv:1808.06640","abstract":"Recent advances in Representation Learning and Adversarial Training seem to succeed in removing unwanted features from the learned representation. We show that demographic information of authors is encoded in--and can be recovered from--the intermediate representations learned by text-based neural classifiers. The implication is that decisions of classifiers trained on textual data are not agnostic to--and likely condition on--demographic attributes. When attempting to remove such demographic information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classifier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data. This behavior is consistent across several tasks, demographic properties and datasets. We explore several techniques to improve the effectiveness of the adversarial component. Our main conclusion is a cautionary one: do not rely on the adversarial training to achieve invariant representation to sensitive features."},{"id":"b022d633a9ee3747ac96b8142fe4cbdc.html","title":"Clustering small-sized collections of short texts","url":"https://link.springer.com/article/10.1007/s10791-017-9324-8","authors":["Lili Kotlerman","Ido Dagan","Oren Kurland"],"date":"2018/08/01","journal":"Information Retrieval Journal","abstract":"The need to cluster small text corpora composed of a few hundreds of short texts rises in various applications; e.g., clustering top-retrieved documents based on their snippets. This clustering task is challenging due to the vocabulary mismatch between short texts and the insufficient corpus-based statistics (e.g., term co-occurrence statistics) due to the corpus size. We address this clustering challenge using a framework that utilizes a set of external knowledge resources that provide information about term relations. Specifically, we use information induced from the resources to estimate similarity between terms and produce term clusters. We also utilize the resources to expand the vocabulary used in the given corpus and thus enhance term clustering. We then project the texts in the corpus onto the term clusters to cluster the texts. We evaluate various instantiations of the proposed framework by varying the\\\\xa0\u2026"},{"id":"624e2f3a32ddcdfe602eb00e1b35c540.html","title":"Setexpander: End-to-end term set expansion based on multi-context term embeddings","url":"https://www.aclweb.org/anthology/C18-2013.pdf","authors":["Jonathan Mamou","Oren Pereg","Moshe Wasserblat","Ido Dagan","Yoav Goldberg","Alon Eirew","Yael Green","Shira Guskin","Peter Izsak","Daniel Korat"],"date":"2018/08","abstract":"We present SetExpander, a corpus-based system for expanding a seed set of terms into a more complete set of terms that belong to the same semantic class. SetExpander implements an iterative end-to end workflow for term set expansion. It enables users to easily select a seed set of terms, expand it, view the expanded set, validate it, re-expand the validated set and store it, thus simplifying the extraction of domain-specific fine-grained semantic classes. SetExpander has been used for solving real-life use cases including integration in an automated recruitment system and an issues and defects resolution system. A video demo of SetExpander is available at https://drive. google. com/open? id= 1e545bB87Autsch36DjnJHmq3HWfSd1Rv."},{"id":"81e4c1f1a9ec1ccb25f338b6a6d94de1.html","title":"Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from Modern Hebrew","url":"https://www.aclweb.org/anthology/C18-1190.pdf","authors":["Adam Amram","Anat Ben David","Reut Tsarfaty"],"date":"2018/08","abstract":"This paper empirically studies the effects of representation choices on neural sentiment analysis for Modern Hebrew, a morphologically rich language (MRL) for which no sentiment analyzer currently exists. We study two dimensions of representational choices:(i) the granularity of the input signal (token-based vs. morpheme-based), and (ii) the level of encoding of vocabulary items (string-based vs. character-based). We hypothesise that for MRLs, languages where multiple meaning-bearing elements may be carried by a single space-delimited token, these choices will have measurable effects on task perfromance, and that these effects may vary for different architectural designs\u2014fully-connected, convolutional or recurrent. Specifically, we hypothesize that morpheme-based representations will have advantages in terms of their generalization capacity and task accuracy, due to their better OOV coverage. To empirically study these effects, we develop a new sentiment analysis benchmark for Hebrew, based on 12K social media comments, and provide two instances of these data: in token-based and morpheme-based settings. Our experiments show that representation choices empirical effects vary with architecture type. While fully-connected and convolutional networks slightly prefer token-based settings, RNNs benefit from a morpheme-based representation, in accord with the hypothesis that explicit morphological information may help generalize. Our endeavour also delivers the first state-of-the-art broad-coverage sentiment analyzer for Hebrew, with over 89% accuracy, alongside an established benchmark to further study the effects of linguistic\\\\xa0\u2026"},{"id":"50545196f0f97f2ea27dffa53230817e.html","title":"Term Set Expansion based on Multi-Context Term Embeddings: an End-to-end Workflow","url":"https://arxiv.org/abs/1807.10104","authors":["Jonathan Mamou","Oren Pereg","Moshe Wasserblat","Ido Dagan","Yoav Goldberg","Alon Eirew","Yael Green","Shira Guskin","Peter Izsak","Daniel Korat"],"date":"2018/07/26","journal":"arXiv preprint arXiv:1807.10104","abstract":"We present SetExpander, a corpus-based system for expanding a seed set of terms into a more complete set of terms that belong to the same semantic class. SetExpander implements an iterative end-to end workflow for term set expansion. It enables users to easily select a seed set of terms, expand it, view the expanded set, validate it, re-expand the validated set and store it, thus simplifying the extraction of domain-specific fine-grained semantic classes. SetExpander has been used for solving real-life use cases including integration in an automated recruitment system and an issues and defects resolution system. A video demo of SetExpander is available at this https URL (some images were blurred for privacy reasons)."},{"id":"9d78d4d30f816c6d7dd0cbb93455ea60.html","title":"Breaking NLI Systems","url":"http://scholar.google.com/scholar?cluster=17876126004664715882&hl=en&oi=scholarr","authors":["Max Glockner","Vered Shwartz","Yoav Goldberg"],"date":"2018/07/18"},{"id":"4cbc18692e9df8f1cae028bc49b175f6.html","title":"Privacy and Fairness in Recommender Systems via Adversarial Training of User Representations","url":"https://arxiv.org/abs/1807.03521","authors":["Yehezkel S Resheff","Yanai Elazar","Moni Shahar","Oren Sar Shalom"],"date":"2018/07/10","journal":"arXiv preprint arXiv:1807.03521","abstract":"Latent factor models for recommender systems represent users and items as low dimensional vectors. Privacy risks of such systems have previously been studied mostly in the context of recovery of personal information in the form of usage records from the training data. However, the user representations themselves may be used together with external data to recover private user information such as gender and age. In this paper we show that user vectors calculated by a common recommender system can be exploited in this way. We propose the privacy-adversarial framework to eliminate such leakage of private information, and study the trade-off between recommender performance and leakage both theoretically and empirically using a benchmark dataset. An advantage of the proposed method is that it also helps guarantee fairness of results, since all implicit knowledge of a set of attributes is scrubbed from the representations used by the model, and thus can\'t enter into the decision making. We discuss further applications of this method towards the generation of deeper and more insightful recommendations."},{"id":"bb8ba07d5e9c3dd706117b67b8d52824.html","title":"Privacy-adversarial user representations in recommender systems","url":"https://www.researchgate.net/profile/Yehezkel_Resheff/publication/326315675_Privacy-Adversarial_User_Representations_in_Recommender_Systems/links/5b8d13b7a6fdcc5f8b7af1e7/Privacy-Adversarial-User-Representations-in-Recommender-Systems.pdf","authors":["Yehezkel S Resheff","Yanai Elazar","Moni Shahar","Oren Sar Shalom"],"date":"2018/07","journal":"arXiv preprint arXiv:1807.03521","abstract":"Latent factor models for recommender systems represent users and items as low dimensional vectors. Privacy risks have been previously studied mostly in the context of recovery of personal information in the form of usage records from the training data. However, the user representations themselves may be used together with external data to recover private user information such as gender and age. In this paper we show that user vectors calculated by a common recommender system can be exploited in this way. We propose the privacy-adversarial framework to eliminate such leakage, and study the trade-off between recommender performance and leakage both theoretically and empirically using a benchmark dataset. We briefly discuss further applications of this method towards the generation of deeper and more insightful recommendations."},{"id":"eef6fab77da3b7b9d613d2484f416a35.html","title":"Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP","url":"https://www.aclweb.org/anthology/W18-2900.pdf","authors":["Georgiana Dinu","Miguel Ballesteros","Avirup Sil","Samuel Bowman","Wael Hamza","Anders S\\\\xf8gaard","Tahira Naseem","Yoav Goldberg"],"date":"2018/07","abstract":"Welcome to the ACL Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP (RELNLP). The workshop took place on July 19th 2018, collocated with the 56th Annual Meeting of the Association for Computational Linguistics in Melbourne, Australia."},{"id":"0f8b776f0b2c15dc2c419b53399843f0.html","title":"Supervised open information extraction","url":"https://www.aclweb.org/anthology/N18-1081.pdf","authors":["Gabriel Stanovsky","Julian Michael","Luke Zettlemoyer","Ido Dagan"],"date":"2018/06","abstract":"We present data and methods that enable a supervised learning approach to Open Information Extraction (Open IE). Central to the approach is a novel formulation of Open IE as a sequence tagging problem, addressing challenges such as encoding multiple extractions for a predicate. We also develop a bi-LSTM transducer, extending recent deep Semantic Role Labeling models to extract Open IE tuples and provide confidence scores for tuning their precision-recall tradeoff. Furthermore, we show that the recently released Question-Answer Meaning Representation dataset can be automatically converted into an Open IE corpus which significantly increases the amount of available training data. Our supervised model outperforms the existing state-of-the-art Open IE systems on benchmark datasets."},{"id":"de7a8198227d55df0a188ecff45b62d4.html","title":"Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation","url":"http://proceedings.mlr.press/v97/gamrian19a.html","authors":["Shani Gamrian","Yoav Goldberg"],"date":"2018/05/31","journal":"arXiv preprint arXiv:1806.07377","abstract":"Despite the remarkable success of Deep RL in learning control policies from raw pixels, the resulting models do not generalize. We demonstrate that a trained agent fails completely when facing small visual changes, and that fine-tuning\u2014the common transfer learning paradigm\u2014fails to adapt to these changes, to the extent that it is faster to re-train the model from scratch. We show that by separating the visual transfer task from the control policy we achieve substantially better sample efficiency and transfer behavior, allowing an agent trained on the source task to transfer well to the target tasks. The visual mapping from the target to the source domain is performed using unaligned GANs, resulting in a control policy that can be further improved using imitation learning from imperfect demonstrations. We demonstrate the approach on synthetic visual variants of the Breakout game, as well as on transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. A visualization of our approach can be seen in\\\\\\\\url {https://youtu. be/4mnkzYyXMn4} and\\\\\\\\url {https://youtu. be/KCGTrQi6Ogo}."},{"id":"f3eb4c0d91148172df9bb6d55a19e008.html","title":"On the Practical Computational Power of Finite Precision RNNs for Language Recognition","url":"https://arxiv.org/abs/1805.04908","authors":["Gail Weiss","Yoav Goldberg","Eran Yahav"],"date":"2018/05/13","journal":"arXiv preprint arXiv:1805.04908","abstract":"While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of RNNs with finite precision whose computation time is linear in the input length. Under these limitations, we show that different RNN variants have different computational power. In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU. This is achieved because LSTMs and ReLU-RNNs can easily implement counting behavior. We show empirically that the LSTM does indeed learn to effectively use the counting mechanism."},{"id":"5356c97145931b980794128c6c314719.html","title":"Paraphrase to explicate: Revealing implicit noun-compound relations","url":"https://arxiv.org/abs/1805.02442","authors":["Vered Shwartz","Ido Dagan"],"date":"2018/05/07","journal":"arXiv preprint arXiv:1805.02442","abstract":"Revealing the implicit semantic relation between the constituents of a noun-compound is important for many NLP applications. It has been addressed in the literature either as a classification task to a set of pre-defined relations or by producing free text paraphrases explicating the relations. Most existing paraphrasing methods lack the ability to generalize, and have a hard time interpreting infrequent or new noun-compounds. We propose a neural model that generalizes better by representing paraphrases in a continuous space, generalizing for both unseen noun-compounds and rare paraphrases. Our model helps improving performance on both the noun-compound paraphrasing and classification tasks."},{"id":"9e25d66dc6cced8645f66336cbd2ecf8.html","title":"Conll-ul: Universal morphological lattices for universal dependency parsing","url":"https://hal.inria.fr/hal-01786125/","authors":["Amir More","\\\\xd6zlem \\\\xc7etino\u011flu","\\\\xc7a\u011fri \\\\xc7\\\\xf6ltekin","Nizar Habash","Beno\\\\xeet Sagot","Djam\\\\xe9 Seddah","Dima Taji","Reut Tsarfaty"],"date":"2018/05/07","abstract":""},{"id":"940fbc16cff71712ce2f5ad3dd109548.html","title":"Breaking NLI Systems with Sentences that Require Simple Lexical Inferences","url":"https://arxiv.org/abs/1805.02266","authors":["Max Glockner","Vered Shwartz","Yoav Goldberg"],"date":"2018/05/06","journal":"arXiv preprint arXiv:1805.02266","abstract":"We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are simpler than the SNLI test set, containing sentences that differ by at most one word from sentences in the training set. Yet, the performance on the new test set is substantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences."},{"id":"321f7e40fd8dac2b87a7e12d4e9f70a7.html","title":"Split and Rephrase: Better Evaluation and a Stronger Baseline","url":"https://arxiv.org/abs/1805.01035","authors":["Roee Aharoni","Yoav Goldberg"],"date":"2018/05/02","journal":"arXiv preprint arXiv:1805.01035","abstract":"Splitting and rephrasing a complex sentence into several shorter sentences that convey the same meaning is a challenging problem in NLP. We show that while vanilla seq2seq models can reach high scores on the proposed benchmark (Narayan et al., 2017), they suffer from memorization of the training set which contains more than 89% of the unique simple sentences from the validation and test sets. To aid this, we present a new train-development-test data split and neural models augmented with a copy-mechanism, outperforming the best reported baseline by 8.68 BLEU and fostering further progress on the task."},{"id":"fc3de3bce8c5e81cf71945c2af2735e5.html","title":"Automatic thesaurus construction for modern Hebrew","url":"https://www.aclweb.org/anthology/L18-1229.pdf","authors":["Chaya Liebeskind","Ido Dagan","Jonathan Schler"],"date":"2018/05","abstract":"Automatic thesaurus construction for Modern Hebrew is a complicated task, due to its high degree of inflectional ambiguity. Linguistics tools, including morphological analyzers, part-of-speech taggers and parsers often have limited in performance on Morphologically Rich Languages (MRLs) such as Hebrew. In this paper, we adopted a schematic methodology for generating a cooccurrence based thesaurus in a MRL and extended the methodology to create distributional similarity thesaurus. We explored three alternative levels of morphological term representations, surface form, lemma, and multiple lemmas, all complemented by the clustering of morphological variants. First, we evaluated both the co-occurrence based method and the distributional similarity method using Hebrew WordNet as our gold standard. However, due to Hebrew WordNet\'s low coverage, we completed our analysis with a manual evaluation. The results showed that for Modern Hebrew corpus-based thesaurus construction, the most directly applied statistical collection, using linguistics tools at the lemma level, is not optimal."},{"id":"3e2a1f54b708ff4dc99b16063a0ae632.html","title":"LaVAN: Localized and Visible Adversarial Noise","url":"https://arxiv.org/abs/1801.02608","authors":["Danny Karmon","Daniel Zoran","Yoav Goldberg"],"date":"2018/01/08","journal":"arXiv preprint arXiv:1801.02608","abstract":"Most works on adversarial examples for deep-learning based image classifiers use noise that, while small, covers the entire image. We explore the case where the noise is allowed to be visible but confined to a small, localized patch of the image, without covering any of the main object (s) in the image. We show that it is possible to generate localized adversarial noises that cover only 2% of the pixels in the image, none of them over the main object, and that are transferable across images and locations, and successfully fool a state-of-the-art Inception v3 model with very high success rates."},{"id":"66b84a1efd8eb787d5298664e8695b65.html","title":"Semantics as a foreign language","url":"https://www.aclweb.org/anthology/D18-1263.pdf","authors":["Gabriel Stanovsky","Ido Dagan"],"date":"2018","abstract":"We propose a novel approach to semantic dependency parsing (SDP) by casting the task as an instance of multi-lingual machine translation, where each semantic representation is a different foreign dialect. To that end, we first generalize syntactic linearization techniques to account for the richer semantic dependency graph structure. Following, we design a neural sequence-to-sequence framework which can effectively recover our graph linearizations, performing almost on-par with previous SDP state-of-the-art while requiring less parallel training annotations. Beyond SDP, our linearization technique opens the door to integration of graph-based semantic representations as features in neural models for downstream applications."},{"id":"90bbbc78edeed9f70532733d19519959.html","title":"Evaluating multiple system summary lengths: A case study","url":"https://www.aclweb.org/anthology/D18-1087.pdf","authors":["Ori Shapira","David Gabay","Hadar Ronen","Judit Bar-Ilan","Yael Amsterdamer","Ani Nenkova","Ido Dagan"],"date":"2018","abstract":"Practical summarization systems are expected to produce summaries of varying lengths, per user needs. While a couple of early summarization benchmarks tested systems across multiple summary lengths, this practice was mostly abandoned due to the assumed cost of producing reference summaries of multiple lengths. In this paper, we raise the research question of whether reference summaries of a single length can be used to reliably evaluate system summaries of multiple lengths. For that, we have analyzed a couple of datasets as a case study, using several variants of the ROUGE metric that are standard in summarization evaluation. Our findings indicate that the evaluation protocol in question is indeed competitive. This result paves the way to practically evaluating varying-length summaries with simple, possibly existing, summarization benchmarks."},{"id":"d2e17254af8ff2877abcc0eb00deb4a2.html","title":"The Natural Language Programming (NLPRO) Project: Turning Text into Executable Code.","url":"http://ceur-ws.org/Vol-2075/NLP4RE_paper10.pdf","authors":["Reut Tsarfaty"],"date":"2018","abstract":"In this paper we present the natural language programming (NLPRO) project (via ERC-StG-2015 grant 677352), where we strive to automatically translate requirements documents directly into the executable code of the systems they describe. To achieve this, we embrace the ambiguity of NL requirements and define a three-fold research agenda wherein we (i) formalize text-to-code translation as a structure prediction task,(ii) propose a formal semantic representation in terms of Live Sequence Charts (LSCs), and (iii) develop and comparatively evaluate novel sentence-based vs. discourse-based models for semantic parsing of requirements documents, and test their accuracy on various case studies. The empirical results of our first research cycle show that the discourse-based models consistently outperform the sentence-based models in constructing a system that reflects the requirements in the document. We conjecture that the formal representation of LSCs, the joint sentencediscourse modeling strategy, and the statistical learning component, are key ingredients for effectively tackling the NLPRO long-standing challenge."},{"id":"d6ed955be0496bc9fe40f314ac6d1cf5.html","title":"Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples","url":"http://proceedings.mlr.press/v80/weiss18a.html","authors":["Gail Weiss","Yoav Goldberg","Eran Yahav"],"date":"2017/11/27","journal":"arXiv preprint arXiv:1711.09576","abstract":"We present a novel algorithm that uses exact learning and abstraction to extract a deterministic finite automaton describing the state dynamics of a given trained RNN. We do this using Angluin\u2019s\\\\\\\\lstar algorithm as a learner and the trained RNN as an oracle. Our technique efficiently extracts accurate automata from trained RNNs, even when the state vectors are large and require fine differentiation."},{"id":"0597766b979c5e42d744585869c89f7c.html","title":"Crowdsourcing question-answer meaning representations","url":"https://arxiv.org/abs/1711.05885","authors":["Julian Michael","Gabriel Stanovsky","Luheng He","Ido Dagan","Luke Zettlemoyer"],"date":"2017/11/16","journal":"arXiv preprint arXiv:1711.05885","abstract":"We introduce Question-Answer Meaning Representations (QAMRs), which represent the predicate-argument structure of a sentence as a set of question-answer pairs. We also develop a crowdsourcing scheme to show that QAMRs can be labeled with very little training, and gather a dataset with over 5,000 sentences and 100,000 questions. A detailed qualitative analysis demonstrates that the crowd-generated question-answer pairs cover the vast majority of predicate-argument relationships in existing datasets (including PropBank, NomBank, QA-SRL, and AMR) along with many previously under-resourced ones, including implicit arguments and relations. The QAMR data and annotation code is made publicly available to enable future work on how best to model these complex phenomena."},{"id":"1ba3f1f79f9fc16c184cd11b1a4585c3.html","title":"Universal Dependencies 2.1","url":"https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1548?locale-attribute=cs","authors":["Joakim Nivre","\u017deljko Agi\u0107","Lars Ahrenberg","Lene Antonsen","Maria Jesus Aranzabe","Masayuki Asahara","Luma Ateyah","Mohammed Attia","Aitziber Atutxa","Liesbeth Augustinus","Elena Badmaeva","Miguel Ballesteros","Esha Banerjee","Sebastian Bank","Verginica Barbu Mititelu","John Bauer","Kepa Bengoetxea","Riyaz Ahmad Bhat","Eckhard Bick","Victoria Bobicev","Carl B\\\\xf6rstell","Cristina Bosco","Gosse Bouma","Sam Bowman","Aljoscha Burchardt","Marie Candito","Gauthier Caron","G\\\\xfcl\u015fen Cebiro\u011flu Eryi\u011fit","Giuseppe GA Celano","Savas Cetin","Fabricio Chalub","Jinho Choi","Silvie Cinkov\\\\xe1","\\\\xc7a\u011fr\u0131 \\\\xc7\\\\xf6ltekin","Miriam Connor","Elizabeth Davidson","Marie\u2010catherine Marneffe","Valeria Paiva","Arantza Ilarraza","Peter Dirix","Kaja Dobrovoljc","Timothy Dozat","Kira Droganova","Puneet Dwivedi","Marhaba Eli","Ali Elkahky","Toma\u017e Erjavec","Rich\\\\xe1rd Farkas","Hector Fernandez Alcalde","Jennifer Foster","Cl\\\\xe1udia Freitas","Katar\\\\xedna Gajdo\u0161ov\\\\xe1","Daniel Galbraith","Marcos Garcia","Moa G\\\\xe4rdenfors","Kim Gerdes","Filip Ginter","Iakes Goenaga","Koldo Gojenola","Memduh G\\\\xf6k\u0131rmak","Yoav Goldberg","Xavier G\\\\xf3mez Guinovart","Berta Gonz\\\\xe1les Saavedra","Matias Grioni","Normunds Gr\u016bz\u012btis","Bruno Guillaume","Nizar Habash","Jan Haji\u010d","Jan Haji\u010d Jr","Linh H\\\\xe0 M\u1ef9","Kim Harris","Dag Haug","Barbora Hladk\\\\xe1","Jaroslava Hlav\\\\xe1\u010dov\\\\xe1","Florinel Hociung","Petter Hohle","Radu Ion","Elena Irimia","Tom\\\\xe1\u0161 Jel\\\\xednek","Anders Johannsen","Fredrik J\\\\xf8rgensen","H\\\\xfcner Ka\u015f\u0131kara","Hiroshi Kanayama","Jenna Kanerva","Tolga Kayadelen","V\\\\xe1clava Kettnerov\\\\xe1","Jesse Kirchner","Natalia Kotsyba","Simon Krek","Veronika Laippala","Lorenzo Lambertino","Tatiana Lando","John Lee","Ph\u01b0\u01a1ng L\\\\xea H\u1ed3ng","Alessandro Lenci","Saran Lertpradit","Herman Leung","Cheuk Ying Li","Josie Li","Keying Li","Nikola Ljube\u0161i\u0107","Olga Loginova","Olga Lyashevskaya","Teresa Lynn","Vivien Macketanz","Aibek Makazhanov","Michael Mandl","Christopher Manning","C\u0103t\u0103lina M\u0103r\u0103nduc","David Mare\u010dek","Katrin Marheinecke","H\\\\xe9ctor Mart\\\\xednez Alonso","Andr\\\\xe9 Martins","Jan Ma\u0161ek","Yuji Matsumoto","Ryan Mcdonald","Gustavo Mendon\\\\xe7a","Niko Miekka","Anna Missil\\\\xe4","C\u0103t\u0103lin Mititelu","Yusuke Miyao","Simonetta Montemagni","Amir More","Laura Moreno Romero","Shinsuke Mori","Bohdan Moskalevskyi","Kadri Muischnek","Kaili M\\\\xfc\\\\xfcrisep","Pinkey Nainwani","Anna Nedoluzhko","Gunta Ne\u0161pore\u2010b\u0113rzkalne","L\u01b0\u01a1ng Nguy\u1ec5n Th\u1ecb","Huy\u1ec1n Nguy\u1ec5n Th\u1ecb Minh","Vitaly Nikolaev","Hanna Nurmi","Stina Ojala","Petya Osenova","Robert \\\\xd6stling","Lilja \\\\xd8vrelid","Elena Pascual","Marco Passarotti","Cenel\u2010augusto Perez","Guy Perrier","Slav Petrov","Jussi Piitulainen","Emily Pitler","Barbara Plank","Martin Popel","Lauma Pretkalni\u0146a","Prokopis Prokopidis"],"date":"2017/11/15","abstract":"Popis Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008)."},{"id":"d69c90172f6b36c4a5c6bb80397bd797.html","title":"Analysis of sentence embedding models using prediction tasks in natural language processing","url":"https://ieeexplore.ieee.org/abstract/document/8030297/","authors":["Yossi Adi","Einat Kermany","Yonatan Belinkov","Ofer Lavi","Yoav Goldberg"],"date":"2017/09/08","journal":"IBM Journal of Research and Development","abstract":""},{"id":"23a0d2f1e8e985d84e523af8378bc938.html","title":"Capturing Dependency Syntax with \u201cDeep\u201d Sequential Models","url":"https://ep.liu.se/konferensartikel.aspx?series=ecp&issue=139&Article_No=1","authors":["Yoav Goldberg"],"date":"2017/09","abstract":"Neural network (\u201cdeep learning\u201d) models are taking over machine learning approaches for language by storm. In particular, recurrent neural networks (RNNs), which are flexible non-markovian models of sequential data, were shown to be effective for a variety of language processing tasks. Somewhat surprisingly, these seemingly purely sequential models are very capable at modeling syntactic phenomena, and using them result in very strong dependency parsers, for a variety of languages. In this talk, I will briefly describe recurrent-networks, and present empirical evidence for their capabilities of learning the subject-verb agreement relation in naturally occuring text, from relatively indirect supervision. This part is based on my joint work with Tal Linzen and Emmanuel Dupoux. I will then describe bi-directional recurrent networks---a simple extension of recurrent networks---and show how they can be used as the\\\\xa0\u2026"},{"id":"3e59d5a82a25f0727855f1221002d7fe.html","title":"Interactive abstractive summarization for event news tweets","url":"https://www.aclweb.org/anthology/D17-2019.pdf","authors":["Ori Shapira","Hadar Ronen","Meni Adler","Yael Amsterdamer","Judit Bar-Ilan","Ido Dagan"],"date":"2017/09","abstract":"We present a novel interactive summarization system that is based on abstractive summarization, derived from a recent consolidated knowledge representation for multiple texts. We incorporate a couple of interaction mechanisms, providing a bullet-style summary while allowing to attain the most important information first and interactively drill down to more specific details. A usability study of our implementation, for event news tweets, suggests the utility of our approach for text exploration."},{"id":"1bc411ae87a402f0753e6a3a466c2ef6.html","title":"Universal Joint Morph-Syntactic Processing: The Open University of Israel\u2019s Submission to The CoNLL 2017 Shared Task","url":"https://www.aclweb.org/anthology/K17-3027.pdf","authors":["Amir More","Reut Tsarfaty"],"date":"2017/08","abstract":"We present the Open University\u2019s submission to the CoNLL 2017 Shared Task on multilingual parsing from raw text to Universal Dependencies. The core of our system is a joint morphological disambiguator and syntactic parser which accepts morphologically analyzed surface tokens as input and returns morphologically disambiguated dependency trees as output. Our parser requires a lattice as input, so we generate morphological analyses of surface tokens using a data-driven morphological analyzer that derives its lexicon from the UD training corpora, and we rely on UDPipe for sentence segmentation and surface-level tokenization. We report our official macro-average LAS is 56.56. Although our model is not as performant as many others, it does not make use of neural networks, therefore we do not rely on word embeddings or any other data source other than the corpora themselves. In addition, we show the utility of a lexicon-backed morphological analyzer for the MRL Modern Hebrew. We use our results on Modern Hebrew to argue that the UD community should define a UD-compatible standard for access to lexical resources, which we argue is crucial for MRLs and low resource languages in particular."},{"id":"fac68f1835a3cf3d20df4074867f1ae6.html","title":"Acquiring predicate paraphrases from news tweets","url":"https://www.aclweb.org/anthology/S17-1019.pdf","authors":["Vered Shwartz","Gabriel Stanovsky","Ido Dagan"],"date":"2017/08","abstract":"We present a simple method for ever-growing extraction of predicate paraphrases from news headlines in Twitter. Analysis of the output of ten weeks of collection shows that the accuracy of paraphrases with different support levels is estimated between 60-86%. We also demonstrate that our resource is to a large extent complementary to existing resources, providing many novel paraphrases. Our resource is publicly available, continuously expanding based on daily news."},{"id":"9c186de4d20b1fa07dd4fc0f4691c764.html","title":"A simple language model based on pmi matrix approximations","url":"https://arxiv.org/abs/1707.05266","authors":["Oren Melamud","Ido Dagan","Jacob Goldberger"],"date":"2017/07/17","journal":"arXiv preprint arXiv:1707.05266","abstract":"In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vec\'s algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings."},{"id":"3e6c159664cb666744f821d46e9703ff.html","title":"Controlling Linguistic Style Aspects in Neural Language Generation","url":"https://arxiv.org/abs/1707.02633","authors":["Jessica Ficler","Yoav Goldberg"],"date":"2017/07/09","journal":"arXiv preprint arXiv:1707.02633","abstract":"Most work on neural natural language generation (NNLG) focus on controlling the content of the generated text. We experiment with controlling several stylistic aspects of the generated text, in addition to its content. The method is based on conditioned RNN language model, where the desired content as well as the stylistic parameters serve as conditioning contexts. We demonstrate the approach on the movie reviews domain and show that it is successful in generating coherent sentences corresponding to the required linguistic style and content."},{"id":"635530fecedb1751200641a5fef8c20f.html","title":"Integrating deep linguistic features in factuality prediction over unified datasets","url":"https://www.aclweb.org/anthology/P17-2056.pdf","authors":["Gabriel Stanovsky","Judith Eckle-Kohler","Yevgeniy Puzikov","Ido Dagan","Iryna Gurevych"],"date":"2017/07","abstract":"Previous models for the assessment of commitment towards a predicate in a sentence (also known as factuality prediction) were trained and tested against a specific annotated dataset, subsequently limiting the generality of their results. In this work we propose an intuitive method for mapping three previously annotated corpora onto a single factuality scale, thereby enabling models to be tested across these corpora. In addition, we design a novel model for factuality prediction by first extending a previous rule-based factuality prediction system and applying it over an abstraction of dependency trees, and then using the output of this system in a supervised classifier. We show that this model outperforms previous methods on all three datasets. We make both the unified factuality corpus and our new model publicly available."},{"id":"7d38b73c1283ebb636cbd613750dad3c.html","title":"Exploring the Syntactic Abilities of RNNs with Multi-task Learning","url":"https://arxiv.org/abs/1706.03542","authors":["Emile Enguehard","Yoav Goldberg","Tal Linzen"],"date":"2017/06/12","journal":"arXiv preprint arXiv:1706.03542","abstract":"Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016). We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. We trained a single RNN to perform both the agreement task and an additional task, either CCG supertagging or language modeling. Multi-task training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syntactic representations than shown before. We also show that easily available agreement training data can improve performance on other syntactic tasks, in particular when only a limited amount of training data is available for those tasks. The multi-task paradigm can also be leveraged to inject grammatical knowledge into language models."},{"id":"dba4ab898ff36312caec5732dd097d9b.html","title":"Greedy transition-based dependency parsing with stack lstms","url":"https://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00285","authors":["Miguel Ballesteros","Chris Dyer","Yoav Goldberg","Noah A Smith"],"date":"2017/06","journal":"Computational Linguistics","abstract":"We introduce a greedy transition-based parser that learns to represent parser states using recurrent neural networks. Our primary innovation that enables us to do this efficiently is a new control structure for sequential neural networks\u2014the stack long short-term memory unit (LSTM). Like the conventional stack data structures used in transition-based parsers, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. Our model captures three facets of the parser\'s state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of transition actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. In addition, we compare two different word representations: (i) standard word vectors based on look-up tables\\\\xa0\u2026"},{"id":"58dbd903aff79525d80cdb2713032350.html","title":"Neural Network Methods for Natural Language Processing","url":"https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037","authors":["Yoav Goldberg"],"date":"2017/04/17","journal":"Synthesis Lectures on Human Language Technologies","abstract":"Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries."},{"id":"f0ddfe2e0edab00b3bc735f028c592a8.html","title":"Towards string-to-tree neural machine translation","url":"https://arxiv.org/abs/1704.04743","authors":["Roee Aharoni","Yoav Goldberg"],"date":"2017/04/16","journal":"arXiv preprint arXiv:1704.04743","abstract":"We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. An experiment on the WMT16 German-English news translation task resulted in an improved BLEU score when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A small-scale human evaluation also showed an advantage to the syntax-aware system."},{"id":"ef0576e7da6d1d8be457e93f4154ee5b.html","title":"The Interplay of Semantics and Morphology in Word Embeddings","url":"https://arxiv.org/abs/1704.01938","authors":["Oded Avraham","Yoav Goldberg"],"date":"2017/04/06","journal":"arXiv preprint arXiv:1704.01938","abstract":"We explore the ability of word embeddings to capture both semantic and morphological similarity, as affected by the different types of linguistic properties (surface form, lemma, morphological tag) used to compose the representation of each word. We train several models, where each uses a different subset of these properties to compose its representations. By evaluating the models on semantic and morphological measures, we reveal some useful insights on the relationship between semantics and morphology."},{"id":"822fd5f88e3ea1a01be041aac96f4434.html","title":"A consolidated open knowledge representation for multiple texts","url":"https://www.aclweb.org/anthology/W17-0902.pdf","authors":["Rachel Wities","Vered Shwartz","Gabriel Stanovsky","Meni Adler","Ori Shapira","Shyam Upadhyay","Dan Roth","Eugenio Mart\\\\xednez-C\\\\xe1mara","Iryna Gurevych","Ido Dagan"],"date":"2017/04","abstract":"We propose to move from Open Information Extraction (OIE) ahead to Open Knowledge Representation (OKR), aiming to represent information conveyed jointly in a set of texts in an open text-based manner. We do so by consolidating OIE extractions using entity and predicate coreference, while modeling information containment between coreferring elements via lexical entailment. We suggest that generating OKR structures can be a useful step in the NLP pipeline, to give semantic applications an easy handle on consolidated information across multiple texts."},{"id":"b08a5db0e8f02c8a16bfed576eedd8b7.html","title":"Discourse Relations and Conjoined VPs: Automated Sense Recognition","url":"https://www.aclweb.org/anthology/E17-4004.pdf","authors":["Valentina Pyatkin","Bonnie Webber"],"date":"2017/04","abstract":"Sense classification of discourse relations is a sub-task of shallow discourse parsing. Discourse relations can occur both across sentences (inter-sentential) and within sentences (intra-sentential), and more than one discourse relation can hold between the same units. Using a newly available corpus of discourse-annotated intra-sentential conjoined verb phrases, we demonstrate a sequential classification pipeline for their multi-label sense classification. We assess the importance of each feature used in the classification, the feature scope, and what is lost in moving from gold standard manual parses to the output of an off-the-shelf parser."},{"id":"2c3af6251160105907ac68faf1da492e.html","title":"Improving a Strong Neural Parser with Conjunction-Specific Features","url":"https://arxiv.org/abs/1702.06733","authors":["Jessica Ficler","Yoav Goldberg"],"date":"2017/02/22","journal":"arXiv preprint arXiv:1702.06733","abstract":"While dependency parsers reach very high overall accuracy, some dependency relations are much harder than others. In particular, dependency parsers perform poorly in coordination construction (ie, correctly attaching the\\" conj\\" relation). We extend a state-of-the-art dependency parser with conjunction-specific features, focusing on the similarity between the conjuncts head words. Training the extended parser yields an improvement in\\" conj\\" attachment as well as in overall dependency parsing accuracy on the Stanford dependency conversion of the Penn TreeBank."},{"id":"7f8db3ae3290c9531048a2a2c4581980.html","title":"DyNet: The Dynamic Neural Network Toolkit","url":"https://arxiv.org/abs/1701.03980","authors":["Graham Neubig","Chris Dyer","Yoav Goldberg","Austin Matthews","Waleed Ammar","Antonios Anastasopoulos","Miguel Ballesteros","David Chiang","Daniel Clothiaux","Trevor Cohn","Kevin Duh","Manaal Faruqui","Cynthia Gan","Dan Garrette","Yangfeng Ji","Lingpeng Kong","Adhiguna Kuncoro","Gaurav Kumar","Chaitanya Malaviya","Paul Michel","Yusuke Oda","Matthew Richardson","Naomi Saphra","Swabha Swayamdipta","Pengcheng Yin"],"date":"2017/01/15","journal":"arXiv preprint arXiv:1701.03980","abstract":"We describe DyNet, a toolkit for implementing neural network models based on dynamic declaration of network structure. In the static declaration strategy that is used in toolkits like Theano, CNTK, and TensorFlow, the user first defines a computation graph (a symbolic representation of the computation), and then examples are fed into an engine that executes this computation and computes its derivatives. In DyNet\'s dynamic declaration strategy, computation graph construction is mostly transparent, being implicitly constructed by executing procedural code that computes the network outputs, and the user is free to use different network structures for each input. Dynamic declaration thus facilitates the implementation of more complicated network architectures, and DyNet is specifically designed to allow users to implement their models in a way that is idiomatic in their preferred programming language (C++ or Python). One challenge with dynamic declaration is that because the symbolic computation graph is defined anew for every training example, its construction must have low overhead. To achieve this, DyNet has an optimized C++ backend and lightweight graph representation. Experiments show that DyNet\'s speeds are faster than or comparable with static declaration toolkits, and significantly faster than Chainer, another dynamic declaration toolkit. DyNet is released open-source under the Apache 2.0 license and available at this http URL."},{"id":"0b252d2e8c3bed7ff58a951f6ffb7321.html","title":"On-the-fly operation batching in dynamic computation graphs","url":"http://papers.nips.cc/paper/6986-on-the-fly-operation-batching-in-dynamic-computation-graphs","authors":["Graham Neubig","Yoav Goldberg","Chris Dyer"],"date":"2017","abstract":"Dynamic neural networks toolkits such as PyTorch, DyNet, and Chainer offer more flexibility for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (eg, TensorFlow, CNTK, and Theano). However, existing toolkits-both static and dynamic-require that the developer organize the computations into the batches necessary for exploiting high-performance data-parallel algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, in computationally efficient batches. On a variety of tasks, we obtain throughput similar to manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually."},{"id":"1a08dc42826af15f1c33facc3fcc4626.html","title":"Neural disambiguation of causal lexical markers based on context","url":"https://www.aclweb.org/anthology/W17-6927.pdf","authors":["Eugenio Mart\\\\xednez-C\\\\xe1mara","Vered Shwartz","Iryna Gurevych","Ido Dagan"],"date":"2017","abstract":"Causation is a psychological tool of humans to understand the world and it is projected in natural language. Causation relates two events, so in order to understand the causal relation of those events and the causal reasoning of humans, the study of causality classification is required. We claim that the use of linguistic features may restrict the representation of causality, and dense vector spaces can provide a better encoding of the causal meaning of an utterance. Herein, we propose a neural network architecture only fed with word embeddings for the task of causality classification. Our results show that our claim holds, and we outperform the state-of-the-art on the AltLex corpus. The source code of our experiments is publicly available. 1"},{"id":"561a50f75224d3193420a15e2036511e.html","title":"Lingpeng Kong, Adhiguna Kuncoro, Gaurav Kumar, Chaitanya Malaviya, Paul Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha Swayamdipta, and Pengcheng Yin. 2017. Dynet\\\\xa0\u2026","url":"http://scholar.google.com/scholar?cluster=621847681578506086&hl=en&oi=scholarr","authors":["Graham Neubig","Chris Dyer","Yoav Goldberg","Austin Matthews","Waleed Ammar","Antonios Anastasopoulos","Miguel Ballesteros","David Chiang","Daniel Clothiaux","Trevor Cohn","Kevin Duh","Manaal Faruqui","Cynthia Gan","Dan Garrette","Yangfeng Ji"],"date":"2017","journal":"arXiv preprint arXiv:1701.03980"},{"id":"87917c098f7cc906640e2b541f793e93.html","title":"Morphological Inflection Generation with Hard Monotonic Attention","url":"https://arxiv.org/abs/1611.01487","authors":["Roee Aharoni","Yoav Goldberg","Israel Ramat-Gan"],"date":"2017","journal":"Proceedings of ACL. https://arxiv. org/abs/1611.01487","abstract":"We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft attention\\\\\\\\cite {bahdanauCB14} models for the task, shedding some light on the features such models extract."},{"id":"abd6de4d9aece33e1820bedfc46b1d21.html","title":"The recognizing textual entailment challenges: Datasets and methodologies","url":"https://link.springer.com/chapter/10.1007/978-94-024-0881-2_42","authors":["Luisa Bentivogli","Ido Dagan","Bernardo Magnini"],"date":"2017","abstract":"While semantic inference has always been a major focus in Computational Linguistics, the topic has benefited of new attention in the field thanks to the Recognizing Textual Entailment (RTE) framework, first launched in 2004, which has provided an operational definition of entailment based on human judgements over portions of text. On top of such definition, a task has been designed, which includes both guidelines for dataset annotation and evaluation metrics for assessing systems\u2019 performance. This chapter presents the successful experience of creating Textual Entailment datasets. We show how, during the years, RTE datasets have been developed in several variants, not only to address complex phenomena underlying entailment, but also to demonstrate the potential application of entailment inference into concrete scenarios, including summarization, knowledge base population, answer validation for\\\\xa0\u2026"},{"id":"af0513286e02ffbdfac8a5502ba91dc2.html","title":"From Raw Text to Universal Dependencies-Look, No Tags!","url":"https://www.aclweb.org/anthology/K17-3022.pdf","authors":["Miryam de Lhoneux","Yan Shao","Ali Basirat","Eliyahu Kiperwasser","Sara Stymne","Yoav Goldberg","Joakim Nivre"],"date":"2017","journal":"Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies","abstract":"We present the Uppsala submission to the CoNLL 2017 shared task on parsing from raw text to universal dependencies. Our system is a simple pipeline consisting of two components. The first performs joint word and sentence segmentation on raw text; the second predicts dependency trees from raw words. The parser bypasses the need for part-of-speech tagging, but uses word embeddings based on universal tag distributions. We achieved a macro-averaged LAS F1 of 65.11 in the official test run, which improved to 70.49 after bug fixes. We obtained the 2nd best result for sentence segmentation with a score of 89.03."},{"id":"e00570c3cac9997af6201418199dacb9.html","title":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP","url":"https://www.aclweb.org/anthology/W17-5300.pdf","authors":["Samuel Bowman","Yoav Goldberg","Felix Hill","Angeliki Lazaridou","Omer Levy","Roi Reichart","Anders S\\\\xf8gaard"],"date":"2017","journal":"Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for NLP","abstract":"This workshop deals with the evaluation of general-purpose vector representations for linguistic units (morphemes, words, phrases, sentences, etc). What distinguishes these representations (or embeddings) is that they are not trained with a specific application in mind, but rather to capture broadly useful features of the represented units. Another way to view their usage is through the lens of transfer learning: The embeddings are trained with one objective, but applied on others."},{"id":"fd25fed5796983cd4473cdb9553fc536.html","title":"Data-driven broad-coverage grammars for opinionated natural language generation (onlg)","url":"https://repository.ubn.ru.nl/bitstream/handle/2066/179462/179462pub.pdf","authors":["Tomer Cagan","Stefan L Frank","Reut Tsarfaty"],"date":"2017","abstract":"Opinionated natural language generation (ONLG) is a new, challenging, NLG task in which we aim to automatically generate human-like, subjective, responses to opinionated articles online. We present a data-driven architecture for ONLG that generates subjective responses triggered by users\u2019 agendas, based on automatically acquired wide-coverage generative grammars. We compare three types of grammatical representations that we design for ONLG. The grammars interleave different layers of linguistic information, and are induced from a new, enriched dataset we developed. Our evaluation shows that generation with Relational-Realizational (Tsarfaty and Sima\u2019an, 2008) inspired grammar gets better language model scores than lexicalized grammarsa la Collins (2003), and that the latter gets better humanevaluation scores. We also show that conditioning the generation on topic models makes generated responses more relevant to the document content."},{"id":"4c8f011e76ec3c29d8e3026c7ca1fb19.html","title":"Semiautomatic construction of cross-period thesaurus","url":"https://dl.acm.org/doi/abs/10.1145/2994151","authors":["Chaya Liebeskind","Ido Dagan","Jonathan Schler"],"date":"2016/12/19","journal":"Journal on Computing and Cultural Heritage (JOCCH)","abstract":"A cross-period (diachronic) thesaurus enables users to search for information using modern terminology and obtain semantically related terms from earlier historical periods. The complex task of supporting the construction of a diachronic thesaurus by a domain expert lexicographer has hardly been addressed computationally until now. In this article, we introduce a semiautomatic iterative Query Expansion (QE) scheme for supporting diachronic thesaurus construction, which identifies candidate related terms based on statistical corpus-based measures. We use ancient-modern period classification to increase the performance of the statistical cooccurrence measures and extend our methods to deal with <i>Multi-Word Expressions</i> (MWEs). We demonstrate the empirical benefit of our scheme for a Jewish cross-period thesaurus and evaluate its impact on recall and on the effectiveness of the lexicographer\u2019s manual efforts."},{"id":"5612ce7247231fcf6009b26236e85546.html","title":"Semi supervised preposition-sense disambiguation using multilingual data","url":"https://www.aclweb.org/anthology/C16-1256.pdf","authors":["Hila Gonen","Yoav Goldberg"],"date":"2016/12","abstract":"Prepositions are very common and very ambiguous, and understanding their sense is critical for understanding the meaning of the sentence. Supervised corpora for the preposition-sense disambiguation task are small, suggesting a semi-supervised approach to the task. We show that signals from unannotated multilingual data can be used to improve supervised preposition-sense disambiguation. Our approach pre-trains an LSTM encoder for predicting the translation of a preposition, and then incorporates the pre-trained encoder as a component in a supervised classification system, and fine-tunes it for the task. The multilingual signals consistently improve results on two preposition-sense datasets."},{"id":"61d02d04db4f7d51ca2cbe0f39005bda.html","title":"Simple and accurate dependency parsing using bidirectional LSTM feature representations","url":"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00101","authors":["Eliyahu Kiperwasser","Yoav Goldberg"],"date":"2016/12","journal":"Transactions of the Association for Computational Linguistics","abstract":"We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese."},{"id":"7050c1c99123b8aea8ded93fa66310c9.html","title":"Easy-first dependency parsing with hierarchical tree LSTMs","url":"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00110","authors":["Eliyahu Kiperwasser","Yoav Goldberg"],"date":"2016/12","journal":"Transactions of the Association for Computational Linguistics","abstract":"We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving very strong accuracies for English and Chinese, without relying on external word embeddings. The parser\u2019s implementation is available for download at the first author\u2019s webpage."},{"id":"bf8b392308d588cf381497a20da0ac18.html","title":"Modeling extractive sentence intersection via subtree entailment","url":"https://www.aclweb.org/anthology/C16-1272.pdf","authors":["Omer Levy","Ido Dagan","Gabriel Stanovsky","Judith Eckle-Kohler","Iryna Gurevych"],"date":"2016/12","abstract":"Sentence intersection captures the semantic overlap of two texts, generalizing over paradigms such as textual entailment and semantic text similarity. Despite its modeling power, it has received little attention because it is difficult for non-experts to annotate. We analyze 200 pairs of similar sentences and identify several underlying properties of sentence intersection. We leverage these insights to design an algorithm that decomposes the sentence intersection task into several simpler annotation tasks, facilitating the construction of a high quality dataset via crowdsourcing. We implement this approach and provide an annotated dataset of 1,764 sentence intersections."},{"id":"cfac7fee6bf1a683658c4f6b9f8786d9.html","title":"Data-driven morphological analysis and disambiguation for morphologically rich languages and universal dependencies","url":"https://www.aclweb.org/anthology/C16-1033.pdf","authors":["Amir More","Reut Tsarfaty"],"date":"2016/12","abstract":"Parsing texts into universal dependencies (UD) in realistic scenarios requires infrastructure for the morphological analysis and disambiguation (MA&D) of typologically different languages as a first tier. MA&D is particularly challenging in morphologically rich languages (MRLs), where the ambiguous space-delimited tokens ought to be disambiguated with respect to their constituent morphemes, each morpheme carrying its own tag and a rich set features. Here we present a novel, language-agnostic, framework for MA&D, based on a transition system with two variants\u2014word-based and morpheme-based\u2014and a dedicated transition to mitigate the biases of variable-length morpheme sequences. Our experiments on a Modern Hebrew case study show state of the art results, and we show that the morpheme-based MD consistently outperforms our word-based variant. We further illustrate the utility and multilingual coverage of our framework by morphologically analyzing and disambiguating the large set of languages in the UD treebanks."},{"id":"fdb3390c52a3dd222fc883df64322fc0.html","title":"Assessing the ability of LSTMs to learn syntax-sensitive dependencies","url":"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00115","authors":["Tal Linzen","Emmanuel Dupoux","Yoav Goldberg"],"date":"2016/12","journal":"Transactions of the Association for Computational Linguistics","abstract":"The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture\u2019s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted\\\\xa0\u2026"},{"id":"4fddef8b611b40b2ddecf5d38a16579e.html","title":"Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure","url":"https://arxiv.org/abs/1611.03641","authors":["Oded Avraham","Yoav Goldberg"],"date":"2016/11/11","journal":"arXiv preprint arXiv:1611.03641","abstract":"We suggest a new method for creating and using gold-standard datasets for word similarity evaluation. Our goal is to improve the reliability of the evaluation, and we do this by redesigning the annotation task to achieve higher inter-rater agreement, and by defining a performance measure which takes the reliability of each annotation decision in the dataset into account."},{"id":"50dd47cebbb2014bda7d43be302a369d.html","title":"Sequence to Sequence Transduction with Hard Monotonic Attention","url":"https://openreview.net/forum?id=HkyYqU9lx","authors":["Roee Aharoni","Yoav Goldberg"],"date":"2016/11/04","journal":"arXiv preprint arXiv:1611.01487","abstract":"We present a supervised sequence to sequence transduction model with a hard attention mechanism which combines the more traditional statistical alignment methods with the power of recurrent neural networks. We evaluate the model on the task of morphological inflection generation and show that it provides state of the art results in various setups compared to the previous neural and non-neural approaches. Eventually we present an analysis of the learned representations for both hard and soft attention models, shedding light on the features such models extract in order to solve the task."},{"id":"56168c4831e2d56bac388ac3728b6420.html","title":"Creating a large benchmark for open information extraction","url":"https://www.aclweb.org/anthology/D16-1252.pdf","authors":["Gabriel Stanovsky","Ido Dagan"],"date":"2016/11","abstract":"Open information extraction (Open IE) was presented as an unrestricted variant of traditional information extraction. It has been gaining substantial attention, manifested by a large number of automatic Open IE extractors and downstream applications. In spite of this broad attention, the Open IE task definition has been lacking\u2013there are no formal guidelines and no large scale gold standard annotation. Subsequently, the various implementations of Open IE resorted to small scale posthoc evaluations, inhibiting an objective and reproducible cross-system comparison. In this work, we develop a methodology that leverages the recent QA-SRL annotation to create a first independent and large scale Open IE annotation, 1 and use it to automatically compare the most prominent Open IE systems."},{"id":"7c03be62f4f86e343924fb5691bf996e.html","title":"Porting an open information extraction system from english to german","url":"https://www.aclweb.org/anthology/D16-1086.pdf","authors":["Tobias Falke","Gabriel Stanovsky","Iryna Gurevych","Ido Dagan"],"date":"2016/11","abstract":"Many downstream NLP tasks can benefit from Open Information Extraction (Open IE) as a semantic representation. While Open IE systems are available for English, many other languages lack such tools. In this paper, we present a straightforward approach for adapting PropS, a rule-based predicate-argument analysis for English, to a new language, German. With this approach, we quickly obtain an Open IE system for German covering 89% of the English rule set. It yields 1.6 n-ary extractions per sentence at 60% precision, making it comparable to systems for English and readily usable in downstream applications. 1"},{"id":"83402dede2e26ad9f97a4e621ef04dcc.html","title":"Practical Neural Networks for NLP: From Theory to Code","url":"https://www.aclweb.org/anthology/papers/D/D16/D16-2001/","authors":["Chris Dyer","Yoav Goldberg","Graham Neubig"],"date":"2016/11","abstract":"This tutorial aims to bring NLP researchers up to speed with the current techniques in deep learning and neural networks, and show them how they can turn their ideas into practical implementations. We will start with simple classification models (logistic regression and multilayer perceptrons) and cover more advanced patterns that come up in NLP such as recurrent networks for sequence tagging and prediction problems, structured networks (eg, compositional architectures based on syntax trees), structured output spaces (sequences and trees), attention for sequence-to-sequence transduction, and feature induction for complex algorithm states. A particular emphasis will be on learning to represent complex objects as recursive compositions of simpler objects. This representation will reflect characterize standard objects in NLP, such as the composition of characters and morphemes into words, and words into sentences and documents. In addition, new opportunities such as learning to embed\\" algorithm states\\" such as those used in transition-based parsing and other sequential structured prediction models (for which effective features may be difficult to engineer by hand) will be covered. Everything in the tutorial will be grounded in code\u2014we will show how to program seemingly complex neural-net models using toolkits based on the computation-graph formalism. Computation graphs decompose complex computations into a DAG, with nodes representing inputs, target outputs, parameters, or (sub) differentiable functions (eg,\\" tanh\\",\\" matrix multiply\\", and\\" softmax\\"), and edges represent data dependencies. These graphs can be run\\" forward\\" to\\\\xa0\u2026"},{"id":"afb3e30acd26c014d7dc3d76d60c0152.html","title":"Cogalex-v shared task: Lexnet-integrated path-based and distributional method for the identification of semantic relations","url":"https://arxiv.org/abs/1610.08694","authors":["Vered Shwartz","Ido Dagan"],"date":"2016/10/27","journal":"arXiv preprint arXiv:1610.08694","abstract":"We present a submission to the CogALex 2016 shared task on the corpus-based identification of semantic relations, using LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. The reported results in the shared task bring this submission to the third place on subtask 1 (word relatedness), and the first place on subtask 2 (semantic relation classification), demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing concrete semantic relations. Combined with a common similarity measure, LexNET performs fairly good on the word relatedness task (subtask 1). The relatively low performance of LexNET and all other systems on subtask 2, however, confirms the difficulty of the semantic relation classification task, and stresses the need to develop additional methods for this task."},{"id":"e4f4c39cfa17e3b92b4598b1541b920f.html","title":"A Neural Network for Coordination Boundary Prediction","url":"https://arxiv.org/abs/1610.03946","authors":["Jessica Ficler","Yoav Goldberg"],"date":"2016/10/13","journal":"arXiv preprint arXiv:1610.03946","abstract":"We propose a neural-network based model for coordination boundary prediction. The network is designed to incorporate two signals: the similarity between conjuncts and the observation that replacing the whole coordination phrase with a conjunct tends to produce a coherent sentences. The modeling makes use of several LSTM networks. The model is trained solely on conjunction annotations in a Treebank, without using external resources. We show improvements on predicting coordination boundaries on the PTB compared to two state-of-the-art parsers; as well as improvement over previous coordination boundary prediction systems on the Genia corpus."},{"id":"5160ec58a9933e6a3bcfc327e3f3b4c2.html","title":"PMI matrix approximations with applications to neural language modeling","url":"https://arxiv.org/abs/1609.01235","authors":["Oren Melamud","Ido Dagan","Jacob Goldberger"],"date":"2016/09/05","journal":"arXiv preprint arXiv:1609.01235","abstract":"The negative sampling (NEG) objective function, used in word2vec, is a simplification of the Noise Contrastive Estimation (NCE) method. NEG was found to be highly effective in learning continuous word representations. However, unlike NCE, it was considered inapplicable for the purpose of learning the parameters of a language model. In this study, we refute this assertion by providing a principled derivation for NEG-based language modeling, founded on a novel analysis of a low-dimensional approximation of the matrix of pointwise mutual information between the contexts and the predicted words. The obtained language modeling is closely related to NCE language models but is based on a simplified objective function. We thus provide a unified formulation for two main language processing tasks, namely word embedding and language modeling, based on the NEG objective function. Experimental results on two popular language modeling benchmarks show comparable perplexity results, with a small advantage to NEG over NCE."},{"id":"083be3210e45be510bc54d472a3719dc.html","title":"Reconsidering Cross-lingual Word Embeddings","url":"http://scholar.google.com/scholar?cluster=14857846587367814311&hl=en&oi=scholarr","authors":["Omer Levy","Anders S\\\\xf8gaard","Yoav Goldberg"],"date":"2016/08/18","journal":"arXiv preprint arXiv:1608.05426","abstract":"While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remains vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of benchmarks. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, is an appealing approach for\\\\xa0\u2026"},{"id":"0afaaece9fc41eecccccb7d105d41158.html","title":"Path-based vs. distributional information in recognizing lexical semantic relations","url":"https://arxiv.org/abs/1608.05014","authors":["Vered Shwartz","Ido Dagan"],"date":"2016/08/17","journal":"arXiv preprint arXiv:1608.05014","abstract":"Recognizing various semantic relations between terms is beneficial for many NLP tasks. While path-based and distributional information sources are considered complementary for this task, the superior results the latter showed recently suggested that the former\'s contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. The empirical results show that this method is effective in the multiclass setting as well. We further show that the path-based information source always contributes to the classification, and analyze the cases in which it mostly complements the distributional information."},{"id":"cd04754ef59f9fb378cb7f86c4ccabd4.html","title":"Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks","url":"https://arxiv.org/abs/1608.04207","authors":["Yossi Adi","Einat Kermany","Yonatan Belinkov","Ofer Lavi","Yoav Goldberg"],"date":"2016/08/15","journal":"arXiv preprint arXiv:1608.04207","abstract":"There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector\'s dimensionality on the resulting representations."},{"id":"1dc97fe8e1e28622e52b11dbf208648c.html","title":"Improving sequence to sequence learning for morphological inflection generation: The BIU-MIT systems for the SIGMORPHON 2016 shared task for morphological reinflection","url":"https://www.aclweb.org/anthology/W16-2007.pdf","authors":["Roee Aharoni","Yoav Goldberg","Yonatan Belinkov"],"date":"2016/08","abstract":"Morphological reinflection is the task of generating a target form given a source form and the morpho-syntactic attributes of the target (and, optionally, of the source). This work presents the submission of Bar Ilan University and the Massachusetts Institute of Technology for the morphological reinflection shared task held at SIGMORPHON 2016. The submission includes two recurrent neural network architectures for learning morphological reinflection from incomplete inflection tables while using several novel ideas for this task: morpho-syntactic attribute embeddings, modeling the concept of templatic morphology, bidirectional input character representations and neural discriminative string transduction. The reported results for the proposed models over the ten languages in the shared task bring this submission to the second/third place (depending on the language) on all three sub-tasks out of eight participating teams, while training only on the Restricted category data."},{"id":"51fcbb29938b1e41d56ae85795d2a468.html","title":"Annotating and predicting non-restrictive noun phrase modifications","url":"https://www.aclweb.org/anthology/P16-1119.pdf","authors":["Gabriel Stanovsky","Ido Dagan"],"date":"2016/08","abstract":"The distinction between restrictive and non-restrictive modification in noun phrases is a well studied subject in linguistics. Automatically identifying non-restrictive modifiers can provide NLP applications with shorter, more salient arguments, which were found beneficial by several recent works. While previous work showed that restrictiveness can be annotated with high agreement, no large scale corpus was created, hindering the development of suitable classification algorithms. In this work we devise a novel crowdsourcing annotation methodology, and an accompanying large scale corpus. Then, we present a robust automated system which identifies non-restrictive modifiers, notably improving over prior methods."},{"id":"55d5ca8a80cf23345d45bdeee0122eae.html","title":"Specifying and annotating reduced argument span via qa-srl","url":"https://www.aclweb.org/anthology/P16-2077.pdf","authors":["Gabriel Stanovsky","Ido Dagan","Meni Adler"],"date":"2016/08","abstract":"Prominent semantic annotations take an inclusive approach to argument span annotation, marking arguments as full constituency subtrees. Some works, however, showed that identifying a reduced argument span can be beneficial for various semantic tasks. While certain practical methods do extract reduced argument spans, such as in Open-IE, these solutions are often ad-hoc and system-dependent, with no commonly accepted standards. In this paper we propose a generic argument reduction criterion, along with an annotation procedure, and show that it can be consistently and intuitively annotated using the recent QA-SRL paradigm."},{"id":"5e0e0f29ec180585ba119b6f101b41e1.html","title":"context2vec: Learning generic context embedding with bidirectional lstm","url":"https://www.aclweb.org/anthology/K16-1006.pdf","authors":["Oren Melamud","Jacob Goldberger","Ido Dagan"],"date":"2016/08","abstract":"Context representations are central to various NLP tasks, such as word sense disambiguation, named entity recognition, coreference resolution, and many more. In this work we present a neural model for efficiently learning a generic context embedding function from large corpora, using bidirectional LSTM. With a very simple application of our context representations, we manage to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word embeddings. We release our code and pretrained models, suggesting they could be useful in a wide variety of NLP tasks."},{"id":"a83698eea977b04862a98b2e3dbb0bf0.html","title":"A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments","url":"https://arxiv.org/abs/1608.05426","authors":["Omer Levy","Anders S\\\\xf8gaard","Yoav Goldberg"],"date":"2016/08","journal":"arXiv preprint arXiv:1608.05426","abstract":"While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remain vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment algorithms, such as IBM Model-1, which demonstrate similar performance to state-of-the-art embedding algorithms on a variety of benchmarks. Overall, we observe that different algorithmic approaches for utilizing the sentence ID feature space result in similar performance. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account."},{"id":"aded7b6453938f3ef1195a357f268014.html","title":"Annotating relation inference in context via question answering","url":"https://www.aclweb.org/anthology/P16-2041.pdf","authors":["Omer Levy","Ido Dagan"],"date":"2016/08","abstract":"We present a new annotation method for collecting data on relation inference in context. We convert the inference task to one of simple factoid question answering, allowing us to easily scale up to 16,000 high-quality examples. Our method corrects a major bias in previous evaluations, making our dataset much more realistic."},{"id":"cacee9ce2a9dfb6c3ab9939c313696b6.html","title":"Deep multi-task learning with low level tasks supervised at lower layers","url":"https://www.aclweb.org/anthology/P16-2038.pdf","authors":["Anders S\\\\xf8gaard","Yoav Goldberg"],"date":"2016/08","abstract":"In all previous work on deep multi-task learning we are aware of, all task supervisions are on the same (outermost) layer. We present a multi-task learning architecture with deep bi-directional RNNs, where different tasks supervision can happen at different layers. We present experiments in syntactic chunking and CCG supertagging, coupled with the additional task of POS-tagging. We show that it is consistently better to have POS supervision at the innermost rather than the outermost layer. We argue that this is because \u201clowlevel\u201d tasks are better kept at the lower layers, enabling the higher-level tasks to make use of the shared representation of the lower-level tasks. Finally, we also show how this architecture can be used for domain adaptation."},{"id":"d19cc96c26d29ee2cbac4a87c430a32b.html","title":"Adding context to semantic data-driven paraphrasing","url":"https://www.aclweb.org/anthology/S16-2013.pdf","authors":["Vered Shwartz","Ido Dagan"],"date":"2016/08","abstract":"Recognizing lexical inferences between pairs of terms is a common task in NLP applications, which should typically be performed within a given context. Such context-sensitive inferences have to consider both term meaning in context as well as the fine-grained relation holding between the terms. Hence, to develop suitable lexical inference methods, we need datasets that are annotated with fine-grained semantic relations in-context. Since existing datasets either provide outof-context annotations or refer to coarsegrained relations, we propose a methodology for adding context-sensitive annotations. We demonstrate our methodology by applying it to phrase pairs from PPDB 2.0, creating a novel dataset of finegrained lexical inferences in-context and showing its utility in developing contextsensitive methods."},{"id":"e9d55a18cc7916e84a7452ee80a4b545.html","title":"The roles of pathbased and distributional information in recognizing lexical semantic relations","url":"http://scholar.google.com/scholar?cluster=15843396639655447207&hl=en&oi=scholarr","authors":["Vered Shwartz","Ido Dagan"],"date":"2016/08","journal":"CoRR, abs/1608.05014","abstract":"Recognizing various semantic relations between terms is crucial for many NLP tasks. While path-based and distributional information sources are considered complementary, the strong results the latter showed on recent datasets suggested that the former\u2019s contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. We demonstrate that these two information sources are indeed complementary, and analyze the contributions of each source."},{"id":"ad80f56895c00a5bcd6ea378c25922d6.html","title":"Coordination Annotation Extension in the Penn Tree Bank","url":"https://arxiv.org/abs/1606.02529","authors":["Jessica Ficler","Yoav Goldberg"],"date":"2016/06/08","journal":"arXiv preprint arXiv:1606.02529","abstract":"Coordination is an important and common syntactic construction which is not handled well by state of the art parsers. Coordinations in the Penn Treebank are missing internal structure in many cases, do not include explicit marking of the conjuncts and contain various errors and inconsistencies. In this work, we initiated manual annotation process for solving these issues. We identify the different elements in a coordination phrase and label each element with its function. We add phrase boundaries when these are missing, unify inconsistencies, and fix errors. The outcome is an extension of the PTB that includes consistent and detailed structures for coordinations. We make the coordination annotation publicly available, in hope that they will facilitate further research into coordination disambiguation."},{"id":"7afeec32c3749f5544f22d221a250c85.html","title":"Improved Parsing for Argument-Clusters Coordination","url":"https://arxiv.org/abs/1606.00294","authors":["Jessica Ficler","Yoav Goldberg"],"date":"2016/06/01","journal":"arXiv preprint arXiv:1606.00294","abstract":"Syntactic parsers perform poorly in prediction of Argument-Cluster Coordination (ACC). We change the PTB representation of ACC to be more suitable for learning by a statistical PCFG parser, affecting 125 trees in the training set. Training on the modified trees yields a slight improvement in EVALB scores on sections 22 and 23. The main evaluation is on a corpus of 4th grade science exams, in which ACC structures are prevalent. On this corpus, we obtain an impressive x2. 7 improvement in recovering ACC structures compared to a parser trained on the original PTB trees."},{"id":"14767cdc9040ff0112915cb824ea1d1e.html","title":"The negochat corpus of human-agent negotiation dialogues","url":"https://www.aclweb.org/anthology/L16-1501.pdf","authors":["Vasily Konovalov","Ron Artstein","Oren Melamud","Ido Dagan"],"date":"2016/05","abstract":"Annotated in-domain corpora are crucial to the successful development of dialogue systems of automated agents, and in particular for developing natural language understanding (NLU) components of such systems. Unfortunately, such important resources are scarce. In this work, we introduce an annotated natural language human-agent dialogue corpus in the negotiation domain. The corpus was collected using Amazon Mechanical Turk following the \u2018Wizard-Of-Oz\u2019approach, where a \u2018wizard\u2019human translates the participants\u2019 natural language utterances in real time into a semantic language. Once dialogue collection was completed, utterances were annotated with intent labels by two independent annotators, achieving high inter-annotator agreement. Our initial experiments with an SVM classifier show that automatically inferring such labels from the utterances is far from trivial. We make our corpus publicly available to serve as an aid in the development of dialogue systems for negotiation agents, and suggest that analogous corpora can be created following our methodology and using our available source code. To the best of our knowledge this is the first publicly available negotiation dialogue corpus."},{"id":"3e2de6f4ba5f4ccdaa91411229ea2948.html","title":"Universal dependencies v1: A multilingual treebank collection","url":"https://www.aclweb.org/anthology/L16-1262.pdf","authors":["Joakim Nivre","Marie-Catherine De Marneffe","Filip Ginter","Yoav Goldberg","Jan Hajic","Christopher D Manning","Ryan McDonald","Slav Petrov","Sampo Pyysalo","Natalia Silveira","Reut Tsarfaty","Daniel Zeman"],"date":"2016/05","abstract":"Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages."},{"id":"2c3028a6234639303d14f67121ddfe42.html","title":"Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss","url":"https://arxiv.org/abs/1604.05529","authors":["Barbara Plank","Anders S\\\\xf8gaard","Yoav Goldberg"],"date":"2016/04/19","journal":"arXiv preprint arXiv:1604.05529","abstract":"Bidirectional long short-term memory (bi-LSTM) networks have recently proven successful for various NLP sequence modeling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues and evaluate bi-LSTMs with word, character, and unicode byte embeddings for POS tagging. We compare bi-LSTMs to traditional POS taggers across languages and data sizes. We also present a novel bi-LSTM model, which combines the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that bi-LSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed."},{"id":"29ad19018791b8b3c422023b98ff620b.html","title":"Improving sentence compression by learning to predict gaze","url":"https://arxiv.org/abs/1604.03357","authors":["Sigrid Klerke","Yoav Goldberg","Anders S\\\\xf8gaard"],"date":"2016/04/12","journal":"arXiv preprint arXiv:1604.03357","abstract":"We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches."},{"id":"05e0f20305ec11dab83b5668ba1c55ba.html","title":"Training with Exploration Improves a Greedy Stack-LSTM Parser","url":"https://arxiv.org/abs/1603.03793","authors":["Miguel Ballesteros","Yoav Goldberg","Chris Dyer","Noah A Smith"],"date":"2016/03/11","journal":"arXiv preprint arXiv:1603.03793","abstract":"We adapt the greedy Stack-LSTM dependency parser of Dyer et al.(2015) to support a training-with-exploration procedure using dynamic oracles (Goldberg and Nivre, 2013) instead of cross-entropy minimization. This form of training, which accounts for model predictions at training time rather than assuming an error-free action history, improves parsing accuracies for both English and Chinese, obtaining very strong results for both languages. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural-network."},{"id":"be4d55b5379f1c14f62fdf21f2a48b10.html","title":"Getting More Out Of Syntax with PropS","url":"https://arxiv.org/abs/1603.01648","authors":["Gabriel Stanovsky","Jessica Ficler","Ido Dagan","Yoav Goldberg"],"date":"2016/03/04","journal":"arXiv preprint arXiv:1603.01648","abstract":"Semantic NLP applications often rely on dependency trees to recognize major elements of the proposition structure of sentences. Yet, while much semantic structure is indeed expressed by syntax, many phenomena are not easily read out of dependency trees, often leading to further ad-hoc heuristic post-processing or to information loss. To directly address the needs of semantic applications, we present PropS--an output representation designed to explicitly and uniformly express much of the proposition structure which is implied from syntax, and an associated tool for extracting it from dependency trees."},{"id":"ac3d0d767fd767737cbf7bf8fb1a00c0.html","title":"The Reciprocity-Symmetry Generalization: Proto-Roles and the Organization of Lexical Meanings","url":"http://www.phil.uu.nl/~yoad/papers/WinterSymmetry.pdf","authors":["Yoad Winter"],"date":"2016/02/26","journal":"Empirical issues in syntax and semantics","abstract":"This paper systematically analyzes the relations between logical symmetry and lexical reciprocity. A new generalization about these phenomena is uncovered, which is referred to as the Reciprocity-Symmetry Generalization (RSG). To analyze the RSG in full generality, we develop a new formal theory of lexical reciprocity building on Dowty\u2019s notion of proto-roles. Because of its foundational nature and plausibility for other languages besides English, the RSG is conjectured to be a language universal. Some general implications of this conjecture are discussed, especially regarding the organization of lexical meanings in different languages, and their relations with cognitive systems of concepts and categorization. Although the RSG is new with this paper, it appears to have been silently sensed since early transformational works in the 1960s, without any general analysis. By uncovering this generalization and accounting for it, the present work removes considerable confusion surrounding the pertinent semantic questions."},{"id":"069d6cabcde4e83b0eee965c44cab4c5.html","title":"Improving Hypernymy Detection with an Integrated Path-based and Distributional Method","url":"https://arxiv.org/abs/1603.06076","authors":["Vered Shwartz","Yoav Goldberg","Ido Dagan"],"date":"2016","journal":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)","abstract":"Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches. Distributional methods, whose supervised variants are the current best performers, and path-based methods, which received less research attention. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network, that achieves results comparable to distributional methods. We then extend the approach to integrate both path-based and distributional signals, significantly improving upon the state-of-the-art on this task."},{"id":"0853377f9eb74885dab4eaa77432d884.html","title":"Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning","url":"https://www.aclweb.org/anthology/K16-1000.pdf","authors":["Stefan Riezler","Yoav Goldberg"],"date":"2016","journal":"Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning","abstract":"The 2016 Conference on Computational Natural Language Learning is the twentieth in the series of annual meetings organized by SIGNLL, the ACL special interest group on natural language learning. CoNLL 2016 will be held on August 11-12, 2016, and is co-located with the 54th annual meeting of the Association for Computational Linguistics (ACL) in Berlin, Germany."},{"id":"d5c02f54315dd573f1b159e89ba7e3f5.html","title":"Collecting better training data using biased agent policies in negotiation dialogues","url":"http://workshop.colips.org/wochat/@iva2016/documents/RP-270.pdf","authors":["Vasily Konovalov","Oren Melamud","Ron Artstein","Ido Dagan"],"date":"2016","journal":"WOCHAT: Workshop on Chatbots and Conversational Agent Technologies","abstract":"When naturally occurring data is characterized by a highly skewed class distribution, supervised learning often benefits from reducing this skew. Human-agent dialogue data is commonly highly skewed when using standard agent policies. Hence, we suggest that agent policies need to be reconsidered in the context of training data collection. Specifically, in this work we implemented biased agent policies that are optimized for data collection in the negotiation domain. Empirical evaluations show that our method is successful in collecting a reasonably balanced corpus in the highly skewed Job-Candidate domain. Furthermore, using this balanced corpus to train a negotiation intent classifier yields notable performance improvements relative to naturally distributed data."},{"id":"e30d12d4ba24be7f9898551fb3c73002.html","title":"A primer on neural network models for natural language processing","url":"http://www.jair.org/index.php/jair/article/view/11030","authors":["Yoav Goldberg"],"date":"2016","journal":"Journal of Artificial Intelligence Research","abstract":"Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation."},{"id":"2ee87eebe615b34c5cbd707a5774d916.html","title":"Universal Dependencies 1.2","url":"https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1548?locale-attribute=cs","authors":["Joakim Nivre","\u017deljko Agi\u0107","Maria Jesus Aranzabe","Masayuki Asahara","Aitziber Atutxa","Miguel Ballesteros","John Bauer","Kepa Bengoetxea","Riyaz Ahmad Bhat","Cristina Bosco","Sam Bowman","Giuseppe GA Celano","Miriam Connor","Marie-Catherine de Marneffe","Arantza Diaz de Ilarraza","Kaja Dobrovoljc","Timothy Dozat","Toma\u017e Erjavec","Rich\\\\xef\\\\xbf\\\\xbdrd Farkas","Jennifer Foster","Daniel Galbraith","Filip Ginter","Iakes Goenaga","Koldo Gojenola","Yoav Goldberg","Berta Gonzales","Bruno Guillaume","Jan Haji\u010d","Dag Haug","Radu Ion","Elena Irimia","Anders Johannsen","Hiroshi Kanayama","Jenna Kanerva","Simon Krek","Veronika Laippala","Alessandro Lenci","Nikola Ljube\u0161i\u0107","Teresa Lynn","Christopher Manning","C\u0103t\u0103lina M\u0103r\u0103nduc","David Mare\u010dek","H\\\\xef\\\\xbf\\\\xbdctor Mart\\\\xef\\\\xbf\\\\xbdnez Alonso","Jan Ma\u0161ek","Yuji Matsumoto","Ryan McDonald","Anna Missil\\\\xef\\\\xbf\\\\xbd","Verginica Mititelu","Yusuke Miyao","Simonetta Montemagni","Shunsuke Mori","Hanna Nurmi","Petya Osenova","Lilja \\\\xef\\\\xbf\\\\xbdvrelid","Elena Pascual","Marco Passarotti","Cenel-Augusto Perez","Slav Petrov","Jussi Piitulainen","Barbara Plank","Martin Popel","Prokopis Prokopidis","Sampo Pyysalo","Loganathan Ramasamy","Rudolf Rosa","Shadi Saleh","Sebastian Schuster","Wolfgang Seeker","Mojgan Seraji","Natalia Silveira","Maria Simi","Radu Simionescu","Katalin Simk\\\\xef\\\\xbf\\\\xbd","Kiril Simov","Aaron Smith","Jan \u0160t\u011bp\\\\xef\\\\xbf\\\\xbdnek","Alane Suhr","Zsolt Sz\\\\xef\\\\xbf\\\\xbdnt\\\\xef\\\\xbf\\\\xbd","Takaaki Tanaka","Reut Tsarfaty","Sumire Uematsu","Larraitz Uria","Viktor Varga","Veronika Vincze","Zden\u011bk \u017dabokrtsk\\\\xef\\\\xbf\\\\xbd","Daniel Zeman","Hanzhi Zhu"],"date":"2015/11/15","abstract":"Popis Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008)."},{"id":"950835f59e8353976ce63e5c8a36a127.html","title":"Knowledge-based textual inference via parse-tree transformations","url":"https://www.jair.org/index.php/jair/article/view/10957","authors":["Roy Bar-Haim","Ido Dagan","Jonathan Berant"],"date":"2015/09/9","journal":"Journal of Artificial Intelligence Research","abstract":"Textual inference is an important component in many applications for understanding natural language. Classical approaches to textual inference rely on logical representations for meaning, which may be regarded as \\"external\\" to the natural language itself. However, practical applications usually adopt shallower lexical or lexical-syntactic representations, which correspond closely to language structure. In many cases, such approaches lack a principled meaning representation and inference framework. We describe an inference formalism that operates directly on language-based structures, particularly syntactic parse trees. New trees are generated by applying inference rules, which provide a unified representation for varying types of inferences. We use manual and automatic methods to generate these rules, which cover generic linguistic structures as well as specific lexical-based inferences. We also present a novel packed data-structure and a corresponding inference algorithm that allows efficient implementation of this formalism. We proved the correctness of the new algorithm and established its efficiency analytically and empirically. The utility of our approach was illustrated on two tasks: unsupervised relation extraction from a large corpus, and the Recognizing Textual Entailment (RTE) benchmarks."},{"id":"e20ced778879995dca17e93e990c7f15.html","title":"Semi-supervised dependency parsing using bilexical contextual features from auto-parsed data","url":"https://www.aclweb.org/anthology/D15-1158.pdf","authors":["Eliyahu Kiperwasser","Yoav Goldberg"],"date":"2015/09","abstract":"We present a semi-supervised approach to improve dependency parsing accuracy by using bilexical statistics derived from auto-parsed data. The method is based on estimating the attachment potential of head-modifier words, by taking into account not only the head and modifier words themselves, but also the words surrounding the head and the modifier. When integrating the learned statistics as features in a graph-based parsing model, we observe nice improvements in accuracy when parsing various English datasets."},{"id":"00ed59ba58aae590332b6a376c62e35c.html","title":"Open ie as an intermediate structure for semantic tasks","url":"https://www.aclweb.org/anthology/P15-2050.pdf","authors":["Gabriel Stanovsky","Ido Dagan"],"date":"2015/07","abstract":"Semantic applications typically extract information from intermediate structures derived from sentences, such as dependency parse or semantic role labeling. In this paper, we study Open Information Extraction\u2019s (Open IE) output as an additional intermediate structure and find that for tasks such as text comprehension, word similarity and word analogy it can be very effective. Specifically, for word analogy, Open IE-based embeddings surpass the state of the art. We suggest that semantic applications will likely benefit from adding Open IE format to their set of potential sentencelevel structures."},{"id":"861dcfb3b9c2fed268936edd4806ae47.html","title":"Integrating query performance prediction in term scoring for diachronic thesaurus","url":"https://www.aclweb.org/anthology/W15-3714.pdf","authors":["Chaya Liebeskind","Ido Dagan"],"date":"2015/07","abstract":"A diachronic thesaurus is a lexical resource that aims to map between modern terms and their semantically related terms in earlier periods. In this paper, we investigate the task of collecting a list of relevant modern target terms for a domain-specific diachronic thesaurus. We propose a supervised learning scheme, which integrates features from two closely related fields: Terminology Extraction and Query Performance Prediction (QPP). Our method further expands modern candidate terms with ancient related terms, before assessing their corpus relevancy with QPP measures. We evaluate the empirical benefit of our method for a thesaurus for a diachronic Jewish corpus."},{"id":"b3cde280409488ac58a404be7a4ed652.html","title":"Seed-Based Event Trigger Labeling: How far can event descriptions get us?","url":"https://www.aclweb.org/anthology/P15-2061.pdf","authors":["Ofer Bronstein","Ido Dagan","Qi Li","Heng Ji","Anette Frank"],"date":"2015/07","abstract":"The task of event trigger labeling is typically addressed in the standard supervised setting: triggers for each target event type are annotated as training data, based on annotation guidelines. We propose an alternative approach, which takes the example trigger terms mentioned in the guidelines as seeds, and then applies an eventindependent similarity-based classifier for trigger labeling. This way we can skip manual annotation for new event types, while requiring only minimal annotated training data for few example events at system setup. Our method is evaluated on the ACE-2005 dataset, achieving 5.7% F1 improvement over a state-of-the-art supervised system which uses the full training data."},{"id":"0f26221273bae9a162a3ce5870eae01e.html","title":"Text categorization from category name in an industry-motivated scenario","url":"https://link.springer.com/article/10.1007/s10579-015-9298-3","authors":["Chaya Liebeskind","Lili Kotlerman","Ido Dagan"],"date":"2015/06/01","journal":"Language resources and evaluation","abstract":"In this work we suggest a novel Text Categorization (TC) scenario, motivated by an ad-hoc industrial need to assign documents to a set of predefined categories, while labeled training data for the categories is not available. The scenario is applicable in many industrial settings and is interesting from the academic perspective. We present a new dataset geared for the main characteristics of the scenario, and utilize it to investigate the name-based TC approach, which uses the category names as its only input and does not require training data. We evaluate and analyze the performance of state-of-the-art methods for this dataset to identify the shortcomings of these methods for our scenario, and suggest ways for overcoming these shortcomings. We utilize statistical correlation measured over a target corpus for improving the state-of-the-art, and offer a different classification scheme based on the characteristics\\\\xa0\u2026"},{"id":"0cf7b529a7108dcbd94af7efab9eec38.html","title":"Efficient global learning of entailment graphs","url":"https://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00220","authors":["Jonathan Berant","Noga Alon","Ido Dagan","Jacob Goldberger"],"date":"2015/06","journal":"Computational Linguistics","abstract":"Entailment rules between predicates are fundamental to many semantic-inference applications. Consequently, learning such rules has been an active field of research in recent years. Methods for learning entailment rules between predicates that take into account dependencies between different rules (e.g., entailment is a transitive relation) have been shown to improve rule quality, but suffer from scalability issues, that is, the number of predicates handled is often quite small. In this article, we present methods for learning transitive graphs that contain tens of thousands of nodes, where nodes represent predicates and edges correspond to entailment rules (termed entailment graphs). Our methods are able to scale to a large number of predicates by exploiting structural properties of entailment graphs such as the fact that they exhibit a \u201ctree-like\u201d property. We apply our methods on two data sets and demonstrate that\\\\xa0\u2026"},{"id":"0fd407c133eb13e6b734949a6cdc0555.html","title":"A simple word embedding model for lexical substitution","url":"https://www.aclweb.org/anthology/W15-1501.pdf","authors":["Oren Melamud","Omer Levy","Ido Dagan"],"date":"2015/06","abstract":"The lexical substitution task requires identifying meaning-preserving substitutes for a target word instance in a given sentential context. Since its introduction in SemEval-2007, various models addressed this challenge, mostly in an unsupervised setting. In this work we propose a simple model for lexical substitution, which is based on the popular skip-gram word embedding model. The novelty of our approach is in leveraging explicitly the context embeddings generated within the skip-gram model, which were so far considered only as an internal component of the learning process. Our model is efficient, very simple to implement, and at the same time achieves state-ofthe-art results on lexical substitution tasks in an unsupervised setting."},{"id":"f00f7bba1384e4463189c1ab1eb255c6.html","title":"Multi-level alignments as an extensible representation basis for textual entailment algorithms","url":"https://www.aclweb.org/anthology/S15-1022.pdf","authors":["Tae-Gil Noh","Sebastian Pad\\\\xf3","Vered Shwartz","Ido Dagan","Vivi Nastase","Kathrin Eichler","Lili Kotlerman","Meni Adler"],"date":"2015/06","abstract":"A major problem in research on Textual Entailment (TE) is the high implementation effort for TE systems. Recently, interoperable standards for annotation and preprocessing have been proposed. In contrast, the algorithmic level remains unstandardized, which makes component re-use in this area very difficult in practice. In this paper, we introduce multi-level alignments as a central, powerful representation for TE algorithms that encourages modular, reusable, multilingual algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages."},{"id":"c732b05f2c9dfd2518055f6f5ac27ee0.html","title":"Improving distributional similarity with lessons learned from word embeddings","url":"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00134","authors":["Omer Levy","Yoav Goldberg","Ido Dagan"],"date":"2015/05/04","journal":"Transactions of the Association for Computational Linguistics","abstract":"Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others."},{"id":"fa2bea0b30c05efab2619813a967366d.html","title":"Universal Dependencies 1.0","url":"https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1548?locale-attribute=cs","authors":["Joakim Nivre","Cristina Bosco","Jinho Choi","Marie-Catherine de Marneffe","Timothy Dozat","Rich\\\\xe1rd Farkas","Jennifer Foster","Filip Ginter","Yoav Goldberg","Jan Haji\u010d","Jenna Kanerva","Veronika Laippala","Alessandro Lenci","Teresa Lynn","Christopher Manning","Ryan McDonald","Anna Missil\\\\xe4","Simonetta Montemagni","Slav Petrov","Sampo Pyysalo","Natalia Silveira","Maria Simi","Aaron Smith","Reut Tsarfaty","Veronika Vincze","Daniel Zeman"],"date":"2015/01/15","abstract":"Popis Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008)."},{"id":"c309731f884b6d356582fbe6007d3a19.html","title":"Textual entailment graphs.","url":"http://u.cs.biu.ac.il/~davidol/lilikotlerman/nleGraphs.pdf","authors":["Lili Kotlerman","Ido Dagan","Bernardo Magnini","Luisa Bentivogli"],"date":"2015/01","journal":"Nat. Lang. Eng.","abstract":"In this work we present a novel type of graphs for Natural Language Processing, namely Textual Entailment Graphs. We describe the complete methodology we developed for the construction of such graphs and provide some baselines for this task by evaluating relevant state-of-the-art technology. We situate our research in the context of Text Exploration, since it was motivated by joint work with industrial partners in the text analytics area. Accordingly, we present our motivating scenario and the first gold-standard dataset of textual entailment graphs. However, while our own motivation and dataset focus on the text exploration setting, we suggest that textual entailment graphs can have different usages and suggest that automatic creation of such graphs is an interesting task for the community."},{"id":"1066c85681fb4d49a62ed763caf90b76.html","title":"Do supervised distributional methods really learn lexical inference relations?","url":"https://www.aclweb.org/anthology/N15-1098.pdf","authors":["Omer Levy","Steffen Remus","Chris Biemann","Ido Dagan"],"date":"2015","abstract":"Distributional representations of words have been recently used in supervised settings for recognizing lexical inference relations between word pairs, such as hypernymy and entailment. We investigate a collection of these state-of-the-art methods, and show that they do not actually learn a relation between two words. Instead, they learn an independent property of a single word in the pair: whether that word is a \u201cprototypical hypernym\u201d."},{"id":"378e485590b47d3399745b774557dc4f.html","title":"Unsupervised acquisition of entailment relations from the Web","url":"https://u.cs.biu.ac.il/~nlp/downloads/publications/S1351324913000156a.pdf","authors":["Idan Szpektor","Hristo Tanev","Ido Dagan","Bonaventura Coppola","Milen Kouylekov"],"date":"2015","journal":"Natural Language Engineering","abstract":"Entailment recognition is a primary generic task in natural language inference, whose focus is to detect whether the meaning of one expression can be inferred from the meaning of the other. Accordingly, many NLP applications would benefit from high coverage knowledgebases of paraphrases and entailment rules. To this end, learning such knowledgebases from the Web is especially appealing due to its huge size as well as its highly heterogeneous content, allowing for a more scalable rule extraction of various domains. However, the scalability of state-of-the-art entailment rule acquisition approaches from the Web is still limited. We present a fully unsupervised learning algorithm for Webbased extraction of entailment relations. We focus on increased scalability and generality with respect to prior work, with the potential of a large-scale Web-based knowledgebase. Our algorithm takes as its input a lexical\u2013syntactic template and searches the Web for syntactic templates that participate in an entailment relation with the input template. Experiments show promising results, achieving performance similar to a state-of-the-art unsupervised algorithm, operating over an offline corpus, but with the benefit of learning rules for different domains with no additional effort."},{"id":"389ef4da1d00afe136c1742b71c61af8.html","title":"Template Kernels for Dependency Parsing","url":"https://www.aclweb.org/anthology/N15-1163.pdf","authors":["Hillel Taub-Tabib","Yoav Goldberg","Amir Globerson"],"date":"2015","abstract":"A common approach to dependency parsing is scoring a parse via a linear function of a set of indicator features. These features are typically manually constructed from templates that are applied to parts of the parse tree. The templates define which properties of a part should combine to create features. Existing approaches consider only a small subset of the possible combinations, due to statistical and computational efficiency considerations. In this work we present a novel kernel which facilitates efficient parsing with feature representations corresponding to a much larger set of combinations. We integrate the kernel into a parse reranking system and demonstrate its effectiveness on four languages from the CoNLL-X shared task. 1"},{"id":"58a971e17063d10a9e35af3c4ca9244e.html","title":"Universal dependencies 1.1","url":"http://scholar.google.com/scholar?cluster=1992226429518703120&hl=en&oi=scholarr","authors":["\u017deljko Agic","Maria Jesus Aranzabe","Aitziber Atutxa","Cristina Bosco","Jinho Choi","Marie-Catherine de Marneffe","Timothy Dozat","Rich\\\\xe1rd Farkas","Jennifer Foster","Filip Ginter","Iakes Goenaga","Koldo Gojenola","Yoav Goldberg","Jan Hajic","Anders Tr\\\\xe6rup Johannsen","Jenna Kanerva","Juha Kuokkala","Veronika Laippala","Alessandro Lenci","Krister Lind\\\\xe9n","Nikola Ljube\u0161ic","Teresa Lynn","Christopher Manning","H\\\\xe9ctor Alonso Mart\\\\xednez","Ryan McDonald","Anna Missil\\\\xe4","Simonetta Montemagni","Joakim Nivre","Hanna Nurmi","Petya Osenova","Slav Petrov","Jussi Piitulainen","Barbara Plank","Prokopis Prokopidis","Sampo Pyysalo","Wolfgang Seeker","Mojgan Seraji","Natalia Silveira","Maria Simi","Kiril Simov","Aaron Smith","Reut Tsarfaty","Veronika Vincze","Daniel Zeman"],"date":"2015","journal":"LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague"},{"id":"77c1301ef4e9418fcbae24fa12c345fb.html","title":"Universal Dependencies Treebanks 1.2 (see http://universaldependencies. org/)","url":"https://arpi.unipi.it/handle/11568/773645","authors":["Joakim Nivre","\u017deljko Agi\u0107","Maria Jesus Aranzabe","Masayuki Asahara","Aitziber Atutxa","Miguel Ballesteros","John Bauer","Kepa Bengoetxea","Riyaz Ahmad Bhat","Cristina Bosco","Sam Bowman","Giuseppe GA Celano","Miriam Connor","Marie-Catherine de Marneffe","Arantza Diaz de Ilarraza","Kaja Dobrovoljc","Timothy Dozat","Toma\u017e Erjavec","Rich\\\\xe1rd Farkas","Jennifer Foster","Daniel Galbraith","Filip Ginter","Iakes Goenaga","Koldo Gojenola","Yoav Goldberg","Berta Gonzales","Bruno Guillaume","Jan Haji\u010d","Dag Haug","Radu Ion","Elena Irimia","Anders Johannsen","Hiroshi Kanayama","Jenna Kanerva","Simon Krek","Veronika Laippala","Alessandro Lenci","Nikola Ljube\u0161i\u0107","Teresa Lynn","Christopher Manning","C\u0103t\u0103lina M\u0103r\u0103nduc","David Mare\u010dek","H\\\\xe9ctor Mart\\\\xednez Alonso","Jan Ma\u0161ek","Yuji Matsumoto","Ryan Mcdonald","Anna Missil\\\\xe4","Verginica Mititelu","Yusuke Miyao","Simonetta Montemagni","Shunsuke Mori","Hanna Nurmi","Petya Osenova","Lilja \\\\xd8vrelid","Elena Pascual","Marco Passarotti","Cenel-Augusto Perez","Slav Petrov","Jussi Piitulainen","Barbara Plank","Martin Popel","Prokopis Prokopidis","Sampo Pyysalo","Loganathan Ramasamy","Rudolf Rosa","Shadi Saleh","Sebastian Schuster","Wolfgang Seeker","Mojgan Seraji","Natalia Silveira","Maria Simi","Radu Simionescu","Katalin Simk\\\\xf3","Kiril Simov","Aaron Smith","Jan \u0160t\u011bp\\\\xe1nek","Alane Suhr","Zsolt Sz\\\\xe1nt\\\\xf3","Takaaki Tanaka","Reut Tsarfaty","Sumire Uematsu","Larraitz Uria","Viktor Varga","Veronika Vincze","Zden\u011bk \u017dabokrtsk\\\\xfd","Daniel Zeman","Hanzhi Zhu"],"date":"2015","abstract":"Universal Dependencies is a project that seeks to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. The annotation scheme is based on (universal) Stanford dependencies (de Marneffe et al., 2006, 2008, 2014), Google universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008)."},{"id":"7e1ea697ba71ec4ee37e37e5c84cffad.html","title":"Word embeddings in Hebrew: Initial results","url":"https://www.openu.ac.il/iscol2015/downloads/ISCOL2015_submission_27_c_16.pdf","authors":["Oded Avraham","Yoav Goldberg"],"date":"2015","abstract":"Word embeddings algorithms got considerable attention in the past few years. However, they were all applied to English, a language with very limited morphology. We present our initial results with inferring word embeddings on Hebrew, a language with a much richer inflectional morphology system. The embeddings seem to provide a mix of semantic and morphological properties. Using lemmatization helps direct the resulting similarities away from the morphological similarity and towards semantic similarity. We are currently looking at improving the control over the aspects of the resulting similarities by investigating more refined and taskdirected lemmatization."},{"id":"c407225f2dcb6fbcce9d1a64f32203e4.html","title":"Inherent Vacuity in Lattice Automata","url":"https://link.springer.com/chapter/10.1007/978-3-319-23534-9_10","authors":["Hila Gonen","Orna Kupferman"],"date":"2015","abstract":" <i>Vacuity checking</i> is traditionally performed after model checking has terminated successfully. It ensures that all the elements of the specification have played a role in its satisfaction by the system. The need to check the quality of specifications is even more acute in <i>property-based design</i>, where the specification is the only input, serving as a basis to the development of the system. <i>Inherent vacuity</i> adapts the theory of vacuity in model checking to the setting of property-based design. Essentially, a specification is inherently vacuous if it can be mutated into a simpler equivalent specification, which is known, in the case of specifications in linear temporal logic, to coincide with the fact the specification is satisfied vacuously in all systems."},{"id":"cafc36c193b4860454d16c7b3c59bcd8.html","title":"Modeling word meaning in context with substitute vectors","url":"https://www.aclweb.org/anthology/N15-1050.pdf","authors":["Oren Melamud","Ido Dagan","Jacob Goldberger"],"date":"2015","abstract":"Context representations are a key element in distributional models of word meaning. In contrast to typical representations based on neighboring words, a recently proposed approach suggests to represent a context of a target word by a substitute vector, comprising the potential fillers for the target word slot in that context. In this work we first propose a variant of substitute vectors, which we find particularly suitable for measuring context similarity. Then, we propose a novel model for representing word meaning in context based on this context representation. Our model outperforms state-of-the-art results on lexical substitution tasks in an unsupervised setting."},{"id":"8639200ba787f1de3565bf2af5102458.html","title":"Semantic parsing using content and context: A case study from requirements elicitation","url":"https://www.aclweb.org/anthology/D14-1136.pdf","authors":["Reut Tsarfaty","Ilia Pogrebezky","Guy Weiss","Yaarit Natan","Smadar Szekely","David Harel"],"date":"2014/10","abstract":"We present a model for the automatic semantic analysis of requirements elicitation documents. Our target semantic representation employs live sequence charts, a multi-modal visual language for scenariobased programming, which can be directly translated into executable code. The architecture we propose integrates sentencelevel and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context. We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static (entities, properties) and dynamic (behavioral scenarios) requirements in the document."},{"id":"887abd66d42324f340116c5fbda75b94.html","title":"Entailment graphs for text analytics in the excitement project","url":"https://link.springer.com/chapter/10.1007/978-3-319-10816-2_2","authors":["Bernardo Magnini","Ido Dagan","G\\\\xfcnter Neumann","Sebastian Pado"],"date":"2014/09/08","abstract":"In the last years, a relevant research line in Natural Language Processing has focused on detecting semantic relations among portions of text, including entailment, similarity, temporal relations, and, with a less degree, causality. The attention on such semantic relations has raised the demand to move towards more informative meaning representations, which express properties of concepts and relations among them. This demand triggered research on \u201cstatement entailment graphs\u201d, where nodes are natural language statements (propositions), comprising of predicates with their arguments and modifiers, while edges represent entailment relations between nodes."},{"id":"1ec9cf537cebcf4d293f5997c6a5d65a.html","title":"Introducing the spmrl 2014 shared task on parsing morphologically-rich languages","url":"https://www.aclweb.org/anthology/W14-6111.pdf","authors":["Djam\\\\xe9 Seddah","Sandra K\\\\xfcbler","Reut Tsarfaty"],"date":"2014/08","abstract":"This first joint meeting on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical English (SPMRL-SANCL) featured a shared task on statistical parsing of morphologically rich languages (SPMRL). The goal of the shared task is to allow to train and test different participating systems on comparable data sets, thus providing an objective measure of comparison between state-of-the-art parsing systems on data data sets from a range of different languages. This 2014 SPMRL shared task is a continuation and extension of the SPMRL shared task, which was co-located with the SPMRL meeting at EMNLP 2013 (Seddah et al., 2013). This paper provides a short overview of the 2014 SPMRL shared task goals, data sets, and evaluation setup. Since the SPMRL 2014 largely builds on the infrastructure established for the SPMRL 2013 shared task, we start by reviewing the previous shared task (\\\\xa7 2) and then proceed to the 2014 SPMRL evaluation settings (\\\\xa7 3), data sets (\\\\xa7 4), and a task summary (\\\\xa7 5). Due to organizational constraints, this overview is published prior to the submission of all system test runs, and a more detailed overview including the description of participating systems and the analysis of their results will follow as part of (Seddah et al., 2014), once the shared task is completed."},{"id":"dc10a6507464bbbc2c69c382d6c4b747.html","title":"Proposition Knowledge Graphs","url":"https://www.aclweb.org/anthology/W14-4504.pdf","authors":["Gabriel Stanovsky","Omer Levy","Ido Dagan"],"date":"2014/08","abstract":"Open Information Extraction (Open IE) is a promising approach for unrestricted Information Discovery (ID). While Open IE is a highly scalable approach, allowing unsupervised relation extraction from open domains, it currently has some limitations. First, it lacks the expressiveness needed to properly represent and extract complex assertions that are abundant in text. Second, it does not consolidate the extracted propositions, which causes simple queries above Open IE assertions to return insufficient or redundant information. To address these limitations, we propose in this position paper a novel representation for ID\u2013Propositional Knowledge Graphs (PKG). PKGs extend the Open IE paradigm by representing semantic inter-proposition relations in a traversable graph. We outline an approach for constructing PKGs from single and multiple texts, and highlight a variety of high-level applications that may leverage PKGs as their underlying information discovery and representation framework."},{"id":"22539529cd9476d9bbe408641765baa6.html","title":"Constrained arc-eager dependency parsing","url":"https://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00184","authors":["Joakim Nivre","Yoav Goldberg","Ryan McDonald"],"date":"2014/06/27","journal":"Computational Linguistics","abstract":"Arc-eager dependency parsers process sentences in a single left-to-right pass over the input and have linear time complexity with greedy decoding or beam search. We show how such parsers can be constrained to respect two different types of conditions on the output dependency graph: span constraints, which require certain spans to correspond to subtrees of the graph, and arc constraints, which require certain arcs to be present in the graph. The constraints are incorporated into the arc-eager transition system as a set of preconditions for each transition and preserve the linear time complexity of the parser."},{"id":"8d80c7c577f601e30f857976a48608f9.html","title":"Linguistic regularities in sparse and explicit word representations","url":"https://www.aclweb.org/anthology/W14-1618.pdf","authors":["Omer Levy","Yoav Goldberg"],"date":"2014/06/26","abstract":"Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.\u2019s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations."},{"id":"0a0793e629a3667f15b0758c8e9cbe5c.html","title":"Intermediary semantic representation through proposition structures","url":"https://www.aclweb.org/anthology/W14-2413.pdf","authors":["Gabriel Stanovsky","Jessica Ficler","Ido Dagan","Yoav Goldberg"],"date":"2014/06","abstract":"We propose an intermediary-level semantic representation, providing a higher level of abstraction than syntactic parse trees, while not committing to decisions in cases such as quantification, grounding or verbspecific roles assignments. The proposal is centered around the proposition structure of the text, and includes also implicit propositions which can be inferred from the syntax but are not transparent in parse trees, such as copular relations introduced by appositive constructions. Other benefits over dependency-trees are explicit marking of logical relations between propositions, explicit marking of multiword predicate such as light-verbs, and a consistent representation for syntacticallydifferent but semantically-similar structures. The representation is meant to serve as a useful input layer for semanticoriented applications, as well as to provide a better starting point for further levels of semantic analysis such as semantic-rolelabeling and semantic-parsing."},{"id":"8c15fc8cfd9b72302ddc2e866d415d44.html","title":"Probabilistic modeling of joint-context in distributional similarity","url":"https://www.aclweb.org/anthology/W14-1619.pdf","authors":["Oren Melamud","Ido Dagan","Jacob Goldberger","Idan Szpektor","Deniz Yuret"],"date":"2014/06","abstract":"Most traditional distributional similarity models fail to capture syntagmatic patterns that group together multiple word features within the same joint context. In this work we introduce a novel generic distributional similarity scheme under which the power of probabilistic models can be leveraged to effectively model joint contexts. Based on this scheme, we implement a concrete model which utilizes probabilistic n-gram language models. Our evaluations suggest that this model is particularly wellsuited for measuring similarity for verbs, which are known to exhibit richer syntagmatic patterns, while maintaining comparable or better performance with respect to competitive baselines for nouns. Following this, we propose our scheme as a framework for future semantic similarity models leveraging the substantial body of work that exists in probabilistic language modeling."},{"id":"a0ba330c683711980a17301d23723753.html","title":"Recognizing implied predicate-argument relationships in textual inference","url":"https://www.aclweb.org/anthology/P14-2120.pdf","authors":["Asher Stern","Ido Dagan"],"date":"2014/06","abstract":"We investigate recognizing implied predicate-argument relationships which are not explicitly expressed in syntactic structure. While prior works addressed such relationships as an extension to semantic role labeling, our work investigates them in the context of textual inference scenarios. Such scenarios provide prior information, which substantially eases the task. We provide a large and freely available evaluation dataset for our task setting, and propose methods to cope with it, while obtaining promising results in empirical evaluations."},{"id":"c2cc03c531e53ce43c1409fd4feb50b0.html","title":"Generating subjective responses to opinionated articles in social media: an agenda-driven architecture and a turing-like test","url":"https://www.aclweb.org/anthology/W14-2708.pdf","authors":["Tomer Cagan","Stefan L Frank","Reut Tsarfaty"],"date":"2014/06","abstract":"Natural language traffic in social media (blogs, microblogs, talkbacks) enjoys vast monitoring and analysis efforts. However, the question whether computer systems can generate such content in order to effectively interact with humans has been only sparsely attended to. This paper presents an architecture for generating subjective responses to opinionated articles based on users\u2019 agenda, documents\u2019 topics, sentiments and a knowledge graph. We present an empirical evaluation method for quantifying the humanlikeness and relevance of the generated responses. We show that responses generated using world knowledge in the input are regarded as more human-like than those that rely on topic, sentiment and agenda only, whereas the use of world knowledge does not affect perceived relevance."},{"id":"cc444c665fd0e2ba82dfe99dea07ee10.html","title":"The excitement open platform for textual inferences","url":"https://www.aclweb.org/anthology/P14-5008.pdf","authors":["Bernardo Magnini","Roberto Zanoli","Ido Dagan","Kathrin Eichler","G\\\\xfcnter Neumann","Tae-Gil Noh","Sebastian Pado","Asher Stern","Omer Levy"],"date":"2014/06","abstract":"This paper presents the Excitement Open Platform (EOP), a generic architecture and a comprehensive implementation for textual inference in multiple languages. The platform includes state-of-art algorithms, a large number of knowledge resources, and facilities for experimenting and testing innovative approaches. The EOP is distributed as an open source software."},{"id":"e408caf4796f7c7a7ef176bc6aef7f67.html","title":"Focused entailment graphs for open ie propositions","url":"https://www.aclweb.org/anthology/W14-1610.pdf","authors":["Omer Levy","Ido Dagan","Jacob Goldberger"],"date":"2014/06","abstract":"Open IE methods extract structured propositions from text. However, these propositions are neither consolidated nor generalized, and querying them may lead to insufficient or redundant information. This work suggests an approach to organize open IE propositions using entailment graphs. The entailment relation unifies equivalent propositions and induces a specific-to-general structure. We create a large dataset of gold-standard proposition entailment graphs, and provide a novel algorithm for automatically constructing them. Our analysis shows that predicate entailment is extremely context-sensitive, and that current lexical-semantic resources do not capture many of the lexical inferences induced by proposition entailment."},{"id":"d917d7444d7b44181080500778cda55d.html","title":"A tabular method for dynamic oracles in transition-based parsing","url":"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00170","authors":["Yoav Goldberg","Francesco Sartorio","Giorgio Satta"],"date":"2014/04/30","journal":"Transactions of the Association for Computational Linguistics","abstract":"We develop parsing oracles for two transition-based dependency parsers, including the arc-standard parser, solving a problem that was left open in (Goldberg and Nivre, 2013). We experimentally show that using these oracles during training yields superior parsing accuracies on many languages."},{"id":"aec38569a54dcc719a4d1d70067ef1f2.html","title":"word2vec Explained: deriving Mikolov et al.\'s negative-sampling word-embedding method","url":"https://arxiv.org/abs/1402.3722","authors":["Yoav Goldberg","Omer Levy"],"date":"2014/02/15","journal":"arXiv preprint arXiv:1402.3722","abstract":"The word2vec software of Tomas Mikolov and colleagues (this https URL) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations."},{"id":"ebb47a91f0bb15246f15e5c6a6c47153.html","title":"The Hebrew Wikipedia Dependency Parsed Corpus Ver. 1.0","url":"http://u.cs.biu.ac.il/~yogo/hebwiki/parsed_hebrew_wiki.pdf","authors":["Yoav Goldberg"],"date":"2014/02/06","journal":"Bar-Ilan University","abstract":"In all these cases, the suffix is not separated from its base word, and instead the complete form is followed by the pseudo-words of its suffix. The benefit of the pseudo-words system is that, after ignoring the pseudo-word tokens, the word sequence has one-to-one correspondence to the raw tokens."},{"id":"90209244ac14aec8ec4f10c2e4b2327a.html","title":"A Note on Latent Semantic Analysis","url":"https://www.cs.bgu.ac.il/~dsp152/wiki.files/lsa.pdf","authors":["Yoav Goldberg"],"date":"2014/01/18","abstract":"We have a corpus of d documents over a vocabulary of v words. We arrange the corpus in a matrix C of dimensions v\\\\xd7 d, where Cij is the amount of association between word i and document j. The amount of association is either the count, or a function based on the count such as PMI, TF-IDF and so on. While choosing the association measure is important for obtaining good performance, it is not important to this explanation. Similarly, the documents can be generalized to any context a word appears in (eg same sentence, k preceding and following words, syntactic relations, and so on). Each row in C is associated with a word and each column is associated with a document. Each word vector reflects the contexts the word appears in, and each document vector reflects the words that appear in it. Based on the intuition that words appearing in similar documents (contexts) are similar, we can measure the similarity between words by measuring the similarity between their corresponding vectors (matrix rows). Similarly, we can measure the similarity between documents using the similarity between their corresponding document vectors (matrix columns). One common way of measuring similarity is the cosine similarity measure: simcos (x, y)=\u2329 x, y\u232a xy"},{"id":"053019288e8838d24a972d9e2e51a4cc.html","title":"Syntax and Parsing of Semitic Languages","url":"https://link.springer.com/chapter/10.1007/978-3-642-45358-8_3","authors":["Reut Tsarfaty"],"date":"2014","abstract":"The grammar of Semitic languages is different from that of English and many other languages. Therefore, general-purpose statistical parsers are not always equally successful when applied to Semitic data. This chapter presents the syntax of Semitic languages and discusses how it challenges existing general-purpose parsing architectures. We then survey the different components of a generative probabilistic parsing system and show how they can be designed and implemented in order to effectively cope with these challenges. We finally present parsing results obtained for Hebrew and Arabic using different technologies in different scenarios. While parsing Semitic languages can already be made quite accurate using the present techniques, remaining challenges leave ample space for future research."},{"id":"24c001fbad34b2c2676e154b9e87cb9c.html","title":"An update and extension of the META-NET Study \u201cEurope\u2019s Languages in the digital age\u201d","url":"http://real.mtak.hu/50245/1/CCURL_2014_META_NET_u.pdf","authors":["Georg Rehm","Hans Uszkoreit","Ido Dagan","Vartkes Goetcherian","Mehmet Ugur Dogan","Tam\\\\xe1s V\\\\xe1radi"],"date":"2014","abstract":"This paper extends and updates the cross-language comparison of LT support for 30 European languages as published in the META-NET Language White Paper Series. The updated comparison confirms the original results and paints an alarming picture: it demonstrates that there are even more dramatic differences in LT support between the European languages."},{"id":"2ef3abae01b7bda89dbbdce17d545da4.html","title":"Neural Word Embedding as Implicit Matrix Factorization","url":"http://papers.nips.cc/paper/5477-neural-word-embedding-as","authors":["Omer Levy","Yoav Goldberg"],"date":"2014","abstract":"We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS\'s solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS\'s factorization."},{"id":"42a87f0868540905eebbd0cd1ee00ec5.html","title":"Israel Ramat","url":"http://scholar.google.com/scholar?cluster=1561064494103251808&hl=en&oi=scholarr","authors":["Omer Levy","Yoav Goldberg","Ido Dagan"],"date":"2014","journal":"Gan"},{"id":"4eb53d36f2cee1f4d18ed1d38d75b313.html","title":"Benchmarking applied semantic inference: the PASCAL recognising textual entailment challenges","url":"https://link.springer.com/chapter/10.1007/978-3-642-45321-2_19","authors":["Roy Bar-Haim","Ido Dagan","Idan Szpektor"],"date":"2014","abstract":"Identifying that the same meaning is expressed by, or can be inferred from, various language expressions is a major challenge for natural language understanding applications such as information extraction, question answering and automatic summarization. Dagan and Glickman [5] proposed <i>Textual Entailment</i>, the task of deciding whether a target text follows from a source text, as a unifying framework for modeling language variability, which has often been addressed in an application-specific manner. In this paper we describe the series of benchmarks developed for the textual entailment recognition task, known as the PASCAL RTE Challenges. As a concrete example, we describe in detail the second RTE challenge, in which our methodology was consolidated, and served as a basis for the subsequent RTE challenges. The impressive success of these challenges established textual entailment as an\\\\xa0\u2026"},{"id":"63d587a3427b0aab2ccb6cd86399c110.html","title":"Dependency-Based Word Embeddings.","url":"https://www.aclweb.org/anthology/P14-2050.pdf","authors":["Omer Levy","Yoav Goldberg"],"date":"2014","abstract":"While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts. In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings. The dependencybased embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings."},{"id":"9cd339ea56cf4c900af2c65300637135.html","title":"The BIUTTE Research Platform for Transformation-based Textual Entailment Recognition","url":"https://www.aclweb.org/anthology/2014.lilt-9.2/","authors":["Asher Stern","Ido Dagan"],"date":"2014","abstract":"Recent progress in research of the Recognizing Textual Entailment (RTE) task shows a constantly-increasing level of complexity in this research field. A way to avoid having this complexity becoming a barrier for researchers, especially for new-comers in the field, is to provide a freely available RTE system with a high level of flexibility and extensibility. In this paper, we introduce our RTE system, BiuTee2, and suggest it as an effective research framework for RTE. In particular, BiuTee follows the prominent transformation-based paradigm for RTE, and offers an accessible platform for research within this approach. We describe each of BiuTee\u2019s components and point out the mechanisms and properties which directly support adaptations and integration of new components. In addition, we describe BiuTee\u2019s visual tracing tool, which provides notable assistance for researchers in refining and \u201cdebugging\u201d their knowledge resources and inference components."},{"id":"e5b365c9c3e05fc8ebd2cf645d202552.html","title":"Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages","url":"https://www.aclweb.org/anthology/W14-6100.pdf","authors":["Yoav Goldberg","Yuval Marton","Ines Rehbein","Yannick Versley","\\\\xd6zlem \\\\xc7etino\u011flu","Joel Tetreault"],"date":"2014","journal":"Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages","abstract":"The papers in these proceedings were presented at the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages (SPMRLSANCL 2014), held in Seattle, USA, on October 18th, 2013, in conjunction with the 25th international Conference on Computational Linguistics (Coling 2014)."},{"id":"c2272c7de6a6649c10f1bb01476c99ae.html","title":"Dynamic-oracle Transition-based Parsing with Calibrated Probabilistic Output","url":"https://www.aclweb.org/anthology/W13-5709.pdf","authors":["Yoav Goldberg"],"date":"2013/11/27","journal":"IWPT-2013","abstract":"We adapt the dynamic-oracle training method of Goldberg and Nivre (2012; 2013) to train classifiers that produce probabilistic output. Evaluation of an Arc-Eager parser on 6 languages shows that the AdaGrad-RDA based training procedure results in models that provide the same high level of accuracy as the averagedperceptron trained models, while being sparser and providing well-calibrated probabilistic output."},{"id":"6c59a493fc3cb9ba9dbd003ed9234e4a.html","title":"Entailment graphs for text exploration.","url":"https://jssp2013.fbk.eu/sites/jssp2013.fbk.eu/files/IdoBernardo.pdf","authors":["Ido Dagan","Bernardo Magnini"],"date":"2013/11/20","abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Total citations"},{"id":"1168a4cdf744ea90edeb21c5588a2cd5.html","title":"Overview of the SPMRL 2013 shared task: cross-framework evaluation of parsing morphologically rich languages","url":"http://doras.dcu.ie/19958/","authors":["Djam\\\\xe9 Seddah","Reut Tsarfaty","Sandra K\\\\xfcbler","Marie Candito","Jinho Choi","Rich\\\\xe1rd Farkas","Jennifer Foster","Iakes Goenaga","Koldo Gojenola","Yoav Goldberg","Spence Green","Nizar Habash","Marco Kuhlmann","Wolfgang Maier","Joakim Nivre","Adam Przepi\\\\xf3rkowski","Ryan Roth","Wolfgang Seeker","Yannick Versley","Veronika Vincze","Marcin Wolinski","Alina Wr\\\\xf3blewska","Eric Villemonte de la Cl\\\\xe9rgerie"],"date":"2013/10/18","abstract":""},{"id":"95612b904b7f970195483f7922cca867.html","title":"Consolidating and Exploring Information via Textual Inference","url":"https://link.springer.com/chapter/10.1007/978-3-319-02432-5_1","authors":["Ido Dagan"],"date":"2013/10/07","abstract":"Effectively consuming information from large amounts of texts, which are often largely redundant in their content, is an old but increasingly pressing challenge. It is well illustrated by the perpetual attempts to move away from the flat result lists of search engines towards more structured fact-based presentations. Some recent attempts at this challenge are based on presenting structured information that was formulated according to pre-defined knowledge schemes, such as Freebase and Google\u2019s knowledge graph. We propose an alternative, as well as complementary, approach that attempts to consolidate and structure all textual statements in a document collection based on the inference relations between them. Generic textual inference techniques, formulated under the Textual Entailment paradigm, are used to consolidate redundant information into unique \u201ccore\u201d statements, and then present them in an\\\\xa0\u2026"},{"id":"ae82a54faca3eb25273d8c4b11b3767c.html","title":"Training Deterministic Parsers with Non-Deterministic Oracles","url":"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00237","authors":["Yoav Goldberg","Joakim Nivre"],"date":"2013/10","journal":"Transactions of the Association for Computational Linguistics","abstract":"Greedy transition-based parsers are very fast but tend to suffer from error propagation. This problem is aggravated by the fact that they are normally trained using oracles that are deterministic and incomplete in the sense that they assume a unique canonical path through the transition system and are only valid as long as the parser does not stray from this path. In this paper, we give a general characterization of oracles that are nondeterministic and complete, present a method for deriving such oracles for transition systems that satisfy a property we call arc decomposition, and instantiate this method for three well-known transition systems from the literature. We say that these oracles are dynamic, because they allow us to dynamically explore alternative and nonoptimal paths during training \u2014 in contrast to\\\\xa0\u2026"},{"id":"50cdd02272751f01c2ff1d9be02d3edd.html","title":"PLIS: A probabilistic lexical inference system","url":"https://www.aclweb.org/anthology/P13-4017.pdf","authors":["Eyal Shnarch","Erel Segal-haLevi","Jacob Goldberger","Ido Dagan"],"date":"2013/08","abstract":"This paper presents PLIS, an open source Probabilistic Lexical Inference System which combines two functionalities:(i) a tool for integrating lexical inference knowledge from diverse resources, and (ii) a framework for scoring textual inferences based on the integrated knowledge. We provide PLIS with two probabilistic implementation of this framework. PLIS is available for download and developers of text processing applications can use it as an off-the-shelf component for injecting lexical knowledge into their applications. PLIS is easily configurable, components can be extended or replaced with user generated ones to enable system customization and further research. PLIS includes an online interactive viewer, which is a powerful tool for investigating lexical inference processes."},{"id":"aa6f9d88ad1e42107d7731893b1c107c.html","title":"A two level model for context sensitive inference rules","url":"https://www.aclweb.org/anthology/P13-1131.pdf","authors":["Oren Melamud","Jonathan Berant","Ido Dagan","Jacob Goldberger","Idan Szpektor"],"date":"2013/08","abstract":"Automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words, operating at the word space level. A recent line of work, which addresses context sensitivity of rules, represented contexts in a latent topic space and computed similarity over topic vectors. We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set."},{"id":"cc4c96c91e1d47d80b5549c886423ffe.html","title":"A unified morpho-syntactic scheme of Stanford dependencies","url":"https://www.aclweb.org/anthology/P13-2103.pdf","authors":["Reut Tsarfaty"],"date":"2013/08","abstract":"Stanford Dependencies (SD) provide a functional characterization of the grammatical relations in syntactic parse-trees. The SD representation is useful for parser evaluation, for downstream applications, and, ultimately, for natural language understanding, however, the design of SD focuses on structurally-marked relations and under-represents morphosyntactic realization patterns observed in Morphologically Rich Languages (MRLs). We present a novel extension of SD, called Unified-SD (U-SD), which unifies the annotation of structurally-and morphologically-marked relations via an inheritance hierarchy. We create a new resource composed of U-SD-annotated constituency and dependency treebanks for the MRL Modern Hebrew, and present two systems that can automatically predict U-SD annotations, for gold segmented input as well as raw texts, with high baseline accuracy. 1 Introduction"},{"id":"d58b7a3b751ff79f736a7f7838240369.html","title":"Recognizing partial textual entailment","url":"https://www.aclweb.org/anthology/P13-2080.pdf","authors":["Omer Levy","Torsten Zesch","Ido Dagan","Iryna Gurevych"],"date":"2013/08","abstract":"Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. It thus cannot capture the notion that the target fragment is \u201calmost entailed\u201d by the given text. The recently suggested idea of partial textual entailment may remedy this problem. We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to this setting. Indeed, our results show that these methods are useful for recognizing partial entailment. We also provide a preliminary assessment of how partial entailment may be used for recognizing (complete) textual entailment."},{"id":"da76c31881d4450521437de3847d553b.html","title":"Using lexical expansion to learn inference rules from sparse data","url":"https://www.aclweb.org/anthology/P13-2051.pdf","authors":["Oren Melamud","Ido Dagan","Jacob Goldberger","Idan Szpektor"],"date":"2013/08","abstract":"Automatic acquisition of inference rules for predicates is widely addressed by computing distributional similarity scores between vectors of argument words. In this scheme, prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability. To improve the learning of such rules in an unsupervised way, we propose to lexically expand sparse argument word vectors with semantically similar words. Our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines."},{"id":"3d242a0f833e12b14f2ef49efa34292e.html","title":"Recognizing textual entailment: Models and applications","url":"https://www.morganclaypool.com/doi/abs/10.2200/s00509ed1v01y201305hlt023","authors":["Ido Dagan","Dan Roth","Mark Sammons","Fabio Massimo Zanzotto"],"date":"2013/07/19","journal":"Synthesis Lectures on Human Language Technologies","abstract":" <b>Download Free Sample</b> "},{"id":"220cfd787c201c2108c1a409c1b605ff.html","title":"Deliverable 8.1: Open platform evaluation and distribution, I cycle","url":"http://scholar.google.com/scholar?cluster=4013713709989633612&hl=en&oi=scholarr","authors":["Amir H Moin","Gunter Neumann","Bernardo Magnini","Roberto Zanoli","Sebastian Pado","Ido Dagan","Asher Stern","Ofer Bronstein","Omer Levy"],"date":"2013/07/10","abstract":"According to the work plan of the project, the first free open source distribution release of the EXCITEMENT Open Platform (EOP), based on the source code developed in work package 4 (WP4) and the knowledge resources provided by work package 5 (WP5), has been"},{"id":"0720d2b0c33dfc69dd016515ea582b42.html","title":"Truthteller: Annotating predicate truth","url":"https://www.aclweb.org/anthology/N13-1091.pdf","authors":["Amnon Lotan","Asher Stern","Ido Dagan"],"date":"2013/06","abstract":"We propose a novel semantic annotation type of assigning truth values to predicate occurrences, and present TruthTeller, a standalone publiclyavailable tool that produces such annotations. TruthTeller integrates a range of semantic phenomena, such as negation, modality, presupposition, implicativity, and more, which were dealt only partly in previous works. Empirical evaluations against human annotations show satisfactory results and suggest the usefulness of this new type of tool for NLP."},{"id":"2ffe549474aef70bbb84e82d5e9cfdb2.html","title":"SemEval-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing Textual Embodiment Challenge","url":"https://digital.library.unt.edu/ark:/67531/metadc993399/","authors":["Myroslava O Dzikovska","Rodney D Nielsen","Chris Brew","Claudia Leacock","Danilo Giampiccolo","Luisa Bentivogli","Peter Clark","Ido Dagan","Hoa Trang Dang"],"date":"2013/06","journal":"Second Joint Conference on Lexical and Computational Semantics (* SEM): Seventh International Workshop on Semantic Evaluation (SemEval 2013)","abstract":"We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2-way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and compare results from 9 participating teams, and discuss future directions."},{"id":"8c7f9a7b72eda6b7a01e30a46cdf2830.html","title":"<b>Design Patterns in Fluid Construction Grammar</b> <b>Luc Steels (editor)</b> Universitat Pompeu Fabra and Sony Computer Science Laboratory, Paris Amsterdam: John\\\\xa0\u2026","url":"https://www.mitpressjournals.org/doi/abs/10.1162/COLI_r_00154","authors":["Nathan Schneider","Reut Tsarfaty"],"date":"2013/06","abstract":"Design Patterns in Fluid Construction Grammar Luc Steels (editor) Universitat Pompeu Fabra and Sony Computer Science Laboratory,"},{"id":"95790c37617c508258b8854efe3726a3.html","title":"UKP-BIU: Similarity and entailment metrics for student response analysis","url":"https://www.aclweb.org/anthology/S13-2048.pdf","authors":["Omer Levy","Torsten Zesch","Ido Dagan","Iryna Gurevych"],"date":"2013/06","abstract":"Our system combines text similarity measures with a textual entailment system. In the main task, we focused on the influence of lexicalized versus unlexicalized features, and how they affect performance on unseen questions and domains. We also participated in the pilot partial entailment task, where our system significantly outperforms a strong baseline."},{"id":"c0a37659a3ef7a08865807f40e5c8016.html","title":"Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge","url":"http://scholar.google.com/scholar?cluster=8805697316973288288&hl=en&oi=scholarr","authors":["Myroslava O Dzikovska","Rodney D Nielsen","Chris Brew","Claudia Leacock","Danilo Giampiccolo","Luisa Bentivogli","Peter Clark","Ido Dagan","Hoa T Dang"],"date":"2013/06","abstract":"We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2-way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and compare results from 9 participating teams, and discuss future directions."},{"id":"3552c6d949fcb36062a7e09854f918e4.html","title":"Automatic thesaurus construction for cross generation corpus","url":"https://dl.acm.org/doi/abs/10.1145/2442080.2442084","authors":["Hadas Zohar","Chaya Liebeskind","Jonathan Schler","Ido Dagan"],"date":"2013/04/11","journal":"Journal on Computing and Cultural Heritage (JOCCH)","abstract":"This article describes methods for semiautomatic thesaurus construction, for a cross generation, cross genre, and cross cultural corpus. Semiautomatic thesaurus construction is a complex task, and applying it on a cross generation corpus brings its own challenges. We used a Jewish juristic corpus containing documents and genres that were written across 2000 years, and contain a mix of different languages, dialects, geographies, and writing styles. We evaluated different first and second order methods, and introduced a special annotation scheme for this problem, which showed that first order methods performed surprisingly well. We found that in our case, improving the coverage is the more difficult task, for this we introduce a new algorithm to increase recall (coverage)\u2014which is applicable to many other problems as well, and demonstrates significant improvement in our corpus."},{"id":"d6d5fd865d700c52b9c13618b3748dda.html","title":"Parsing morphologically rich languages: Introduction to the special issue","url":"https://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00133","authors":["Reut Tsarfaty","Djam\\\\xe9 Seddah","Sandra K\\\\xfcbler","Joakim Nivre"],"date":"2013/03","journal":"Computational linguistics","abstract":"Parsing is a key task in natural language processing. It involves predicting, for each natural language sentence, an abstract representation of the grammatical entities in the sentence and the relations between these entities. This representation provides an interface to compositional semantics and to the notions of \u201cwho did what to whom.\u201d The last two decades have seen great advances in parsing English, leading to major leaps also in the performance of applications that use parsers as part of their backbone, such as systems for information extraction, sentiment analysis, text summarization, and machine translation. Attempts to replicate the success of parsing English for other languages have often yielded unsatisfactory results. In particular, parsing languages with complex word structure and flexible word order has been shown to require non-trivial adaptation. This special issue reports on methods that successfully\\\\xa0\u2026"},{"id":"dd4d2b676e414e0c4445bc5254fbaee3.html","title":"Word Segmentation, Unknown-word Resolution, and Morphological Agreement in a Hebrew Parsing System","url":"https://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00137","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2013/03","journal":"Computational Linguistics","abstract":"We present a constituency parsing system for Modern Hebrew. The system is based on the PCFG-LA parsing method of Petrov et al. , which is extended in various ways in order to accommodate the specificities of Hebrew as a morphologically rich language with a small treebank. We show that parsing performance can be enhanced by utilizing a language resource external to the treebank, specifically, a lexicon-based morphological analyzer. We present a computational model of interfacing the external lexicon and a treebank-based parser, also in the common case where the lexicon and the treebank follow different annotation schemes. We show that Hebrew word-segmentation and constituency-parsing can be performed jointly using CKY lattice parsing. Performing the tasks jointly is effective, and substantially outperforms a pipeline-based model. We suggest modeling grammatical agreement in a constituency\\\\xa0\u2026"},{"id":"366ca3dd4aa01b55b5a9356f316a8b5f.html","title":"A Dataset of Syntactic-Ngrams over Time from a Very Large Corpus of English Books","url":"https://research.google/pubs/pub41603.pdf","authors":["Yoav Goldberg","Jon Orwant"],"date":"2013","abstract":"We created a dataset of syntactic-ngrams (counted dependency-tree fragments) based on a corpus of 3.5 million English books. The dataset includes over 10 billion distinct items covering a wide range of syntactic configurations. It also includes temporal information, facilitating new kinds of research into lexical semantics over time. This paper describes the dataset, the syntactic representation, and the kinds of information provided."},{"id":"63150439e95494cad82a162062bbbfbe.html","title":"Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages","url":"https://www.aclweb.org/anthology/W13-4900.pdf","authors":["Yoav Goldberg","Yuval Marton","Ines Rehbein","Yannick Versley"],"date":"2013","journal":"Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages","abstract":"The papers in these proceedings were presented at the fourth Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL 2013), held in Seattle, USA, on October 18th, 2013, in conjunction with the Conference on Empirical Methods in Natural Language Processing (EMNLP 2013). SPMRL is endorsed by the ACL SIGPARSE and SIGLEX interest groups and provides a forum for research in parsing morphologically-rich languages, with the goal of identifying cross-cutting issues in the annotation and parsing methodology for such languages, which typically have more flexible word order and/or higher word-form variation than English."},{"id":"63c84d150a628d1d8e60f38b2e6b2b62.html","title":"Book Review: Design Patterns in Fluid Construction Grammar edited by Luc Steels","url":"https://aclanthology.org/J13-2006.pdf","authors":["Nathan Schneider","Reut Tsarfaty"],"date":"2013","journal":"Computational Linguistics","abstract":"In computational modeling of natural language phenomena, there are at least three modes of research. The currently dominant statistical paradigm typically prioritizes instance coverage: Data-driven methods seek to use as much information observed in data as possible in order to generalize linguistic analyses to unseen instances. A second approach prioritizes detailed description of grammatical phenomena, that is, forming and defending theories with a focus on a small number of instances. A third approach might be called integrative: Rather than addressing phenomena in isolation, different approaches are brought together to address multiple challenges in a unified framework, and the behavior of the system is demonstrated with a small number of instances. Design"},{"id":"6e30e8fd2a3925f46ece6e9d58596d69.html","title":"Recognizing textual entailment","url":"https://pdfs.semanticscholar.org/e9cb/15132afd04ae93ba693044fd73f6746f58c4.pdf","authors":["Ido Dagan","Dan Roth","Mark Sammons","Fabio Zanzotto"],"date":"2013","abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Total citations"},{"id":"8b6c041d53a2627ccb548aaf993a92a0.html","title":"Universal Dependency Annotation for Multilingual Parsing.","url":"https://www.aclweb.org/anthology/P13-2017.pdf","authors":["Ryan T McDonald","Joakim Nivre","Yvonne Quirmbach-Brundage","Yoav Goldberg","Dipanjan Das","Kuzman Ganchev","Keith B Hall","Slav Petrov","Hao Zhang","Oscar T\\\\xe4ckstr\\\\xf6m","Claudia Bedini","N\\\\xfaria Bertomeu Castell\\\\xf3","Jungmee Lee"],"date":"2013","abstract":"We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English, Swedish, Spanish, French and Korean. To show the usefulness of such a resource, we present a case study of crosslingual transfer parsing with more reliable evaluation than has been possible before. This \u2018universal\u2019treebank is made freely available in order to facilitate research on multilingual dependency parsing. 1"},{"id":"9fc950d91120cb30652343715fc96fa5.html","title":"A Non-Monotonic Arc-Eager Transition System for Dependency Parsing","url":"https://www.aclweb.org/anthology/W13-3518.pdf","authors":["Matthew Honnibal","Yoav Goldberg","Markn Johnson"],"date":"2013","abstract":"Previous incremental parsers have used monotonic state transitions. However, transitions can be made to revise previous decisions quite naturally, based on further information."},{"id":"c321a86bcb1bcbad1b67d5e44022472e.html","title":"Efficient Implementation of Beam-Search Incremental Parsers","url":"https://www.aclweb.org/anthology/P13-2111.pdf","authors":["Yoav Goldberg","Kai Zhao","Liang Huang"],"date":"2013","abstract":"Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O (n2), rather than linear time, because each statetransition is actually implemented as an O (n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O (1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of\u223c 2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences."},{"id":"e1f2be14ce3ff1c744b95dcfe8b3b701.html","title":"A dynamic oracle for arc-eager dependency parsing","url":"https://www.aclweb.org/anthology/C12-1059.pdf","authors":["Yoav Goldberg","Joakim Nivre"],"date":"2012/12","abstract":"The standard training regime for transition-based dependency parsers makes use of an oracle, which predicts an optimal transition sequence for a sentence and its gold tree. We present an improved oracle for the arc-eager transition system, which provides a set of optimal transitions for every valid parser configuration, including configurations from which the gold tree is not reachable. In such cases, the oracle provides transitions that will lead to the best reachable tree from the given configuration. The oracle is efficient to implement and provably correct. We use the oracle to train a deterministic left-to-right dependency parser that is less sensitive to error propagation, using an online training procedure that also explores parser configurations resulting from non-optimal sequences of transitions. This new parser outperforms greedy parsers trained using conventional oracles on a range of data sets, with an average improvement of over 1.2 LAS points and up to almost 3 LAS points on some data sets."},{"id":"7cc8e2bc310a0dd3761182f531053e03.html","title":"Semantic annotation for textual entailment recognition","url":"https://link.springer.com/chapter/10.1007/978-3-642-37798-3_2","authors":["Assaf Toledo","Sophia Katrenko","Stavroula Alexandropoulou","Heidi Klockmann","Asher Stern","Ido Dagan","Yoad Winter"],"date":"2012/10/27","abstract":"We introduce a new semantic annotation scheme for the Recognizing Textual Entailment (RTE) dataset as well as a manually annotated dataset that uses this scheme. The scheme addresses three types of modification that license entailment patterns: <i>restrictive</i>, <i>appositive</i> and <i>conjunctive</i>, with a formal semantic specification of these patterns\u2019 contribution for establishing entailment. These inferential constructions were found to occur in 77.68% of the entailments in the RTE 1-3 corpora. They were annotated with cross-annotator agreement of 70.73% on average. A central aim of our annotations is to examine components that address these phenomena in RTE systems. Specifically, the new annotated dataset is used for examining a syntactic rule base within the BIUTEE recognizer, a publicly available entailment system. According to our tests, the rule base is rarely used to process the phenomena\\\\xa0\u2026"},{"id":"3bffee13958cc941eca266f577e5560b.html","title":"Event orientated adnominals and compositionality","url":"https://www.phil.uu.nl/~yoad/papers/WinterZwartsEventAdnominals.pdf","authors":["Yoad Winter","Joost Zwarts"],"date":"2012/10","journal":"Unpublished Ms., to appear in Proceedings of Annual Meeting of the Israeli Association of Theoretical Linguistics","abstract":"When a noun is modified by an adnominal, as in blue door, short giraffe, or book on the table, the modifier is often analyzed as applying to an argument of the predicate that the noun denotes. We call this argument the referential argument of the noun (Williams 1981, Higginbotham 1985). Consider for instance (1).(1) a.[[blue door]]= \u03bbx. door (x)< x is blue b.[[short giraffe]]= \u03bbx. giraffe (x)< x is short (relative to giraffes) c.[[book on the table]]= \u03bbx. book (x)< x is on the table"},{"id":"f9bea65f13ea0df7d40bbf30535758a5.html","title":"Textual entailment","url":"https://nlpado.de/~sebastian/pub/papers/ox13_pado_preprint.pdf","authors":["Sebastian Pad\\\\xf3","Ido Dagan"],"date":"2012/08/29","journal":"Oxford Handbook of Computational Linguistics. Oxford University Press, Oxford","abstract":"Textual entailment is a binary relation between two natural language texts (the so-called text and hypothesis) that holds when readers of the text would agree that the hypothesis is most likely true (Peter is snoring\u2192 A man sleeps). The recognition of textual entailment requires an account of linguistic variability (ie, the possibility to realize a certain state of affairs in different ways, as in Peter buys the car\u2194 The car is purchased by Peter) as well as of the derivation of additional knowledge (as in Peter buys the car\u2192 Peter owns the car). In contrast to classical (logics-based) inference, textual entailment also covers cases of very probable, but still defeasible, entailment (A hurricane hit Peter\u2019s town\u2192 Peter\u2019s town was damaged). A substantial part of human common-sense reasoning involves such defeasible inferences. As a consequence, textual entailment is of considerable interest for many real-world language processing tasks where it can serve as a generic, application-independent framework for semantic inference. This chapter discusses the history of textual entailment, relevant linguistic phenomena, approaches to recognizing textual entailment, and its integration in various NLP tasks."},{"id":"084cc39218d8d2d987106fe995f846a1.html","title":"Houvardas, J., and E. Stamatatos (2006).","url":"http://library.oapen.org/bitstream/handle/20.500.12657/27654/1002351.pdf?sequence=1#page=308","authors":["V Keselj","F Peng","N Cercone","C Thomas","M Koppel","N Akiva","Ido Dagan","J Schler","S Argamon","S Raghavan","A Kovashka","R Mooney","Y Seroussi","I Zukerman","F Bohnert"],"date":"2012/07/16","journal":"Digital Humanities 2012","abstract":"Dogon languages are spoken predominately in eastern Mali in West Africa. The Dogon were made famous by Marcel Griaule, a French anthropologist who pioneered Ethnography in France, and worked with the Dogon between 1931-1956. He reported that the Dogon had advanced astronomical knowledge of the Sirius binary star system, knowledge that is not possible without telescope. Since then, the Dogon have been shrouded in controversy and mystery."},{"id":"dc7f74d882353edb2ee95865c8a4b3f2.html","title":"Domain adaptation of a dependency parser with a class-class selectional preference model","url":"https://www.aclweb.org/anthology/W12-3308.pdf","authors":["Raphael Cohen","Yoav Goldberg","Michael Elhadad"],"date":"2012/07/09","abstract":"When porting parsers to a new domain, many of the errors are related to wrong attachment of out-of-vocabulary words. Since there is no available annotated data to learn the attachment preferences of the target domain words, we attack this problem using a model of selectional preferences based on domainspecific word classes. Our method uses Latent Dirichlet Allocations (LDA) to learn a domain-specific Selectional Preference model in the target domain using un-annotated data. The model provides features that model the affinities among pairs of words in the domain. To incorporate these new features in the parsing model, we adopt the co-training approach and retrain the parser with the selectional preferences features. We apply this method for adapting Easy First, a fast nondirectional parser trained on WSJ, to the biomedical domain (Genia Treebank). The Selectional Preference features reduce error by 4.5% over the co-training baseline."},{"id":"6e83f7989aac934e6f9adccfa3b66608.html","title":"Efficient tree-based approximation for entailment graph learning","url":"https://www.aclweb.org/anthology/P12-1013.pdf","authors":["Jonathan Berant","Ido Dagan","Meni Adler","Jacob Goldberger"],"date":"2012/07","abstract":"Learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years. In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates (termed entailment graphs). We first identify that entailment graphs exhibit a \u201ctree-like\u201d property and are very similar to a novel type of graph termed forest-reducible graph. We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges, where each iteration takes linear time. We compare our approximation algorithm to a recently-proposed state-of-the-art exact algorithm and show that it is more efficient and scalable both theoretically and empirically, while its output quality is close to that given by the optimal solution of the exact algorithm."},{"id":"7a70d4523bfbbdd4fd00f9d2565604d6.html","title":"Learning verb inference rules from linguistically-motivated evidence","url":"https://www.aclweb.org/anthology/D12-1018.pdf","authors":["Hila Weisman","Jonathan Berant","Idan Szpektor","Ido Dagan"],"date":"2012/07","abstract":"Learning inference relations between verbs is at the heart of many semantic applications. However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework. We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task."},{"id":"84ece2bdea9fc53240c553f8913b71b6.html","title":"Entailment-based text exploration with application to the health-care domain","url":"https://www.aclweb.org/anthology/P12-3014.pdf","authors":["Meni Adler","Jonathan Berant","Ido Dagan"],"date":"2012/07","abstract":"We present a novel text exploration model, which extends the scope of state-of-the-art technologies by moving from standard concept-based exploration to statement-based exploration. The proposed scheme utilizes the textual entailment relation between statements as the basis of the exploration process. A user of our system can explore the result space of a query by drilling down/up from one statement to another, according to entailment relations specified by an entailment graph and an optional concept taxonomy. As a prominent use case, we apply our exploration system and illustrate its benefit on the health-care domain. To the best of our knowledge this is the first implementation of an exploration system at the statement level that is based on the textual entailment relation."},{"id":"8ad97e761f4c0dcd426cb897a86fa496.html","title":"Biutee: A modular open-source system for recognizing textual entailment","url":"https://www.aclweb.org/anthology/P12-3013.pdf","authors":["Asher Stern","Ido Dagan"],"date":"2012/07","abstract":"This paper introduces BIUTEE 1, an opensource system for recognizing textual entailment. Its main advantages are its ability to utilize various types of knowledge resources, and its extensibility by which new knowledge resources and inference components can be easily integrated. These abilities make BIUTEE an appealing RTE system for two research communities:(1) researchers of end applications, that can benefit from generic textual inference, and (2) RTE researchers, who can integrate their novel algorithms and knowledge resources into our system, saving the time and effort of developing a complete RTE system from scratch. Notable assistance for these researchers is provided by a visual tracing tool, by which researchers can refine and \u201cdebug\u201d their knowledge resources and inference components."},{"id":"bcdd132a9ddde1cf69065c99824d5c71.html","title":"Efficient search for transformation-based inference","url":"https://www.aclweb.org/anthology/P12-1030.pdf","authors":["Asher Stern","Roni Stern","Ido Dagan","Ariel Felner"],"date":"2012/07","abstract":"This paper addresses the search problem in textual inference, where systems need to infer one piece of text from another. A prominent approach to this task is attempts to transform one text into the other through a sequence of inference-preserving transformations, aka a proof, while estimating the proof\u2019s validity. This raises a search challenge of finding the best possible proof. We explore this challenge through a comprehensive investigation of prominent search algorithms and propose two novel algorithmic components specifically designed for textual inference: a gradient-style evaluation function, and a locallookahead node expansion method. Evaluations, using the open-source system, BIUTEE, show the contribution of these ideas to search efficiency and proof quality."},{"id":"fb6cd0a51751f417d524071bb68ce84b.html","title":"Crowdsourcing inference-rule evaluation","url":"https://www.aclweb.org/anthology/P12-2031.pdf","authors":["Naomi Zeichner","Jonathan Berant","Ido Dagan"],"date":"2012/07","abstract":"The importance of inference rules to semantic applications has long been recognized and extensive work has been carried out to automatically acquire inference-rule resources. However, evaluating such resources has turned out to be a non-trivial task, slowing progress in the field. In this paper, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed \u201cinstance-based evaluation\u201d method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators."},{"id":"fc7e018e1c6b4dfcff915c54a97fd25c.html","title":"Joint evaluation of morphological segmentation and syntactic parsing","url":"https://www.aclweb.org/anthology/P12-2002.pdf","authors":["Reut Tsarfaty","Joakim Nivre","Evelina Andersson"],"date":"2012/07","abstract":"We present novel metrics for parse evaluation in joint segmentation and parsing scenarios where the gold sequence of terminals is not known in advance. The protocol uses distance-based metrics defined for the space of trees over lattices. Our metrics allow us to precisely quantify the performance gap between non-realistic parsing scenarios (assuming gold segmented and tagged input) and realistic ones (not assuming gold segmentation and tags). Our evaluation of segmentation and parsing for Modern Hebrew sheds new light on the performance of the best parsing systems to date in the different scenarios."},{"id":"231349fed4d08f989e39f0bb24ac1905.html","title":"Precision-biased Parsing and High-Quality Parse Selection","url":"https://arxiv.org/abs/1205.4387","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2012/05/20","journal":"Arxiv preprint arXiv:1205.4387","abstract":"We introduce precision-biased parsing: a parsing task which favors precision over recall by allowing the parser to abstain from decisions deemed uncertain. We focus on dependency-parsing and present an ensemble method which is capable of assigning parents to 84% of the text tokens while being over 96% accurate on these tokens. We use the precision-biased parsing task to solve the related high-quality parse-selection task: finding a subset of high-quality (accurate) trees in a large collection of parsed text. We present a method for choosing over a third of the input trees while keeping unlabeled dependency parsing accuracy of 97% on these trees. We also present a method which is not based on an ensemble but rather on directly predicting the risk associated with individual parser decisions. In addition to its efficiency, this method demonstrates that a parsing system can provide reasonable estimates of confidence in its predictions without relying on ensembles or aggregate corpus counts."},{"id":"9a501fac2b05fd383c7a3519da295bcc.html","title":"Task-specific Word-Clustering for Part-of-Speech Tagging","url":"https://arxiv.org/abs/1205.4298","authors":["Yoav Goldberg"],"date":"2012/05/19","journal":"Arxiv preprint arXiv:1205.4298","abstract":"While the use of cluster features became ubiquitous in core NLP tasks, most cluster features in NLP are based on distributional similarity. We propose a new type of clustering criteria, specific to the task of part-of-speech tagging. Instead of distributional similarity, these clusters are based on the beha vior of a baseline tagger when applied to a large corpus. These cluster features provide similar gains in accuracy to those achieved by distributional-similarity derived clusters. Using both types of cluster features together further improve tagging accuracies. We show that the method is effective for both the in-domain and out-of-domain scenarios for English, and for French, German and Italian. The effect is larger for out-of-domain text."},{"id":"38f01f81bd6f9e50b621a53d11f993d8.html","title":"Proceedings of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages","url":"https://www.aclweb.org/anthology/W12-3400.pdf","authors":["Marianna Apidianaki","Ido Dagan","Jennifer Foster","Yuval Marton","Djam\\\\xe9 Seddah","Reut Tsarfaty"],"date":"2012","abstract":"Morphologically Rich Languages (MRLs) are languages in which grammatical relations such as Subject, Predicate, and Object, are largely indicated morphologically (eg, through inflection) instead of positionally. This poses serious challenges for current (English-centric) syntactic and semantic processing. Furthermore, since grammatical relations provide the interface to compositional semantics, morpho-syntactic phenomena may significantly complicate processing the syntax\u2013semantics interface. In statistical parsing, English parsing performance has reached a high plateau in certain genres. Semantic processing of English has similarly seen much progress in recent years. MRL processing presents new challenges, such as optimal morphological representation, non position-centric algorithms, or different semantic distance measures."},{"id":"4dd02b8444502483fbe695cce53cd47b.html","title":"A probabilistic lexical model for ranking textual inferences","url":"https://www.aclweb.org/anthology/S12-1032.pdf","authors":["Eyal Shnarch","Ido Dagan","Jacob Goldberger"],"date":"2012","abstract":"Identifying textual inferences, where the meaning of one text follows from another, is a general underlying task within many natural language applications. Commonly, it is approached either by generative syntactic-based methods or by \u201clightweight\u201d heuristic lexical models. We suggest a model which is confined to simple lexical information, but is formulated as a principled generative probabilistic model. We focus our attention on the task of ranking textual inferences and show substantially improved results on a recently investigated question answering data set."},{"id":"6ac5d14ef61d0d803a18a0b25f4d997b.html","title":"Statistical thesaurus construction for a morphologically rich language","url":"https://www.aclweb.org/anthology/S12-1009.pdf","authors":["Chaya Liebeskind","Ido Dagan","Jonathan Schler"],"date":"2012","abstract":"Corpus-based thesaurus construction for Morphologically Rich Languages (MRL) is a complex task, due to the morphological variability of MRL. In this paper we explore alternative term representations, complemented by clustering of morphological variants. We introduce a generic algorithmic scheme for thesaurus construction in MRL, and demonstrate the empirical benefit of our methodology for a Hebrew thesaurus."},{"id":"cefd7a13c7131c5ea387e0abc5d2230a.html","title":"Cross-Framework Evaluation for Statistical Parsing","url":"https://www.diva-portal.org/smash/record.jsf?pid=diva2:585582","authors":["Joakim Nivre","Reut Tsarfaty","Evelina Andersson"],"date":"2012","abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Scholar articles"},{"id":"c4c63c710c90188a9520efbb513d3085.html","title":"Introduction to the EXCITEMENT project: towards an open platform for EXploring Customer Interactions through Textual entailMENT","url":"https://www.isca-speech.org/archive/avios_12/avio_024.html","authors":["Moshe Wasserblat","Ezra Daya","Eyal Hurvitz","Maya Gorodetsky","Dmitri Volsky","Ido Dagan","Meni Adler","Asher Steren","Sebastian Pado","Tae-Gil Noh","Britta Zeller","G\\\\xfcnter Neumann","Kathrin Eichler","Rui Wang","Gabriele Fidanza","Giorgio Gianforme","Matthias Meisdrock","Bernardom Magnini","Luisa Bentivogli","Roberto Zanoli","Alberto Lavelli"],"date":"2012","abstract":"Identifying semantic inferences between text units is a major underlying language processing task, needed in practically all text understanding applications. While such inferences are broadly needed, there are currently no generic semantic \u201cengines\u201d or platforms for broad textual inference. The primary scientific motivation for the EXCITEMENT project is to change this ineffective state of affairs and to offer an encompassing open source platform for textual inference. On the industrial side, EXCITEMENT is focused on the text analytics and speech analytics markets and follows the increasing demand for automatically analyzing customer interactions, which today cross multiple channels including speech, email, chat and social media."},{"id":"df461587fbf8682009caca3df1bbcabb.html","title":"Sentence clustering via projection over term clusters","url":"https://www.aclweb.org/anthology/S12-1005.pdf","authors":["Lili Kotlerman","Ido Dagan","Maya Gorodetsky","Ezra Daya"],"date":"2012","abstract":"This paper presents a novel sentence clustering scheme based on projecting sentences over term clusters. The scheme incorporates external knowledge to overcome lexical variability and small corpus size, and outperforms common sentence clustering methods on two reallife industrial datasets."},{"id":"78c3b7e65a109338cc25fffa50916b5d.html","title":"Automatic Syntactic Processing of Modern Hebrew","url":"https://www.cs.bgu.ac.il/~elhadad/nlpproj/pub/yoav-phd.pdf","authors":["Yoav Goldberg"],"date":"2011/11","abstract":"Natural language is the primary means of communication between humans. Language is composed of words, which are combined to form sentences, which, in turn, are combined to form larger units such as paragraphs. While single words can convey myriad meanings, it is the combination of words into sentences that allows for efficient communication and the realization of complex ideas. When words are combined to form a sentence, their combination is governed by a set of rules, called the syntax, or the grammar, of the language. These syntactic rules pose constraints on the ways in which words can be combined (\u201ckid cake the ate a\u201d is clearly ungrammatical), and assign meanings to valid configurations of words (\u201cthe kid ate a cake\u201d and \u201cthe cake ate a kid\u201d are both well formed sentences and contain the exact same words, yet they convey two entirely different messages. The first is an everyday event, while the second is a creepy event from a bizarre horror movie). In other words, sentences must obey structural constraints posed by the syntax, and the structure of the sentence determines its meaning. In many cases, the sentence structure can be recovered even when the sentence words are unknown: consider \u201cthe plumpets ghoked a gloomp\u201d\u2013without knowing what plumpets, ghoked or gloomp are (these are all made-up words), we know that there is an action of \u201cghoking\u201d, there is something called \u201cplumpets\u201d(probably composed of several \u201cplumpet\u201d) which is the subject of the ghoking action (which happened in the past) and that there is something called a \u201cgloomp\u201d which is the object of the ghoking action. This is an important property, that\\\\xa0\u2026"},{"id":"e0f5fc9dbccafd6b332fb833c34815ff.html","title":"The Seventh PASCAL Recognizing Textual Entailment Challenge.","url":"http://allenai.org/content/team/peterc/publications/RTE7_overview.proceedings.pdf","authors":["Luisa Bentivogli","Peter Clark","Ido Dagan","Danilo Giampiccolo"],"date":"2011/11","abstract":"This paper presents the Seventh Recognizing Textual Entailment (RTE-7) challenge. This year\u2019s challenge replicated the exercise proposed in RTE-6, consisting of a Main Task, in which Textual Entailment is performed on a real corpus in the Update Summarization scenario; a Main subtask aimed at detecting novel information; and a KBP Validation Task, in which RTE systems had to validate the output of systems participating in the KBP Slot Filling Task. Thirteen teams participated in the Main Task (submitting 33 runs) and 5 in the Novelty Detection Subtask (submitting 13 runs). The KBP Validation Task was undertaken by 2 participants which submitted 5 runs. The ablation test experiment, introduced in RTE-5 to evaluate the impact of knowledge resources used by the systems participating in the Main Task and extended also to tools in RTE-6, was also repeated in RTE-7."},{"id":"12d35eb89a1e48e0c7e3edf2e9cd37d9.html","title":"Knowledge and Tree-Edits in Learnable Entailment Proofs.","url":"https://www.researchgate.net/profile/Lili_Kotlerman/publication/267917672_Knowledge_and_Tree-Edits_in_Learnable_Entailment_Proofs/links/547472650cf2778985abddae/Knowledge-and-Tree-Edits-in-Learnable-Entailment-Proofs.pdf","authors":["Asher Stern","Shachar Mirkin","Eyal Shnarch","Lili Kotlerman","Ido Dagan","Amnon Lotan","Jonathan Berant"],"date":"2011/10/25","abstract":"This paper describes BIUTEE-Bar Ilan University Textual Entailment Engine. BIUTEE is a natural language inference system in which the hypothesis is proven by the text, based on linguistic-and world-knowledge resources, as well as syntactically motivated tree transformations. The main progress in BIUTEE in the last year is a new confidence model that estimates the validity of the proof found by BIUTEE."},{"id":"87948e71862a3e179cb566575508c69e.html","title":"Proceedings of the Second Workshop on Statistical Parsing of Morphologically Rich Languages","url":"https://www.aclweb.org/anthology/W11-3800.pdf","authors":["Djam\\\\xe9 Seddah","Reut Tsarfaty","Jennifer Foster"],"date":"2011/10","abstract":"Welcome to the second workshop on Statistical Parsing of Morphologically Rich Languages! Following the warm reception of the first official SPMRL workshop at NAACL-HLT 2010, our aim with the second workshop is to build upon the success of the first and offer a platform to the growing community of people who are interested in developing tools and resources for parsing MRLs. We decided to collocate with the International Workshop on Parsing Technologies (IWPT), both because the themes of the two events are so closely related and because the seeds of the SPMRL workshop were planted during IWPT 2009 in Paris. The warm welcome and support of the IWPT community made it our unequivocal choice, and we are honored and pleased to collocate our second SPMRL workshop with this year\u2019s IWPT event"},{"id":"6b7d4738b564dde1198537a1d1b65025.html","title":"A support tool for deriving domain taxonomies from wikipedia","url":"https://www.aclweb.org/anthology/R11-1069.pdf","authors":["Lili Kotlerman","Zemer Avital","Ido Dagan","Amnon Lotan","Ofer Weintraub"],"date":"2011/09","abstract":"Organizing data into category hierarchies (taxonomies) is useful for content discovery, search, exploration and analysis. In industrial settings targeted taxonomies for specific domains are mostly created manually, typically by domain experts, which is time consuming and requires a high level of expertise. This paper presents an algorithm and an implemented interactive system for automatically generating target-domain taxonomies based on the Wikipedia Category Hierarchy. The system also enables human post-editing, facilitated by intelligent assistance."},{"id":"94e66652d864fe89d2a0664de7e69ac9.html","title":"A confidence model for syntactically-motivated entailment proofs","url":"https://www.aclweb.org/anthology/R11-1063.pdf","authors":["Asher Stern","Ido Dagan"],"date":"2011/09","abstract":"This paper presents a novel method for recognizing textual entailment which derives the hypothesis from the text through a sequence of parse tree transformations. Unlike related approaches based on tree-edit-distance, we employ transformations which better capture linguistic structures of entailment. This is achieved by (a) extending an earlier deterministic knowledge-based algorithm with syntactically-motivated on-the-fly transformations, and (b) by introducing an algorithm that uniformly learns costs for all types of transformations. Our evaluations and analysis support the validity of this approach."},{"id":"dc51e24a091120c15e119bed64321e7f.html","title":"Sparse Canonical Correlation Analysis for Biomarker Discovery: A Case Study in Tuberculosis","url":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.232.9577&rep=rep1&type=pdf#page=81","authors":["Juho Rousu","Daniel D Agranoff","John Shawe-Taylor","Delmiro Fernandez-Reyes"],"date":"2011/07/20","journal":"Machine Learning in Systems Biology","abstract":"Biomarker discovery from\u2019omics data is a challenging task due to the high dimensionality of data and the relative scarcity of samples. Here we explore the potential of canonical correlation analysis, a family of methods that finds correlated components in two views. In particular we use the recently introduced technique of sparse canonical correlation analysis that finds a projection directions that are primally sparse in one of the views and dually sparse in the other view. Our experiments show that the method is able to discover meaningful feature combinations that may have use as biomarkers for tuberculosis."},{"id":"2f284473cf82cf4c1d68e45ee852ce7e.html","title":"Evaluating dependency parsing: Robust and heuristics-free cross-annotation evaluation","url":"https://www.aclweb.org/anthology/D11-1036.pdf","authors":["Reut Tsarfaty","Joakim Nivre","Evelina Andersson"],"date":"2011/07","abstract":"Methods for evaluating dependency parsing using attachment scores are highly sensitive to representational variation between dependency treebanks, making cross-experimental evaluation opaque. This paper develops a robust procedure for cross-experimental evaluation, based on deterministic unificationbased operations for harmonizing different representations and a refined notion of tree edit distance for evaluating parse hypotheses relative to multiple gold standards. We demonstrate that, for different conversions of the Penn Treebank into dependencies, performance trends that are observed for parsing results in isolation change or dissolve completely when parse hypotheses are normalized and brought into the same common ground."},{"id":"8f6891b1dac4a7963b34fbb3c75567f9.html","title":"Towards a probabilistic model for lexical entailment","url":"https://www.aclweb.org/anthology/W11-2402.pdf","authors":["Eyal Shnarch","Jacob Goldberger","Ido Dagan"],"date":"2011/07","abstract":"While modeling entailment at the lexical-level is a prominent task, addressed by most textual entailment systems, it has been approached mostly by heuristic methods, neglecting some of its important aspects. We present a probabilistic approach for this task which covers aspects such as differentiating various resources by their reliability levels, considering the length of the entailed sentence, the number of its covered terms and the existence of multiple evidence for the entailment of a term. The impact of our model components is validated by evaluations, which also show that its performance is in line with the best published entailment systems."},{"id":"ae7d28826457a8cb324c98aab352023f.html","title":"Classification-based contextual preferences","url":"https://www.aclweb.org/anthology/W11-2403.pdf","authors":["Shachar Mirkin","Ido Dagan","Lili Kotlerman","Idan Szpektor"],"date":"2011/07","abstract":"This paper addresses context matching in textual inference. We formulate the task under the Contextual Preferences framework which broadly captures contextual aspects of inference. We propose a generic classificationbased scheme under this framework which coherently attends to context matching in inference and may be employed in any inferencebased task. As a test bed for our scheme we use the Name-based Text Categorization (TC) task. We define an integration of Contextual Preferences into the TC setting and present a concrete self-supervised model which instantiates the generic scheme and is applied to address context matching in the TC task. Experiments on standard TC datasets show that our approach outperforms the state of the art in context modeling for Name-based TC."},{"id":"671dfed607987823a04e02b5df8f6d37.html","title":"Joint Hebrew segmentation and parsing using a PCFG-LA lattice parser","url":"https://www.aclweb.org/anthology/P11-2124.pdf","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2011/06/19","abstract":"We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser. We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task. This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs."},{"id":"d7ed5380a7279526cde23e0d9bfbb62a.html","title":"Cross-partition clustering: revealing corresponding themes across related datasets","url":"https://www.tandfonline.com/doi/abs/10.1080/0952813X.2010.490960","authors":["Zvika Marx","Ido Dagan","Eli Shamir"],"date":"2011/06/01","journal":"Journal of Experimental & Theoretical Artificial Intelligence","abstract":"This article studies the task of discovering correspondences across related domains based on real-world data collections. We address this task through a designated extension of distributional data-clustering methods. The method is empirically demonstrated on synthetic data as well as on texts addressing different religions, where the goal is to identify commonalities shared by all religions. This article generalises and demonstrates the empirical improvement relative to our previous studies on this subject, as well as to other comparable methods."},{"id":"10b5842436697e001ee40dc30fa49b92.html","title":"Language-independent parsing with empty elements","url":"https://www.aclweb.org/anthology/P11-2037.pdf","authors":["Shu Cai","David Chiang","Yoav Goldberg"],"date":"2011/06","abstract":"We present a simple, language-independent method for integrating recovery of empty elements into syntactic parsing. This method outperforms the best published method we are aware of on English and a recently published method on Chinese."},{"id":"59577f4b618b4002841b83dd3cf1fea2.html","title":"A probabilistic modeling framework for lexical entailment","url":"https://www.aclweb.org/anthology/P11-2098.pdf","authors":["Eyal Shnarch","Jacob Goldberger","Ido Dagan"],"date":"2011/06","abstract":"Recognizing entailment at the lexical level is an important and commonly-addressed component in textual inference. Yet, this task has been mostly approached by simplified heuristic methods. This paper proposes an initial probabilistic modeling framework for lexical entailment, with suitable EM-based parameter estimation. Our model considers prominent entailment factors, including differences in lexical-resources reliability and the impacts of transitivity and multiple evidence. Evaluations show that the proposed model outperforms most prior systems while pointing at required future improvements."},{"id":"6d6b69b7692ec0389facfef65815db0b.html","title":"Global learning of typed entailment rules","url":"https://www.aclweb.org/anthology/P11-1062.pdf","authors":["Jonathan Berant","Ido Dagan","Jacob Goldberger"],"date":"2011/06","abstract":"Extensive knowledge bases of entailment rules between predicates are crucial for applied semantic inference. In this paper we propose an algorithm that utilizes transitivity constraints to learn a globally-optimal set of entailment rules for typed predicates. We model the task as a graph learning problem and suggest methods that scale the algorithm to larger graphs. We apply the algorithm over a large data set of extracted predicate instances, from which a resource of typed entailment rules has been recently released (Schoenmackers et al., 2010). Our results show that using global transitivity information substantially improves performance over this resource and several baselines, and that our scaling methods allow us to increase the scope of global learning of entailment-rule graphs."},{"id":"47b9f7c2dc31c45aa816e19449a2ee78.html","title":"Learning entailment relations by global graph structure optimization","url":"https://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00085","authors":["Jonathan Berant","Ido Dagan","Jacob Goldberger"],"date":"2011/03","journal":"Computational Linguistics","abstract":"Identifying entailment relations between predicates is an important part of applied semantic inference. In this article we propose a global inference algorithm that learns such entailment rules. First, we define a graph structure over predicates that represents entailment relations as directed edges. Then, we use a global transitivity constraint on the graph to learn the optimal set of edges, formulating the optimization problem as an Integer Linear Program. The algorithm is applied in a setting where, given a target concept, the algorithm learns on the fly all entailment rules between predicates that co-occur with this concept. Results show that our global algorithm improves performance over baseline algorithms by more than 10%."},{"id":"d74df2d5d948faabfb2f7316eed43fc5.html","title":"Transliterated pairs acquisition in medical Hebrew","url":"https://www.researchgate.net/profile/Michael_Elhadad/publication/266407564_Transliterated_Pairs_Acquisition_in_Medical_Hebrew/links/54b504610cf28ebe92e4acc0.pdf","authors":["Raphael Cohen","Yoav Goldberg","Michael Elhadad"],"date":"2011/01/23","journal":"Proc. Machine Translation and Morphologically-rich Languages Workshop","abstract":"The phonetic transcription of a word from a source language using a different script is called transliteration. Transliterations affect Information Extraction (IE) in two ways. First, it takes time for a transliterated word to make it into a technical lexicon, making recognition difficult. A second problem is the variability of ways a foreign word can be rendered phonetically, leading in most cases (except for very short words) to many possible spellings of the word and, therefore, making lexicon-based recognition difficult. In this paper, we present a method for automatically acquiring transliterated words and their source word in order to improve a technical lexicon, addressing both problems: spelling variants and unknown tokens."},{"id":"8f323dc44c0fea4bbe4f2ad3dc2a64c1.html","title":"Learning sparser perceptron models","url":"https://pdfs.semanticscholar.org/6a4b/296ed7102cde2da400fac7ce6f5f78d6d2de.pdf","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2011","abstract":"The averaged-perceptron learning algorithm is simple, versatile and effective. However, when used in NLP settings it tends to produce very dense solutions, while much sparser ones are also possible. We present a simple modification to the perceptron algorithm which allows it to produce sparser solutions while remaining accurate and computationally efficient. We test the method on a multiclass classification task, a structured prediction task, and a guided learning task. In all of the experiments the method produced models which are about 4-5 times smaller than the averaged perceptron, while remaining as accurate."},{"id":"d59078681ba9bb15faf2072d96be5b73.html","title":"Rich parameterization improves RNA structure prediction","url":"https://www.liebertpub.com/doi/abs/10.1089/cmb.2011.0184","authors":["Shay Zakov","Yoav Goldberg","Michael Elhadad","Michal Ziv-Ukelson"],"date":"2011","journal":"Research in Computational Molecular Biology","abstract":"Current approaches to RNA structure prediction range from physics-based methods, which rely on thousands of experimentally measured thermodynamic parameters, to machine-learning (ML) techniques. While the methods for parameter estimation are successfully shifting toward ML-based approaches, the model parameterizations so far remained fairly constant. We study the potential contribution of increasing the amount of information utilized by RNA folding prediction models to the improvement of their prediction quality. This is achieved by proposing novel models, which refine previous ones by examining more types of structural elements, and larger sequential contexts for these elements. Our proposed fine-grained models are made practical thanks to the availability of large training sets, advances in machine-learning, and recent accelerations to RNA folding algorithms. We show that the application of more\\\\xa0\u2026"},{"id":"10753ba2b719d51be50e0b4c96461a0a.html","title":"Rule Chaining and Approximate Match in textual inference.","url":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.418.7637&rep=rep1&type=pdf","authors":["Asher Stern","Eyal Shnarch","Shachar Mirkin","Lili Kotlerman","Naomi Zeichner","Ido Dagan","Amnon Lotan","Jonathan Berant"],"date":"2010/10/27","abstract":"This paper describes the participation of Bar-Ilan university in the sixth RTE challenge. Our textual-entailment engine, BiUTEE, was enhanced with new components that introduce chaining of lexical-entailment rules, and tackle the problem of approximately matching the text and the hypothesis after all available knowledge of entailment rules was utilized. We have also re-engineered our system aiming at an open-source open architecture. BiUTEE\u2019s performance is better than the median of all-submissions, and outperforms significantly an IR-oriented baseline."},{"id":"c3cc66f3aabc821a16bd7675c2c14f9a.html","title":"Directional distributional similarity for lexical inference","url":"https://u.cs.biu.ac.il/~nlp/downloads/publications/directional-distsim.pdf","authors":["Lili Kotlerman","Ido Dagan","Idan Szpektor","Maayan Zhitomirsky-Geffet"],"date":"2010/10/01","journal":"Natural Language Engineering","abstract":"Distributional word similarity is most commonly perceived as a symmetric relation. Yet, directional relations are abundant in lexical semantics and in many Natural Language Processing (NLP) settings that require lexical inference, making symmetric similarity measures less suitable for their identification. This paper investigates the nature of directional (asymmetric) similarity measures that aim to quantify distributional feature inclusion. We identify desired properties of such measures for lexical inference, specify a particular measure based on Average Precision that addresses these properties, and demonstrate the empirical benefit of directional measures for two different NLP datasets."},{"id":"86b29b844e11848052d42357a5ee6658.html","title":"Recognising entailment within discourse","url":"https://www.aclweb.org/anthology/C10-1087.pdf","authors":["Shachar Mirkin","Jonathan Berant","Ido Dagan","Eyal Shnarch"],"date":"2010/08","abstract":"Texts are commonly interpreted based on the entire discourse in which they are situated. Discourse processing has been shown useful for inference-based application; yet, most systems for textual entailment\u2013a generic paradigm for applied inference\u2013have only addressed discourse considerations via off-the-shelf coreference resolvers. In this paper we explore various discourse aspects in entailment inference, suggest initial solutions for them and investigate their impact on entailment performance. Our experiments suggest that discourse provides useful information, which significantly improves entailment inference, and should be better addressed by future entailment systems."},{"id":"74f931cf2438281038668db5ae922381.html","title":"Inspecting the structural biases of dependency parsing algorithms","url":"https://www.aclweb.org/anthology/W10-2927.pdf","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2010/07/15","abstract":"We propose the notion of a structural bias inherent in a parsing system with respect to the language it is aiming to parse. This structural bias characterizes the behaviour of a parsing system in terms of structures it tends to under-and over-produce. We propose a Boosting-based method for uncovering some of the structural bias inherent in parsing systems. We then apply our method to four English dependency parsers (an Arc-Eager and Arc-Standard transition-based parsers, and first-and second-order graph-based parsers). We show that all four parsers are biased with respect to the kind of annotation they are trained to parse. We present a detailed analysis of the biases that highlights specific differences and commonalities between the parsing systems, and improves our understanding of their strengths and weaknesses."},{"id":"3abbb47ac85de956d2bc52e7078ce2b1.html","title":"Global learning of focused entailment graphs","url":"https://www.aclweb.org/anthology/P10-1124.pdf","authors":["Jonathan Berant","Ido Dagan","Jacob Goldberger"],"date":"2010/07","abstract":"We propose a global algorithm for learning entailment relations between predicates. We define a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program. We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms."},{"id":"4a5b539f7217e5407e4c90c2661f5b4d.html","title":"Generating entailment rules from framenet","url":"https://www.aclweb.org/anthology/P10-2045.pdf","authors":["Roni Ben Aharon","Idan Szpektor","Ido Dagan"],"date":"2010/07","abstract":"Many NLP tasks need accurate knowledge for semantic inference. To this end, mostly WordNet is utilized. Yet Word-Net is limited, especially for inference between predicates. To help filling this gap, we present an algorithm that generates inference rules between predicates from FrameNet. Our experiment shows that the novel resource is effective and complements WordNet in terms of rule coverage."},{"id":"a9d9a4979cd3119029f0a856f4eade6c.html","title":"Assessing the role of discourse references in entailment inference","url":"https://www.aclweb.org/anthology/P10-1123.pdf","authors":["Shachar Mirkin","Ido Dagan","Sebastian Pad\\\\xf3"],"date":"2010/07","abstract":"Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal."},{"id":"27e14d0b3b2b2e684036995b6310e72a.html","title":"Easy first dependency parsing of modern Hebrew","url":"https://www.aclweb.org/anthology/W10-1412.pdf","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2010/06/05","abstract":"We investigate the performance of an easyfirst, non-directional dependency parser on the Hebrew Dependency treebank. We show that with a basic feature set the greedy parser\u2019s accuracy is on a par with that of a first-order globally optimized MST parser. The addition of morphological-agreement feature improves the parsing accuracy, making it on-par with a second-order globally optimized MST parser. The improvement due to the morphological agreement information is persistent both when gold-standard and automatically-induced morphological information is used."},{"id":"cbf97a7f6e67100e67fafbe3570780e7.html","title":"Statistical parsing of morphologically rich languages (SPMRL): what, how and whither","url":"https://www.aclweb.org/anthology/W10-1401.pdf","authors":["Reut Tsarfaty","Djam\\\\xe9 Seddah","Yoav Goldberg","Sandra K\\\\xfcbler","Marie Candito","Jennifer Foster","Yannick Versley","Ines Rehbein","Lamia Tounsi"],"date":"2010/06/05","abstract":"The term Morphologically Rich Languages (MRLs) refers to languages in which significant information concerning syntactic units and relations is expressed at word-level. There is ample evidence that the application of readily available statistical parsing models to such languages is susceptible to serious performance degradation. The first workshop on statistical parsing of MRLs hosts a variety of contributions which show that despite languagespecific idiosyncrasies, the problems associated with parsing MRLs cut across languages and parsing frameworks. In this paper we review the current state-of-affairs with respect to parsing MRLs and point out central challenges. We synthesize the contributions of researchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. The overarching analysis suggests itself as a source of directions for future investigations."},{"id":"33097872c29f71ec97c7bbb1c1d83dfb.html","title":"An efficient algorithm for easy-first non-directional dependency parsing","url":"https://www.aclweb.org/anthology/N10-1115.pdf","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2010/06/02","abstract":"We present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a deterministic, best-first, O (nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models."},{"id":"4503d0737497a8b27c4b0d96c024c2eb.html","title":"Modeling morphosyntactic agreement in constituency-based parsing of Modern Hebrew","url":"https://www.aclweb.org/anthology/W10-1405.pdf","authors":["Reut Tsarfaty","Khalil Sima\u2019an"],"date":"2010/06","abstract":"We show that na\u0131ve modeling of morphosyntactic agreement in a Constituency-Based (CB) statistical parsing model is worse than none, whereas a linguistically adequate way of modeling inflectional morphology in CB parsing leads to improved performance. In particular, we show that an extension of the Relational-Realizational (RR) model that incorporates agreement features is superior to CB models that treat morphosyntax as statesplits (SP), and that the RR model benefits more from inflectional features. We focus on parsing Hebrew and report the best result to date, F184. 13 for parsing off of gold-tagged text, 5% error reduction from previous results."},{"id":"a47b0631c39a199376ec36bfefb68035.html","title":"Statistical parsing of morphologically rich languages (spmrl) what, how and whither","url":"https://www.aclweb.org/anthology/W10-1401.pdf","authors":["Reut Tsarfaty","Djam\\\\xe9 Seddah","Yoav Goldberg","Sandra K\\\\xfcbler","Yannick Versley","Marie Candito","Jennifer Foster","Ines Rehbein","Lamia Tounsi"],"date":"2010/06","abstract":"The term Morphologically Rich Languages (MRLs) refers to languages in which significant information concerning syntactic units and relations is expressed at word-level. There is ample evidence that the application of readily available statistical parsing models to such languages is susceptible to serious performance degradation. The first workshop on statistical parsing of MRLs hosts a variety of contributions which show that despite languagespecific idiosyncrasies, the problems associated with parsing MRLs cut across languages and parsing frameworks. In this paper we review the current state-of-affairs with respect to parsing MRLs and point out central challenges. We synthesize the contributions of researchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. The overarching analysis suggests itself as a source of directions for future investigations."},{"id":"570be9f151d9611db20cfbdd6150eb43.html","title":"Building Textual Entailment Specialized Data Sets: a Methodology for Isolating Linguistic Phenomena Relevant to Inference.","url":"https://u.cs.biu.ac.il/~nlp/downloads/publications/478_Paper.pdf","authors":["Luisa Bentivogli","Elena Cabrio","Ido Dagan","Danilo Giampiccolo","Medea Lo Leggio","Bernardo Magnini"],"date":"2010/05/19","abstract":"This paper proposes a methodology for the creation of specialized data sets for Textual Entailment, made of monothematic Text-Hypothesis pairs (ie pairs in which only one linguistic phenomenon relevant to the entailment relation is highlighted and isolated). The annotation procedure assumes that humans have knowledge about the linguistic phenomena relevant to inference, and a classification of such phenomena both into fine grained and macro categories is suggested. We experimented with the proposed methodology over a sample of pairs taken from the RTE-5 data set, and investigated critical issues arising when entailment, contradiction or unknown pairs are considered. The result is a new resource, which can be profitably used both to advance the comprehension of the linguistic phenomena relevant to entailment judgments and to make a first step towards the creation of large-scale specialized data sets."},{"id":"3c38f6978236e8bee9cf0efa5d664f23.html","title":"A Resource for Investigating the Impact of Anaphora and Coreference on Inference.","url":"https://www.academia.edu/download/31326476/LREC-2010_Abad-etal.pdf","authors":["Azad Abad","Luisa Bentivogli","Ido Dagan","Danilo Giampiccolo","Shachar Mirkin","Emanuele Pianta","Asher Stern"],"date":"2010/05","abstract":"Discourse phenomena play a major role in text processing tasks. However, so far relatively little study has been devoted to the relevance of discourse phenomena for inference. Therefore, an experimental study was carried out to assess the relevance of anaphora and coreference for Textual Entailment (TE), a prominent inference framework. First, the annotation of anaphoric and coreferential links in the RTE-5 Search data set was performed according to a specifically designed annotation scheme. As a result, a new data set was created where all anaphora and coreference instances in the entailing sentences which are relevant to the entailment judgment are solved and annotated. A by-product of the annotation is a new \u201caugmented\u201d data set, where all the referring expressions which need to be resolved in the entailing sentences are replaced by explicit expressions. Starting from the final output of the annotation, the actual impact of discourse phenomena on inference engines was investigated, identifying the kind of operations that the systems need to apply to address discourse phenomena and trying to find direct mappings between these operation and annotation types."},{"id":"9ec92712b724c9bbb8370d4bc1d00fe5.html","title":"Learning an expert from human annotations in statistical machine translation: The case of out-of-vocabulary words","url":"http://www.mt-archive.info/10/EAMT-2010-Aziz.pdf","authors":["Wilker Aziz","Marc Dymetman","Shachar Mirkin","Lucia Specia","Nicola Cancedda","Ido Dagan"],"date":"2010/05","journal":"Proceedings of the 14th annual meeting of the European Association for Machine Translation (EAMT), Saint-Rapha, France","abstract":"We present a general method for incorporating an \u201cexpert\u201d model into a Statistical Machine Translation (SMT) system, in order to improve its performance on a particular \u201carea of expertise\u201d, and apply this method to the specific task of finding adequate replacements for Out-of-Vocabulary (OOV) words. Candidate replacements are paraphrases and entailed phrases, obtained using monolingual resources. These candidate replacements are transformed into \u201cdynamic biphrases\u201d, generated at decoding time based on the context of each source sentence. Standard SMT features are enhanced with a number of new features aimed at scoring translations produced by using different replacements. Active learning is used to discriminatively train the model parameters from human assessments of the quality of translations. The learning framework yields an SMT system which is able to deal with sentences containing OOV words but also guarantees that the performance is not degraded for input sentences without OOV words. Results of experiments on English-French translation show that this method outperforms previous work addressing OOV words in terms of acceptability."},{"id":"e64dfc7541585e9e64111eda4db55b3b.html","title":"The fourth pascal recognizing textual entailment challenge","url":"https://www.microsoft.com/en-us/research/publication/the-fourth-pascal-recognizing-textual-entailment-challenge/","authors":["Ido Dagan","Bill Dolan","Bernardo Magnini","Dan Roth"],"date":"2010/01/1","journal":"Journal of Natural Language Engineering","abstract":"The goal of identifying textual entailment\u2013whether one piece of text can be plausibly inferred from another\u2013has emerged in recent years as a generic core problem in natural language understanding. Work in this area has been largely driven by the PASCAL Recognizing Textual Entailment (RTE) challenges, which are a series of annual competitive meetings. The current work exhibits strong ties to some earlier lines of research, particularly automatic acquisition of paraphrases and lexical semantic relationships and unsupervised inference in applications such as question answering, information extraction and summarization. It has also opened the way to newer lines of research on more involved inference methods, on knowledge representations needed to support this natural language understanding challenge and on the use of learning methods in this context. RTE has fostered an active and growing community of researchers focused on the problem of applied entailment. This special issue of the JNLE provides an opportunity to showcase some of the most important work in this emerging area."},{"id":"287a7c3db53ad6ae7b72c878383d75be.html","title":"Recognizing textual entailment: Rational, evaluation and approaches\u2013erratum","url":"https://www.cambridge.org/core/journals/natural-language-engineering/article/recognizing-textual-entailment-rational-evaluation-and-approaches-erratum/A8332663248862777F4665C08BA33E9F","authors":["Ido Dagan","Bill Dolan","Bernardo Magnini","Dan Roth"],"date":"2010/01","journal":"Natural Language Engineering","abstract":"<div class=\\"gsh_csp\\">Due to publisher error, this article was omitted from the printed issue of <i>Natural Language Engineering</i> volume 15 issue 4."},{"id":"36304af58bf01d02088b608126e36847.html","title":"Semantic Inference at the LexicalSyntactic Level","url":"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.465.8266","authors":["Roy Bar-Haim","Ido Dagan","Iddo Greental","Eyal Shnarch"],"date":"2010","abstract":"Semantic inference is an important component in many natural language understanding applications. Classi-cal approaches to semantic inference rely on complex logical representations. However, practical applications usually adopt shallower lexical or lexical-syntactic rep-resentations, but lack a principled inference framework. We propose a generic semantic inference framework that operates directly on syntactic trees. New trees are inferred by applying entailment rules, which provide a unified representation for varying types of inferences. Rules were generated by manual and automatic meth-ods, covering generic linguistic structures as well as specific lexical-based inferences. Initial empirical eval-uation in a Relation Extraction setting supports the va-lidity of our approach."},{"id":"6048aa0f858c5600128dc5d6089ddb93.html","title":"Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages","url":"https://www.aclweb.org/anthology/W10-1400.pdf","authors":["Yuval Marton","Nizar Habash","Owen Rambow","Bharat Ram Ambati","Samar Husain","Sambhav Jain","Dipti Misra Sharma","Rajeev Sangal","Kepa Bengoetxea","Koldo Gojenola","Reut Tsarfaty","Khalil Sima\'an","Tagyoung Chung","Matt Post","Daniel Gildea","Wolfgang Maier","Mohammed Attia","Jennifer Foster","Deirdre Hogan","Joseph Le Roux","Lamia Tounsi","Josef van Genabith","Bharat Ram Ambati","Samar Husain","Joakim Nivre","Rajeev Sangal","Yoav Goldberg","Michael Elhadad"],"date":"2010","journal":"Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages","abstract":"The idea of organizing this workshop was sparked following very interesting discussions that occurred during EACL09 among various researchers working on statistical parsing of different types of languages. Indeed, an opportunity to discuss the issues that we were all experiencing was much needed, and it seemed such a good idea that we decided to take advantage of IWPT\u201909, which was held that year in Paris, to organize a panel on this topic. We planned to have presentations on the various issues faced by this small emerging community, which would allow us to share our sometimes similar solutions for parsing different languages."},{"id":"aaccb377fbb1a5ce7958a4533348a88e.html","title":"RELATIONAL-REALIZATIONAL SYNTAX: AN ARCHITECTURE FOR SPECIFYING AND LEARNING MORPHOSYNTACTIC DESCRIPTIONS","url":"http://cslipublications.stanford.edu/LFG/15/lfg10.pdf#page=458","authors":["Reut Tsarfaty","Miriam Butt","Tracy Holloway King"],"date":"2010","journal":"Proceedings of LFG10","abstract":"This paper presents a novel architecture for specifying rich morphosyntactic representations and learning the associated grammars from annotated data. The key idea underlying the architecture is the application of the traditional notion of a \u201cparadigm\u201d to the syntactic domain. N-place predicates associated with paradigm cells are viewed as relational networks that are realized recursively by combining and ordering cells from other paradigms. The complete morphosyntactic representation of a sentence is then viewed as a nested integrated structure interleaving function and form by means of realization rules. This architecture, called Relational-Realizational, has a simple instantiation as a generative probabilistic model of which parameters can be statistically learned from treebank data. An application of this model to Hebrew allows for accurate description of word-order and argument marking patterns familiar from Semitic traditional grammars. The associated treebank grammar can be used for statistical parsing and is shown to improve state-of-the-art parsing results for Hebrew. The availability of a simple, formal, robust, implementable and statistically interpretable working model opens new horizons in computational linguistics\u2014at least in principle, we should now be able to quantify typological trends which have so far been stated informally or only tacitly reflected in corpus statistics."},{"id":"c8ea359cc051c007642077454d3ff037.html","title":"Relational-realizational parsing","url":"https://dare.uva.nl/record/1/317909","authors":["Reut Tsarfaty"],"date":"2010","abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Total citations"},{"id":"c0a9af16b754dac8c722f908f3e84053.html","title":"The Fifth PASCAL Recognizing Textual Entailment Challenge.","url":"http://www.cs.utexas.edu/users/pclark/papers/RTE6_overview.proceedings.pdf","authors":["Luisa Bentivogli","Peter Clark","Ido Dagan","Danilo Giampiccolo"],"date":"2009/11/17","abstract":"This paper presents the Sixth Recognizing Textual Entailment (RTE-6) challenge. This year a major innovation was introduced, as the traditional Main Task was replaced by a new task, similar to the RTE-5 Search Pilot, in which Textual Entailment is performed on a real corpus in the Update Summarization scenario. A subtask was also proposed, aimed at detecting novel information. To continue the effort of testing RTE in NLP applications, a KBP Validation Pilot Task was set up, in which RTE systems had to validate the output of systems participating in the KBP Slot Filling Task. Eighteen teams participated in the Main Task (48 submitted runs) and 9 in the Novelty Detection Subtask (22 submitted runs). As for the Pilot, 10 runs were submitted by 3 participants. Finally, the exploratory effort started in RTE-5 to perform resource evaluation through ablation tests was not only reiterated in RTE-6, but also extended to tools."},{"id":"f4f82016234ba692c1c09bfe041dddb8.html","title":"Improving text categorization bootstrapping via unsupervised learning","url":"https://dl.acm.org/doi/abs/10.1145/1596515.1596516","authors":["Alfio Gliozzo","Carlo Strapparava","Ido Dagan"],"date":"2009/10/14","journal":"ACM Transactions on Speech and Language Processing (TSLP)","abstract":"We propose a text-categorization bootstrapping algorithm in which categories are described by relevant seed words. Our method introduces two unsupervised techniques to improve the initial categorization step of the bootstrapping scheme: (i) using latent semantic spaces to estimate the similarity among documents and words, and (ii) the Gaussian mixture algorithm, which differentiates relevant and nonrelevant category information using statistics from unlabeled examples. In particular, this second step maps the similarity scores to class posterior probabilities, and therefore reduces sensitivity to keyword-dependent variations in scores. The algorithm was evaluated on two text categorization tasks, and obtained good performance using only the category names as initial seeds. In particular, the performance of the proposed method proved to be equivalent to a pure supervised approach trained on 70--160 labeled\\\\xa0\u2026"},{"id":"f609c2a4a0b9c93c141dfba118e24b8b.html","title":"Hebrew dependency parsing: Initial results","url":"https://www.aclweb.org/anthology/W09-3819.pdf","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2009/10/07","abstract":"We describe a newly available Hebrew Dependency Treebank, which is extracted from the Hebrew (constituency) Treebank. We establish some baseline unlabeled dependency parsing performance on Hebrew, based on two state-of-the-art parsers, MST-parser and MaltParser. The evaluation is performed both in an artificial setting, in which the data is assumed to be properly morphologically segmented and POS-tagged, and in a real-world setting, in which the parsing is performed on automatically segmented and POS-tagged text. We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than Malt-Parser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. 1 Introduction"},{"id":"6abfe5e67c814f33dd299b144dde5006.html","title":"G NATURAL","url":"https://www.cambridge.org/core/journals/natural-language-engineering/article/nle-volume-15-issue-4-cover-and-front-matter/4A25FD66DCC0B968E660DA35BB9A9FF3","authors":["Textual Entailment","Ido Dagan","Bill Dolan","Bernardo Magnini","Dan Roth"],"date":"2009/10/04","abstract":""},{"id":"06ac57a0198ad34ad194edb99b2e2c06.html","title":"Considering discourse references in textual entailment annotation","url":"https://u.cs.biu.ac.il/~nlp/downloads/publications/Final-GL2009_19_Bentivogli-et-al.pdf","authors":["Luisa Bentivogli","Ido Dagan","Hoa Trang Dang","Danilo Giampiccolo","Medea Lo Leggio","Bernardo Magnini"],"date":"2009/09/17","journal":"Proceedings of the 5th International Conference on Generative Approaches to the Lexicon (GL 2009)","abstract":"In the 2009 Recognizing Textual Entailment challenge a Search Pilot task has been introduced, aimed at finding all the sentences in a corpus which entail a set of given hypotheses. The preparation of the data set for this task has provided an opportunity to better understand some phenomena concerning textual entailment recognition in a natural setting. This paper focuses on some problematic issues related to resolving coreferences to entities, space, time and events at the corpus level, as emerged during the annotation of the data set for the textual entailment Search Pilot."},{"id":"45e440116b78da2d93732d8fd44aa96d.html","title":"Ontology Evaluation through Text Classi\ufb01cation","url":"http://books.google.com/books?hl=en&lr=&id=uu5rdVCN5xEC&oi=fnd&pg=PA210&dq=info:hTzRfV69hp0J:scholar.google.com&ots=Gd2S5zrhls&sig=hbVD6VY5pom0IEl8OUdoZ5Yxfzo","authors":["DavidGabay YaelNetzer","Meni Adler","Yoav Goldberg","Michael Elhadad"],"date":"2009/09/01","journal":"Advances in Web and Network Technologies and Information Management: AP Web/WAIM 2009 International Workshops: WCMT 2009, RTBI 2009, DBIR-ENQOIR 2009, and PAIS 2009","abstract":"We present a new method to evaluate a search ontology, which relies on mapping ontology instances to textual documents. On the basis of this mapping, we evaluate the adequacy of ontology relations by measuring their classi\ufb01cation potential over the textual documents. This data-driven method provides concrete feedback to ontology maintainers and a quantitative estimation of the functional adequacy of the ontology relations towards search experience improvement. We speci\ufb01cally evaluate whether an ontology relation can help a semantic search engine support exploratory search."},{"id":"c38251d137a2773296ab0c85dc64a28e.html","title":"Bootstrapping distributional feature vector quality","url":"https://www.mitpressjournals.org/doi/abs/10.1162/coli.08-032-r1-06-96","authors":["Maayan Zhitomirsky-Geffet","Ido Dagan"],"date":"2009/09","journal":"Computational linguistics","abstract":"This article presents a novel bootstrapping approach for improving the quality of feature vector weighting in distributional word similarity. The method was motivated by attempts to utilize distributional similarity for identifying the concrete semantic relationship of lexical entailment. Our analysis revealed that a major reason for the rather loose semantic similarity obtained by distributional similarity methods is insufficient quality of the word feature vectors, caused by deficient feature weighting. This observation led to the definition of a bootstrapping scheme which yields improved feature weights, and hence higher quality feature vectors. The underlying idea of our approach is that features which are common to similar words are also most characteristic for their meanings, and thus should be promoted. This idea is realized via a bootstrapping step applied to an initial standard approximation of the similarity space. The\\\\xa0\u2026"},{"id":"3e8984effd526c09448cb72b55a295f6.html","title":"On the role of lexical features in sequence labeling","url":"https://www.aclweb.org/anthology/D09-1119.pdf","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2009/08/06","abstract":"We use the technique of SVM anchoring to demonstrate that lexical features extracted from a training corpus are not necessary to obtain state of the art results on tasks such as Named Entity Recognition and Chunking. While standard models require as many as 100K distinct features, we derive models with as little as 1K features that perform as well or better on different domains. These robust reduced models indicate that the way rare lexical features contribute to classification in NLP is not fully understood. Contrastive error analysis (with and without lexical features) indicates that lexical features do contribute to resolving some semantic and complex syntactic ambiguities\u2013but we find this contribution does not generalize outside the training corpus. As a general strategy, we believe lexical features should not be directly derived from a training corpus but instead, carefully inferred and selected from other sources."},{"id":"246abffed77355d01a343297f4162bbf.html","title":"Extracting lexical reference rules from Wikipedia","url":"https://www.aclweb.org/anthology/P09-1051.pdf","authors":["Eyal Shnarch","Libby Barak","Ido Dagan"],"date":"2009/08","abstract":"This paper describes the extraction from Wikipedia of lexical reference rules, identifying references to term meanings triggered by other terms. We present extraction methods geared to cover the broad range of the lexical reference relation and analyze them extensively. Most extraction methods yield high precision levels, and our rule-base is shown to perform better than other automatically constructed baselines in a couple of lexical expansion and matching tasks. Our rule-base yields comparable performance to Word-Net while providing largely complementary information."},{"id":"36a6c393473c1057deb6d2a8e77c9ec7.html","title":"Proceedings of the 2009 Workshop on Applied Textual Inference (TextInfer)","url":"https://www.aclweb.org/anthology/W09-2500.pdf","authors":["Chris Callison-Burch","Ido Dagan","Christopher D Manning","Marco Pennacchiotti","Fabio Massimo Zanzotto"],"date":"2009/08","abstract":"Applied textual inference has attracted a significant amount of attention in recent years. Recognizing textual entailments and detecting semantic equivalences between texts are at the core of many NLP tasks, including question answering, information extraction, text summarization, and many others. Developing generic algorithms and resources for inference and paraphrasing would therefore be applicable to a broad range of NLP applications."},{"id":"4298588df528af5efd006a429b1a42e8.html","title":"A compact forest for scalable inference over entailment and paraphrase rules","url":"https://www.aclweb.org/anthology/D09-1110.pdf","authors":["Roy Bar-Haim","Jonathan Berant","Ido Dagan"],"date":"2009/08","abstract":"A large body of recent research has been investigating the acquisition and application of applied inference knowledge. Such knowledge may be typically captured as entailment rules, applied over syntactic representations. Efficient inference with such knowledge then becomes a fundamental problem. Starting out from a formalism for entailment-rule application we present a novel packed data-structure and a corresponding algorithm for its scalable implementation. We proved the validity of the new algorithm and established its efficiency analytically and empirically."},{"id":"5fa570874927817520732397562698b7.html","title":"Augmenting wordnet-based inference with argument mapping","url":"https://www.aclweb.org/anthology/W09-2504.pdf","authors":["Idan Szpektor","Ido Dagan"],"date":"2009/08","abstract":"WordNet is a useful resource for lexical inference in applications. Inference over predicates, however, often requires a change in argument positions, which is not specified in WordNet. We propose a novel framework for augmenting WordNet-based inferences over predicates with corresponding argument mappings. We further present a concrete implementation of this framework, which yields substantial improvement to WordNet-based inference."},{"id":"6ab88ccdcceae5b95e900f0d722ccf0b.html","title":"Source-language entailment modeling for translating unknown terms","url":"https://www.aclweb.org/anthology/P09-1089.pdf","authors":["Shachar Mirkin","Lucia Specia","Nicola Cancedda","Ido Dagan","Marc Dymetman","Idan Szpektor"],"date":"2009/08","abstract":"This paper addresses the task of handling unknown terms in SMT. We propose using source-language monolingual models and resources to paraphrase the source text prior to translation. We further present a conceptual extension to prior work by allowing translations of entailed texts rather than paraphrases only. A method for performing this process efficiently is presented and applied to some 2500 sentences with unknown terms. Our experiments show that the proposed approach substantially increases the number of properly translated texts."},{"id":"a752134babbb40d886facc6688c7f2bb.html","title":"Directional distributional similarity for lexical expansion","url":"https://www.aclweb.org/anthology/P09-2018.pdf","authors":["Lili Kotlerman","Ido Dagan","Idan Szpektor","Maayan Geffet"],"date":"2009/08","abstract":"Distributional word similarity is most commonly perceived as a symmetric relation. Yet, one of its major applications is lexical expansion, which is generally asymmetric. This paper investigates the nature of directional (asymmetric) similarity measures, which aim to quantify distributional feature inclusion. We identify desired properties of such measures, specify a particular one based on averaged precision, and demonstrate the empirical benefit of directional measures for expansion."},{"id":"c7f427814335133e753722eb2075da21.html","title":"An alternative to head-driven approaches for parsing a (relatively) free word-order language","url":"https://www.aclweb.org/anthology/D09-1088.pdf","authors":["Reut Tsarfaty","Khalil Sima\u2019an","Remko Scha"],"date":"2009/08","abstract":"Applying statistical parsers developed for English to languages with freer wordorder has turned out to be harder than expected. This paper investigates the adequacy of different statistical parsing models for dealing with a (relatively) free word-order language. We show that the recently proposed Relational-"},{"id":"d4bcb0d9b7909120e3c45017a8eeeccc.html","title":"Gaiku: Generating Haiku with word associations norms","url":"https://www.aclweb.org/anthology/W09-2005.pdf","authors":["Yael Netzer","David Gabay","Yoav Goldberg","Michael Elhadad"],"date":"2009/06/04","abstract":"Traditional lexical knowledge bases such as Word-Net formalize a limited set of systematic relations that exist between words, such as synonymy, polysemy, hypernymy. When such relations are composed, they maintain their systematicity, and do not create surprising, unexpected word associations. The human mind is not limited to such systematic relations, and people tend to associate words to each other with a rich set of relations, such as non systematic paradigmatic (doctor-nurse) and syntagmatic relations (mash-potato) as identified by Saussure (1949). Such associations rely on cultural (mash-television), emotional (math-yuck) and personal experience (autumn-Canada)."},{"id":"e16c1d5ef3af513d254bcc55d2ce0bb6.html","title":"Text categorization from category name via lexical reference","url":"https://www.aclweb.org/anthology/N09-2009.pdf","authors":["Libby Barak","Ido Dagan","Eyal Shnarch"],"date":"2009/06","abstract":"Requiring only category names as user input is a highly attractive, yet hardly explored, setting for text categorization. Earlier bootstrapping results relied on similarity in LSA space, which captures rather coarse contextual similarity. We suggest improving this scheme by identifying concrete references to the category name\u2019s meaning, obtaining a special variant of lexical expansion."},{"id":"33dde6aa7a9aff70f99f5a81d2898f0b.html","title":"Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and EM-HMM-based lexical probabilities","url":"https://www.aclweb.org/anthology/E09-1038.pdf","authors":["Yoav Goldberg","Reut Tsarfaty","Meni Adler","Michael Elhadad"],"date":"2009/03","abstract":"We present a framework for interfacing a PCFG parser with lexical information from an external resource following a different tagging scheme than the treebank. This is achieved by defining a stochastic mapping layer between the two resources. Lexical probabilities for rare events are estimated in a semi-supervised manner from a lexicon and large unannotated corpora. We show that this solution greatly enhances the performance of an unlexicalized Hebrew PCFG parser, resulting in state-of-the-art Hebrew parsing results both when a segmentation oracle is assumed, and in a real-word parsing scenario of parsing unsegmented tokens."},{"id":"76cad631d5f07bf85fc3ed01aac63e7e.html","title":"Evaluating the inferential utility of lexical-semantic resources","url":"https://www.aclweb.org/anthology/E09-1064.pdf","authors":["Shachar Mirkin","Ido Dagan","Eyal Shnarch"],"date":"2009/03","abstract":"Lexical-semantic resources are used extensively for applied semantic inference, yet a clear quantitative picture of their current utility and limitations is largely missing. We propose system-and application-independent evaluation and analysis methodologies for resources\u2019 performance, and systematically apply them to seven prominent resources. Our findings identify the currently limited recall of available resources, and indicate the potential to improve performance by examining non-standard relation types and by distilling the output of distributional methods. Further, our results stress the need to include auxiliary information regarding the lexical and logical contexts in which a lexical inference is valid, as well as its prior validity likelihood."},{"id":"03ff5fa5fd63ffc8ff17dc355bcf970d.html","title":"It\u2019s time for a semantic engine!","url":"https://nlp.cs.nyu.edu/sk-symposium/slides/IdoDagan.pdf","authors":["Ido Dagan"],"date":"2009/01/07","abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Scholar articles"},{"id":"99a44e060091f7d8b9388203338a4b9b.html","title":"It\u2019s time for a semantic inference engine","url":"https://www.aclweb.org/anthology/W09-3701.pdf","authors":["Ido Dagan"],"date":"2009/01","abstract":"A common computational goal is to encapsulate the modeling of a target phenomenon within a unified and comprehensive\u201d engine\u201d, which addresses a broad range of the required processing tasks. This goal is followed in common modeling of the morphological and syntactic levels of natural language, where most processing tasks are encapsulated within morphological analyzers and syntactic parsers. In this talk I suggest that computational modeling of the semantic level should also focus on encapsulating the various processing tasks within a unified module (engine). The input/output specification of such engine (API) can be based on the textual entailment paradigm, which will be described in brief and suggested as an attractive framework for applied semantic inference. The talk will illustrate an initial proposal for the engine\u2019s API, designed to be embedded within the prominent language processing applications. Finally, I will sketch the entailment formalism and efficient inference algorithm developed at Bar-Ilan University, which illustrates a principled transformational (rather than interpretational) approach towards developing a comprehensive semantic engine."},{"id":"04ce634194a47b27df397198c722020d.html","title":"Bar-Ilan University\u2019s submission to RTE5","url":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.172.7660&rep=rep1&type=pdf","authors":["Shachar Mirkin","Roy Bar-Haim","Jonathan Berant","Ido Dagan","Eyal Shnarch","Asher Stern","Idan Szpektor"],"date":"2009","journal":"Proceedings of the Second Text Analysis Conference (TAC 2009), Gaithersburg, MD","abstract":"This year we focused on the search pilot, and enhanced our RTE-4 system to address two issues introduced by this novel setting: candidate texts search and document-level discourse analysis. Our system achieved the highest score on the search task, and may be viewed as a first step towards fully addressing this challenging task."},{"id":"debf8acb460673024ad1d8d6f3c298a4.html","title":"Addressing Discourse and Document Structure in the RTE Search Task.","url":"http://www-nlp.stanford.edu/joberant/homepage_files/publications/TAC09.pdf","authors":["Shachar Mirkin","Roy Bar-Haim","Ido Dagan","Eyal Shnarch","Asher Stern","Idan Szpektor","Jonathan Berant"],"date":"2009","abstract":"This paper describes Bar-Ilan University\u2019s submissions to RTE-5. This year we focused on the Search pilot, enhancing our entailment system to address two main issues introduced by this new setting: scalability and, primarily, document-level discourse. Our system achieved the highest score on the Search task amongst participating groups, and proposes first steps towards addressing this challenging setting."},{"id":"ffc12df710805712e7a198bc95045458.html","title":"Ontology evaluation through text classification","url":"https://link.springer.com/chapter/10.1007/978-3-642-03996-6_20","authors":["Yael Netzer","David Gabay","Meni Adler","Yoav Goldberg","Michael Elhadad"],"date":"2009","journal":"Advances in Web and Network Technologies, and Information Management","abstract":"We present a new method to evaluate a <i>search ontology</i>, which relies on mapping ontology instances to textual documents. On the basis of this mapping, we evaluate the adequacy of ontology relations by measuring their classification potential over the textual documents. This data-driven method provides concrete feedback to ontology maintainers and a quantitative estimation of the functional adequacy of the ontology relations towards search experience improvement. We specifically evaluate whether an ontology relation can help a semantic search engine support exploratory search."},{"id":"0b9361fa280b91ac84569108564b9a96.html","title":"Efficient Semantic Deduction and Approximate Matching over Compact Parse Forests.","url":"http://www.academia.edu/download/31704216/BIU.proceedings.pdf","authors":["Roy Bar-Haim","Ido Dagan","Shachar Mirkin","Eyal Shnarch","Idan Szpektor","Jonathan Berant","Iddo Greental"],"date":"2008/11/17","abstract":"Semantic inference is often modeled as application of entailment rules, which specify generation of entailed sentences from a source sentence. Efficient generation and representation of entailed consequents is a fundamental problem common to such inference methods. We present a new data structure, termed compact forest, which allows efficient generation and representation of entailed consequents, each represented as a parse tree. Rule-based inference is complemented with a new approximate matching measure inspired by tree kernels, which is computed efficiently over compact forests. Our system also makes use of novel large-scale entailment rule bases, derived from Wikipedia as well as from information about predicates and their argument mapping, gathered from available lexicons and complemented by unsupervised learning."},{"id":"6768f47c801f14b38cf2ae0ba6ad56de.html","title":"The Fourth PASCAL Recognizing Textual Entailment Challenge.","url":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.232.4661&rep=rep1&type=pdf","authors":["Danilo Giampiccolo","Hoa Trang Dang","Bernardo Magnini","Ido Dagan","Elena Cabrio","Bill Dolan"],"date":"2008/11/17","abstract":"In 2008 the Recognizing Textual Entailment Challenge (RTE-4) was proposed for the first time as a track at the Text Analysis Conference (TAC). Another important innovation introduced in this campaign was a three-judgment task, which required the systems to make a further distinction between pairs where the entailment does not hold because the content of H is contradicted by the content of T, and pairs where the entailment cannot be determined because the truth of H cannot be verified on the basis of the content of T. A classic twoway task was also offered. RTE-4 attracted 26 teams, more than half of whom submitted runs for the new 3-way task. This paper describes the preparation of the dataset, and gives an overview of the results achieved by the participating systems."},{"id":"4d320b44272cab5621e3ebf0dd6cf139.html","title":"Learning entailment rules for unary templates","url":"https://www.aclweb.org/anthology/C08-1107.pdf","authors":["Idan Szpektor","Ido Dagan"],"date":"2008/08","abstract":"Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules-entailment rules between templates with a single variable. In this paper we investigate two approaches for unsupervised learning of such rules and compare the proposed methods with a binary rule learning method. The results show that the learned unary rule-sets outperform the binary rule-set. In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure."},{"id":"6cf94a5e83c485bbe5be6e2c999c86b1.html","title":"A single generative model for joint morphological segmentation and syntactic parsing","url":"https://www.aclweb.org/anthology/P08-1043.pdf","authors":["Yoav Goldberg","Reut Tsarfaty"],"date":"2008/06","journal":"Proceedings of ACL-08: HLT","abstract":"Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence. These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance. Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity. Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far."},{"id":"79e2356e45a4f2867347beae6e6bf550.html","title":"Contextual preferences","url":"https://www.aclweb.org/anthology/P08-1078.pdf","authors":["Idan Szpektor","Ido Dagan","Roy Bar-Haim","Jacob Goldberger"],"date":"2008/06","abstract":"The validity of semantic inferences depends on the contexts in which they are applied. We propose a generic framework for handling contextual considerations within applied inference, termed Contextual Preferences. This framework defines the various context-aware components needed for inference and their relationships. Contextual preferences extend and generalize previous notions, such as selectional preferences, while experiments show that the extended framework allows improving inference quality on real application data."},{"id":"b9e181b98fef348fea904a8408b9e6e6.html","title":"EM can find pretty good HMM POS-taggers (when given a good start)","url":"https://www.aclweb.org/anthology/P08-1085.pdf","authors":["Yoav Goldberg","Meni Adler","Michael Elhadad"],"date":"2008/06","abstract":"We address the task of unsupervised POS tagging. We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries. We present a family of algorithms to compute effective initial estimations p (t| w). We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline. We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods."},{"id":"e1fd6c462359251a510ac720d8d0ad7a.html","title":"splitSVM: fast, space-efficient, non-heuristic, polynomial kernel computation for NLP applications","url":"https://www.aclweb.org/anthology/P08-2060.pdf","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2008/06","abstract":"We present a fast, space efficient and nonheuristic method for calculating the decision function of polynomial kernel classifiers for NLP applications. We apply the method to the MaltParser system, resulting in a Java parser that parses over 50 sentences per second on modest hardware without loss of accuracy (a 30 time speedup over existing methods). The method implementation is available as the open-source splitSVM Java library."},{"id":"ed55eb939f974dbb3f7043f257a5ad05.html","title":"Unsupervised lexicon-based resolution of unknown words for full morphological analysis","url":"https://www.aclweb.org/anthology/P08-1083.pdf","authors":["Meni Adler","Yoav Goldberg","David Gabay","Michael Elhadad"],"date":"2008/06","abstract":"Morphological disambiguation proceeds in 2 stages:(1) an analyzer provides all possible analyses for a given token and (2) a stochastic disambiguation module picks the most likely analysis in context. When the analyzer does not recognize a given token, we hit the problem of unknowns. In large scale corpora, unknowns appear at a rate of 5 to 10%(depending on the genre and the maturity of the lexicon)."},{"id":"67954225b1fe80eaa35e1648dab262f4.html","title":"Tagging a Hebrew Corpus: the Case of Participles.","url":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.218.3638&rep=rep1&type=pdf","authors":["Meni Adler","Yael Dahan Netzer","Yoav Goldberg","David Gabay","Michael Elhadad"],"date":"2008/05","abstract":"We report on an effort to build a corpus of Modern Hebrew tagged with parts of speech and morphology. We designed a tagset specific to Hebrew while focusing on four aspects: the tagset should be consistent with common linguistic knowledge; there should be maximal agreement among taggers as to the tags assigned to maintain consistency; the tagset should be useful for machine taggers and learning algorithms; and the tagset should be effective for applications relying on the tags as input features. In this paper, we illustrate these issues by explaining our decision to introduce a tag for beinoni forms in Hebrew. We explain how this tag is defined, and how it helped us improve manual tagging accuracy to a high-level, while improving automatic tagging and helping in the task of syntactic chunking."},{"id":"8208dcb3b10b0be9f1e058f7279c5f81.html","title":"Word-Based or Morpheme-Based? Annotation Strategies for Modern Hebrew Clitics.","url":"https://www.cs.brandeis.edu/~marc/misc/proceedings/lrec-2008/pdf/361_paper.pdf","authors":["Reut Tsarfaty","Yoav Goldberg"],"date":"2008/05","abstract":"Morphologically rich languages pose a challenge to the annotators of treebanks with respect to the status of orthographic (spacedelimited) words in the syntactic parse trees. In such languages an orthographic word may carry various, distinct, sorts of information and the question arises whether we should represent such words as a sequence of their constituent morphemes (ie, a Morpheme-Based annotation strategy) or whether we should preserve their special orthographic status within the trees (ie, a Word-Based annotation strategy). In this paper we empirically address this challenge in the context of the development of Language Resources for Modern Hebrew. We compare and contrast the Morpheme-Based and Word-Based annotation strategies of pronominal clitics in Modern Hebrew and we show that the Word-Based strategy is more adequate for the purpose of training statistical parsers as it provides a better PP-attachment disambiguation capacity and a better alignment with initial surface forms. Our findings in turn raise new questions concerning the interaction of morphological and syntactic processing of which investigation is facilitated by the parallel treebank we made available."},{"id":"45ca9ec60b793d5868d78eee9bff2b6f.html","title":"Identification of transliterated foreign words in Hebrew script","url":"https://link.springer.com/chapter/10.1007/978-3-540-78135-6_40","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2008/02/17","abstract":"We present a loosely-supervised method for context-free identification of transliterated foreign names and borrowed words in Hebrew text. The method is purely statistical and does not require the use of any lexicons or linguistic analysis tool for the source languages (Hebrew, in our case). It also does not require any manually annotated data for training \u2013 we learn from noisy data acquired by over-generation. We report precision/recall results of 80/82 for a corpus of 4044 unique words, containing 368 foreign words."},{"id":"fa2947992d954c2fbdb39d7139af1db3.html","title":"Natural language as the basis for meaning representation and inference","url":"https://link.springer.com/chapter/10.1007/978-3-540-78135-6_14","authors":["Ido Dagan","Roy Bar-Haim","Idan Szpektor","Iddo Greental","Eyal Shnarch"],"date":"2008/02/17","abstract":"Semantic inference is an important component in many natural language understanding applications. Classical approaches to semantic inference rely on logical representations for meaning, which may be viewed as being \u201cexternal\u201d to the natural language itself. However, practical applications usually adopt shallower lexical or lexical-syntactic representations, which correspond closely to language structure. In many cases, such approaches lack a principled meaning representation and inference framework. We describe a generic semantic inference framework that operates directly on language-based structures, particularly syntactic trees. New trees are inferred by applying entailment rules, which provide a unified representation for varying types of inferences. Rules were generated by manual and automatic methods, covering generic linguistic structures as well as specific lexical-based inferences. Initial\\\\xa0\u2026"},{"id":"870b3f0c00ded4260e555d89a2bea8bf.html","title":"Identi\ufb01cation of Transliterated Foreign Words in Hebrew Script","url":"http://books.google.com/books?hl=en&lr=&id=ReeQ0ZzovpEC&oi=fnd&pg=PA466&dq=info:e7r_LFyWeoAJ:scholar.google.com&ots=f7SM-jmL1E&sig=joVOlt1aCjiVXgWRiv1d0xX8wkQ","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2008/02/08","journal":"Computational Linguistics and Intelligent Text Processing: 9th International Conference, CICLing 2008, Haifa, Israel, February 17-23, 2008, Proceedings","abstract":"We present a loosely-supervised method for context-free identi\ufb01cation of transliterated foreign names and borrowed words in Hebrew text. The method is purely statistical and does not require the use of any lexicons or linguistic analysis tool for the source languages (Hebrew, in our case). It also does not require any manually annotated data for training\u2013we learn from noisy data acquired by over-generation. We report precision/recall results of 80/82 for a corpus of 4044 unique words, containing 368 foreign words."},{"id":"6c9f93edb630e76b22fe500c4aca2532.html","title":"Efficient semantic inference over language expressions","url":"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.163.6244","authors":["Roy Bar-Haim","Ido Dagan"],"date":"2008","abstract":"over Language Expressions According to the traditional formal semantics approach, inference is conducted at the logical level. Texts are first translated into some logical form and then new propositions are inferred from interpreted texts by a logical theorem prover. However, text understanding systems usually employ shallower lexical and lexical-syntactic representations, sometimes augmented with partial semantic annotations like word senses, named-entity classes and semantic roles. While practical semantic inference is mostly performed over linguistic rather than logical representations, such practices are typically partial and quite ad-hoc, and lack a clear formalism that specifies how inference knowledge should be represented and applied."},{"id":"b0310423ae5f42da5436cf7e0967c2dd.html","title":"Language and Speech Processing Treebank Grammars","url":"http://www.tsarfaty.com/pdfs/TreebankGrammars.pdf","authors":["Reut Tsarfaty"],"date":"2007/11/13","abstract":"Objective Functions\u2022 Most Probable Word sequence\u2022 Most Probable POS sequence over words\u2022 Most Probable Parse"},{"id":"434ffa9f8626a035035d103458e654e7.html","title":"Accurate unlexicalized parsing for modern Hebrew","url":"https://link.springer.com/chapter/10.1007/978-3-540-74628-7_8","authors":["Reut Tsarfaty","Khalil Sima\u2019an"],"date":"2007/09/03","abstract":"Many state-of-the-art statistical parsers for English can be viewed as Probabilistic Context-Free Grammars (PCFGs) acquired from treebanks consisting of phrase-structure trees enriched with a variety of contextual, derivational (e.g., markovization) and lexical information. In this paper we empirically investigate the applicability and adequacy of the unlexicalized variety of such parsing models to Modern Hebrew, a Semitic language that differs in structure and characteristics from English. We show that contrary to experience with parsing the WSJ, the markovized, head-driven unlexicalized variety does not necessarily outperform plain PCFGs for Semitic languages. We demonstrate that enriching unlexicalized PCFGs with morphologically marked agreement features percolated up the parse tree (e.g., definiteness) outperforms plain PCFGs as well as a simple head-driven variation on the MH treebank. We\\\\xa0\u2026"},{"id":"7d5c53ee4c4ebbb55d43c26e2fe5de05.html","title":"Semantic inference at the lexical-syntactic level","url":"https://www.aaai.org/Papers/AAAI/2007/AAAI07-138.pdf","authors":["Roy Bar-Haim","Ido Dagan","Iddo Greental","Eyal Shnarch"],"date":"2007/07/22","journal":"Proceedings of the National Conference on Artificial Intelligence","abstract":"Semantic inference is an important component in many natural language understanding applications. Classical approaches to semantic inference rely on complex logical representations. However, practical applications usually adopt shallower lexical or lexical-syntactic representations, but lack a principled inference framework. We propose a generic semantic inference framework that operates directly on syntactic trees. New trees are inferred by applying entailment rules, which provide a unified representation for varying types of inferences. Rules were generated by manual and automatic methods, covering generic linguistic structures as well as specific lexical-based inferences. Initial empirical evaluation in a Relation Extraction setting supports the validity of our approach."},{"id":"4348378a55df37984d64c453a0f2daf5.html","title":"SVM model tampering and anchored learning: a case study in Hebrew NP chunking","url":"https://www.aclweb.org/anthology/P07-1029.pdf","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2007/06","abstract":"We study the issue of porting a known NLP method to a language with little existing NLP resources, specifically Hebrew SVM-based chunking. We introduce two SVM-based methods\u2013Model Tampering and Anchored Learning. These allow fine grained analysis of the learned SVM models, which provides guidance to identify errors in the training corpus, distinguish the role and interaction of lexical features and eventually construct a model with\u223c 10% error reduction. The resulting chunker is shown to be robust in the presence of noise in the training corpus, relies on less lexical features than was previously understood and achieves an F-measure performance of 92.2 on automatically PoS-tagged text. The SVM analysis methods also provide general insight on SVM-based chunking."},{"id":"57c141b24df6ebbf3892015c6bf9a060.html","title":"Three-dimensional parametrization for parsing morphologically rich languages","url":"https://www.aclweb.org/anthology/W07-2219.pdf","authors":["Reut Tsarfaty","Khalil Sima\u2019an"],"date":"2007/06","abstract":"Current parameters of accurate unlexicalized parsers based on Probabilistic Context-Free Grammars (PCFGs) form a twodimensional grid in which rewrite events are conditioned on both horizontal (headoutward) and vertical (parental) histories. In Semitic languages, where arguments may move around rather freely and phrasestructures are often shallow, there are additional morphological factors that govern the generation process. Here we propose that agreement features percolated up the parse-tree form a third dimension of parametrization that is orthogonal to the previous two. This dimension differs from mere \u201cstate-splits\u201d as it applies to a whole set of categories rather than to individual ones and encodes linguistically motivated co-occurrences between them. This paper presents extensive experiments with extensions of unlexicalized PCFGs for parsing Modern Hebrew in which tuning the parameters in three dimensions gradually leads to improved performance. Our best result introduces a new, stronger, lower bound on the performance of treebank grammars for parsing Modern Hebrew, and is on a par with current results for parsing Modern Standard Arabic obtained by a fully lexicalized parser trained on a much larger treebank."},{"id":"5df50aeda25d073bf4f6283ace88ec5f.html","title":"Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing","url":"https://www.aclweb.org/anthology/W07-1400.pdf","authors":["Satoshi Sekine","Kentaro Inui","Ido Dagan","Bill Dolan","Danilo Giampiccolo","Bernardo Magnini"],"date":"2007/06","abstract":"Recognizing and generating textual entailment and paraphrases are regarded as important technologies in a broad range of NLP applications, including, information extraction, summarization, question answering, information retrieval, machine translation and text generation. Both textual entailment and paraphrasing address relevant aspects of natural language semantics. Entailment is a directional relation between two expressions in which one of them implies the other, whereas paraphrase is a relation in which two expressions convey essentially the same meaning. Indeed, paraphrase can be defined as bi-directional entailment. While it may be debatable how such semantic definitions can be made well-founded, in practice we have already seen evidence that such knowledge is essential for many applications."},{"id":"72070cce0526d11d20178d22f74528b2.html","title":"Semantic inference at the lexical-syntactic level for textual entailment recognition","url":"https://www.aclweb.org/anthology/W07-1422.pdf","authors":["Roy Bar-Haim","Ido Dagan","Iddo Greental","Idan Szpektor","Moshe Friedman"],"date":"2007/06","abstract":"We present a new framework for textual entailment, which provides a modular integration between knowledge-based exact inference and cost-based approximate matching. Diverse types of knowledge are uniformly represented as entailment rules, which were acquired both manually and automatically. Our proof system operates directly on parse trees, and infers new trees by applying entailment rules, aiming to strictly generate the target hypothesis from the source text. In order to cope with inevitable knowledge gaps, a cost function is used to measure the remaining \u201cdistance\u201d from the hypothesis."},{"id":"87034cf3cfe1d19ef201126e77a5e45b.html","title":"Instance-based evaluation of entailment rule acquisition","url":"https://www.aclweb.org/anthology/P07-1058.pdf","authors":["Idan Szpektor","Eyal Shnarch","Ido Dagan"],"date":"2007/06","abstract":"Obtaining large volumes of inference knowledge, such as entailment rules, has become a major factor in achieving robust semantic processing. While there has been substantial research on learning algorithms for such knowledge, their evaluation methodology has been problematic, hindering further research. We propose a novel evaluation methodology for entailment rules which explicitly addresses their semantic properties and yields satisfactory human agreement levels. The methodology is used to compare two state of the art learning algorithms, exposing critical issues for future progress."},{"id":"9a4c6d1499d7f4766ada44e122fb9b76.html","title":"Cross lingual and semantic retrieval for cultural heritage appreciation","url":"https://www.aclweb.org/anthology/W07-0909.pdf","authors":["Idan Szpektor","Ido Dagan","Alon Lavie","Danny Shacham","Shuly Wintner"],"date":"2007/06","abstract":"We describe a system which enhances the experience of museum visits by providing users with language-technology-based information retrieval capabilities. The system consists of a cross-lingual search engine, augmented by state of the art semantic expansion technology, specifically designed for the domain of the museum (history and archaeology of Israel). We discuss the technology incorporated in the system, its adaptation to the specific domain and its contribution to cultural heritage appreciation."},{"id":"e17099d1e53d079850a55bdfa4041762.html","title":"The third pascal recognizing textual entailment challenge","url":"https://www.aclweb.org/anthology/W07-1401.pdf","authors":["Danilo Giampiccolo","Bernardo Magnini","Ido Dagan","Bill Dolan"],"date":"2007/06","abstract":"This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year\u2019s dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges."},{"id":"18e4e985bcf05a56d09d0e9ca74251a4.html","title":"Participants in action: aspectual meanings and thematic relations interplay in the semantics of semitic morphology","url":"https://dare.uva.nl/record/1/279377","authors":["Reut Tsarfaty"],"date":"2007","journal":"Lecture Notes in Computer Science","abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Total citations"},{"id":"379e79b49a8eab584804097e1edf3df6.html","title":"Dimensions of Parameterization for Modern Hebrew Statistical Parsing","url":"https://dare.uva.nl/personal/pure/en/publications/dimensions-of-parameterization-for-modern-hebrew-statistical-parsing(775232c3-7544-4b87-980d-d419064df780).html","authors":["Reut Tsarfaty","K Sima\'an"],"date":"2007","abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Scholar articles"},{"id":"389caeff16417bd0c7ebb5e070bbde51.html","title":"Learning canonical forms of entailment rules","url":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.7039&rep=rep1&type=pdf","authors":["Idan Szpektor","Ido Dagan"],"date":"2007","journal":"Proceedings of RANLP","abstract":"We propose a modular approach to paraphrase and entailment-rule learning that addresses the morphosyntactic variability of lexical-syntactic templates. Using an entailment module that captures generic morpho-syntactic regularities, we transform every identified template into a canonical form. This way, statistics from different template variations are accumulated for a single template form. Additionally, morpho-syntactic redundant rules are not acquired. This scheme also yields more informative evaluation for the acquisition quality, since the bias towards rules with many frequent variations is avoided."},{"id":"c3dbb4fedb7c83dcbeb79422fdb2a2eb.html","title":"Toward Better Understanding of Hebrew NP Chunks","url":"https://www.researchgate.net/profile/Michael_Elhadad/publication/228938196_Toward_Better_Understanding_of_Hebrew_NP_Chunks/links/0deec5241441fc4321000000/Toward-Better-Understanding-of-Hebrew-NP-Chunks.pdf","authors":["Yoav Goldberg","Michael Elhadad"],"date":"2007","journal":"ISCOL/BIFSAI 2007","abstract":"In (Goldberg and Elhadad, 2007) we presented two techniques (SVM Model Tampering and Anchored Learning) for investigating the SVM learning process and resulting models. These techniques were applied to the task of SVM based Hebrew NP Chunking. The results were better understanding of SVM based chunking, of the role lexical features play in the learned models, of the Hebrew NP chunking task definition, and identification of several deficencies in the NP Chunking corpus. This paper focuses on the Hebrew specific issues raised by that work, which are presented with more detail."},{"id":"65d52be5a124d42dff5148a667493ab2.html","title":"Feature instability as a criterion for selecting potential style markers","url":"https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.20428","authors":["Moshe Koppel","Navot Akiva","Ido Dagan"],"date":"2006/09","journal":"Journal of the American Society for Information Science and Technology","abstract":"We introduce a new measure on linguistic features, called <i>stability</i>, which captures the extent to which a language element such as a word or a syntactic construct is replaceable by semantically equivalent elements. This measure may be perceived as quantifying the degree of available \u201csynonymy\u201d for a language item. We show that frequent, but unstable, features are especially useful as discriminators of an author\'s writing style."},{"id":"45cfe46ea40253ffb2801ae02565bc00.html","title":"Semantic similarity: what for?","url":"https://dl.acm.org/doi/abs/10.5555/1641976.1641978","authors":["Ido Dagan"],"date":"2006/07/23","abstract":"Linguistic similarity has been a prominent notion and tool in computational linguistics and related areas, as elaborated nicely in the announcement of this workshop. Yet, what exactly counts as\\" similarity\\", or when two linguistic concepts should be regarded as similar, often remains rather vague and ill posed, which is in fact quite typical for unsupervised notions. This talk will focus on similarity at the semantic level, and will explore the perspective that different notions of similarity may be defined relative to concrete modeling goals. In particular, I will refer to the two major goals in semantic modeling: predicting likelihood of occurrence, which is the typical goal in disambiguation and language modeling, and recognizing target meanings, which is the typical semantic goal in text understanding applications such as question answering, information extraction, summarization and information retrieval. We will discuss each\\\\xa0\u2026"},{"id":"05d51e612ab445c97e2ab8b870317935.html","title":"Noun phrase chunking in hebrew: Influence of lexical and morphological features","url":"https://www.aclweb.org/anthology/P06-1087.pdf","authors":["Yoav Goldberg","Meni Adler","Michael Elhadad"],"date":"2006/07/17","abstract":"We present a method for Noun Phrase chunking in Hebrew. We show that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew, and propose an alternative definition of Simple NPs. We review syntactic properties of Hebrew related to noun phrases, which indicate that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English. As a confirmation, we apply methods known to work well for English to Hebrew data. These methods give low results (F from 76 to 86) in Hebrew. We then discuss our method, which applies SVM induction over lexical and morphological features. Morphological features improve the average precision by~ 0.5%, recall by~ 1%, and F-measure by~ 0.75, resulting in a system with average performance of 93% precision, 93.4% recall and 93.2 F-measure.*"},{"id":"83678268951bc5b47e2de63976effcd1.html","title":"Integrating pattern-based and distributional similarity methods for lexical entailment acquisition","url":"https://dl.acm.org/citation.cfm?id=1273148","authors":["Shachar Mirkin","Ido Dagan","Maayan Geffet"],"date":"2006/07/17","abstract":"This paper addresses the problem of acquiring lexical semantic relationships, applied to the lexical entailment relation. Our main contribution is a novel conceptual integration between the two distinct acquisition paradigms for lexical relations-the pattern-based and the distributional similarity approaches. The integrated method exploits mutual complementary information of the two approaches to obtain candidate relations and informative characterizing features. Then, a small size training set is used to construct a more accurate supervised classifier, showing significant increase in both recall and precision over the original approaches."},{"id":"38f862bf4d463343098055c7dcc46336.html","title":"Lexical reference: a semantic matching subtask","url":"https://www.aclweb.org/anthology/W06-1621.pdf","authors":["Oren Glickman","Eyal Shnarch","Ido Dagan"],"date":"2006/07","abstract":"Semantic lexical matching is a prominent subtask within text understanding applications. Yet, it is rarely evaluated in a direct manner. This paper proposes a definition for lexical reference which captures the common goals of lexical matching. Based on this definition we created and analyzed a test dataset that was utilized to directly evaluate, compare and improve lexical matching models. We suggest that such decomposition of the global semantic matching task is critical in order to fully understand and improve individual components."},{"id":"4c9632cd78856a1790313b50e7598cf8.html","title":"Integrated morphological and syntactic disambiguation for Modern Hebrew","url":"https://www.aclweb.org/anthology/P06-3009.pdf","authors":["Reut Tsarfaty"],"date":"2006/07","abstract":"Current parsing models are not immediately applicable for languages that exhibit strong interaction between morphology and syntax, eg, Modern Hebrew (MH), Arabic and other Semitic languages. This work represents a first attempt at modeling morphological-syntactic interaction in a generative probabilistic framework to allow for MH parsing. We show that morphological information selected in tandem with syntactic categories is instrumental for parsing Semitic languages. We further show that redundant morphological information helps syntactic disambiguation."},{"id":"e7b23d88f3d8aada5a9bd77bd2528d4f.html","title":"Direct word sense matching for lexical substitution","url":"https://www.aclweb.org/anthology/P06-1057.pdf","authors":["Ido Dagan","Oren Glickman","Alfio Gliozzo","Efrat Marmorshtein","Carlo Strapparava"],"date":"2006/07","abstract":"This paper investigates conceptually and empirically the novel sense matching task, which requires to recognize whether the senses of two synonymous words match in context. We suggest direct approaches to the problem, which avoid the intermediate step of explicit word sense disambiguation, and demonstrate their appealing advantages and stimulating potential for future research."},{"id":"077dfdde815619943bba05016805fe5e.html","title":"Investigating lexical substitution scoring for subtitle generation","url":"https://www.aclweb.org/anthology/W06-2907.pdf","authors":["Oren Glickman","Ido Dagan","Walter Daelemans","Mikaela Keller","Samy Bengio"],"date":"2006/06","abstract":"This paper investigates an isolated setting of the lexical substitution task of replacing words with their synonyms. In particular, we examine this problem in the setting of subtitle generation and evaluate state of the art scoring methods that predict the validity of a given substitution. The paper evaluates two context independent models and two contextual models. The major findings suggest that distributional similarity provides a useful complementary estimate for the likelihood that two Wordnet synonyms are indeed substitutable, while proper modeling of contextual constraints is still a challenging task for future research."},{"id":"ac032669ca54aa97422d2d5d728224d6.html","title":"Machine Learning Challenges: Evaluating Predictive Uncertainty, Visual Object Classification, and Recognizing Textual Entailment, First Pascal Machine Learning Challenges\\\\xa0\u2026","url":"http://books.google.com/books?hl=en&lr=&id=TT73BwAAQBAJ&oi=fnd&pg=PA1&dq=info:2B7TYnvmxIEJ:scholar.google.com&ots=cuSWKZ0ub4&sig=QengItjD-BscyLQ6e52zVtBBBdY","authors":["Joaquin Qui\\\\xf1onero-Candela","Ido Dagan","Bernardo Magnini","Florence D\'Alch\\\\xe9-Buc"],"date":"2006/04/07","abstract":""},{"id":"6ca16bf224f26cf8e52547c1e1886b53.html","title":"The second pascal recognising textual entailment challenge","url":"http://scholar.google.com/scholar?cluster=4198264021694391846&hl=en&oi=scholarr","authors":["R Bar Haim","Ido Dagan","Bill Dolan","Lisa Ferro","Danilo Giampiccolo","Bernardo Magnini","Idan Szpektor"],"date":"2006/04","journal":"Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment"},{"id":"9c5957fdcd5fa17ba98e95c40647ba40.html","title":"Investigating a generic paraphrase-based approach for relation extraction","url":"https://www.aclweb.org/anthology/E06-1052.pdf","authors":["Lorenza Romano","Milen Kouylekov","Idan Szpektor","Ido Dagan","Alberto Lavelli"],"date":"2006/04","abstract":"Unsupervised paraphrase acquisition has been an active research field in recent years, but its effective coverage and performance have rarely been evaluated. We propose a generic paraphrase-based approach for Relation Extraction (RE), aiming at a dual goal: obtaining an applicative evaluation scheme for paraphrase acquisition and obtaining a generic and largely unsupervised configuration for RE. We analyze the potential of our approach and evaluate an implemented prototype of it using an RE dataset. Our findings reveal a high potential for unsupervised paraphrase acquisition. We also identify the need for novel robust models for matching paraphrases in texts, which should address syntactic complexity and variability."},{"id":"4aee3c38b8acdef7c2e8ea8ff940ec6d.html","title":"Evaluating Predictive Uncertainty, Visual Objects Classification and Recognising textual entailment: selected proceedings of the First PASCAL Machine Learning Challenges Workshop","url":"https://hal.archives-ouvertes.fr/hal-00343082/","authors":["Joaquin Candela-Quinonero","Ido Dagan","Bernardo Magnini","Florence d\'Alch\\\\xe9-Buc"],"date":"2006"},{"id":"57fd14689e91cc51f4d21e3b3d366240.html","title":"Reviewers for Volume 32","url":"https://www.aclweb.org/anthology/J06-4013.pdf","authors":["Jan Alexandersson","Michael Hess","Tom Morton","Thorsten Brants","Derrick Higgins","Mark-Jan Nederhof","Chris Brew","Graeme Hirst","Ani Nenkova","Claire Cardie","Julia Hockenmaier","Miles Osborne","John Carroll","Rebecca Hwa","David Palmer","Ciprian Chelba","Mark Johnson","Massimo Poesio","Francine Chen","Ron Kaplan","Detlef Prescher","Keh-jiann Chen","Nikiforos Karamanis Phil Resnik","David Chiang","Andy Kehler","Martin Reynaert","Massi Ciaramita","Rodger Kibble","Maarten de Rijke","Christopher Collins","Adam Kilgarriff","Fabio Rinaldi","Michael Collins","Dan Klein","Graeme Ritchie","Ann Copestake","Kevin Knight","Brian Roark","Richard Craggs","Philipp Koehn","Dan Roth","Walter Daelemans","Andras Kornai","Michel Simard","Ido Dagan","Emiel Krahmer","Radu Soricut","Paul Deane","Sadao Kurohashi","Richard Sproat","Mona Diab","Mirella Lapata","Michael Strube","John Dowding","Lillian Lee","G \\\\xf6khan Tur","Phil Edmonds","Beth Levin","Sebastian Varges","David Eichmann","Gina Levow","Richard Wicentowski","Jason Eisner","Hang Li","Dekai Wu","Roger Evans","Elizabeth Liddy","Fei Xia","Radu Florian","Marc Light","Steve Young","George Foster","Jimmy Lin","Menno van Zaanen","Claire Gardent","Benardo Magnini","Annie Zaenen","John Goldsmith","Chris Manning","Fabio Massimo Zanzotto","Gregory Grefenstette Mark Maybury","Pierre Zweigenbaum","Peter Heeman","Mary McGee Wood"],"date":"2006","abstract":"This journal has a knowledgeable and hard-working editorial board, listed on the inside front cover of each issue; but for most submissions we also enlist the aid of specialist reviewers outside the editorial board. The editor of Computational Linguistics would like to express his gratitude to the external reviewers listed below, who anonymously reviewed papers for the journal during the preparation of this volume (Volume 32). Their generosity, judicious judgment, and prompt response substantially helped us to publish a journal that both is timely and maintains exacting scientific standards; it is a genuine pleasure to thank them collectively for their dedicated service."},{"id":"77c05da025c4f65765e74de314495332.html","title":"Strategic intelligence analysis: from information processing to meaning-making","url":"https://link.springer.com/chapter/10.1007/11760146_43","authors":["Yair Neuman","Liran Elihay","Meni Adler","Yoav Goldberg","Amir Viner"],"date":"2006","journal":"Intelligence and Security Informatics","abstract":"Strategic intelligence involves the efforts to understand the \\"Big Picture\\" emerging from data sources. Concerning textual data, this process involves the extraction of meaning from textual information. In this paper we present a new methodology for meaning extraction from news articles. This methodology is based on the gradual construction of visual maps from processed textual information. The methodology is not a substitute for meaning-making by human agents but combines computational power with human evaluation and provides a tool for identifying emerging patterns of meaning."},{"id":"960a59600d6774e62c7843b8501ab56b.html","title":"Evaluating Predictive Uncertainty, Visual Object Categorization and Textual Entailment, volume 3944 of","url":"http://scholar.google.com/scholar?cluster=9651992554516653204&hl=en&oi=scholarr","authors":["J Quinonero-Candela","Ido Dagan","B Magnini","F D\'Alch\\\\xe9-Buc"],"date":"2006"},{"id":"a7d22394cfc478d924d3ed3552b0ee17.html","title":"The interplay of syntax and morphology in building parsing models for modern hebrew","url":"http://www.tsarfaty.com/pdfs/esslli06.pdf","authors":["Reut Tsarfaty"],"date":"2006","journal":"Proceedings of ESSLI Student Session","abstract":"As of yet, there is no statistical parser for Modern Hebrew (MH). Current practice in building parsing models is not immediately applicable to languages that exhibit strong interaction between syntax and morphology, eg Modern Hebrew, Arabic and other Semitic languages. We suggest that incorporating morphological and morphosyntactic information into the parsing model is essential for parsing Semitic languages. Using a morphological analyzer, a part-of-speech tagger, and a PCFG-based general purpose parser, we segment and parse unseen MH sentences using a small annotated corpus. The Parseval scores obtained are not comparable to those of, eg, state-of-the-art models for English, due to remaining syntactic ambiguity and limited morphological treatment. We conjecture that adequate morphological and syntactic processing of MH should be done in a unified framework in which morphology and syntax can freely interact and share information in both directions."},{"id":"af3bab1afab18851235b3122f37762f2.html","title":"Machine Learning Challenges","url":"https://link.springer.com/content/pdf/10.1007/11736790.pdf","authors":["Joaquin Qui\\\\xf1onero-Candela","Ido Dagan","BernardoMagnini Florenced\u2019Alch\\\\xe9-Buc"],"date":"2006","abstract":"The first PASCAL Machine Learning Challenges Workshop (MLCW 2005)(see, www. pascal-network. org/Workshops/PC04/) was held in Southampton, UK, during April 11-13, 2005. This conference was organized by the Challenges programme of the European Network of Excellence PASCAL (Pattern Analysis, Statistical modelling and ComputationAl Learning) in the framework of the IST Programme of the European Community. First annually and now quarterly, the PASCAL Challenges Programme plays the role of selecting and sponsoring challenging tasks, either practical or theoretical. The aim is to raise difficult machine learning questions and to motivate innovative research and development of new approaches. Financial support covers all the work concerning the cleaning and labelling of the data as well as the preparation of evaluation tools for ranking the results. For the first round of the programme, four\\\\xa0\u2026"},{"id":"b345040e8dc3672f3afe1292c893fb02.html","title":"Causative constructions and aspectual meanings: a case study from Semitic derivational morphology","url":"http://www.academia.edu/download/5445850/10.1.1.98.1080.pdf#page=255","authors":["Reut Tsarfaty"],"date":"2005/12/19","journal":"Fifteenth Amsterdam Colloquium","abstract":"This work aims at identifying aspectual properties of events denoted by morphological causatives in Modern Hebrew (MH). The main purpose of this investigation is to establish a clear connection between causative constructions and aspectual meanings, two notions that are not so easily correlated. A secondary goal is to argue for the systematic aspectual contribution of Semitic derivational morphology. Our theory is inspired by Smith\u2019s causal chain and builds on a thematic account of Semitic derivational morphology. Combining a formal and empirical investigation we argue that the MH causative template Hiph\u2019il shifts the viewpoint of an event onto its initiation and development phases, making it more appropriate for imperfective use."},{"id":"a7290f0c471ad322dfcd68636b8c2312.html","title":"A generalized framework for revealing analogous themes across related topics","url":"https://www.aclweb.org/anthology/H05-1123.pdf","authors":["Zvika Marx","Ido Dagan","Eli Shamir"],"date":"2005/10","abstract":"This work addresses the task of identifying thematic correspondences across subcorpora focused on different topics. We introduce an unsupervised algorithmic framework based on distributional data clustering, which generalizes previous initial works on this task. The empirical results reveal interesting commonalities of different religions. We evaluate the results through measuring the overlap of our clusters with clusters compiled manually by experts. The tested variants of our framework are shown to outperform alternative methods applicable to the task."},{"id":"cc459f92cb45f9eb1bd2bd5685165579.html","title":"Investigating unsupervised learning for text categorization bootstrapping","url":"https://www.aclweb.org/anthology/H05-1017.pdf","authors":["Alfio Gliozzo","Carlo Strapparava","Ido Dagan"],"date":"2005/10","abstract":"We propose a generalized bootstrapping algorithm in which categories are described by relevant seed features. Our method introduces two unsupervised steps that improve the initial categorization step of the bootstrapping scheme:(i) using Latent Semantic space to obtain a generalized similarity measure between instances and features, and (ii) the Gaussian Mixture algorithm, to obtain uniform classification probabilities for unlabeled examples. The algorithm was evaluated on two Text Categorization tasks and obtained state-of-theart performance using only the category names as initial seeds."},{"id":"25f30dac32395ce6751cefb384eef71a.html","title":"Participants in Action: The interplay of aspectual meanings and thematic relations in the semantics of Semitic morphology","url":"https://link.springer.com/chapter/10.1007/978-3-540-75144-1_15","authors":["Reut Tsarfaty"],"date":"2005/09/12","abstract":"This work aims to demonstrate that event structure and thematic relations are closely intertwined. Specifically, we show that in Modern Hebrew the choice of a morphological template has profound effects on the event structure of derived verbs. These effects are correlated with the thematic features marked by the templates, and are mediated by the aspectual classification of the lexical material provided by roots."},{"id":"e0a399eddbe3023cc37b7b69f369b01a.html","title":"A probabilistic lexical approach to textual entailment","url":"https://u.cs.biu.ac.il/~nlp/downloads/publications/ProbabilisticLexical.pdf","authors":["Oren Glickman","Ido Dagan","Moshe Koppel"],"date":"2005/07/30","journal":"IJCAI","abstract":"The textual entailment problem is to determine if a given text entails a given hypothesis. This paper describes first a general generative probabilistic setting for textual entailment. We then focus on the sub-task of recognizing whether the lexical concepts present in the hypothesis are entailed from the text. This problem is recast as one of text categorization in which the classes are the vocabulary words. We make novel use of Na\\\\xefve Bayes to model the problem in an entirely unsupervised fashion. Empirical tests suggest that the method is effective and compares favorably with state-of-theart heuristic scoring approaches."},{"id":"076c7595eb2913b2c8adaa1c8fe7c019.html","title":"A probabilistic classification approach for lexical textual entailment","url":"https://www.aaai.org/Papers/AAAI/2005/AAAI05-166.pdf","authors":["Oren Glickman","Ido Dagan","Moshe Koppel"],"date":"2005/07/09","abstract":"The textual entailment task\u2013determining if a given text entails a given hypothesis\u2013provides an abstraction of applied semantic inference. This paper describes first a general generative probabilistic setting for textual entailment. We then focus on the sub-task of recognizing whether the lexical concepts present in the hypothesis are entailed from the text. This problem is recast as one of text categorization in which the classes are the vocabulary words. We make novel use of Na\\\\xefve Bayes to model the problem in an entirely unsupervised fashion. Empirical tests suggest that the method is effective and compares favorably with state-of-the-art heuristic scoring approaches."},{"id":"0dd14fb8b94af23afaecf1123a850a45.html","title":"The distributional inclusion hypotheses and lexical entailment","url":"https://www.aclweb.org/anthology/P05-1014.pdf","authors":["Maayan Geffet","Ido Dagan"],"date":"2005/06","abstract":"This paper suggests refinements for the Distributional Similarity Hypothesis. Our proposed hypotheses relate the distributional behavior of pairs of words to lexical entailment\u2013a tighter notion of semantic similarity that is required by many NLP applications. To automatically explore the validity of the defined hypotheses we developed an inclusion testing algorithm for characteristic features of two words, which incorporates corpus and web-based feature sampling to overcome data sparseness. The degree of hypotheses validity was then empirically tested and manually analyzed with respect to the word sense level. In addition, the above testing algorithm was exploited to improve lexical entailment acquisition."},{"id":"1b9ef5c6a4daa07311494d1bc36e217b.html","title":"Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005)","url":"https://www.aclweb.org/anthology/W05-0600.pdf","authors":["Ido Dagan","Daniel Gildea"],"date":"2005/06","abstract":"The 2005 Conference on Computational Natural Language Learning (CoNLL-2005) is the ninth in a series of meetings organized by SIGNLL, the ACL special interest group on natural language learning. This year\u2019s CoNLL will be held in Ann Arbor, Michigan, on June 29 and 30, in conjunction with the ACL 2005 conference."},{"id":"98415e2b116a891509ba065048c9071f.html","title":"Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment","url":"https://www.aclweb.org/anthology/W05-1200.pdf","authors":["Bill Dolan","Ido Dagan"],"date":"2005/06","abstract":"The last few years have seen a surge in interest in modeling techniques aimed at measuring semantic equivalence and entailment, with work on paraphrase acquisition/generation, WordNetbased expansion, distributional similarity, supervised learning of semantic variability in information extraction, and the identification of patterns in template-based QA. Being able to identify when two strings\u201d mean the same thing\u201d or that one entails the other are crucial abilities for a broad range of NLP-related applications, ranging from question answering to summarization."},{"id":"de5d41afe8d24446dd55ed02b7176d7a.html","title":"A probabilistic setting and lexical coocurrence model for textual entailment","url":"https://www.aclweb.org/anthology/W05-1208.pdf","authors":["Oren Glickman","Ido Dagan"],"date":"2005/06","abstract":"This paper proposes a general probabilistic setting that formalizes a probabilistic notion of textual entailment. We further describe a particular preliminary model for lexical-level entailment, based on document cooccurrence probabilities, which follows the general setting. The model was evaluated on two application independent datasets, suggesting the relevance of such probabilistic approaches for entailment modeling."},{"id":"012a416bc1af060b74f6e125465bfbf1.html","title":"The PASCAL recognising textual entailment challenge","url":"https://link.springer.com/chapter/10.1007/11736790_9","authors":["Ido Dagan","Oren Glickman","Bernardo Magnini"],"date":"2005/04/11","abstract":"This paper describes the PASCAL Network of Excellence first <i>Recognising Textual Entailment</i> (RTE-1) Challenge benchmark. The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task."},{"id":"c877742498663de107846c9ced987a68.html","title":"A lexical alignment model for probabilistic textual entailment","url":"https://link.springer.com/chapter/10.1007/11736790_16","authors":["Oren Glickman","Ido Dagan","Moshe Koppel"],"date":"2005/04/11","abstract":"This paper describes the Bar-Ilan system participating in the Recognising Textual Entailment Challenge. The paper proposes first a general probabilistic setting that formalizes the notion of textual entailment. We then describe a concrete alignment-based model for lexical entailment, which utilizes web co-occurrence statistics in a bag of words representation. Finally, we report the results of the model on the <i>Recognising Textual Entailment</i> challenge dataset along with some analysis."},{"id":"b1e29f9af9a87902a68f2dbcd937d05d.html","title":"Web based probabilistic textual entailment","url":"https://www.researchgate.net/profile/Oren_Glickman/publication/244447093_Web_Based_Probabilistic_Textual_Entailment/links/5e22ee3fa6fdcc101571cd62/Web-Based-Probabilistic-Textual-Entailment.pdf","authors":["Oren Glickman","Ido Dagan","Moshe Koppel"],"date":"2005/04","journal":"Proceedings of the 1st Pascal Challenge Workshop","abstract":"This paper proposes a general probabilistic setting that formalizes the notion of textual entailment. In addition we describe a concrete model for lexical entailment based on web co-occurrence statistics in a bag of words representation."},{"id":"ba7df1bf2c1f7dab0f447e6da49a0fc7.html","title":"binyanim ba\'avir\': An investigation of Aspect Semantics in Modern Hebrew","url":"https://eprints.illc.uva.nl/755/","authors":["Reut Tsarfaty"],"date":"2005/03/3","abstract":""},{"id":"c62857c09c01c877edaca09715b200fa.html","title":"Connecting Causative Constructions and Aspectual Meanings: A Case Study from Semitic Derivational Morphology.","url":"https://dare.uva.nl/record/1/246535","authors":["Reut Tsarfaty"],"date":"2005","abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Total citations"},{"id":"53bffdf4b20ff8b686f849ae8ffde9cd.html","title":"Acquiring lexical paraphrases from a single corpus","url":"http://books.google.com/books?hl=en&lr=&id=ejXv2scn794C&oi=fnd&pg=PA81&dq=info:U3mr-q-oL7UJ:scholar.google.com&ots=tjjJp7e0r4&sig=Hx015kFwysH8i8jua3AINYI222M","authors":["Oren Glickman","Ido Dagan"],"date":"2004/11/30","journal":"Recent Advances in Natural Language Processing III. John Benjamins Publishing, Amsterdam, Netherlands","abstract":"This paper studies the potential of extracting lexical paraphrases from a single corpus, focusing on the extraction of verb paraphrases. Most previous approaches detect individual paraphrase instances within a pair (or set) of comparable corpora, each of them containing roughly the same information, and rely on the substantial level of correspondence of such corpora. We present a novel method that successfully detects isolated paraphrase instances within a single corpus without relying on any a-priori structure and information."},{"id":"b41106d0c341a9cf61d8cf6c52affcbb.html","title":"Unsupervised and supervised exploitation of semantic domains in lexical disambiguation","url":"https://www.sciencedirect.com/science/article/pii/S088523080400018X","authors":["Alfio Gliozzo","Carlo Strapparava","Ido Dagan"],"date":"2004/07/01","journal":"Computer Speech & Language","abstract":""},{"id":"87419e07868d01024622c6708f495c63.html","title":"Scaling web-based acquisition of entailment relations","url":"https://www.aclweb.org/anthology/W04-3206.pdf","authors":["Idan Szpektor","Hristo Tanev","Ido Dagan","Bonaventura Coppola"],"date":"2004/07","abstract":"Paraphrase recognition is a critical step for natural language interpretation. Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases. However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited. We present a fully unsupervised learning algorithm for Web-based extraction of entailment relations, an extended model of paraphrases. We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base. Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates. Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods."},{"id":"b5c85f1d4583f358df934488cad9a4dc.html","title":"Probabilistic textual entailment: Generic applied modeling of language variability","url":"http://www.academia.edu/download/42124110/ProbabilisticTE_fv07.pdf","authors":["Ido Dagan","Oren Glickman"],"date":"2004/01/26","journal":"Learning Methods for Text Understanding and Mining","abstract":"A most prominent phenomenon of natural languages is variability\u2013stating the same meaning in various ways. Robust language processing applications\u2013like Information Retrieval (IR), Question Answering (QA), Information Extraction (IE), text summarization and machine translation\u2013must recognize the different forms in which their inputs and requested outputs might be expressed. Today, inferences about language variability are often performed by practical systems at a\\" shallow\\" semantic level, due to the fact that robust semantic interpretation into logic-based meaning-level representations is not feasible. However, there is yet no generally applicable framework for modeling variability in an application independent manner. Consequently this problem is treated mostly independently within individual systems, and usually to a quite limited extent. In this paper we outline a proposal for a generic model for recognizing language variability at a shallow semantic level, its implementation as a practical engine to be leveraged within a variety of applications, and several learning tasks that it poses. Our approach is based on a notion of textual entailment between text expressions, capturing that the meaning of one expression can be inferred from the other. We propose an inference model that approximates entailment without any explicit interpretation into meaning representations, but rather operating directly over lexical-syntactic units. The model consists of a knowledge base of basic patterns along with compositional probabilistic inference rules, and can be implemented as a practical Prolog-style engine. We further propose learning approaches for\\\\xa0\u2026"},{"id":"d311e9a2932cf4038190078842ecfc76.html","title":"Philosophy of cognition Why people think computers can\u2019t","url":"https://www.academia.edu/download/30693332/MMAI.pdf","authors":["Reut Tsarfaty"],"date":"2004/01/07","abstract":"The purpose of this paper is to present Marvin Minsky\u2019s view of Artificial intelligence, based on three of his papers. This paper is organized as follows. In section 1 I provide a brief introduction to AI and to Marvin Minsky\u2019s work. In sections 2/3 I discuss questions about the limits/capabilities of machines as presented in [Marvin Minsky, 1982], In section 4 I present a theory suggested [Marvin Minsky, 1974] to support his ideas and consequently I show an application of that theory for dealing with different forms of humor as presented in [Marvin Minsky, 1981]. The approaches and examples I present in sections 2 to 4 are all adopted from [Marvin Minsky, 1974][Marvin Minsky, 1981] and [Marvin Minsky, 1982]."},{"id":"0baed24442f248b8dc6267ca34fa10de.html","title":"Statistical and Learning Methods in Natural Language Processing","url":"http://cs.haifa.ac.il/~shuly/teaching/04/statnlp/hpos4.pdf","authors":["Ido Dagan","Shuly Wintner"],"date":"2004","abstract":"Objective: reducing the degree of morphological ambiguity using statistical data automatically derived from large Hebrew corpora, in order to improve the recall of Hebrew search engines."},{"id":"ccf0d46458e9f640653017c756967ac7.html","title":"Introduction to the special issue on word sense disambiguation","url":"https://www.infona.pl/resource/bwmeta1.element.elsevier-bb4d8d31-d995-3a05-b47f-e6163620d56e","authors":["Judita Preiss","Mark Stevenson"],"date":"2004","journal":"Computer Speech & Language"},{"id":"ce6bcde0ac1d08035764c91e3c41168a.html","title":"Feature vector quality and distributional similarity","url":"https://www.aclweb.org/anthology/C04-1036.pdf","authors":["Maayan Geffet","Ido Dagan"],"date":"2004","abstract":"We suggest a new goal and evaluation criterion for word similarity measures. The new criterion-meaning-entailing substitutability-fits the needs of semantic-oriented NLP applications and can be evaluated directly (independent of an application) at a good level of human agreement. Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality. Finally, a novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance."},{"id":"014a7d85f2e80a459fe01e803357e5de.html","title":"Course on Empirical Learning Methods in NLP\u2013References","url":"http://cs.haifa.ac.il/~shuly/teaching/04/statnlp/bib.doc","authors":["Ido Dagan"],"date":"2003/06","journal":"Presented at Trento University","abstract":"Foundations of Statistical Natural Language Processing by Christopher D. Manning and Hinrich Sch\\\\xfctze, MIT Press, 1999 (Second printing with corrections\u20132000). The home page for the book is at http://mitpress. mit. edu/catalog/item/default. asp? ttype= 2&tid= 3391."},{"id":"49367d9a91485f51f8761adc4a08de96.html","title":"Identifying structure across pre-partitioned data","url":"https://proceedings.neurips.cc/paper/2003/file/de7092ba6df4276921d27a3704c57998-Paper.pdf","authors":["Zvika Marx","Ido Dagan","Eli Shamir"],"date":"2003","journal":"Advances in neural information processing systems","abstract":"We propose an information-theoretic clustering approach that incorporates a pre-known partition of the data, aiming to identify common clusters that cut across the given partition. In the standard clustering setting the formation of clusters is guided by a single source of feature information. The newly utilized pre-partition factor introduces an additional bias that counterbalances the impact of the features whenever they become correlated with this known partition. The resulting algorithmic framework was applied successfully to synthetic data, as well as to identifying text-based cross-religion correspondences."},{"id":"4f542800f18f2bf1d6d447e9ad61df87.html","title":"A corpus-independent feature set for style-based text categorization","url":"http://www.academia.edu/download/47390282/A_Corpus-Independent_Feature_Set_for_Sty20160720-6271-vkiogp.pdf","authors":["Moshe Koppel","Navot Akiva","Ido Dagan"],"date":"2003","journal":"IJCAI-2003 Workshop on Computational Approaches to Text Style and Synthesis, Acapulco, Mexico","abstract":"We suggest a corpus-independent feature set appropriate for style-based text categorization problems. To achieve this, we introduce a new measure on linguistic features, called stability, which captures the extent to which a language element, such as a word or syntactic construct, is replaceable by semantically equivalent elements. This measure may be perceived as quantifying the degree of available \u201csynonymy\u201d for a language item. We show that frequent but unstable features are especially useful for stylebased text categorization."},{"id":"4c2cd1fbf8ef923b39adca6a259d538c.html","title":"Cross-dataset clustering: Revealing corresponding themes across multiple corpora","url":"https://www.aclweb.org/anthology/W02-2009.pdf","authors":["Ido Dagan","Zvika Marx","Eli Shamir"],"date":"2002","abstract":"We present a method for identifying corresponding themes across several corpora that are focused on related, but distinct, domains. This task is approached through simultaneous clustering of keyword sets extracted from the analyzed corpora. Our algorithm extends the informationbottleneck soft clustering method for a suitable setting consisting of several datasets. Experimentation with topical corpora reveals similar aspects of three distinct religions. The evaluation is by way of comparison to clusters constructed manually by an expert."},{"id":"94bfa76f3463a9c9a6ddb9056f0187e4.html","title":"Crosscomponent clustering for template induction","url":"https://u.cs.biu.ac.il/~nlp/downloads/publications/marx_dagan_02.pdf","authors":["Zvika Marx","Ido Dagan","Eli Shamir"],"date":"2002","journal":"Workshop on Text Learning (TextML-2002), Sydney, Australia","abstract":"We suggest an unsupervised approach to template induction for information extraction, through detecting sub-topics and themes that cut across the documents of a topical corpus. We introduce a new method \u0153 cross component clustering \u0153 that simultaneously clusters the components forming our setting, each of which consists of the words of a single article. Our algorithm is derived from the Information Bottleneck clustering algorithm. The resulting clusters are found to be in systematic correspondence with sets of terms that are used in filling the slots of the MUC3/4 ready-made template, which was used for evaluation."},{"id":"c2af36e469a64077d95e491fa3bbacc5.html","title":"Coupled clustering: a method for detecting structural correspondence","url":"http://www.jmlr.org/papers/v3/marx02a.html","authors":["Zvika Marx","Ido Dagan","Joachim M Buhmann","Eli Shamir"],"date":"2002","journal":"Journal of Machine Learning Research","abstract":"This paper proposes a new paradigm and a computational framework for revealing equivalencies (analogies) between sub-structures of distinct composite systems that are initially represented by unstructured data sets. For this purpose, we introduce and investigate a variant of traditional data clustering, termed coupled clustering, which outputs a configuration of corresponding subsets of two such representative sets. We apply our method to synthetic as well as textual data. Its achievements in detecting topical correspondences between textual corpora are evaluated through comparison to performance of human experts."},{"id":"ce69c1c885af4e2a9b27d425aadd67ac.html","title":"Conceptual mapping through keyword coupled clustering","url":"https://link.springer.com/article/10.1007/BF02512360","authors":["Zvika Marx","Ido Dagan"],"date":"2001/09/01","journal":"Mind & Society","abstract":"This paper introduces coupled clustering\u2014a novel computational framework for detecting corresponding themes in unstructured data. Gaining its inspiration from the structure mapping theory, our framework utilizes unsupervised statistical learning tools for automatic construction of aligned representations reflecting the context of the particular mapping being made. The coupled clustering algorithm is demonstrated and evaluated through detecting conceptual correspondences in textual corpora. In its current phase, the method is primarily oriented towards context-dependent feature-based similarity. However, it is preliminary demonstrated how it could be utilized for identification of relational commonalities, as well."},{"id":"e45ec26c0545a79ca6ef645cb2044ca5.html","title":"Incorporating compositional evidence in memory-based partial parsing","url":"https://www.aclweb.org/anthology/P00-1007.pdf","authors":["Yuval Krymolowski","Ido Dagan"],"date":"2000/10","abstract":"In this paper, a memory-based parsing method is extended for handling compositional structures. The method is oriented for learning to parse any selected subset of target syntactic structures. It is local, yet can handle also compositional structures. Parts of speech as well as embedded instances are being used simultaneously. The output is a partial parse in which instances of the target structures are marked."},{"id":"3e6d28b7b14ce5005e8677fb2591de90.html","title":"Contextual word similarity","url":"http://books.google.com/books?hl=en&lr=&id=VoOLvxyX0BUC&oi=fnd&pg=PA459&dq=info:I1JJMg01ohIJ:scholar.google.com&ots=ww8_IF6On1&sig=4fvfT0dbIh2iEIASvrtv60qpKGk","authors":["Ido Dagan"],"date":"2000/07/25","journal":"Handbook of Natural Language Processing","abstract":"Identifying different types of similarities between words has been an important goal in Natural Language Processing (NLP). This chapter describes the basic statistical approach for computing the degree of similarity between words. In this approach a word is represented by a word co-occurrence vector in which each entry corresponds to another word in the lexicon. The value of an entry specifies the frequency of joint occurrence of the two words in the corpus; that is, the frequency in which they cooccur within some particular relations in the text. The degree of similarity between a pair of words is then computed by some similarity or distance measure that is applied to the corresponding pairs of vectors."},{"id":"c1399d12880bb85a064a4594c8a33ab8.html","title":"A comprehensive bilingual word alignment system","url":"https://link.springer.com/chapter/10.1007/978-94-017-2535-4_4","authors":["Yaacov Choueka","Ehud S Conley","Ido Dagan"],"date":"2000","abstract":"This chapter describes a general, comprehensive and robust word-alignment system and its application to the Hebrew-English language pair. A major goal of the system architecture is to assume as little as possible about its input and about the relative nature of the two languages, while allowing the use of (minimal) specific monolingual pre-processing resources when required. The system thus receives as input a pair of raw parallel texts and requires only a tokeniser (and possibly a lemmatiser) for each language. After tokenisation (and lemmatisation if necessary), a rough initial alignment is obtained for the texts using a version of Fung and McKeown\u2019s <i>DK-vec</i> algorithm (Fung und McKeown, 1997; Fung, this volume). The initial alignment is given as input to a version of the <i>word_ align</i> algorithm (Dagan, Church and Gale, 1993), an extension of Model 2 in the IBM statistical translation model. <i>Word_align</i>\\\\xa0\u2026"},{"id":"07451fd8d5c4b7598e4ded7e392cdc49.html","title":"Committee-based sample selection for probabilistic classifiers","url":"https://www.jair.org/index.php/jair/article/view/10244","authors":["Shlomo Argamon-Engelson","Ido Dagan"],"date":"1999/11/15","journal":"Journal of Artificial Intelligence Research","abstract":"In many real-world learning tasks it is expensive to acquire a sufficient number of labeled examples for training. This paper investigates methods for reducing annotation cost by <i>sample selection</i>. In this approach, during training the learning program examines many unlabeled examples and selects for labeling only those that are most informative at each stage. This avoids redundantly labeling examples that contribute little new information."},{"id":"60d05697f0456c3b41ae4429a4a52c97.html","title":"Similarity-based models of word cooccurrence probabilities","url":"https://link.springer.com/article/10.1023/A:1007537716579","authors":["Ido Dagan","Lillian Lee","Fernando CN Pereira"],"date":"1999/02/01","journal":"Machine learning","abstract":"In many applications of natural language processing (NLP) it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations \u201ceat a peach\u201d and \u201deat a beach\u201d is more likely. Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on \u201cmost similar\u201d words."},{"id":"6f4f514535cf2b3f607491a387391218.html","title":"Eric Brill Dept. of Computer Science brill@ cs. jhu. edu Johns Hopkins University Baltimore, MD, USA James Brooks Dept. of Computer and jbrooks@ gradient. cis. upenn. edu\\\\xa0\u2026","url":"https://link.springer.com/content/pdf/10.1007/978-94-017-2390-9.pdf#page=307","authors":["Kenneth Church","Michael Collins","Bruce Croft","Ido Dagan","Pim van der Eijk","Helmut Feldweg","Hideo Fujii","Cap Gemini"],"date":"1999","journal":"Natural Language Processing Using Very Large Corpora","abstract":"Index accent/diacritic restoration, 100-109, 112, 113, 117,118 acronymic abbreviations, 147 acronyms, 52 adaptive language modeling, 122 adjacent character groups, 139 words, 141 algorithm Baum-Welch, 5, 6, 27, 28, 34, 36-38 inside-outside, 197 convergence, 198 local minima, 199 online, 125 supervised training, 38 unsupervised learning, 33 Viterbi, 1, 15, 103 aligment, 225 character-based, 211 document elements, 267 multiparagraph, 265 offset probability, 216 thresholds, 217 translation probability, 212 word alignment, 209 ambiguity, 275 accent/diacritic, 100, 117 class, 2-4, 7-9 morphological, 47 rate, 6 type, 8 authorship identification, 105 baseline error, 152 Baum-Welch training, 38, 41 Bayesian classifier, 105-107, 111,114-116, 118, 119"},{"id":"ce63226241a938f295ce325dafa36346.html","title":"Proceedings of the Fourth Workshop on Very Large Corpora","url":"http://scholar.google.com/scholar?cluster=10700900845674491184&hl=en&oi=scholarr","authors":["Eva Ejerhed","Ido Dagan"],"date":"1999"},{"id":"f624b6d0daf4c2a29b945e3bad5f637d.html","title":"Detecting sub-topic correspondence through bipartite term clustering","url":"https://www.aclweb.org/anthology/W99-0907.pdf","authors":["Zvika Marx","Ido Dagan","Eli Shamir"],"date":"1999","abstract":"Ido DAGAN The Institute for Info. Retrieval and Computational Linguistics, The Department of Mathematics and Computer Science, Bar-Ilan University Ramat-Gan 52900, Israel, dagan@ cs. biu. ac. il related to each other?"},{"id":"2fcaed52c32c5d18dc7b5a1a74cd543f.html","title":"A memory-based approach to learning shallow natural language patterns","url":"https://arxiv.org/abs/cmp-lg/9806011","authors":["Shlomo Argamon","Ido Dagan","Yuval Krymolowski"],"date":"1998/06/16","journal":"arXiv preprint cmp-lg/9806011","abstract":"Recognizing shallow linguistic patterns, such as basic syntactic relationships between words, is a common task in applied natural language and text processing. The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata. This paper presents a novel memory-based learning method that recognizes shallow patterns in new text based on a bracketed training corpus. The training data are stored as-is, in efficient suffix-tree data structures. Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus. This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training. The paper presents experimental results for recognizing noun phrase, subject-verb and verb-object patterns in English. Since the learning approach enables easy porting to new domains, we plan to apply it to syntactic patterns in other languages and to sub-language patterns for information extraction."},{"id":"b82fe3f77de359f6673efc620b9a37ac.html","title":"Mining text using keyword distributions","url":"https://link.springer.com/article/10.1023/A:1008623632443","authors":["Ronen Feldman","Ido Dagan","Haym Hirsh"],"date":"1998/05/01","journal":"Journal of Intelligent Information Systems","abstract":"Knowledge Discovery in Databases (KDD) focuses on the computerized exploration of large amounts of data and on the discovery of interesting patterns within them. While most work on KDD has been concerned with structured databases, there has been little work on handling the huge amount of information that is available only in unstructured textual form. This paper describes the KDT system for Knowledge Discovery in Text, in which documents are labeled by keywords, and knowledge discovery is performed by analyzing the co-occurrence frequencies of the various keywords labeling the documents. We show how this keyword-frequency approach supports a range of KDD operations, providing a suitable foundation for knowledge discovery and exploration for collections of unstructured text."},{"id":"f6657b3b66341ff0080b271029dc1907.html","title":"Technomonitor.","url":"https://elibrary.ru/item.asp?id=4862864","authors":["Ido Dagan"],"date":"1998","journal":"Online","abstract":"Discusses the trends and technological developments for automating some text-processing tasks and ways these developments can help persons deal with major information access problems. Details on the use of information extraction techniques; Use of manual classification to classify documents by end-users and information specialists."},{"id":"6c146c916385cd07dcb42f878d68e261.html","title":"Similarity-based methods for word sense disambiguation","url":"https://arxiv.org/abs/cmp-lg/9708010","authors":["Ido Dagan","Lillian Lee","Fernando Pereira"],"date":"1997/08/18","journal":"arXiv preprint cmp-lg/9708010","abstract":"We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency. The similarity-based methods perform up to 40% better on this particular task. We also conclude that events that occur only once in the training set have major impact on similarity-based estimates."},{"id":"21b250aedc96eebe4590324736b128e0.html","title":"Mistake-driven learning in text categorization","url":"https://arxiv.org/abs/cmp-lg/9706006","authors":["Ido Dagan","Yael Karov","Dan Roth"],"date":"1997/06/09","journal":"arXiv preprint cmp-lg/9706006","abstract":"Learning problems in the text processing domain often map the text to a space whose dimensions are the measured features of the text, eg, its words. Three characteristic properties of this domain are (a) very high dimensionality,(b) both the learned concepts and the instances reside very sparsely in the feature space, and (c) a high variation in the number of active features in an instance. In this work we study three mistake-driven learning algorithms for a typical task of this nature--text categorization. We argue that these algorithms--which categorize documents by learning a linear separator in the feature space--have a few properties that make them ideal for this domain. We then show that a quantum leap in performance is achieved when we further modify the algorithms to better address some of the specific characteristics of the domain. In particular, we demonstrate (1) how variation in document length can be tolerated by either normalizing feature weights or by using negative weights,(2) the positive effect of applying a threshold range in training,(3) alternatives in considering feature frequency, and (4) the benefits of discarding features while training. Overall, we present an algorithm, a variation of Littlestone\'s Winnow, which performs significantly better than any other algorithm tested on this task using a similar feature set."},{"id":"fd0266c3b2104a3775538ca2925665fc.html","title":"Termight: Coordinating humans and machines in bilingual terminology acquisition","url":"https://link.springer.com/article/10.1023/A:1007926723945","authors":["Ido Dagan","Ken Church"],"date":"1997/03/01","journal":"Machine translation","abstract":"We propose a semi-automatic tool, termight, that supports the construction of bilingual glossaries. Termight consists of two components which address the two subtasks in glossary construction: (a) preparing a monolingual list of all technical terms in a source-language document, and (b) finding the translations for these terms in parallel source\u2013target documents. As a first step (in each component) the tool extracts automatically candidate terms and candidate translations, based on term-extraction and word-alignment algorithms. It then performs several additional preprocessing steps which greatly facilitate human post-editing of the candidate lists. These steps include grouping and sorting of candidates and associating example concordance lines with each candidate. Finally, the data prepared in preprocessing is presented to the user via an interactive interface which supports quick post-editing operations\\\\xa0\u2026"},{"id":"7877fe6d45cbf14af01ffcf40d791f4d.html","title":"Mistake-driven learning with thesaurus for text categorization","url":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.1482&rep=rep1&type=pdf","authors":["Takefumi Yamazaki","Ido Dagan"],"date":"1997","journal":"Proceedings of NLPRS-97, the Natural Language Processing Pacific Rim Symposium","abstract":"This paper extends the mistake-driven learner WINNOW to better utilize thesauri for text categorization. In our method not only words but also semantic categories given by the thesaurus are used as features in a classier. New ltering and disambiguation methods are used as pre-processing to solve the problems caused by the use of the thesaurus. In order to verify our methods, we test a large body of tagged Japanese newspaper articles created by RWCP1. Experimental results show that WINNOW with thesauri attains high accuracy and that the proposed ltering and disambiguation methods also contribute to the improved accuracy."},{"id":"3081b2e38a3451a780ab21173573a59d.html","title":"Minimizing manual annotation cost in supervised training from corpora","url":"https://arxiv.org/abs/cmp-lg/9606030","authors":["Sean P Engelson","Ido Dagan"],"date":"1996/06/24","journal":"arXiv preprint cmp-lg/9606030","abstract":"Corpus-based methods for natural language processing often use supervised training, requiring expensive manual annotation of training corpora. This paper investigates methods for reducing annotation cost by {\\\\\\\\it sample selection}. In this approach, during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage. This avoids redundantly annotating examples that contribute little new information. This paper extends our previous work on {\\\\\\\\it committee-based sample selection} for probabilistic classifiers. We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-of-speech tagging. We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs. In particular, the simplest method, which has no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger."},{"id":"155d74a2c2b1f1d73f4690d11aaf58cc.html","title":"Computational Linguistics ISSN 0891-2017","url":"https://www.aclweb.org/anthology/J96-2000.pdf","authors":["Julia Hirschberg","Robert C Berwick","Graeme Hirst","Richard Sproat","Pierre Isabelle","Robert JP Ingria","Hiyan Alshawi","Mark Johnson","John Lafferty","Lori Levin","Johanna Moore","Stephen Pulman","Mats Rooth","Keh-Yih Su","David Weir","Anne Abeille","Patrick Blackburn","Susan Brennan","Matt Crocker","Ido Dagan","Masaaki Nagata","Megumi Kameyama","Ed Stabler","Kees van Deemter","Gregory Ward","Pete Whitelock","Eric Brill","Richard Crouch","David Johnson","Julian Kupiec","Emmanuel Roche","Hinrich Schiitze","Marilyn Walker"],"date":"1996/06","journal":"Computational Linguistics","abstract":"Creative Commons License ACL materials are Copyright\\\\xa9 1963\u20132019 ACL; other materials are copyrighted by their respective copyright holders. Materials prior to 2016 here are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License. Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a Creative Commons Attribution 4.0 International License."},{"id":"e273ff05dead65b08f7cb2a369e7a6ce.html","title":"Keyword-based browsing and analysis of large document sets","url":"https://www.academia.edu/download/49469460/Keyboard_sdair.pdf","authors":["Ido Dagan","Ronen Feldman","Haym Hirsh"],"date":"1996/04","journal":"Proceedings of the symposium on document analysis and information retrieval (SDAIR-96), Las Vegas, Nevada","abstract":"Knowledge Discovery in Databases (KDD) focuses on the computerized exploration of large amounts of data and on the discovery of interesting patterns within them. While most work on KDD has been concerned with structured databases, there has been little work on handling the huge amount of information that is available only in unstructured textual form. This paper describes the KDT system for Knowledge Discovery in Texts. It is built on top of a text-categorization paradigm where text articles are annotated with keywords organized in a hierarchical structure. Knowledge discovery is performed by analyzing the co-occurrence frequencies of"},{"id":"0a2068bc17d19c0ba8eed9575f7255d8.html","title":"Efficient algorithms for mining and manipulating associations in texts","url":"http://scholar.google.com/scholar?cluster=18025103419565185400&hl=en&oi=scholarr","authors":["Ronen Feldman","Ido Dagan","W Kloesgen"],"date":"1996","journal":"CYBERNETICS AND SYSTEMS RESEARCH"},{"id":"cf7012f358a3ec4f7fb64609256b5612.html","title":"Proceedings of the Fourth Workshop on Very Large Corpora: 4 August 1996, University of Copenhagen, Copenhagen, Denmark","url":"http://scholar.google.com/scholar?cluster=5478008953305989381&hl=en&oi=scholarr","authors":["Eva Ejerhed","Ido Dagan"],"date":"1996"},{"id":"c8fe48dfb2d539788cc6542c574a26dd.html","title":"Syntax and lexical statistics in anaphora resolution","url":"https://www.tandfonline.com/doi/abs/10.1080/08839519508945492","authors":["Ido Dagan","John Justeson","Shalom Lappin","Herbert Leass","Amnon Ribak"],"date":"1995/11/01","journal":"Applied Artificial Intelligence an International Journal","abstract":"We describe a syntactically based salience algorithm for pronominal anaphora resolution and a procedure for reevaluating the decisions of the algorithm on the basis of statistically modeled lexical semantic/pragmatic preferences. We report the results of an extensive blind test of both systems on computer manual text. We discuss the implications of these results for the comparative roles of syntactically defined salience and statistically measured lexical preference in determining the references of pronouns in text."},{"id":"2f2a7fdec0ea4023eabcd7505fc946f8.html","title":"Sample selection in natural language learning","url":"https://link.springer.com/chapter/10.1007/3-540-60925-3_50","authors":["Sean P Engelson","Ido Dagan"],"date":"1995/08/20","abstract":"Many corpus-based methods for natural language processing are based on supervised training, requiring expensive manual annotation of training corpora. This paper investigates reducing annotation cost by <i>sample selection</i>. In this approach, the learner examines many unlabeled examples and selects for labeling only those that are most informative at each stage of training. In this way it is possible to avoid redundantly annotating examples that contribute little new information. The paper first analyzes the issues that need to be addressed when constructing a sample selection algorithm, arguing for the attractiveness of <i>committee-based</i> selection methods. We then focus on selection for training probabilistic classifiers, which are commonly applied to problems in statistical natural language processing. We report experimental results of applying a specific type of committee-based selection during training of a\\\\xa0\u2026"},{"id":"95d6afb64f31ec4e3663f9dad7e778e3.html","title":"Knowledge Discovery in Textual Databases (KDT).","url":"https://www.aaai.org/Papers/KDD/1995/KDD95-012.pdf","authors":["Ronen Feldman","Ido Dagan"],"date":"1995/08/20","journal":"KDD","abstract":"The information age is characterized by a rapid growth in the amount of information available in electronic media. Traditional data handling methods are not adequate to cope with this information flood. Knowledge Discovery in Databases (KDD) is a new paradigm that focuses on computerized exploration of large amounts of data and on discovery of relevant and interesting patterns within them. While most work on KDD is concerned with structured databases, it is clear that this paradigm is required for handling the huge amount of information that is available only in unstructured textual form. To apply traditional KDD on texts it is necessary to impose some structure on the data that would be rich enough to allow for interesting KDD operations. On the other hand, we have to consider the severe limitations of current text processing technology and define rather simple structures that can be extracted from texts fairly automatically and in a reasonable cost. We propose using a text categorization paradigm to annotate text articles with meaningful concepts that are organized in hierarchical structure. We suggest that this relatively simple annotation is rich enough to provide the basis for a KDD framework, enabling data summarization, exploration of interesting patterns, and trend analysis. This research combines the KDD and text categorization paradigms and suggests advances to the state of the art in both areas."},{"id":"aa590dd151ce35f0ce299459ce113836.html","title":"Knowledge discovery in textual database (KDT)","url":"http://scholar.google.com/scholar?cluster=18353641162458083912&hl=en&oi=scholarr","authors":["Ronen Feldman","Ido Dagan"],"date":"1995/08","journal":"Proceedings of the first ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"},{"id":"9c85dc0bff9d5bb3443b442c3f59b850.html","title":"Committee-based sampling for training probabilistic classifiers","url":"https://www.sciencedirect.com/science/article/pii/B978155860377650027X","authors":["Ido Dagan","Sean P Engelson"],"date":"1995/01/1","abstract":"<div><div class=\\"gsh_csp\\">In many real-world learning tasks, it is expensive to acquire a sufficient number of labeled examples for training. This paper proposes a general method for efficiently training probabilistic classifiers, by selecting for training only the more informative examples in a stream of unlabeled examples. The method, <i>committee-based sampling</i>, evaluates the informativeness of an example by measuring the degree of disagreement between several model variants. These variants (the committee) are drawn randomly from a probability distribution conditioned by the training set selected so far (Monte-Carlo sampling). The method is particularly attractive because it evaluates the expected information gain from a training example implicitly, making the model both easy to implement and generally applicable."},{"id":"cf2ba650e9ce914bb94fd4eed5240407.html","title":"Selective sampling in natural language learning","url":"https://www.aaai.org/Papers/BISFAI/1995/BISFAI95-006.pdf","authors":["Ido Dagan","Sean P Engelson"],"date":"1995","journal":"Proceedings of the IJCAI Workshop on New Approaches to Learning for Natural Language Processing","abstract":"Many corpus-based methods for natural language processing are based on supervised training, requiring expensive manual annotation of training corpora. This paper investigates reducing annotation cost by selective sampling. In this approach, the learner examines many unlabeled examples and selects for labeling only those that are most informative at each stage of training. In this way it is possible to avoid redundantly annotating examples that contribute little new information. The paper first analyzes the issues that need to be addressed when constructing a selective sampling algorithm, arguing for the attractiveness of committee-based sampling methods. We then focus on selective sampling for training probabilistic classifiers, which are commonly applied to problems in statistical natural language processing. We report experimental results of applying a specific type of committee-based sampling during\\\\xa0\u2026"},{"id":"64d3c6cd50efa419e523ab419851fc4e.html","title":"Termight: Identifying and translating technical terminology","url":"https://www.aclweb.org/anthology/A94-1006.pdf","authors":["Ido Dagan","Kenneth Church"],"date":"1994/10","abstract":"We propose a semi-automatic tool, termight, that helps professional translators and terminologists identify technical terms and their translations. The tool makes use of part-of-speech tagging and wordalignment programs to extract candidate terms and their translations. Although the extraction programs are far from perfect, it isn\'t too hard for the user to filter out the wheat from the chaff. The extraction algorithms emphasize completeness. Alternative proposals are likely to miss important but infrequent terms/translations. To reduce the burden on the user during the filtering phase, candidates are presented in a convenient order, along with some useful concordance evidence, in an interface that is designed to minimize keystrokes. Termight is currently being used by the translators at AT\u2022 T Business Translation Services (formerly AT&T Language Line Services)."},{"id":"da844f8c1d91e2c59a12497cfa4bc525.html","title":"Similarity-based estimation of word cooccurrence probabilities","url":"https://arxiv.org/abs/cmp-lg/9405001","authors":["Ido Dagan","Fernando Pereira","Lillian Lee"],"date":"1994/05/02","journal":"arXiv preprint cmp-lg/9405001","abstract":"In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations``eat a peach\'\'and``eat a beach\'\'is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on``most similar\'\'words. We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz\'s back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error."},{"id":"1e44682b6fc01c7b4520d65b96e79101.html","title":"Word sense disambiguation using a second language monolingual corpus","url":"https://dl.acm.org/doi/abs/10.5555/203987.203991","authors":["Ido Dagan","Alon Itai"],"date":"1994","journal":"Computational linguistics","abstract":"This paper presents a new approach for resolving lexical ambiguities in one language using statistical data from a monolingual corpus of another language. This approach exploits the differences between mappings of words to senses in different languages. The paper concentrates on the problem of target word selection in machine translation, for which the approach is directly applicable. The presented algorithm identifies syntactic relations between words, using a source language parser, and maps the alternative interpretations of these relations to the target language, using a bilingual lexicon. The preferred senses are then selected according to statistics on lexical relations in the target language. The selection is based on a statistical model and on a constraint propagation algorithm, which simultaneously handles all ambiguities in the sentence. The method was evaluated using three sets of Hebrew and German\\\\xa0\u2026"},{"id":"c7546971458b6cacf6f7ec987af945ba.html","title":"Aligning parallel texts: Do methods developed for English-French generalize to Asian languages","url":"https://pdfs.semanticscholar.org/8d02/1434c8da92634a49cead486826789af8ffff.pdf","authors":["Kenneth Church","Ido Dagan","William Gale","Pascale Fung","Jon Helfman","Bala Satish"],"date":"1993/08/10","journal":"Proceedings of the Pacific Asia Conference on Formal and Computational Linguistics","abstract":"Parallel texts have recently received considerable attention in machine translation (eg, Brown et al, 1990), bilingual lexicography (eg, Klavans and Tzoukermann, 1990), and terminology research for human translators (eg, Isabelle, 1992). We have been most interested in the terminology application. How would Microsoft, or some other software vendor, want the term\\" dialog box\\" to be translated in their manuals? Technical terms such as\\" dialog box\\" are difficult for translators because they are generally not as familiar with the subject domain as either the author of the source, text or the reader of the target text. Parallel texts could be used to help translators overcome their lack of domain expertise by providing them with the ability to search previously translated documents for examples of potentially difficult terminology and see how they were translated in the past.\\" Existing translations contain more solutions to more translation problems than any other existing resource.\\"(Isabelle, 1992)"},{"id":"c099d29175a9e7637b61eb5e85ccc637.html","title":"Contextual word similarity and estimation from sparse data","url":"https://www.aclweb.org/anthology/P93-1022.pdf","authors":["Ido Dagan","Shaul Marcus","Shaul Markovitch"],"date":"1993/06","abstract":"In recent years there is much interest in word cooccurrence relations, such as n-grams, verbobject combinations, or cooccurrence within a limited context. This paper discusses how to estimate the probability of cooccurrences that do not occur in the training data. We present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words, as determined by an appropriate word similarity metric. Our evaluation suggests that this method performs better than existing smoothing methods, and may provide an alternative to class based models."},{"id":"51839a106221f5f871fd99fa113a6d08.html","title":"Robust bilingual word alignment for machine aided translation","url":"https://www.aclweb.org/anthology/W93-0301.pdf","authors":["Ido Dagan","Kenneth Church","Willian Gale"],"date":"1993","abstract":"We have developed a new program called word_align for aligning parallel text, text such as the Canadian Hansards that are available in two or more languages. The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown el al.\'s Model 2 (Brown et al., 1993), modified and extended to deal with robustness issues. Word_align was tested on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology."},{"id":"64c7352c2d13671309ccf313e199c8dd.html","title":"A Combined Symbolic-empirical Approach for the Automatic Translation of Compounds","url":"http://scholar.google.com/scholar?cluster=1034527605485457882&hl=en&oi=scholarr","authors":["Ulrike Rackow","Ido Dagan","Ulrike Schwall"],"date":"1992"},{"id":"c764559b57dc36e3a96389360c949e64.html","title":"Automatic translation of noun compounds","url":"https://www.aclweb.org/anthology/C92-4201.pdf","authors":["Ulrike Rackow","Ido Dagan","Ulrike Schwall"],"date":"1992","abstract":"It is widely known that the word formation mechanism of compounding is highly productive, in Ger man as well as in English, and that efficient strategies have to bc, lcvelopcd to dcal with this linguistic phe nomenon in any kind of NI, 1 system. Although this fact is generally agreed upon and a lot of linguistic rc search has been it, me, it has not bccn possible so fat to, levelop a general and overall pro,: cdure to solve the probh: m in a satisfactory aud ade, lnatc way ((: f."},{"id":"cb9605429ecfb914b29ffa4026d66d36.html","title":"Multilingual statistical approaches for natural language disambiguation","url":"http://scholar.google.com/scholar?cluster=14432641086509145325&hl=en&oi=scholarr","authors":["Ido Dagan"],"date":"1992","journal":"Doctoral dissertation, Israel Institute of Technology, Haifa, Israel"},{"id":"6926933dde62e104e653b3f9f889841f.html","title":"A set expression based inheritance system","url":"https://link.springer.com/article/10.1007/BF01531060","authors":["Ido Dagan","Alon Itai"],"date":"1991/09/01","journal":"Annals of Mathematics and Artificial Intelligence","abstract":"This paper describes a new formalism for inheritance systems, based on the formal semantics of set expressions. Using the formalism, it is possible to define new semantic classes by arbitrary set expressions operating on previously defined classes. Thus generalizing both<i>IS-A</i> links and<i>IS-NOT-A</i> links and adding the set intersection operation. We present an efficient algorithm which follows these definitions to deduce the properties implied by the inheritance network, i.e., the properties of the classes containing a given element. The application which motivated the development of the formalism, namely semantic disambiguation of natural language, is also described."},{"id":"74b1418cbd702ad840be60af010be171.html","title":"A statistical filter for resolving pronoun references","url":"http://books.google.com/books?hl=en&lr=&id=NVltkYpCcrYC&oi=fnd&pg=PA125&dq=info:8-Nv59vv7CUJ:scholar.google.com&ots=GctFbMffof&sig=q1K3JztnvBRz7C4g3hFdR8e9NM0","authors":["YA Feldman","A Bruckstein"],"date":"1991/06/17","journal":"Artificial Intelligence and Computer Vision","abstract":"Current methods for resolving pronoun references assume the existence of selectional constraints to \ufb01lter out semantically inappropriate referents. However, such constraints are usually not available for broad domains, because of the need to acquire a huge amount of information. This work suggests to use statistical data on cooccurrence patterns instead of selec-tional constraints. These patterns were collected automatically from large corpora and were used to \ufb01lter out implausible candidates. The method was combined with Hobbs\u2019 algorithm to resolve references of the pronoun \u201cit\u201d. The results of an initial experiment are presented, and show that the addition of the statistical data to the disambiguation mechanism indeed improves its performance."},{"id":"b37dbe147579d39415eff857e43ce94b.html","title":"Two languages are more informative than one","url":"https://www.aclweb.org/anthology/P91-1017.pdf","authors":["Ido Dagan","Alon Itai","Ulrike Schwall"],"date":"1991/06","abstract":"This paper presents a new approach for resolving lexical ambiguities in one language using statistical data on lexical relations in another language. This approach exploits the differences between mappings of words to senses in different languages. We concentrate on the problem of target word selection in machine translation, for which the approach is directly applicable, and employ a statistical model for the selection mechanism. The model was evaluated using two sets of Hebrew and German examples and was found to be very useful for disambiguation."},{"id":"fb8d9cd9d844dd65ca69939b3fcbd6cc.html","title":"Lexical disambiguation: sources of information and their statistical realization","url":"https://www.aclweb.org/anthology/P91-1048.pdf","authors":["Ido Dagan"],"date":"1991/06","abstract":"Lexieal disambiguation can be achieved using different sources of information. Aiming at high performance of automatic disambiguation it is important to know the relative importance and applicability of the various sources. In this paper we classify several sources of information and show how some of them can be achieved using statistical data. First evaluations indicate the extreme importance of local information, which mainly represents lexical associations and seleetional restrictions for syntactically related words."},{"id":"1eed31525b16aa798617227c1b5d0f8e.html","title":"Automatic processing of large corpora for the resolution of anaphora references","url":"https://www.aclweb.org/anthology/C90-3063.pdf","authors":["Ido Dagan","Alon Itai"],"date":"1990","abstract":"Manual acquisition of semantic constraints in broad domains is very expensive. This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus. To a large extent, these statistics reflect, semantic constraints and thus are used to disambiguate anaphora references and syntactic ambiguities. The scherne was implemented by gathering statistics on the output of other linguistic tools. An experiment was performed to resolve references of the pronoun\\" it\\" in sentences that were randomly selected from the corpus. Ttle results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis {\'or a useful disambiguat. ion tool."},{"id":"fea291ae6775f7f3a0215c00351a417b.html","title":"Automatic acquisition of constraints for the resolution of anaphora references and syntactic ambiguities","url":"http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/1990/CS/CS0626.pdf","authors":["Ido Dagan","Alon Itai"],"date":"1990","abstract":"Manual acquisition of semantic constraints in broad domains is very expensive. This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus. To a large extent, these statistics reflect semantic constraints and ihus are used to disambiguate anaphora references and syntactic ambiguities. The scheme was implemented by gathering statistics on the output of other linguistic tools. An experiment was performed to resolve references of the pronoun\\" it\\" in sentences that were randomly selected from the corpus. The results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis for a useful disambiguation tool."},{"id":"4807d52cbaea0716b5fb956c677403ac.html","title":"Semantic disambiguation in MT: A prototype and its consequences","url":"http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-info.cgi/1989/CS/CS0548","authors":["Ido Dagan","Alon Itai"],"date":"1989","abstract":"This paper describes a program and a fonnal model for disambiguation in machine translation. The model uses semantic constraints and was implemented for English-to-Hebrew translation. It solves in a unified way both syntactic and lexical (target word selection) ambiguities. The model reduces the redundancy in the representation of semantic constraints and thus reduces the overall size of the semantic database as well as facilitating its acquisition. Our model implies that the same constraints may be used for translation in both directions. Using this property we suggest a scheme for the automatic acquisition of the semantic database using corpora from both languages."},{"id":"09ae66e278dedb19bec2afe87e068c54.html","title":"Cited by 179","url":"https://scholar.google.co.il/scholar?oi=bibs&hl=en&oe=ASCII&cites=1143417000908024923&as_sdt=5","authors":["Ron Yair Pinter ","Ido Dagan","Martin Charles Golumbic"],"date":"1988","journal":"Discrete Applied Mathematics"},{"id":"06a5017b08491d0db7b848ef357908e8.html","title":"Natural Language Knowledge Graphs","url":"https://www.tsdconference.org/tsd2016/video/1609_TSD_NLKG.pdf","authors":["Ido Dagan"],"abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Scholar articles"},{"id":"106125728a2b5cee0dcf985bc56dd697.html","title":"19. CONTEXTUAL WORD SIMILARITY","url":"https://gramatica.usc.es/~gamallo/aulas/MasterLexicografia/biblio/Dagan_similarity.pdf","authors":["Ido Dagan"],"abstract":"Identifying different types of similarities between words has been an important goal in Natural Language Processing (NLP). This chapter describes the basic statistical approach for computing the degree of similarity between words. In this approach a word is represented by a word co-occurrence vector in which each entry corresponds to another word in the lexicon. The value of an entry specifies the frequency of joint occurrence of the two words in the corpus, that is, the frequency in which they co-occur within some particular relationships in the text. The degree of similarity between a pair of words is then computed by some similarity or distance measure that is applied to the corresponding pairs of vectors. This chapter describes in detail different types of lexical relationships that can be used for constructing word cooccurrence vectors. It then defines a schematic form for vector-based similarity measures and describes concrete measures that correspond to the general form. We also give examples for word similarities identified by corpus-based measures, along with \u201cexplanations\u201d of the major data elements that entailed the observed similarity. This chapter describes in detail the basic vector-based approach for computing word similarity. Similarities may be computed between different lexical units, such as word strings, word lemmas, and multi-word terms or phrases. We shall use the term \u201cword\u201d to denote a lexical unit but the discussion applies to other units as well."},{"id":"1e29264bba1829756f9f0d020c9d77c2.html","title":"EXploring Customer Interactions through Textual EntailMENT","url":"https://pdfs.semanticscholar.org/03a7/28dc1876f503419a6b2a42e68253ce773346.pdf","authors":["Ido Dagan","Bernardo Magnini","Guenter Neumann","Sebastian Pado"],"abstract":"EXCITEMENT is a 3-year research project (1/2012-12/2014) funded by the European Commission under FP7. The project consortium includes NICE Systems LTD (Israel) as coordinator, four academic partners, University of Bar Ilan (Israel), DFKI (Germany), FBK (Italy), University of Heidelberg (Germany), and two industrial partners, Almawave SRL (Italy) and OMQ GmbH (Germany). The main topic of the project is identifying semantic inferences between text units, a major language processing task, needed in practically all text understanding applications. While such inferences are broadly needed, there are currently no generic semantic engines or platforms for broad textual inference. The primary scientific motivation for the EXCITEMENT project is to change this ineffective state of affairs and to offer an encompassing open source platform for textual inference. On the industrial side, EXCITEMENT is focused on the text analytics market and follows the increasing demand for automatically analyzing customer interactions, which today cross multiple channels including speech, email, chat and social media."},{"id":"272083e25f6dfd114b07c87388d0a7cc.html","title":"Supporting Material for the Paper: Rich Parameterization Improves RNA Structure Prediction","url":"https://pdfs.semanticscholar.org/ff40/06c112812cef905889c00a55830843452896.pdf","authors":["Shay Zakov","Yoav Goldberg","Michael Elhadad","Michal Ziv-Ukelson"],"abstract":"We specify here the features which are used by our models. Each feature description is composed of two parts: a description of a structural element, and a (possibly empty) description of a sequential context. All models discussed in the paper are obtained by combining a set of structural elements St with a set of sequential contexts Co, and producing all corresponding features (ie producing a feature for each structural element in St and a corresponding sequential context from Co). In Section 1 we define the different loop types, which are used for categorizing structural elements. In Section 2 we give the three sets of structural elements Stbase, Stmed, and Sthigh, and in Section 3 we give the three sets of sequential contexts Cobase, Comed, and Cohigh, which are used in our models. All examples in the text refer to the sequence-folding (x, y) depicted in Fig. 1."},{"id":"2b29ba0a705f1d8f7105b6f46b129826.html","title":"Cited by 3","url":"https://scholar.google.co.il/scholar?oi=bibs&hl=en&oe=ASCII&cites=3365042071955341514&as_sdt=5","authors":["Micah Shlain","Hillel Taub-Tabib","Shoval Sadde","Yoav Goldberg"]},{"id":"3019b98e0410cebf419f065ee1657a0e.html","title":"Bringing Textual Descriptions to Life: Semantic Parsing of Requirements Documents into Executable Scenarios","url":"https://www.openu.ac.il/iscol2015/downloads/ISCOL2015_submission6_c_2.pdf","authors":["Ilia Pogrebezky","Smadar Szekely","Reut Tsarfaty","David Harel"],"abstract":"We present an end-to-end framework for translating natural language (NL) requirements into executable systems. Specifically, we implement semantic parsers for two sorts of input:(i) requirements documents written in a controlled natural language (CNL), and (ii) requirements documents written in NL. For each requirements document, we output the system model (SM) architecture along with a set of live sequence charts (LSCs) that capture the dynamic behavior of the specified system. Our parsers are embedded in PlayGo, a development environment for specifying (playing-in) requirements as LSCs and executing (playing-out) the resulting behavior. Thus, our output systems may be depicted visually, executed interactively, or provided as a standalone Java executable. PlayGo further allows for post-editing the predicted output via a friendly GUI, thus enabling the rapid development of parallel text: code data for statistical natural language programming."},{"id":"33305d7315d62c18a86d723794dc9843.html","title":"Generating Language from BlissSymbols Using Semantic Authoring","url":"https://www.cs.bgu.ac.il/~yoavg/publications/Issac2006bliss.doc","authors":["Yael Netzer","Ofer Biller","Michael Elhadad","Yoav Goldberg"],"abstract":"Objectives"},{"id":"3d7489853315475323bbc3d69d762a7e.html","title":"Full Access","url":"https://dl.acm.org/doi/abs/10.5555/1868771","authors":["Djam\\\\xc3\\\\xa9 Seddah","Sandra K\\\\xc3\\\\xbcbler","Reut Tsarfaty"],"abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_oci_field\\">Scholar articles"},{"id":"42ac00538bfea6bb9d85b5ca4809b653.html","title":"Effective Paraphrase Expansion in Addressing Lexical Variability","url":"https://fruct.org/publications/abstract-AINL-FRUCT-2016/files/Kon.pdf","authors":["Vasily Konovalov","Meni Adler","Ido Dagan"],"abstract":"In this paper, we investigate the contribution of automatically generated translation-based paraphrases to address lexical variability with application in dialogue systems. We compare the proposed methods with the state of the art approach. Furthermore, we define the desired criteria for the pivot language to generate the paraphrases, and find that the performance of paraphrase expansion correlates well with the averaged smoothed BLEU measure. The results suggest that:(1) The paraphrase expansion leads to better performance than the bag of words baseline.(2) The gain in performance often comes from a few most effective pivot languages.(3) The differences between machine translation engines are not reflected in empirical evaluation. By using the most effective pivot languages we can save the expenses to generate additional paraphrases, and as a result, to save the resources to train a classification model."},{"id":"437c48ba73d2f1c4d7b0dfb2a50eba23.html","title":"Evaluating recurrent neural networks as cognitive models of syntax","url":"http://tallinzen.net/media/papers/linzen_et_al_rnn_agreement.pdf","authors":["Tal Linzen","Emile Enguehard","Yoav Goldberg","Emmanuel Dupoux"],"abstract":"We analyze the ability of modern recurrent neural networks (RNNs) to learn subject-verb number agreement, a phenomenon considered to require a representation of the structure of the sentence; our goal was to assess the potential of RNNs to serve as cognitive models of syntax. Given explicit supervision, the network made few errors overall, but error rates increased in more complex sentences, and the pattern of errors was qualitatively different than in human experiments. Multi-task learning narrowed the gap between RNNs and humans. In summary, RNNs may be able to develop structural representations, but require strong supervision to do so."},{"id":"5c7779b7da21cd0798143533511b1137.html","title":"Contextual Preferences for Name-based Text Categorization","url":"http://scholar.google.com/scholar?cluster=6265193869646916067&hl=en&oi=scholarr","authors":["Shachar Mirkin","Ido Dagan","Lili Kotlerman","Idan Szpektor"],"abstract":"Name-based Text Categorization (TC) is an unsupervised setting of TC where the only input provided is the category name. Prior work suggested considering for categorization only candidate documents that contain terms that directly entail the category name. A major issue with this method\u2013the ambiguity in entailing terms\u2013was addressed by matching between vector-space representation of contexts for the rule and the tested document. We propose a novel context model for identifying invalid rule application, which utilizes classifiers for context representation. We suggest a general scheme that may be utilized in any inference-based task, and demonstrate an implementation for TC based on selfsupervised classifiers. Our experiments on standard TC datasets show that our model outperforms state-of-the-art context models."},{"id":"5c7e0ce0e9a028be87ff2d5d523e0df2.html","title":"Acquiring lexical paraphrases from a single corpus 1 Index of Subjects and Terms 11","url":"https://u.cs.biu.ac.il/~nlp/downloads/publications/ranlp03-book.pdf","authors":["Oren Glickman","Ido Dagan"],"abstract":"This paper studies the potential of extracting lexical paraphrases from a single corpus, focusing on the extraction of verb paraphrases. Most previous approaches detect individual paraphrase instances within a pair (or set) of comparable corpora, each of them containing roughly the same information, and rely on the substantial level of correspondence of such corpora. We present a novel method that successfully detects isolated paraphrase instances within a single corpus without relying on any a-priori structure and information."},{"id":"5e32e7324862cabf64b51a0159fcdf5b.html","title":"19.1. 1. Applications of word similarity","url":"http://u.cs.biu.ac.il/~nlp/downloads/publications/contextual_word_similarity.doc","authors":["Ido Dagan"],"abstract":"The concept of word similarity was traditionally captured within thesauri. A thesaurus is a lexicographic resource that specifies semantic relationships between words, listing for each word related words such as synonyms, hyponyms and hypernyms. Thesauri have been used to assist writers in selecting appropriate words and terms and in enriching the vocabulary of a text. To this end, modern word processors provide a thesaurus as a built in tool."},{"id":"62ebf0cdd1d86e1f0dfbad1f84ea27cd.html","title":"Evaluating Relation Inference via Question Answering Supplementary Material","url":"http://scholar.google.com/scholar?cluster=10835016893165936010&hl=en&oi=scholarr","authors":["Omer Levy","Ido Dagan","Extracting Open IE Assertions"],"abstract":"We extracted over 63 million unique subjectrelation-object triplets from Google\u2019s Syntactic N-grams (Goldberg and Orwant, 2013), including the number of times each one appeared in the corpus. This collection represents over 1.5 billion distinct appearances. The assertions may include multiword phrases as relations or arguments, for example:(chocolate, is made from, the cocoa bean): 22"},{"id":"6907fcfb38f94e6dee6dde5c78874822.html","title":"Findings of the First WMT Shared Task on Sign Language Translation","url":"https://www.statmt.org/wmt22/pdf/2022.wmt-1.71.pdf","authors":["Mathias M\\\\xc3\\\\xbcller","Sarah Ebling","Eleftherios Avramidis","Alessia Battisti","Mich\\\\xc3\\\\xa8le Berger","HfH Zurich","Richard Bowden","Annelies Braffort","Necati Cihan Camg\\\\xc3\\\\xb6z","Cristina Espa\\\\xc3\\\\xb1a-Bonet","Roman Grundkiewicz","Zifan Jiang","Oscar Koller","Amit Moryossef","Regula Perrollaz","Sabine Reinhard","Annette Rios","Dimitar Shterionov","Sandra Sidler-Miserez","Katja Tissi","Davy Van Landuyt"],"abstract":"This paper presents the results of the First WMT Shared Task on Sign Language Translation (WMT-SLT22) 1. This shared task is concerned with automatic translation between signed and spoken 2 languages. The task is novel in the sense that it requires processing visual information (such as video frames or human pose estimation) beyond the wellknown paradigm of text-to-text machine translation (MT). The task featured two tracks, translating from Swiss German Sign Language (DSGS) to German and vice versa. Seven teams participated in this first edition of the task, all submitting to the DSGS-to-German track. Besides a system ranking and system papers describing state-of-the-art techniques, this shared task makes the following scientific contributions: novel corpora, reproducible baseline systems and new protocols and software for human evaluation. Finally, the task also resulted in the first publicly available set of system outputs and human evaluation scores for sign language translation."},{"id":"70d48ea580d8e75ea3821f1cd052fdff.html","title":"Dialog Natural Language Understanding using a Generic Textual Inference System","url":"http://events.eventact.com/afeka/aclp2012/Dialogue%20Natural%20Language%20Understanding_Segal-halevi%20et%20al.pdf","authors":["Erel Segal-haLevi","Ido Dagan"],"abstract":"One of the components of a dialog system is the Natural Language Understanding (NLU) component. This component accepts natural language text, and returns the meaning of that text, in some formal application-specific meaning representation. One of the difficulties in building NLU components is the variability in natural language-the many different ways by which a human can express the same meaning. We propose to tackle this difficulty by using a generic Textual Entailment (TE) system-a system that can calculate, for each pair of texts, whether the meaning of one of them can be inferred from the other. A single TE system can be used for various NLU components in various domains."},{"id":"7386c9bc4926e78c16ccef0a03ca472d.html","title":"Publications of last four years","url":"http://www.coli.uni-saarland.de/~hansu/bio.pdf","authors":["Georg Rehm","Hans Uszkoreit","Ido Dagan","Vartkes Goetcherian","Mehmet Ugur Dogan","Coskun Mermer","Tam\\\\xe1s V\\\\xe1radi","Sabine Kirchmeier-Andersen","Gerhard Stickel","Meirion Prys Jones","Stefan Oeter","Sigve Gramstad"],"abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Scholar articles"},{"id":"74e3d00a3965638acadf295ef50e9363.html","title":"Razvan Bunescu (Ohio University, USA) Evgeniy Gabrilovich (Yahoo! Research, USA)","url":"https://www.aaai.org/Papers/Workshops/2008/WS-08-15/WS08-15-014.pdf","authors":["Rada Mihalcea","Eugene Agichtein","Einat Amitay","Mikhail Bilenko","Chris Brew","Timothy Chklovski","Massimiliano Ciaramita","Andras Csomai","Silviu Cucerzan","Ido Dagan","Ravi Kumar","Oren Kurland","Lillian Lee","Elizabeth Liddy","Daniel Marcu","Shaul Markovitch","Raymond Mooney","Vivi Nastase","Bo Pang","Marius Pasca","Ted Pedersen","Simone Paolo Ponzetto","Dragomir Radev","Dan Roth","Peter Turney"],"abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Scholar articles"},{"id":"7a1224ccfe886c2efe568e3105d2cbaf.html","title":"Agreement Matters: Challenges of Translating into a Morphologically Rich Language, and the Advantages of a Syntax-Based System","url":"http://cl.haifa.ac.il/MT/abstracts/goldberg.pdf","authors":["Yoav Goldberg"],"abstract":"Consider the following (simple) English sentences:\u201cI drive a car.\u201d,\u201cI don\u2019t know how to drive\u201d,\u201cI wash the car\u201d,\u201cI wash the floor\u201d. Translating them to Hebrew using Google\u2019s statistical MT system, yields: \u043e \u0434 \u0432 \u0434 \u0434 (I drive (masculine) a car); \u0434\u0430 \u043e\u0436 \u0430 \u0434 (I don\u2019t know (feminine) how to drive); \u0439 \u043c \u0434 \u043e \u0434 \u0432 \u043e (I wash (masculine) the car); and \u0438\u043a\u043c \u043e \u043e\u0438 \u043d \u0434 (I wash (feminine) the floor). While amusing and not quite politically correct, these are all arguably very good translations: without explicit gender marking, the translator can not know if the speaker is masculine or feminine, and he (she?) resorts to deciding based on her (his?) cultural knowledge."},{"id":"83786244456cea14dac19af8426c49c8.html","title":"Transition-Based Morphological Disambiguation","url":"https://www.openu.ac.il/iscol2015/downloads/ISCOL2015_submission_31_d_1.pdf","authors":["Amir More","Reut Tsarfaty"],"abstract":"In Morphologically Rich Languages (MRLs), sentences are composed of ambiguous space-delimited tokens that ought to be disambiguated with respect to their constituent morphemes. Previous work on Morphological Disambiguation (MD) of MRLs has had variable success, with Semitic languages having sub-par results for downstream applications. Here we propose novel MD transition-based systems, both word-based and morphemebased, and tackle the challenge introduced by the variable length of hypothesized morpheme sequences. Our experiments show that transition-based morphemebased MD consistently outperforms the word-based variant, while providing new state of the art results on Hebrew MD."},{"id":"89414bf78ecc707ea93ae764e39ff979.html","title":"Remote Participants","url":"https://openreview.net/pdf?id=Ju3H1n2XHch#page=49","authors":["Timothy Baldwin","Verginica Barbu Mititelu","Emily M Bender","Archna Bhatia","IHMC Florida","US Bernd Bohnet","Francis Bond","TU Nanyang","SG Cem Bozsahin","TR Ankara","Ryan Cotterell","William Croft","US Alburquerque","Miryam de Lhoneux","Marie-Catherine de Marneffe","US Columbus","Jamie Findlay","Daniel Flickinger","Kim Gerdes","FR Orsay","Voula Giouli","Tunga Gungor","Jan Hajic","Dag Haug","Uxoa I\\\\xc3\\\\xb1urrieta","ES Donostia","Laura Kallmeyer","Christo Kirov","Maria Koptjevskaja Tamm","Artur Kulmizev","Lori Levin","Natalia Levshina","NL Psycholinguistics\\\\xe2\\\\x80\\\\x93Nijmegen","Teresa Lynn","Stella Markantonatou","Nurit Melnik","Paola Merlo","Yusuke Miyao","Kadri Muischnek","Joakim Nivre","Petya Osenova","BG Sofia","Stephen Pepper","James Pustejovsky","US Waltham","Alexandre Rademaker","Carlos Ramisch","Manfred Sailer","DE Main","Agata Savary","Emmanuel Schang","Nathan Schneider","Ivelina Stoyanova","Sara Stymne","Reut Tsarfaty","Francis M Tyers","Meagan Vigus","Aline Villavicencio","Veronika Vincze","Ekaterina Vylomova","Nianwen Xue","David Yarowsky","US Baltimore","Amir Zeldes","Daniel Zeman","Tim Zingler"],"journal":"Universals of Linguistic Idiosyncrasy in Multilingual Computational Linguistics","abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_oci_field\\">Scholar articles"},{"id":"aa0a10562bda6e66044ed770b1e29c62.html","title":"Supplementary Material-Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples","url":"http://proceedings.mlr.press/v80/weiss18a/weiss18a-supp.pdf","authors":["Gail Weiss","Yoav Goldberg","Eran Yahav"],"abstract":"The algorithm maintains an observation table (S, E, T) that records whether strings belong to U. In Algorithm 1, this table is represented by the two-dimensional array T, with dimensions| S|\\\\xd7| E|, where, informally, we can view S as a set of words that lead from the initial state to states of the hypothesized automaton, and E as a set of words serving as experiments to separate states. The table T itself maps a word w\u2208(S\u222a S\\\\xb7 \u03a3)\\\\xb7 E to True if w\u2208 U and False otherwise."},{"id":"ae4a99f691a8e74575a160aa824b1c13.html","title":"Index of Subjects and Terms 11","url":"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.295.2659&rep=rep1&type=pdf","authors":["Oren Glickman","Ido Dagan"],"abstract":"This paper studies the potential of extracting lexical paraphrases from a single corpus, focusing on the extraction of verb paraphrases. Most previous approaches detect individual paraphrase instances within a pair (or set) of comparable corpora, each of them containing roughly the same information, and rely on the substantial level of correspondence of such corpora. We present a novel method that successfully detects isolated paraphrase instances within a single corpus without relying on any a-priori structure and information."},{"id":"de643c74ed6d9c622c2d54d311623faf.html","title":"Learning to Exploit Structured Resources for Lexical Inference","url":"https://pdfs.semanticscholar.org/cee9/4959cf360f86587a7aea2106d29d525d6bc5.pdf","authors":["Vered Shwartz Omer Levy","Ido Dagan","Jacob Goldberger"],"abstract":"Massive knowledge resources, such as Wikidata, can provide valuable information for lexical inference, especially for proper-names. Prior resource-based approaches typically select the subset of each resource\u2019s relations which are relevant for a particular given task. The selection process is done manually, limiting these approaches to smaller resources such as WordNet, which lacks coverage of propernames and recent terminology. This paper presents a supervised framework for automatically selecting an optimized subset of resource relations for a given target inference task. Our approach enables the use of large-scale knowledge resources, thus providing a rich source of high-precision inferences over proper-names."},{"id":"bfa74531b8ebc7edf02b82719f6a33c2.html","title":"Be principled! A Probabilistic Model for Lexical Entailment","url":"https://www.cs.bgu.ac.il/~adlerm/iscol11/bisfai11_submission_4.pdf","authors":["Eyal Shnarch","Jacob Goldberger","Ido Dagan"],"abstract":"Recognizing lexical entailment is a prominent task addressed by most textual entailment systems. Nevertheless, it is approached mostly by heuristic methods which do not address major aspects of the lexical entailment scenario. We present, for the first time, a principled probabilistic approach for this task. Our model covers prominent aspects such as integrating various resources with different reliability, taking into account the length of the entailed sentence, the number of its covered terms and the existence of multiple evidence for the entailment of a term. Evaluations validate the impact of our model components and show that its performance is in line with the best published entailment systems."},{"id":"c5fd579e186300e4057f4743d25713ea.html","title":"Larraitz Uria, Viktor Varga, Veronika Vincze, Zdenek \u02c7Zabokrtsk\\\\xfd, Daniel Zeman, and Hanzhi Zhu. 2015","url":"http://scholar.google.com/scholar?cluster=1847268529356725851&hl=en&oi=scholarr","authors":["Joakim Nivre","Maria Jesus Aranzabe \u02c7Zeljko Agic","Masayuki Asahara","Aitziber Atutxa","Miguel Ballesteros","John Bauer","Kepa Bengoetxea","Riyaz Ahmad Bhat","Cristina Bosco","Sam Bowman","Giuseppe GA Celano","Miriam Connor","Marie-Catherine de Marneffe","Arantza Diaz de Ilarraza","Kaja Dobrovoljc","Timothy Dozat","Tomaz Erjavec","Rich\\\\xe1rd Farkas","Jennifer Foster","Daniel Galbraith","Filip Ginter","Iakes Goenaga","Koldo Gojenola","Yoav Goldberg","Berta Gonzales","Bruno Guillaume","Jan Hajic","Dag Haug","Radu Ion","Elena Irimia","Anders Johannsen","Hiroshi Kanayama","Jenna Kanerva","Simon Krek","Veronika Laippala","Alessandro Lenci","Nikola Ljube\u0161ic","Teresa Lynn","Christopher Manning","Catalina Maranduc","David Marecek","H\\\\xe9ctor Mart\u0131nez Alonso","Jan Ma\u0161ek","Yuji Matsumoto","Ryan McDonald","Anna Missil\\\\xe4","Verginica Mititelu","Yusuke Miyao","Simonetta Montemagni","Shunsuke Mori","Hanna Nurmi","Petya Osenova","Lilja \\\\xd8vrelid","Elena Pascual","Marco Passarotti","Cenel-Augusto Perez","Slav Petrov","Jussi Piitulainen","Barbara Plank","Martin Popel","Prokopis Prokopidis","Sampo Pyysalo","Rudolf Rosa Loganathan Ramasamy","Shadi Saleh","Sebastian Schuster","Wolfgang Seeker","Mojgan Seraji","Natalia Silveira","Maria Simi","Radu Simionescu","Katalin Simk\\\\xf3","Kiril Simov","Aaron Smith","Jan \u0160tep\\\\xe1nek","Alane Suhr","Zsolt Sz\\\\xe1nt\\\\xf3","Takaaki Tanaka","Reut Tsarfaty","Sumire Uematsu"],"journal":"Universal dependencies"},{"id":"d158e7a5a2044e54734a6b17a7222245.html","title":"Symposium on Foundations of Artificial Intelligence","url":"https://www.aaai.org/Papers/BISFAI/1995/BISFAI95-028.pdf","authors":["Mat\\\\xedas Alvarado","Ofer Arieli","Arnon Avron","Consistent Data from Inconsistent","A Knowledge-Bases","Chad Burkey","Charles B Callaway","Jacques Calmet","Valery Cherniaev","Ido Dagan","Jonathan Ginzburg","Dan Givoli","John Grant","Jaroslaw Gryz","Lawrence Henschen","Chuen-Hsuen Jeff Ho","Karsten Homann"],"abstract":"<div class=\\"gs_scl\\"><div class=\\"gsc_vcd_field\\">Scholar articles"},{"id":"d3c7f03e12e765196aa72e2f9f7044a0.html","title":"B-56: Modeling Word Meaning in Context with Substitute Vectors","url":"http://scholar.google.com/scholar?cluster=12103993865900108166&hl=en&oi=scholarr","authors":["Oren Melamud","Ido Dagan","Jacob Goldberger"]}]'),Vl={};function q8(n){try{return decodeURIComponent(n.replace(/\\x/g,"%"))}catch(e){return n.replace(/\\x([0-9A-Fa-f]{2,4})/g,function(){return String.fromCharCode(parseInt(arguments[1],16))})}}I1.forEach(n=>{const e=n.date?n.date.slice(0,4):"";e in Vl||(Vl[e]=[]),Vl[e].push(n),n.authors=n.authors.map(q8)});const A1=JSON.parse('{"PhD Student":[{"name":"Amit Moryossef","scholarId":"Aaj_RBEAAAAJ","startYear":2018,"advisors":["Yoav Goldberg"],"previousDegrees":["MSc Student"]},{"name":"Royi Rassin","startYear":2020,"advisors":["Yoav Goldberg"],"previousDegrees":["MSc Student"],"homepage":"https://royi-rassin.netlify.app/"},{"name":"Avi Caciularu","startYear":2019,"advisors":["Ido Dagan","Jacob Goldberger"],"homepage":"http://aviclu.github.io/"},{"name":"Mosh Levy","startYear":2020,"advisors":["Yoav Goldberg"],"image":false},{"name":"Omer Goldman","startYear":2020,"advisors":["Reut Tsarfaty"]},{"name":"Uri Goren","startYear":2019,"advisors":["Reut Tsarfaty"],"homepage":"http://www.goren.ml"},{"name":"Amir David Nissan Cohen","startYear":2019,"advisors":["Yoav Goldberg"],"endYear":2023},{"name":"Ori Shapira","startYear":2017,"advisors":["Ido Dagan","Yael Amsterdamer"],"previousDegrees":["MSc Student"],"homepage":"https://orishapira.wordpress.com/"},{"name":"Natalie Shapira","startYear":2018,"advisors":["Yoav Goldberg"],"homepage":"https://twitter.com/NatalieShapira"},{"name":"Tzuf Paz-Argaman","image":false,"startYear":2019,"advisors":["Reut Tsarfaty"],"endYear":2023},{"name":"Shauli Ravfogel","startYear":2020,"advisors":["Yoav Goldberg"],"endYear":2024,"previousDegrees":["MSc Student"],"homepage":"https://shauli-ravfogel.netlify.app/"},{"name":"Valentina Pyatkin","scholarId":"E9EgKkMAAAAJ","startYear":2019,"advisors":["Reut Tsarfaty","Ido Dagan"],"homepage":"https://valentinapy.github.io"},{"name":"Ori Ernst","startYear":2019,"advisors":["Ido Dagan","Jacob Goldberger"],"homepage":"https://oriern.netlify.app/"},{"name":"Ayal Klein","startYear":2019,"advisors":["Ido Dagan"],"endYear":2023,"previousDegrees":["MSc Student"],"interests":"Semantics, Semantic Representations, Psycholinguistics","homepage":"https://kleinay.github.io/"},{"name":"Yanai Elazar","scholarId":"7p_Ce8kAAAAJ","startYear":2016,"advisors":["Yoav Goldberg"],"endYear":2022,"previousDegrees":["MSc Student"],"homepage":"https://yanaiela.github.io/","interests":"Interpretability and Analysis, Commonsense Reasoning, Biases"},{"name":"Ohad Rozen","startYear":2018,"advisors":["Ido Dagan"],"endYear":2022,"interests":"Inference, Question answering"},{"name":"Moran Baruch","startYear":2019,"advisors":["Yoav Goldberg"],"endYear":2022},{"name":"Matan Ben Noach","image":false,"startYear":2020,"advisors":["Yoav Goldberg","Jacob Goldberger"],"endYear":2024,"previousDegrees":["MSc Student"]},{"name":"Alon Jacovi","startYear":2019,"advisors":["Yoav Goldberg"],"previousDegrees":["MSc Student"],"homepage":"https://alonjacovi.github.io/","interests":"XAI for NLP"},{"name":"Arie Cattan","startYear":2020,"advisors":["Ido Dagan"],"previousDegrees":["MSc Student"]},{"name":"Paul Roit","startYear":2018,"advisors":["Yoav Goldberg","Ido Dagan"],"endYear":2024,"previousDegrees":["MSc Student"]},{"name":"Eran Hirsch","startYear":"2022","advisors":["Ido Dagan"],"endYear":"2026"},{"name":"Alon Eirew","startYear":2022,"advisors":["Ido Dagan"],"previousDegrees":["MSc Student"],"homepage":"https://www.linkedin.com/in/aloneirew/"}],"MSc Student":[{"name":"Shachar Rosenman","startYear":2019,"advisors":["Yoav Goldberg"]},{"name":"Ruben Wolhandler","startYear":2021,"advisors":["Ido Dagan"],"homepage":"https://www.linkedin.com/in/ruben-wolhandler/"},{"name":"Shir Ashury Tahan","startYear":2022,"advisors":["Yoav Goldberg"]},{"name":"Alon Jacoby","startYear":2022,"advisors":["Yoav Goldberg"],"image":false},{"name":"Daniela Brook Weiss","aliases":["Daniela Stepanov"],"startYear":2018,"advisors":["Ido Dagan"],"previousDegrees":["MSc Student"]},{"name":"Eyal Orbach","startYear":2017,"advisors":["Yoav Goldberg"],"previousDegrees":["MSc Student"]},{"name":"Eviatar Nachshoni","startYear":2022,"advisors":["Ido Dagan"]},{"name":"Aryeh Tiktinsky","startYear":2018,"advisors":["Yoav Goldberg"]},{"name":"Ari Bornstein","aliases":["Aaron Bornstein"],"startYear":2017,"advisors":["Ido Dagan"]},{"name":"Aviv Weinstein","startYear":2019,"advisors":["Yoav Goldberg"],"endYear":2021,"interests":"Relation Extraction"},{"name":"David Guriel","startYear":2019,"advisors":["Reut Tsarfaty"],"endYear":2021,"interests":"Morphology, Reinflection, minimal supervision algorithms"},{"name":"Elad Ben Zaken","startYear":2021,"advisors":["Yoav Goldberg"],"homepage":"https://www.linkedin.com/in/elad-ben-zaken-4904a1127/","interests":"Machine Learning, Deep Learning & NLP"},{"name":"Amit Seker","startYear":2016,"advisors":["Reut Tsarfaty"]},{"name":"Eylon Gueta","startYear":2019,"advisors":["Reut Tsarfaty"]},{"name":"Itai Mondshine","startYear":2022,"advisors":["Reut Tsarfaty"]},{"name":"Refael Shaked Greenfeld","startYear":2020,"advisors":["Reut Tsarfaty"],"homepage":"https://www.linkedin.com/in/refael-shaked-greenfeld/","interests":"Deep Learning & NLP, Scuba Diving"},{"name":"Ben Hagag","startYear":2020,"advisors":["Reut Tsarfaty"],"endYear":2022,"previousDegrees":["MSc Student"]},{"name":"Dan Bareket","startYear":2018,"advisors":["Reut Tsarfaty"]},{"name":"Elron Bandel","startYear":2020,"advisors":["Yoav Goldberg","Yanai Elazar"],"homepage":"https://www.linkedin.com/in/elron/"},{"name":"Asaf Achi Mordechai","startYear":2019,"advisors":["Yoav Goldberg","Reut Tsarfaty"],"endYear":2021,"homepage":"https://github.com/asafam","interests":"NBA fantasy, sci-fi books, international soccer"},{"name":"Mor Peled","startYear":2019,"advisors":["Yoav Goldberg"],"interests":"Deep learning, Health care & Rock\'nroll"},{"name":"Shmuel Amar","startYear":2019,"advisors":["Ido Dagan"],"homepage":"https://github.com/shmuelamar","interests":"AI, ML, DL, NLP, Big Data"},{"name":"Leon Pesahov","startYear":null,"image":false,"advisors":["Ido Dagan"]}],"Alumni":[{"name":"Gabriel Stanovsky","startYear":2014,"advisors":["Ido Dagan"],"previousDegrees":["PhD Student"],"endYear":2018,"homepage":"https://gabrielstanovsky.github.io/","current":{"position":"Faculty","employer":"Hebrew University"}},{"name":"Roee Aharoni","startYear":2016,"advisors":["Yoav Goldberg"],"endYear":2020,"previousDegrees":["PhD Student"],"homepage":"https://www.roeeaharoni.com","current":{"position":"Research Scientist","employer":"Google"}},{"name":"Omer Levy","startYear":2012,"advisors":["Yoav Goldberg","Ido Dagan"],"endYear":2016,"previousDegrees":["PhD Student"],"homepage":"https://www.cs.tau.ac.il/~levyomer/","current":{"position":"Senior Lecturer","employer":"Tel Aviv University"}},{"name":"Karin Brisker","startYear":2017,"advisors":["Yoav Goldberg"],"endYear":2020,"previousDegrees":["MSc Student"],"current":{"position":"Data Scientist ","employer":"Microsoft, healthcare team"}},{"name":"Daniel Juravski","startYear":2018,"advisors":["Yoav Goldberg"],"endYear":2020,"previousDegrees":["MSc Student"]},{"name":"Ofer Sabo","startYear":2018,"advisors":["Yoav Goldberg","Ido Dagan"],"endYear":2020,"previousDegrees":["MSc Student"],"current":{"position":"NLP Researcher","employer":"Zebra Medical"}},{"name":"Vered Shwartz","startYear":2015,"advisors":["Ido Dagan"],"endYear":2019,"previousDegrees":["PhD Student"],"homepage":"https://vered1986.github.io","current":{"position":"Postdoctoral Researcher","employer":"Allen Institute for AI (AI2) & University of Washington"}},{"name":"Eliyahu Kiperwasser","startYear":2014,"advisors":["Yoav Goldberg"],"endYear":2019,"previousDegrees":["PhD Student"],"homepage":"http://elki.cc","current":{"position":"Senior research scientist","employer":"eBay"}},{"name":"Hila Gonen","scholarId":"URThmtMAAAAJ","startYear":2015,"advisors":["Yoav Goldberg"],"endYear":2020,"previousDegrees":["PhD Student"],"homepage":"https://u.cs.biu.ac.il/~gonenhi/","current":{"position":"Postdoc","employer":"Amazon"}},{"name":"Noa Lubin","startYear":2017,"advisors":["Yoav Goldberg"],"endYear":2019,"previousDegrees":["MSc Student"],"homepage":"https://www.linkedin.com/in/noalu/","current":{"position":"Senior Machine Learning Researcher","employer":"Diagnostic Robotics"}},{"name":"Lili Kotlerman","image":false,"startYear":2010,"advisors":["Ido Dagan"],"endYear":2016,"previousDegrees":["PhD Student"],"current":{"position":"Senior NLP researcher","employer":"Intuition Robotics"}},{"name":"Shany Barhom","startYear":2017,"advisors":["Ido Dagan"],"endYear":2020,"previousDegrees":["MSc Student"],"interests":"Coreference, discourse and representation learning","current":{"position":"Algorithm Researcher","employer":"BeyondMinds"}},{"name":"Oren Melamud","startYear":2012,"advisors":["Ido Dagan","Jacob Goldberger"],"endYear":2016,"previousDegrees":["PhD Student"],"homepage":"https://u.cs.biu.ac.il/~melamuo/","current":{"position":"Director of Data Science","employer":"Keywee Inc."}},{"name":"Zvika Marx","image":false,"startYear":null,"previousDegrees":["Ph.D"],"endYear":2005,"advisors":["Ido Dagan","Eli Shamir"]},{"name":"Oren Glickman","startYear":null,"previousDegrees":["Ph.D"],"endYear":2006,"advisors":["Ido Dagan","Moshe Koppel"]},{"name":"Maayan Gefet","image":false,"startYear":null,"previousDegrees":["Ph.D"],"endYear":2006,"advisors":["Ido Dagan","Dror Feitelson"]},{"name":"Roy Bar-Haim","image":false,"startYear":null,"previousDegrees":["Ph.D"],"endYear":2009,"advisors":["Ido Dagan"]},{"name":"Jonathan Berant","homepage":"http://www.cs.tau.ac.il/~joberant/","startYear":null,"previousDegrees":["Ph.D"],"endYear":2012,"advisors":["Ido Dagan"],"current":{"position":"Assistant Professor","employer":"Tel Aviv University"}},{"name":"Eyal Shnarch","image":false,"startYear":null,"previousDegrees":["Ph.D"],"endYear":2014,"advisors":["Ido Dagan","Jacob Goldberger"]},{"name":"Asher Stern","image":false,"homepage":"https://www.linkedin.com/in/asherstern/","startYear":null,"previousDegrees":["Ph.D"],"endYear":2015,"advisors":["Ido Dagan"],"current":{"position":"Applied Researcher","employer":"eBay"}},{"name":"Shlomit Hazan","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":1997,"advisors":["Ido Dagan","Ronen Feldman"]},{"name":"Erez Lotan","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":1998,"advisors":["Ido Dagan"]},{"name":"Alex Avramovitch","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":1998,"advisors":["Ido Dagan"]},{"name":"Shelly Katz","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":1998,"advisors":["Ido Dagan","Ariel Frank"]},{"name":"Roman Mitnitsky","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":1998,"advisors":["Ido Dagan"]},{"name":"Michal Finkelstein-Landau","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":1999,"advisors":["Ido Dagan"]},{"name":"Marina Risher","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2001,"advisors":["Ido Dagan"]},{"name":"Ehud Conley","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2002,"advisors":["Ido Dagan"]},{"name":"Idan Szpektor","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2005,"advisors":["Ido Dagan","Yossi Matias"]},{"name":"Shachar Mirkin","homepage":"https://sites.google.com/site/shacharmirkin","startYear":null,"previousDegrees":["M.Sc"],"endYear":2006,"advisors":["Ido Dagan","Ari Rappoport"],"current":{"position":"NLP Researcher","employer":"Digimind"}},{"name":"Moshe Friedman","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2006,"advisors":["Ido Dagan","Moshe Koppel"]},{"name":"Tal Itzhak Ron","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2006,"advisors":["Ido Dagan"]},{"name":"Efrat Hershkovitz","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2006,"advisors":["Ido Dagan"]},{"name":"Libby Berkovitch","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2008,"advisors":["Ido Dagan"]},{"name":"Ephi Sachs","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2008,"advisors":["Ido Dagan"]},{"name":"Chen Erez","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2009,"advisors":["Ido Dagan"]},{"name":"Chaya Liebeskind","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2009,"advisors":["Ido Dagan"],"current":{"position":"Lecturer","employer":"Machon Lev"}},{"name":"Roni Ben Aharon","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2010,"advisors":["Ido Dagan"]},{"name":"Hadas Zohar","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2011,"advisors":["Ido Dagan","Jonathan Schler"]},{"name":"Naomi Frankel","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2012,"advisors":["Ido Dagan","Meni Adler"]},{"name":"Amnon Lotan","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2012,"advisors":["Ido Dagan"]},{"name":"Hila Weisman","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2013,"advisors":["Ido Dagan","Idan Szpektor"]},{"name":"Ofer Bronstein","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2015,"advisors":["Ido Dagan"]},{"name":"Vasily Konovalov","image":false,"startYear":null,"previousDegrees":["M.Sc"],"endYear":2016,"advisors":["Ido Dagan"]},{"name":"Rachel wities","startYear":2004,"advisors":["Ido Dagan"],"endYear":2007,"previousDegrees":["MSc Student"],"current":{"position":"NLP Data Scientist","employer":"Zebra Medical Vision"}},{"name":"Yehudit Meged","startYear":2017,"previousDegrees":["M.Sc"],"endYear":2020,"advisors":["Ido Dagan"]},{"name":"Assaf Toledo","homepage":"https://www.linkedin.com/in/assaf-toledo-a3608b27/","startYear":null,"previousDegrees":["Post-doc"],"endYear":null,"current":{"position":"Research Staff Member","employer":"IBM Research"}}],"Employee":[{"name":"Victoria Basmov","startYear":2018,"advisors":["Reut Tsarfaty"]},{"name":"Yuval Krymolowski","startYear":1999,"advisors":["Ido Dagan"],"endYear":2006,"previousDegrees":["PhD Student"],"interests":"event discovery, parsing, lexical semantics","current":{"position":"Lab employee","employer":"BIU NLP Lab"}},{"name":"Shoval Sadde","startYear":2018,"advisors":["Reut Tsarfaty"],"endYear":2020,"current":{"position":"Computational Linguist","employer":"AI2"}}],"Faculty":[{"name":"Ido Dagan","scholarId":"YzGAGtoAAAAJ","homepage":"https://u.cs.biu.ac.il/~dagan/"},{"name":"Reut Tsarfaty","scholarId":"0Eome9MAAAAJ","homepage":"https://research.biu.ac.il/researcher/prof-reut-tsarfaty/"},{"name":"Yoav Goldberg","scholarId":"0rskDKgAAAAJ","homepage":"https://www.cs.bgu.ac.il/~yoavg/uni/"}]}'),ff={},R1=[];for(const n of Object.values(A1))for(const e of n){if(ff[e.name]=e,e.id=e.name.toLowerCase().replace(/ /g,"_"),e.imagePath="image"in e&&!e.image?"unknown":e.id,e.aliases)for(const t of e.aliases)ff[t]=e;R1.push(e)}const Y8=new E("cdk-dir-doc",{providedIn:"root",factory:function Q8(){return me(Q)}}),K8=/^(ar|ckb|dv|he|iw|fa|nqo|ps|sd|ug|ur|yi|.*[-_](Adlm|Arab|Hebr|Nkoo|Rohg|Thaa))(?!.*[-_](Latn|Cyrl)($|-|_))($|-|_)/i;let pf,P1=(()=>{class n{constructor(t){if(this.value="ltr",this.change=new it,t){const r=t.documentElement?t.documentElement.dir:null;this.value=function X8(n){const e=(null==n?void 0:n.toLowerCase())||"";return"auto"===e&&"undefined"!=typeof navigator&&(null==navigator?void 0:navigator.language)?K8.test(navigator.language)?"rtl":"ltr":"rtl"===e?"rtl":"ltr"}((t.body?t.body.dir:null)||r||"ltr")}}ngOnDestroy(){this.change.complete()}}return n.\u0275fac=function(t){return new(t||n)(v(Y8,8))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})(),So=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({}),n})();try{pf="undefined"!=typeof Intl&&Intl.v8BreakIterator}catch(n){pf=!1}let Co,Gi,gf,Pt=(()=>{class n{constructor(t){this._platformId=t,this.isBrowser=this._platformId?function lR(n){return n===W0}(this._platformId):"object"==typeof document&&!!document,this.EDGE=this.isBrowser&&/(edge)/i.test(navigator.userAgent),this.TRIDENT=this.isBrowser&&/(msie|trident)/i.test(navigator.userAgent),this.BLINK=this.isBrowser&&!(!window.chrome&&!pf)&&"undefined"!=typeof CSS&&!this.EDGE&&!this.TRIDENT,this.WEBKIT=this.isBrowser&&/AppleWebKit/i.test(navigator.userAgent)&&!this.BLINK&&!this.EDGE&&!this.TRIDENT,this.IOS=this.isBrowser&&/iPad|iPhone|iPod/.test(navigator.userAgent)&&!("MSStream"in window),this.FIREFOX=this.isBrowser&&/(firefox|minefield)/i.test(navigator.userAgent),this.ANDROID=this.isBrowser&&/android/i.test(navigator.userAgent)&&!this.TRIDENT,this.SAFARI=this.isBrowser&&/safari/i.test(navigator.userAgent)&&this.WEBKIT}}return n.\u0275fac=function(t){return new(t||n)(v(Xu))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();function Gl(n){return function J8(){if(null==Co&&"undefined"!=typeof window)try{window.addEventListener("test",null,Object.defineProperty({},"passive",{get:()=>Co=!0}))}finally{Co=Co||!1}return Co}()?n:!!n.capture}function Z8(){if(null==Gi){if("object"!=typeof document||!document||"function"!=typeof Element||!Element)return Gi=!1,Gi;if("scrollBehavior"in document.documentElement.style)Gi=!0;else{const n=Element.prototype.scrollTo;Gi=!!n&&!/\{\s*\[native code\]\s*\}/.test(n.toString())}}return Gi}function Ui(n){return n.composedPath?n.composedPath()[0]:n.target}function mf(){return"undefined"!=typeof __karma__&&!!__karma__||"undefined"!=typeof jasmine&&!!jasmine||"undefined"!=typeof jest&&!!jest||"undefined"!=typeof Mocha&&!!Mocha}function N1(n){return cn((e,t)=>n<=t)}function u5(n,e){return n===e}function dn(n){return xe((e,t)=>{ot(n).subscribe(ve(t,()=>t.complete(),wc)),!t.closed&&e.subscribe(t)})}function bf(n){return null!=n&&"false"!=`${n}`}function F1(n,e=0){return function h5(n){return!isNaN(parseFloat(n))&&!isNaN(Number(n))}(n)?Number(n):e}function $l(n){return Array.isArray(n)?n:[n]}function Be(n){return null==n?"":"string"==typeof n?n:`${n}px`}function ra(n){return n instanceof ht?n.nativeElement:n}let f5=(()=>{class n{create(t){return"undefined"==typeof MutationObserver?null:new MutationObserver(t)}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})(),p5=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({providers:[f5]}),n})();class g5 extends Je{constructor(e,t){super()}schedule(e,t=0){return this}}const ql={setInterval(n,e,...t){const{delegate:i}=ql;return null!=i&&i.setInterval?i.setInterval(n,e,...t):setInterval(n,e,...t)},clearInterval(n){const{delegate:e}=ql;return((null==e?void 0:e.clearInterval)||clearInterval)(n)},delegate:void 0};class yf extends g5{constructor(e,t){super(e,t),this.scheduler=e,this.work=t,this.pending=!1}schedule(e,t=0){var i;if(this.closed)return this;this.state=e;const r=this.id,a=this.scheduler;return null!=r&&(this.id=this.recycleAsyncId(a,r,t)),this.pending=!0,this.delay=t,this.id=null!==(i=this.id)&&void 0!==i?i:this.requestAsyncId(a,this.id,t),this}requestAsyncId(e,t,i=0){return ql.setInterval(e.flush.bind(e,this),i)}recycleAsyncId(e,t,i=0){if(null!=i&&this.delay===i&&!1===this.pending)return t;null!=t&&ql.clearInterval(t)}execute(e,t){if(this.closed)return new Error("executing a cancelled action");this.pending=!1;const i=this._execute(e,t);if(i)return i;!1===this.pending&&null!=this.id&&(this.id=this.recycleAsyncId(this.scheduler,this.id,null))}_execute(e,t){let r,i=!1;try{this.work(e)}catch(a){i=!0,r=a||new Error("Scheduled action threw falsy error")}if(i)return this.unsubscribe(),r}unsubscribe(){if(!this.closed){const{id:e,scheduler:t}=this,{actions:i}=t;this.work=this.state=this.scheduler=null,this.pending=!1,Ji(i,this),null!=e&&(this.id=this.recycleAsyncId(t,e,null)),this.delay=null,super.unsubscribe()}}}const j1={now:()=>(j1.delegate||Date).now(),delegate:void 0};class xo{constructor(e,t=xo.now){this.schedulerActionCtor=e,this.now=t}schedule(e,t=0,i){return new this.schedulerActionCtor(this,e).schedule(i,t)}}xo.now=j1.now;class vf extends xo{constructor(e,t=xo.now){super(e,t),this.actions=[],this._active=!1}flush(e){const{actions:t}=this;if(this._active)return void t.push(e);let i;this._active=!0;do{if(i=e.execute(e.state,e.delay))break}while(e=t.shift());if(this._active=!1,i){for(;e=t.shift();)e.unsubscribe();throw i}}}const wf=new vf(yf),m5=wf,B1=new Set;let aa,y5=(()=>{class n{constructor(t){this._platform=t,this._matchMedia=this._platform.isBrowser&&window.matchMedia?window.matchMedia.bind(window):w5}matchMedia(t){return(this._platform.WEBKIT||this._platform.BLINK)&&function v5(n){if(!B1.has(n))try{aa||(aa=document.createElement("style"),aa.setAttribute("type","text/css"),document.head.appendChild(aa)),aa.sheet&&(aa.sheet.insertRule(`@media ${n} {body{ }}`,0),B1.add(n))}catch(e){console.error(e)}}(t),this._matchMedia(t)}}return n.\u0275fac=function(t){return new(t||n)(v(Pt))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();function w5(n){return{matches:"all"===n||""===n,media:n,addListener:()=>{},removeListener:()=>{}}}let H1=(()=>{class n{constructor(t,i){this._mediaMatcher=t,this._zone=i,this._queries=new Map,this._destroySubject=new _e}ngOnDestroy(){this._destroySubject.next(),this._destroySubject.complete()}isMatched(t){return z1($l(t)).some(r=>this._registerQuery(r).mql.matches)}observe(t){let a=Oh(z1($l(t)).map(o=>this._registerQuery(o).observable));return a=vl(a.pipe(si(1)),a.pipe(N1(1),function b5(n,e=wf){return xe((t,i)=>{let r=null,a=null,o=null;const s=()=>{if(r){r.unsubscribe(),r=null;const c=a;a=null,i.next(c)}};function l(){const c=o+n,d=e.now();if(d<c)return r=this.schedule(void 0,c-d),void i.add(r);s()}t.subscribe(ve(i,c=>{a=c,o=e.now(),r||(r=e.schedule(l,n),i.add(r))},()=>{s(),i.complete()},void 0,()=>{a=r=null}))})}(0))),a.pipe(H(o=>{const s={matches:!1,breakpoints:{}};return o.forEach(({matches:l,query:c})=>{s.matches=s.matches||l,s.breakpoints[c]=l}),s}))}_registerQuery(t){if(this._queries.has(t))return this._queries.get(t);const i=this._mediaMatcher.matchMedia(t),a={observable:new ye(o=>{const s=l=>this._zone.run(()=>o.next(l));return i.addListener(s),()=>{i.removeListener(s)}}).pipe(vw(i),H(({matches:o})=>({query:t,matches:o})),dn(this._destroySubject)),mql:i};return this._queries.set(t,a),a}}return n.\u0275fac=function(t){return new(t||n)(v(y5),v(ae))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();function z1(n){return n.map(e=>e.split(",")).reduce((e,t)=>e.concat(t)).map(e=>e.trim())}function Yl(n,e){return(n.getAttribute(e)||"").match(/\S+/g)||[]}const V1="cdk-describedby-message",Ql="cdk-describedby-host";let _f=0,C5=(()=>{class n{constructor(t,i){this._platform=i,this._messageRegistry=new Map,this._messagesContainer=null,this._id=""+_f++,this._document=t,this._id=me($r)+"-"+_f++}describe(t,i,r){if(!this._canBeDescribed(t,i))return;const a=Df(i,r);"string"!=typeof i?(G1(i,this._id),this._messageRegistry.set(a,{messageElement:i,referenceCount:0})):this._messageRegistry.has(a)||this._createMessageElement(i,r),this._isElementDescribedByMessage(t,a)||this._addMessageReference(t,a)}removeDescription(t,i,r){var a;if(!i||!this._isElementNode(t))return;const o=Df(i,r);if(this._isElementDescribedByMessage(t,o)&&this._removeMessageReference(t,o),"string"==typeof i){const s=this._messageRegistry.get(o);s&&0===s.referenceCount&&this._deleteMessageElement(o)}0===(null===(a=this._messagesContainer)||void 0===a?void 0:a.childNodes.length)&&(this._messagesContainer.remove(),this._messagesContainer=null)}ngOnDestroy(){var t;const i=this._document.querySelectorAll(`[${Ql}="${this._id}"]`);for(let r=0;r<i.length;r++)this._removeCdkDescribedByReferenceIds(i[r]),i[r].removeAttribute(Ql);null===(t=this._messagesContainer)||void 0===t||t.remove(),this._messagesContainer=null,this._messageRegistry.clear()}_createMessageElement(t,i){const r=this._document.createElement("div");G1(r,this._id),r.textContent=t,i&&r.setAttribute("role",i),this._createMessagesContainer(),this._messagesContainer.appendChild(r),this._messageRegistry.set(Df(t,i),{messageElement:r,referenceCount:0})}_deleteMessageElement(t){var i,r;null===(r=null===(i=this._messageRegistry.get(t))||void 0===i?void 0:i.messageElement)||void 0===r||r.remove(),this._messageRegistry.delete(t)}_createMessagesContainer(){if(this._messagesContainer)return;const t="cdk-describedby-message-container",i=this._document.querySelectorAll(`.${t}[platform="server"]`);for(let a=0;a<i.length;a++)i[a].remove();const r=this._document.createElement("div");r.style.visibility="hidden",r.classList.add(t),r.classList.add("cdk-visually-hidden"),this._platform&&!this._platform.isBrowser&&r.setAttribute("platform","server"),this._document.body.appendChild(r),this._messagesContainer=r}_removeCdkDescribedByReferenceIds(t){const i=Yl(t,"aria-describedby").filter(r=>0!=r.indexOf(V1));t.setAttribute("aria-describedby",i.join(" "))}_addMessageReference(t,i){const r=this._messageRegistry.get(i);(function D5(n,e,t){const i=Yl(n,e);i.some(r=>r.trim()==t.trim())||(i.push(t.trim()),n.setAttribute(e,i.join(" ")))})(t,"aria-describedby",r.messageElement.id),t.setAttribute(Ql,this._id),r.referenceCount++}_removeMessageReference(t,i){const r=this._messageRegistry.get(i);r.referenceCount--,function S5(n,e,t){const r=Yl(n,e).filter(a=>a!=t.trim());r.length?n.setAttribute(e,r.join(" ")):n.removeAttribute(e)}(t,"aria-describedby",r.messageElement.id),t.removeAttribute(Ql)}_isElementDescribedByMessage(t,i){const r=Yl(t,"aria-describedby"),a=this._messageRegistry.get(i),o=a&&a.messageElement.id;return!!o&&-1!=r.indexOf(o)}_canBeDescribed(t,i){if(!this._isElementNode(t))return!1;if(i&&"object"==typeof i)return!0;const r=null==i?"":`${i}`.trim(),a=t.getAttribute("aria-label");return!(!r||a&&a.trim()===r)}_isElementNode(t){return t.nodeType===this._document.ELEMENT_NODE}}return n.\u0275fac=function(t){return new(t||n)(v(Q),v(Pt))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();function Df(n,e){return"string"==typeof n?`${e||""}/${n}`:n}function G1(n,e){n.id||(n.id=`${V1}-${e}-${_f++}`)}function $1(n){return 0===n.buttons||0===n.offsetX&&0===n.offsetY}function q1(n){const e=n.touches&&n.touches[0]||n.changedTouches&&n.changedTouches[0];return!(!e||-1!==e.identifier||null!=e.radiusX&&1!==e.radiusX||null!=e.radiusY&&1!==e.radiusY)}const I5=new E("cdk-input-modality-detector-options"),A5={ignoreKeys:[18,17,224,91,16]},oa=Gl({passive:!0,capture:!0});let R5=(()=>{class n{constructor(t,i,r,a){this._platform=t,this._mostRecentTarget=null,this._modality=new Ut(null),this._lastTouchMs=0,this._onKeydown=o=>{var s,l;null!==(l=null===(s=this._options)||void 0===s?void 0:s.ignoreKeys)&&void 0!==l&&l.some(c=>c===o.keyCode)||(this._modality.next("keyboard"),this._mostRecentTarget=Ui(o))},this._onMousedown=o=>{Date.now()-this._lastTouchMs<650||(this._modality.next($1(o)?"keyboard":"mouse"),this._mostRecentTarget=Ui(o))},this._onTouchstart=o=>{q1(o)?this._modality.next("keyboard"):(this._lastTouchMs=Date.now(),this._modality.next("touch"),this._mostRecentTarget=Ui(o))},this._options=Object.assign(Object.assign({},A5),a),this.modalityDetected=this._modality.pipe(N1(1)),this.modalityChanged=this.modalityDetected.pipe(function d5(n,e=Qn){return n=null!=n?n:u5,xe((t,i)=>{let r,a=!0;t.subscribe(ve(i,o=>{const s=e(o);(a||!n(r,s))&&(a=!1,r=s,i.next(o))}))})}()),t.isBrowser&&i.runOutsideAngular(()=>{r.addEventListener("keydown",this._onKeydown,oa),r.addEventListener("mousedown",this._onMousedown,oa),r.addEventListener("touchstart",this._onTouchstart,oa)})}get mostRecentModality(){return this._modality.value}ngOnDestroy(){this._modality.complete(),this._platform.isBrowser&&(document.removeEventListener("keydown",this._onKeydown,oa),document.removeEventListener("mousedown",this._onMousedown,oa),document.removeEventListener("touchstart",this._onTouchstart,oa))}}return n.\u0275fac=function(t){return new(t||n)(v(Pt),v(ae),v(Q),v(I5,8))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();const L5=new E("cdk-focus-monitor-default-options"),Kl=Gl({passive:!0,capture:!0});let Q1=(()=>{class n{constructor(t,i,r,a,o){this._ngZone=t,this._platform=i,this._inputModalityDetector=r,this._origin=null,this._windowFocused=!1,this._originFromTouchInteraction=!1,this._elementInfo=new Map,this._monitoredElementCount=0,this._rootNodeFocusListenerCount=new Map,this._windowFocusListener=()=>{this._windowFocused=!0,this._windowFocusTimeoutId=window.setTimeout(()=>this._windowFocused=!1)},this._stopInputModalityDetector=new _e,this._rootNodeFocusAndBlurListener=s=>{for(let c=Ui(s);c;c=c.parentElement)"focus"===s.type?this._onFocus(s,c):this._onBlur(s,c)},this._document=a,this._detectionMode=(null==o?void 0:o.detectionMode)||0}monitor(t,i=!1){const r=ra(t);if(!this._platform.isBrowser||1!==r.nodeType)return I(null);const a=function t5(n){if(function e5(){if(null==gf){const n="undefined"!=typeof document?document.head:null;gf=!(!n||!n.createShadowRoot&&!n.attachShadow)}return gf}()){const e=n.getRootNode?n.getRootNode():null;if("undefined"!=typeof ShadowRoot&&ShadowRoot&&e instanceof ShadowRoot)return e}return null}(r)||this._getDocument(),o=this._elementInfo.get(r);if(o)return i&&(o.checkChildren=!0),o.subject;const s={checkChildren:i,subject:new _e,rootNode:a};return this._elementInfo.set(r,s),this._registerGlobalListeners(s),s.subject}stopMonitoring(t){const i=ra(t),r=this._elementInfo.get(i);r&&(r.subject.complete(),this._setClasses(i),this._elementInfo.delete(i),this._removeGlobalListeners(r))}focusVia(t,i,r){const a=ra(t);a===this._getDocument().activeElement?this._getClosestElementsInfo(a).forEach(([s,l])=>this._originChanged(s,i,l)):(this._setOrigin(i),"function"==typeof a.focus&&a.focus(r))}ngOnDestroy(){this._elementInfo.forEach((t,i)=>this.stopMonitoring(i))}_getDocument(){return this._document||document}_getWindow(){return this._getDocument().defaultView||window}_getFocusOrigin(t){return this._origin?this._originFromTouchInteraction?this._shouldBeAttributedToTouch(t)?"touch":"program":this._origin:this._windowFocused&&this._lastFocusOrigin?this._lastFocusOrigin:t&&this._isLastInteractionFromInputLabel(t)?"mouse":"program"}_shouldBeAttributedToTouch(t){return 1===this._detectionMode||!(null==t||!t.contains(this._inputModalityDetector._mostRecentTarget))}_setClasses(t,i){t.classList.toggle("cdk-focused",!!i),t.classList.toggle("cdk-touch-focused","touch"===i),t.classList.toggle("cdk-keyboard-focused","keyboard"===i),t.classList.toggle("cdk-mouse-focused","mouse"===i),t.classList.toggle("cdk-program-focused","program"===i)}_setOrigin(t,i=!1){this._ngZone.runOutsideAngular(()=>{this._origin=t,this._originFromTouchInteraction="touch"===t&&i,0===this._detectionMode&&(clearTimeout(this._originTimeoutId),this._originTimeoutId=setTimeout(()=>this._origin=null,this._originFromTouchInteraction?650:1))})}_onFocus(t,i){const r=this._elementInfo.get(i),a=Ui(t);!r||!r.checkChildren&&i!==a||this._originChanged(i,this._getFocusOrigin(a),r)}_onBlur(t,i){const r=this._elementInfo.get(i);!r||r.checkChildren&&t.relatedTarget instanceof Node&&i.contains(t.relatedTarget)||(this._setClasses(i),this._emitOrigin(r,null))}_emitOrigin(t,i){t.subject.observers.length&&this._ngZone.run(()=>t.subject.next(i))}_registerGlobalListeners(t){if(!this._platform.isBrowser)return;const i=t.rootNode,r=this._rootNodeFocusListenerCount.get(i)||0;r||this._ngZone.runOutsideAngular(()=>{i.addEventListener("focus",this._rootNodeFocusAndBlurListener,Kl),i.addEventListener("blur",this._rootNodeFocusAndBlurListener,Kl)}),this._rootNodeFocusListenerCount.set(i,r+1),1==++this._monitoredElementCount&&(this._ngZone.runOutsideAngular(()=>{this._getWindow().addEventListener("focus",this._windowFocusListener)}),this._inputModalityDetector.modalityDetected.pipe(dn(this._stopInputModalityDetector)).subscribe(a=>{this._setOrigin(a,!0)}))}_removeGlobalListeners(t){const i=t.rootNode;if(this._rootNodeFocusListenerCount.has(i)){const r=this._rootNodeFocusListenerCount.get(i);r>1?this._rootNodeFocusListenerCount.set(i,r-1):(i.removeEventListener("focus",this._rootNodeFocusAndBlurListener,Kl),i.removeEventListener("blur",this._rootNodeFocusAndBlurListener,Kl),this._rootNodeFocusListenerCount.delete(i))}--this._monitoredElementCount||(this._getWindow().removeEventListener("focus",this._windowFocusListener),this._stopInputModalityDetector.next(),clearTimeout(this._windowFocusTimeoutId),clearTimeout(this._originTimeoutId))}_originChanged(t,i,r){this._setClasses(t,i),this._emitOrigin(r,i),this._lastFocusOrigin=i}_getClosestElementsInfo(t){const i=[];return this._elementInfo.forEach((r,a)=>{(a===t||r.checkChildren&&a.contains(t))&&i.push([a,r])}),i}_isLastInteractionFromInputLabel(t){const{_mostRecentTarget:i,mostRecentModality:r}=this._inputModalityDetector;if("mouse"!==r||!i||i===t||"INPUT"!==t.nodeName&&"TEXTAREA"!==t.nodeName||t.disabled)return!1;const a=t.labels;if(a)for(let o=0;o<a.length;o++)if(a[o].contains(i))return!0;return!1}}return n.\u0275fac=function(t){return new(t||n)(v(ae),v(Pt),v(R5),v(Q,8),v(L5,8))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();const K1="cdk-high-contrast-black-on-white",X1="cdk-high-contrast-white-on-black",Sf="cdk-high-contrast-active";let J1=(()=>{class n{constructor(t,i){this._platform=t,this._document=i,this._breakpointSubscription=me(H1).observe("(forced-colors: active)").subscribe(()=>{this._hasCheckedHighContrastMode&&(this._hasCheckedHighContrastMode=!1,this._applyBodyHighContrastModeCssClasses())})}getHighContrastMode(){if(!this._platform.isBrowser)return 0;const t=this._document.createElement("div");t.style.backgroundColor="rgb(1,2,3)",t.style.position="absolute",this._document.body.appendChild(t);const i=this._document.defaultView||window,r=i&&i.getComputedStyle?i.getComputedStyle(t):null,a=(r&&r.backgroundColor||"").replace(/ /g,"");switch(t.remove(),a){case"rgb(0,0,0)":case"rgb(45,50,54)":case"rgb(32,32,32)":return 2;case"rgb(255,255,255)":case"rgb(255,250,239)":return 1}return 0}ngOnDestroy(){this._breakpointSubscription.unsubscribe()}_applyBodyHighContrastModeCssClasses(){if(!this._hasCheckedHighContrastMode&&this._platform.isBrowser&&this._document.body){const t=this._document.body.classList;t.remove(Sf,K1,X1),this._hasCheckedHighContrastMode=!0;const i=this.getHighContrastMode();1===i?t.add(Sf,K1):2===i&&t.add(Sf,X1)}}}return n.\u0275fac=function(t){return new(t||n)(v(Pt),v(Q))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})(),O5=(()=>{class n{constructor(t){t._applyBodyHighContrastModeCssClasses()}}return n.\u0275fac=function(t){return new(t||n)(v(J1))},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({imports:[p5]}),n})();const F5=new E("mat-sanity-checks",{providedIn:"root",factory:function N5(){return!0}});let Dn=(()=>{class n{constructor(t,i,r){this._sanityChecks=i,this._document=r,this._hasDoneGlobalChecks=!1,t._applyBodyHighContrastModeCssClasses(),this._hasDoneGlobalChecks||(this._hasDoneGlobalChecks=!0)}_checkIsEnabled(t){return!mf()&&("boolean"==typeof this._sanityChecks?this._sanityChecks:!!this._sanityChecks[t])}}return n.\u0275fac=function(t){return new(t||n)(v(J1),v(F5,8),v(Q))},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({imports:[So,So]}),n})();function j5(n){return class extends n{constructor(...e){super(...e),this._disabled=!1}get disabled(){return this._disabled}set disabled(e){this._disabled=bf(e)}}}function e_(n,e){return class extends n{constructor(...t){super(...t),this.defaultColor=e,this.color=e}get color(){return this._color}set color(t){const i=t||this.defaultColor;i!==this._color&&(this._color&&this._elementRef.nativeElement.classList.remove(`mat-${this._color}`),i&&this._elementRef.nativeElement.classList.add(`mat-${i}`),this._color=i)}}}function B5(n){return class extends n{constructor(...e){super(...e),this._disableRipple=!1}get disableRipple(){return this._disableRipple}set disableRipple(e){this._disableRipple=bf(e)}}}class z5{constructor(e,t,i,r=!1){this._renderer=e,this.element=t,this.config=i,this._animationForciblyDisabledThroughCss=r,this.state=3}fadeOut(){this._renderer.fadeOutRipple(this)}}const t_={enterDuration:225,exitDuration:150},Cf=Gl({passive:!0}),n_=["mousedown","touchstart"],i_=["mouseup","mouseleave","touchend","touchcancel"];class V5{constructor(e,t,i,r){this._target=e,this._ngZone=t,this._isPointerDown=!1,this._activeRipples=new Map,this._pointerUpEventsRegistered=!1,r.isBrowser&&(this._containerElement=ra(i))}fadeInRipple(e,t,i={}){const r=this._containerRect=this._containerRect||this._containerElement.getBoundingClientRect(),a=Object.assign(Object.assign({},t_),i.animation);i.centered&&(e=r.left+r.width/2,t=r.top+r.height/2);const o=i.radius||function G5(n,e,t){const i=Math.max(Math.abs(n-t.left),Math.abs(n-t.right)),r=Math.max(Math.abs(e-t.top),Math.abs(e-t.bottom));return Math.sqrt(i*i+r*r)}(e,t,r),s=e-r.left,l=t-r.top,c=a.enterDuration,d=document.createElement("div");d.classList.add("mat-ripple-element"),d.style.left=s-o+"px",d.style.top=l-o+"px",d.style.height=2*o+"px",d.style.width=2*o+"px",null!=i.color&&(d.style.backgroundColor=i.color),d.style.transitionDuration=`${c}ms`,this._containerElement.appendChild(d);const u=window.getComputedStyle(d),f=u.transitionDuration,p="none"===u.transitionProperty||"0s"===f||"0s, 0s"===f,g=new z5(this,d,i,p);d.style.transform="scale3d(1, 1, 1)",g.state=0,i.persistent||(this._mostRecentTransientRipple=g);let m=null;return!p&&(c||a.exitDuration)&&this._ngZone.runOutsideAngular(()=>{const b=()=>this._finishRippleTransition(g),D=()=>this._destroyRipple(g);d.addEventListener("transitionend",b),d.addEventListener("transitioncancel",D),m={onTransitionEnd:b,onTransitionCancel:D}}),this._activeRipples.set(g,m),(p||!c)&&this._finishRippleTransition(g),g}fadeOutRipple(e){if(2===e.state||3===e.state)return;const t=e.element,i=Object.assign(Object.assign({},t_),e.config.animation);t.style.transitionDuration=`${i.exitDuration}ms`,t.style.opacity="0",e.state=2,(e._animationForciblyDisabledThroughCss||!i.exitDuration)&&this._finishRippleTransition(e)}fadeOutAll(){this._getActiveRipples().forEach(e=>e.fadeOut())}fadeOutAllNonPersistent(){this._getActiveRipples().forEach(e=>{e.config.persistent||e.fadeOut()})}setupTriggerEvents(e){const t=ra(e);!t||t===this._triggerElement||(this._removeTriggerEvents(),this._triggerElement=t,this._registerEvents(n_))}handleEvent(e){"mousedown"===e.type?this._onMousedown(e):"touchstart"===e.type?this._onTouchStart(e):this._onPointerUp(),this._pointerUpEventsRegistered||(this._registerEvents(i_),this._pointerUpEventsRegistered=!0)}_finishRippleTransition(e){0===e.state?this._startFadeOutTransition(e):2===e.state&&this._destroyRipple(e)}_startFadeOutTransition(e){const t=e===this._mostRecentTransientRipple,{persistent:i}=e.config;e.state=1,!i&&(!t||!this._isPointerDown)&&e.fadeOut()}_destroyRipple(e){var t;const i=null!==(t=this._activeRipples.get(e))&&void 0!==t?t:null;this._activeRipples.delete(e),this._activeRipples.size||(this._containerRect=null),e===this._mostRecentTransientRipple&&(this._mostRecentTransientRipple=null),e.state=3,null!==i&&(e.element.removeEventListener("transitionend",i.onTransitionEnd),e.element.removeEventListener("transitioncancel",i.onTransitionCancel)),e.element.remove()}_onMousedown(e){const t=$1(e),i=this._lastTouchStartEvent&&Date.now()<this._lastTouchStartEvent+800;!this._target.rippleDisabled&&!t&&!i&&(this._isPointerDown=!0,this.fadeInRipple(e.clientX,e.clientY,this._target.rippleConfig))}_onTouchStart(e){if(!this._target.rippleDisabled&&!q1(e)){this._lastTouchStartEvent=Date.now(),this._isPointerDown=!0;const t=e.changedTouches;for(let i=0;i<t.length;i++)this.fadeInRipple(t[i].clientX,t[i].clientY,this._target.rippleConfig)}}_onPointerUp(){!this._isPointerDown||(this._isPointerDown=!1,this._getActiveRipples().forEach(e=>{!e.config.persistent&&(1===e.state||e.config.terminateOnPointerUp&&0===e.state)&&e.fadeOut()}))}_registerEvents(e){this._ngZone.runOutsideAngular(()=>{e.forEach(t=>{this._triggerElement.addEventListener(t,this,Cf)})})}_getActiveRipples(){return Array.from(this._activeRipples.keys())}_removeTriggerEvents(){this._triggerElement&&(n_.forEach(e=>{this._triggerElement.removeEventListener(e,this,Cf)}),this._pointerUpEventsRegistered&&i_.forEach(e=>{this._triggerElement.removeEventListener(e,this,Cf)}))}}const U5=new E("mat-ripple-global-options");let r_=(()=>{class n{constructor(t,i,r,a,o){this._elementRef=t,this._animationMode=o,this.radius=0,this._disabled=!1,this._isInitialized=!1,this._globalOptions=a||{},this._rippleRenderer=new V5(this,i,t,r)}get disabled(){return this._disabled}set disabled(t){t&&this.fadeOutAllNonPersistent(),this._disabled=t,this._setupTriggerEventsIfEnabled()}get trigger(){return this._trigger||this._elementRef.nativeElement}set trigger(t){this._trigger=t,this._setupTriggerEventsIfEnabled()}ngOnInit(){this._isInitialized=!0,this._setupTriggerEventsIfEnabled()}ngOnDestroy(){this._rippleRenderer._removeTriggerEvents()}fadeOutAll(){this._rippleRenderer.fadeOutAll()}fadeOutAllNonPersistent(){this._rippleRenderer.fadeOutAllNonPersistent()}get rippleConfig(){return{centered:this.centered,radius:this.radius,color:this.color,animation:Object.assign(Object.assign(Object.assign({},this._globalOptions.animation),"NoopAnimations"===this._animationMode?{enterDuration:0,exitDuration:0}:{}),this.animation),terminateOnPointerUp:this._globalOptions.terminateOnPointerUp}}get rippleDisabled(){return this.disabled||!!this._globalOptions.disabled}_setupTriggerEventsIfEnabled(){!this.disabled&&this._isInitialized&&this._rippleRenderer.setupTriggerEvents(this.trigger)}launch(t,i=0,r){return"number"==typeof t?this._rippleRenderer.fadeInRipple(t,i,Object.assign(Object.assign({},this.rippleConfig),r)):this._rippleRenderer.fadeInRipple(0,0,Object.assign(Object.assign({},this.rippleConfig),t))}}return n.\u0275fac=function(t){return new(t||n)(C(ht),C(ae),C(Pt),C(U5,8),C(Fi,8))},n.\u0275dir=Re({type:n,selectors:[["","mat-ripple",""],["","matRipple",""]],hostAttrs:[1,"mat-ripple"],hostVars:2,hostBindings:function(t,i){2&t&&Li("mat-ripple-unbounded",i.unbounded)},inputs:{color:["matRippleColor","color"],unbounded:["matRippleUnbounded","unbounded"],centered:["matRippleCentered","centered"],radius:["matRippleRadius","radius"],animation:["matRippleAnimation","animation"],disabled:["matRippleDisabled","disabled"],trigger:["matRippleTrigger","trigger"]},exportAs:["matRipple"]}),n})(),$5=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({imports:[Dn,Dn]}),n})();const q5=["mat-button",""],Y5=["*"],K5=["mat-button","mat-flat-button","mat-icon-button","mat-raised-button","mat-stroked-button","mat-mini-fab","mat-fab"],X5=e_(j5(B5(class{constructor(n){this._elementRef=n}})));let a_=(()=>{class n extends X5{constructor(t,i,r){super(t),this._focusMonitor=i,this._animationMode=r,this.isRoundButton=this._hasHostAttributes("mat-fab","mat-mini-fab"),this.isIconButton=this._hasHostAttributes("mat-icon-button");for(const a of K5)this._hasHostAttributes(a)&&this._getHostElement().classList.add(a);t.nativeElement.classList.add("mat-button-base"),this.isRoundButton&&(this.color="accent")}ngAfterViewInit(){this._focusMonitor.monitor(this._elementRef,!0)}ngOnDestroy(){this._focusMonitor.stopMonitoring(this._elementRef)}focus(t,i){t?this._focusMonitor.focusVia(this._getHostElement(),t,i):this._getHostElement().focus(i)}_getHostElement(){return this._elementRef.nativeElement}_isRippleDisabled(){return this.disableRipple||this.disabled}_hasHostAttributes(...t){return t.some(i=>this._getHostElement().hasAttribute(i))}}return n.\u0275fac=function(t){return new(t||n)(C(ht),C(Q1),C(Fi,8))},n.\u0275cmp=mt({type:n,selectors:[["button","mat-button",""],["button","mat-raised-button",""],["button","mat-icon-button",""],["button","mat-fab",""],["button","mat-mini-fab",""],["button","mat-stroked-button",""],["button","mat-flat-button",""]],viewQuery:function(t,i){if(1&t&&Uu(r_,5),2&t){let r;Wr(r=Vr())&&(i.ripple=r.first)}},hostAttrs:[1,"mat-focus-indicator"],hostVars:5,hostBindings:function(t,i){2&t&&(za("disabled",i.disabled||null),Li("_mat-animation-noopable","NoopAnimations"===i._animationMode)("mat-button-disabled",i.disabled))},inputs:{disabled:"disabled",disableRipple:"disableRipple",color:"color"},exportAs:["matButton"],features:[kr],attrs:q5,ngContentSelectors:Y5,decls:4,vars:5,consts:[[1,"mat-button-wrapper"],["matRipple","",1,"mat-button-ripple",3,"matRippleDisabled","matRippleCentered","matRippleTrigger"],[1,"mat-button-focus-overlay"]],template:function(t,i){1&t&&(Su(),Y(0,"span",0),Os(1),G(),zt(2,"span",1)(3,"span",2)),2&t&&(ie(2),Li("mat-button-ripple-round",i.isRoundButton||i.isIconButton),re("matRippleDisabled",i._isRippleDisabled())("matRippleCentered",i.isIconButton)("matRippleTrigger",i._getHostElement()))},dependencies:[r_],styles:[".mat-button .mat-button-focus-overlay,.mat-icon-button .mat-button-focus-overlay{opacity:0}.mat-button:hover:not(.mat-button-disabled) .mat-button-focus-overlay,.mat-stroked-button:hover:not(.mat-button-disabled) .mat-button-focus-overlay{opacity:.04}@media(hover: none){.mat-button:hover:not(.mat-button-disabled) .mat-button-focus-overlay,.mat-stroked-button:hover:not(.mat-button-disabled) .mat-button-focus-overlay{opacity:0}}.mat-button,.mat-icon-button,.mat-stroked-button,.mat-flat-button{box-sizing:border-box;position:relative;-webkit-user-select:none;user-select:none;cursor:pointer;outline:none;border:none;-webkit-tap-highlight-color:rgba(0,0,0,0);display:inline-block;white-space:nowrap;text-decoration:none;vertical-align:baseline;text-align:center;margin:0;min-width:64px;line-height:36px;padding:0 16px;border-radius:4px;overflow:visible}.mat-button::-moz-focus-inner,.mat-icon-button::-moz-focus-inner,.mat-stroked-button::-moz-focus-inner,.mat-flat-button::-moz-focus-inner{border:0}.mat-button.mat-button-disabled,.mat-icon-button.mat-button-disabled,.mat-stroked-button.mat-button-disabled,.mat-flat-button.mat-button-disabled{cursor:default}.mat-button.cdk-keyboard-focused .mat-button-focus-overlay,.mat-button.cdk-program-focused .mat-button-focus-overlay,.mat-icon-button.cdk-keyboard-focused .mat-button-focus-overlay,.mat-icon-button.cdk-program-focused .mat-button-focus-overlay,.mat-stroked-button.cdk-keyboard-focused .mat-button-focus-overlay,.mat-stroked-button.cdk-program-focused .mat-button-focus-overlay,.mat-flat-button.cdk-keyboard-focused .mat-button-focus-overlay,.mat-flat-button.cdk-program-focused .mat-button-focus-overlay{opacity:.12}.mat-button::-moz-focus-inner,.mat-icon-button::-moz-focus-inner,.mat-stroked-button::-moz-focus-inner,.mat-flat-button::-moz-focus-inner{border:0}.mat-raised-button{box-sizing:border-box;position:relative;-webkit-user-select:none;user-select:none;cursor:pointer;outline:none;border:none;-webkit-tap-highlight-color:rgba(0,0,0,0);display:inline-block;white-space:nowrap;text-decoration:none;vertical-align:baseline;text-align:center;margin:0;min-width:64px;line-height:36px;padding:0 16px;border-radius:4px;overflow:visible;transform:translate3d(0, 0, 0);transition:background 400ms cubic-bezier(0.25, 0.8, 0.25, 1),box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1)}.mat-raised-button::-moz-focus-inner{border:0}.mat-raised-button.mat-button-disabled{cursor:default}.mat-raised-button.cdk-keyboard-focused .mat-button-focus-overlay,.mat-raised-button.cdk-program-focused .mat-button-focus-overlay{opacity:.12}.mat-raised-button::-moz-focus-inner{border:0}.mat-raised-button._mat-animation-noopable{transition:none !important;animation:none !important}.mat-stroked-button{border:1px solid currentColor;padding:0 15px;line-height:34px}.mat-stroked-button .mat-button-ripple.mat-ripple,.mat-stroked-button .mat-button-focus-overlay{top:-1px;left:-1px;right:-1px;bottom:-1px}.mat-fab{box-sizing:border-box;position:relative;-webkit-user-select:none;user-select:none;cursor:pointer;outline:none;border:none;-webkit-tap-highlight-color:rgba(0,0,0,0);display:inline-block;white-space:nowrap;text-decoration:none;vertical-align:baseline;text-align:center;margin:0;min-width:64px;line-height:36px;padding:0 16px;border-radius:4px;overflow:visible;transform:translate3d(0, 0, 0);transition:background 400ms cubic-bezier(0.25, 0.8, 0.25, 1),box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1);min-width:0;border-radius:50%;width:56px;height:56px;padding:0;flex-shrink:0}.mat-fab::-moz-focus-inner{border:0}.mat-fab.mat-button-disabled{cursor:default}.mat-fab.cdk-keyboard-focused .mat-button-focus-overlay,.mat-fab.cdk-program-focused .mat-button-focus-overlay{opacity:.12}.mat-fab::-moz-focus-inner{border:0}.mat-fab._mat-animation-noopable{transition:none !important;animation:none !important}.mat-fab .mat-button-wrapper{padding:16px 0;display:inline-block;line-height:24px}.mat-mini-fab{box-sizing:border-box;position:relative;-webkit-user-select:none;user-select:none;cursor:pointer;outline:none;border:none;-webkit-tap-highlight-color:rgba(0,0,0,0);display:inline-block;white-space:nowrap;text-decoration:none;vertical-align:baseline;text-align:center;margin:0;min-width:64px;line-height:36px;padding:0 16px;border-radius:4px;overflow:visible;transform:translate3d(0, 0, 0);transition:background 400ms cubic-bezier(0.25, 0.8, 0.25, 1),box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1);min-width:0;border-radius:50%;width:40px;height:40px;padding:0;flex-shrink:0}.mat-mini-fab::-moz-focus-inner{border:0}.mat-mini-fab.mat-button-disabled{cursor:default}.mat-mini-fab.cdk-keyboard-focused .mat-button-focus-overlay,.mat-mini-fab.cdk-program-focused .mat-button-focus-overlay{opacity:.12}.mat-mini-fab::-moz-focus-inner{border:0}.mat-mini-fab._mat-animation-noopable{transition:none !important;animation:none !important}.mat-mini-fab .mat-button-wrapper{padding:8px 0;display:inline-block;line-height:24px}.mat-icon-button{padding:0;min-width:0;width:40px;height:40px;flex-shrink:0;line-height:40px;border-radius:50%}.mat-icon-button i,.mat-icon-button .mat-icon{line-height:24px}.mat-button-ripple.mat-ripple,.mat-button-focus-overlay{top:0;left:0;right:0;bottom:0;position:absolute;pointer-events:none;border-radius:inherit}.mat-button-ripple.mat-ripple:not(:empty){transform:translateZ(0)}.mat-button-focus-overlay{opacity:0;transition:opacity 200ms cubic-bezier(0.35, 0, 0.25, 1),background-color 200ms cubic-bezier(0.35, 0, 0.25, 1)}._mat-animation-noopable .mat-button-focus-overlay{transition:none}.mat-button-ripple-round{border-radius:50%;z-index:1}.mat-button .mat-button-wrapper>*,.mat-flat-button .mat-button-wrapper>*,.mat-stroked-button .mat-button-wrapper>*,.mat-raised-button .mat-button-wrapper>*,.mat-icon-button .mat-button-wrapper>*,.mat-fab .mat-button-wrapper>*,.mat-mini-fab .mat-button-wrapper>*{vertical-align:middle}.mat-form-field:not(.mat-form-field-appearance-legacy) .mat-form-field-prefix .mat-icon-button,.mat-form-field:not(.mat-form-field-appearance-legacy) .mat-form-field-suffix .mat-icon-button{display:inline-flex;justify-content:center;align-items:center;font-size:inherit;width:2.5em;height:2.5em}.mat-flat-button::before,.mat-raised-button::before,.mat-fab::before,.mat-mini-fab::before{margin:calc(calc(var(--mat-focus-indicator-border-width, 3px) + 2px) * -1)}.mat-stroked-button::before{margin:calc(calc(var(--mat-focus-indicator-border-width, 3px) + 3px) * -1)}.cdk-high-contrast-active .mat-button,.cdk-high-contrast-active .mat-flat-button,.cdk-high-contrast-active .mat-raised-button,.cdk-high-contrast-active .mat-icon-button,.cdk-high-contrast-active .mat-fab,.cdk-high-contrast-active .mat-mini-fab{outline:solid 1px}"],encapsulation:2,changeDetection:0}),n})(),J5=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({imports:[$5,Dn,Dn]}),n})();const Z5=["addListener","removeListener"],e4=["addEventListener","removeEventListener"],t4=["on","off"];function xf(n,e,t,i){if(te(t)&&(i=t,t=void 0),i)return xf(n,e,t).pipe(Lh(i));const[r,a]=function r4(n){return te(n.addEventListener)&&te(n.removeEventListener)}(n)?e4.map(o=>s=>n[o](e,s,t)):function n4(n){return te(n.addListener)&&te(n.removeListener)}(n)?Z5.map(o_(n,e)):function i4(n){return te(n.on)&&te(n.off)}(n)?t4.map(o_(n,e)):[];if(!r&&Ec(n))return $e(o=>xf(o,e,t))(ot(n));if(!r)throw new TypeError("Invalid event target");return new ye(o=>{const s=(...l)=>o.next(1<l.length?l:l[0]);return r(s),()=>a(s)})}function o_(n,e){return t=>i=>n[t](e,i)}const Eo={schedule(n){let e=requestAnimationFrame,t=cancelAnimationFrame;const{delegate:i}=Eo;i&&(e=i.requestAnimationFrame,t=i.cancelAnimationFrame);const r=e(a=>{t=void 0,n(a)});return new Je(()=>null==t?void 0:t(r))},requestAnimationFrame(...n){const{delegate:e}=Eo;return((null==e?void 0:e.requestAnimationFrame)||requestAnimationFrame)(...n)},cancelAnimationFrame(...n){const{delegate:e}=Eo;return((null==e?void 0:e.cancelAnimationFrame)||cancelAnimationFrame)(...n)},delegate:void 0};new class o4 extends vf{flush(e){this._active=!0;const t=this._scheduled;this._scheduled=void 0;const{actions:i}=this;let r;e=e||i.shift();do{if(r=e.execute(e.state,e.delay))break}while((e=i[0])&&e.id===t&&i.shift());if(this._active=!1,r){for(;(e=i[0])&&e.id===t&&i.shift();)e.unsubscribe();throw r}}}(class a4 extends yf{constructor(e,t){super(e,t),this.scheduler=e,this.work=t}requestAsyncId(e,t,i=0){return null!==i&&i>0?super.requestAsyncId(e,t,i):(e.actions.push(this),e._scheduled||(e._scheduled=Eo.requestAnimationFrame(()=>e.flush(void 0))))}recycleAsyncId(e,t,i=0){var r;if(null!=i?i>0:this.delay>0)return super.recycleAsyncId(e,t,i);const{actions:a}=e;null!=t&&(null===(r=a[a.length-1])||void 0===r?void 0:r.id)!==t&&(Eo.cancelAnimationFrame(t),e._scheduled=void 0)}});let Tf,l4=1;const Xl={};function s_(n){return n in Xl&&(delete Xl[n],!0)}const c4={setImmediate(n){const e=l4++;return Xl[e]=!0,Tf||(Tf=Promise.resolve()),Tf.then(()=>s_(e)&&n()),e},clearImmediate(n){s_(n)}},{setImmediate:d4,clearImmediate:u4}=c4,Jl={setImmediate(...n){const{delegate:e}=Jl;return((null==e?void 0:e.setImmediate)||d4)(...n)},clearImmediate(n){const{delegate:e}=Jl;return((null==e?void 0:e.clearImmediate)||u4)(n)},delegate:void 0};new class f4 extends vf{flush(e){this._active=!0;const t=this._scheduled;this._scheduled=void 0;const{actions:i}=this;let r;e=e||i.shift();do{if(r=e.execute(e.state,e.delay))break}while((e=i[0])&&e.id===t&&i.shift());if(this._active=!1,r){for(;(e=i[0])&&e.id===t&&i.shift();)e.unsubscribe();throw r}}}(class h4 extends yf{constructor(e,t){super(e,t),this.scheduler=e,this.work=t}requestAsyncId(e,t,i=0){return null!==i&&i>0?super.requestAsyncId(e,t,i):(e.actions.push(this),e._scheduled||(e._scheduled=Jl.setImmediate(e.flush.bind(e,void 0))))}recycleAsyncId(e,t,i=0){var r;if(null!=i?i>0:this.delay>0)return super.recycleAsyncId(e,t,i);const{actions:a}=e;null!=t&&(null===(r=a[a.length-1])||void 0===r?void 0:r.id)!==t&&(Jl.clearImmediate(t),e._scheduled=void 0)}});function l_(n,e=wf){return function g4(n){return xe((e,t)=>{let i=!1,r=null,a=null,o=!1;const s=()=>{if(null==a||a.unsubscribe(),a=null,i){i=!1;const c=r;r=null,t.next(c)}o&&t.complete()},l=()=>{a=null,o&&t.complete()};e.subscribe(ve(t,c=>{i=!0,r=c,a||ot(n(c)).subscribe(a=ve(t,s,l))},()=>{o=!0,(!i||!a||a.closed)&&t.complete()}))})}(()=>function b4(n=0,e,t=m5){let i=-1;return null!=e&&(Tp(e)?t=e:i=e),new ye(r=>{let a=function m4(n){return n instanceof Date&&!isNaN(n)}(n)?+n-t.now():n;a<0&&(a=0);let o=0;return t.schedule(function(){r.closed||(r.next(o++),0<=i?this.schedule(void 0,i):r.complete())},a)})}(n,e))}let c_=(()=>{class n{constructor(t,i,r){this._ngZone=t,this._platform=i,this._scrolled=new _e,this._globalSubscription=null,this._scrolledCount=0,this.scrollContainers=new Map,this._document=r}register(t){this.scrollContainers.has(t)||this.scrollContainers.set(t,t.elementScrolled().subscribe(()=>this._scrolled.next(t)))}deregister(t){const i=this.scrollContainers.get(t);i&&(i.unsubscribe(),this.scrollContainers.delete(t))}scrolled(t=20){return this._platform.isBrowser?new ye(i=>{this._globalSubscription||this._addGlobalListener();const r=t>0?this._scrolled.pipe(l_(t)).subscribe(i):this._scrolled.subscribe(i);return this._scrolledCount++,()=>{r.unsubscribe(),this._scrolledCount--,this._scrolledCount||this._removeGlobalListener()}}):I()}ngOnDestroy(){this._removeGlobalListener(),this.scrollContainers.forEach((t,i)=>this.deregister(i)),this._scrolled.complete()}ancestorScrolled(t,i){const r=this.getAncestorScrollContainers(t);return this.scrolled(i).pipe(cn(a=>!a||r.indexOf(a)>-1))}getAncestorScrollContainers(t){const i=[];return this.scrollContainers.forEach((r,a)=>{this._scrollableContainsElement(a,t)&&i.push(a)}),i}_getWindow(){return this._document.defaultView||window}_scrollableContainsElement(t,i){let r=ra(i),a=t.getElementRef().nativeElement;do{if(r==a)return!0}while(r=r.parentElement);return!1}_addGlobalListener(){this._globalSubscription=this._ngZone.runOutsideAngular(()=>xf(this._getWindow().document,"scroll").subscribe(()=>this._scrolled.next()))}_removeGlobalListener(){this._globalSubscription&&(this._globalSubscription.unsubscribe(),this._globalSubscription=null)}}return n.\u0275fac=function(t){return new(t||n)(v(ae),v(Pt),v(Q,8))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})(),d_=(()=>{class n{constructor(t,i,r){this._platform=t,this._change=new _e,this._changeListener=a=>{this._change.next(a)},this._document=r,i.runOutsideAngular(()=>{if(t.isBrowser){const a=this._getWindow();a.addEventListener("resize",this._changeListener),a.addEventListener("orientationchange",this._changeListener)}this.change().subscribe(()=>this._viewportSize=null)})}ngOnDestroy(){if(this._platform.isBrowser){const t=this._getWindow();t.removeEventListener("resize",this._changeListener),t.removeEventListener("orientationchange",this._changeListener)}this._change.complete()}getViewportSize(){this._viewportSize||this._updateViewportSize();const t={width:this._viewportSize.width,height:this._viewportSize.height};return this._platform.isBrowser||(this._viewportSize=null),t}getViewportRect(){const t=this.getViewportScrollPosition(),{width:i,height:r}=this.getViewportSize();return{top:t.top,left:t.left,bottom:t.top+r,right:t.left+i,height:r,width:i}}getViewportScrollPosition(){if(!this._platform.isBrowser)return{top:0,left:0};const t=this._document,i=this._getWindow(),r=t.documentElement,a=r.getBoundingClientRect();return{top:-a.top||t.body.scrollTop||i.scrollY||r.scrollTop||0,left:-a.left||t.body.scrollLeft||i.scrollX||r.scrollLeft||0}}change(t=20){return t>0?this._change.pipe(l_(t)):this._change}_getWindow(){return this._document.defaultView||window}_updateViewportSize(){const t=this._getWindow();this._viewportSize=this._platform.isBrowser?{width:t.innerWidth,height:t.innerHeight}:{width:0,height:0}}}return n.\u0275fac=function(t){return new(t||n)(v(Pt),v(ae),v(Q,8))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})(),Ef=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({}),n})(),u_=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({imports:[So,Ef,So,Ef]}),n})();class kf{attach(e){return this._attachedHost=e,e.attach(this)}detach(){let e=this._attachedHost;null!=e&&(this._attachedHost=null,e.detach())}get isAttached(){return null!=this._attachedHost}setAttachedHost(e){this._attachedHost=e}}class h_ extends kf{constructor(e,t,i,r){super(),this.component=e,this.viewContainerRef=t,this.injector=i,this.componentFactoryResolver=r}}class w4 extends kf{constructor(e,t,i,r){super(),this.templateRef=e,this.viewContainerRef=t,this.context=i,this.injector=r}get origin(){return this.templateRef.elementRef}attach(e,t=this.context){return this.context=t,super.attach(e)}detach(){return this.context=void 0,super.detach()}}class _4 extends kf{constructor(e){super(),this.element=e instanceof ht?e.nativeElement:e}}class S4 extends class D4{constructor(){this._isDisposed=!1,this.attachDomPortal=null}hasAttached(){return!!this._attachedPortal}attach(e){return e instanceof h_?(this._attachedPortal=e,this.attachComponentPortal(e)):e instanceof w4?(this._attachedPortal=e,this.attachTemplatePortal(e)):this.attachDomPortal&&e instanceof _4?(this._attachedPortal=e,this.attachDomPortal(e)):void 0}detach(){this._attachedPortal&&(this._attachedPortal.setAttachedHost(null),this._attachedPortal=null),this._invokeDisposeFn()}dispose(){this.hasAttached()&&this.detach(),this._invokeDisposeFn(),this._isDisposed=!0}setDisposeFn(e){this._disposeFn=e}_invokeDisposeFn(){this._disposeFn&&(this._disposeFn(),this._disposeFn=null)}}{constructor(e,t,i,r,a){super(),this.outletElement=e,this._componentFactoryResolver=t,this._appRef=i,this._defaultInjector=r,this.attachDomPortal=o=>{const s=o.element,l=this._document.createComment("dom-portal");s.parentNode.insertBefore(l,s),this.outletElement.appendChild(s),this._attachedPortal=o,super.setDisposeFn(()=>{l.parentNode&&l.parentNode.replaceChild(s,l)})},this._document=a}attachComponentPortal(e){const i=(e.componentFactoryResolver||this._componentFactoryResolver).resolveComponentFactory(e.component);let r;return e.viewContainerRef?(r=e.viewContainerRef.createComponent(i,e.viewContainerRef.length,e.injector||e.viewContainerRef.injector),this.setDisposeFn(()=>r.destroy())):(r=i.create(e.injector||this._defaultInjector||wt.NULL),this._appRef.attachView(r.hostView),this.setDisposeFn(()=>{this._appRef.viewCount>0&&this._appRef.detachView(r.hostView),r.destroy()})),this.outletElement.appendChild(this._getComponentRootNode(r)),this._attachedPortal=e,r}attachTemplatePortal(e){let t=e.viewContainerRef,i=t.createEmbeddedView(e.templateRef,e.context,{injector:e.injector});return i.rootNodes.forEach(r=>this.outletElement.appendChild(r)),i.detectChanges(),this.setDisposeFn(()=>{let r=t.indexOf(i);-1!==r&&t.remove(r)}),this._attachedPortal=e,i}dispose(){super.dispose(),this.outletElement.remove()}_getComponentRootNode(e){return e.hostView.rootNodes[0]}}let C4=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({}),n})();const f_=Z8();class x4{constructor(e,t){this._viewportRuler=e,this._previousHTMLStyles={top:"",left:""},this._isEnabled=!1,this._document=t}attach(){}enable(){if(this._canBeEnabled()){const e=this._document.documentElement;this._previousScrollPosition=this._viewportRuler.getViewportScrollPosition(),this._previousHTMLStyles.left=e.style.left||"",this._previousHTMLStyles.top=e.style.top||"",e.style.left=Be(-this._previousScrollPosition.left),e.style.top=Be(-this._previousScrollPosition.top),e.classList.add("cdk-global-scrollblock"),this._isEnabled=!0}}disable(){if(this._isEnabled){const e=this._document.documentElement,i=e.style,r=this._document.body.style,a=i.scrollBehavior||"",o=r.scrollBehavior||"";this._isEnabled=!1,i.left=this._previousHTMLStyles.left,i.top=this._previousHTMLStyles.top,e.classList.remove("cdk-global-scrollblock"),f_&&(i.scrollBehavior=r.scrollBehavior="auto"),window.scroll(this._previousScrollPosition.left,this._previousScrollPosition.top),f_&&(i.scrollBehavior=a,r.scrollBehavior=o)}}_canBeEnabled(){if(this._document.documentElement.classList.contains("cdk-global-scrollblock")||this._isEnabled)return!1;const t=this._document.body,i=this._viewportRuler.getViewportSize();return t.scrollHeight>i.height||t.scrollWidth>i.width}}class T4{constructor(e,t,i,r){this._scrollDispatcher=e,this._ngZone=t,this._viewportRuler=i,this._config=r,this._scrollSubscription=null,this._detach=()=>{this.disable(),this._overlayRef.hasAttached()&&this._ngZone.run(()=>this._overlayRef.detach())}}attach(e){this._overlayRef=e}enable(){if(this._scrollSubscription)return;const e=this._scrollDispatcher.scrolled(0);this._config&&this._config.threshold&&this._config.threshold>1?(this._initialScrollPosition=this._viewportRuler.getViewportScrollPosition().top,this._scrollSubscription=e.subscribe(()=>{const t=this._viewportRuler.getViewportScrollPosition().top;Math.abs(t-this._initialScrollPosition)>this._config.threshold?this._detach():this._overlayRef.updatePosition()})):this._scrollSubscription=e.subscribe(this._detach)}disable(){this._scrollSubscription&&(this._scrollSubscription.unsubscribe(),this._scrollSubscription=null)}detach(){this.disable(),this._overlayRef=null}}class p_{enable(){}disable(){}attach(){}}function Mf(n,e){return e.some(t=>n.bottom<t.top||n.top>t.bottom||n.right<t.left||n.left>t.right)}function g_(n,e){return e.some(t=>n.top<t.top||n.bottom>t.bottom||n.left<t.left||n.right>t.right)}class E4{constructor(e,t,i,r){this._scrollDispatcher=e,this._viewportRuler=t,this._ngZone=i,this._config=r,this._scrollSubscription=null}attach(e){this._overlayRef=e}enable(){this._scrollSubscription||(this._scrollSubscription=this._scrollDispatcher.scrolled(this._config?this._config.scrollThrottle:0).subscribe(()=>{if(this._overlayRef.updatePosition(),this._config&&this._config.autoClose){const t=this._overlayRef.overlayElement.getBoundingClientRect(),{width:i,height:r}=this._viewportRuler.getViewportSize();Mf(t,[{width:i,height:r,bottom:r,right:i,top:0,left:0}])&&(this.disable(),this._ngZone.run(()=>this._overlayRef.detach()))}}))}disable(){this._scrollSubscription&&(this._scrollSubscription.unsubscribe(),this._scrollSubscription=null)}detach(){this.disable(),this._overlayRef=null}}let k4=(()=>{class n{constructor(t,i,r,a){this._scrollDispatcher=t,this._viewportRuler=i,this._ngZone=r,this.noop=()=>new p_,this.close=o=>new T4(this._scrollDispatcher,this._ngZone,this._viewportRuler,o),this.block=()=>new x4(this._viewportRuler,this._document),this.reposition=o=>new E4(this._scrollDispatcher,this._viewportRuler,this._ngZone,o),this._document=a}}return n.\u0275fac=function(t){return new(t||n)(v(c_),v(d_),v(ae),v(Q))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();class M4{constructor(e){if(this.scrollStrategy=new p_,this.panelClass="",this.hasBackdrop=!1,this.backdropClass="cdk-overlay-dark-backdrop",this.disposeOnNavigation=!1,e){const t=Object.keys(e);for(const i of t)void 0!==e[i]&&(this[i]=e[i])}}}class I4{constructor(e,t){this.connectionPair=e,this.scrollableViewProperties=t}}class A4{constructor(e,t,i,r,a,o,s,l,c,d=!1){this._portalOutlet=e,this._host=t,this._pane=i,this._config=r,this._ngZone=a,this._keyboardDispatcher=o,this._document=s,this._location=l,this._outsideClickDispatcher=c,this._animationsDisabled=d,this._backdropElement=null,this._backdropClick=new _e,this._attachments=new _e,this._detachments=new _e,this._locationChanges=Je.EMPTY,this._backdropClickHandler=u=>this._backdropClick.next(u),this._backdropTransitionendHandler=u=>{this._disposeBackdrop(u.target)},this._keydownEvents=new _e,this._outsidePointerEvents=new _e,r.scrollStrategy&&(this._scrollStrategy=r.scrollStrategy,this._scrollStrategy.attach(this)),this._positionStrategy=r.positionStrategy}get overlayElement(){return this._pane}get backdropElement(){return this._backdropElement}get hostElement(){return this._host}attach(e){!this._host.parentElement&&this._previousHostParent&&this._previousHostParent.appendChild(this._host);const t=this._portalOutlet.attach(e);return this._positionStrategy&&this._positionStrategy.attach(this),this._updateStackingOrder(),this._updateElementSize(),this._updateElementDirection(),this._scrollStrategy&&this._scrollStrategy.enable(),this._ngZone.onStable.pipe(si(1)).subscribe(()=>{this.hasAttached()&&this.updatePosition()}),this._togglePointerEvents(!0),this._config.hasBackdrop&&this._attachBackdrop(),this._config.panelClass&&this._toggleClasses(this._pane,this._config.panelClass,!0),this._attachments.next(),this._keyboardDispatcher.add(this),this._config.disposeOnNavigation&&(this._locationChanges=this._location.subscribe(()=>this.dispose())),this._outsideClickDispatcher.add(this),"function"==typeof(null==t?void 0:t.onDestroy)&&t.onDestroy(()=>{this.hasAttached()&&this._ngZone.runOutsideAngular(()=>Promise.resolve().then(()=>this.detach()))}),t}detach(){if(!this.hasAttached())return;this.detachBackdrop(),this._togglePointerEvents(!1),this._positionStrategy&&this._positionStrategy.detach&&this._positionStrategy.detach(),this._scrollStrategy&&this._scrollStrategy.disable();const e=this._portalOutlet.detach();return this._detachments.next(),this._keyboardDispatcher.remove(this),this._detachContentWhenStable(),this._locationChanges.unsubscribe(),this._outsideClickDispatcher.remove(this),e}dispose(){var e;const t=this.hasAttached();this._positionStrategy&&this._positionStrategy.dispose(),this._disposeScrollStrategy(),this._disposeBackdrop(this._backdropElement),this._locationChanges.unsubscribe(),this._keyboardDispatcher.remove(this),this._portalOutlet.dispose(),this._attachments.complete(),this._backdropClick.complete(),this._keydownEvents.complete(),this._outsidePointerEvents.complete(),this._outsideClickDispatcher.remove(this),null===(e=this._host)||void 0===e||e.remove(),this._previousHostParent=this._pane=this._host=null,t&&this._detachments.next(),this._detachments.complete()}hasAttached(){return this._portalOutlet.hasAttached()}backdropClick(){return this._backdropClick}attachments(){return this._attachments}detachments(){return this._detachments}keydownEvents(){return this._keydownEvents}outsidePointerEvents(){return this._outsidePointerEvents}getConfig(){return this._config}updatePosition(){this._positionStrategy&&this._positionStrategy.apply()}updatePositionStrategy(e){e!==this._positionStrategy&&(this._positionStrategy&&this._positionStrategy.dispose(),this._positionStrategy=e,this.hasAttached()&&(e.attach(this),this.updatePosition()))}updateSize(e){this._config=Object.assign(Object.assign({},this._config),e),this._updateElementSize()}setDirection(e){this._config=Object.assign(Object.assign({},this._config),{direction:e}),this._updateElementDirection()}addPanelClass(e){this._pane&&this._toggleClasses(this._pane,e,!0)}removePanelClass(e){this._pane&&this._toggleClasses(this._pane,e,!1)}getDirection(){const e=this._config.direction;return e?"string"==typeof e?e:e.value:"ltr"}updateScrollStrategy(e){e!==this._scrollStrategy&&(this._disposeScrollStrategy(),this._scrollStrategy=e,this.hasAttached()&&(e.attach(this),e.enable()))}_updateElementDirection(){this._host.setAttribute("dir",this.getDirection())}_updateElementSize(){if(!this._pane)return;const e=this._pane.style;e.width=Be(this._config.width),e.height=Be(this._config.height),e.minWidth=Be(this._config.minWidth),e.minHeight=Be(this._config.minHeight),e.maxWidth=Be(this._config.maxWidth),e.maxHeight=Be(this._config.maxHeight)}_togglePointerEvents(e){this._pane.style.pointerEvents=e?"":"none"}_attachBackdrop(){const e="cdk-overlay-backdrop-showing";this._backdropElement=this._document.createElement("div"),this._backdropElement.classList.add("cdk-overlay-backdrop"),this._animationsDisabled&&this._backdropElement.classList.add("cdk-overlay-backdrop-noop-animation"),this._config.backdropClass&&this._toggleClasses(this._backdropElement,this._config.backdropClass,!0),this._host.parentElement.insertBefore(this._backdropElement,this._host),this._backdropElement.addEventListener("click",this._backdropClickHandler),this._animationsDisabled||"undefined"==typeof requestAnimationFrame?this._backdropElement.classList.add(e):this._ngZone.runOutsideAngular(()=>{requestAnimationFrame(()=>{this._backdropElement&&this._backdropElement.classList.add(e)})})}_updateStackingOrder(){this._host.nextSibling&&this._host.parentNode.appendChild(this._host)}detachBackdrop(){const e=this._backdropElement;if(e){if(this._animationsDisabled)return void this._disposeBackdrop(e);e.classList.remove("cdk-overlay-backdrop-showing"),this._ngZone.runOutsideAngular(()=>{e.addEventListener("transitionend",this._backdropTransitionendHandler)}),e.style.pointerEvents="none",this._backdropTimeout=this._ngZone.runOutsideAngular(()=>setTimeout(()=>{this._disposeBackdrop(e)},500))}}_toggleClasses(e,t,i){const r=$l(t||[]).filter(a=>!!a);r.length&&(i?e.classList.add(...r):e.classList.remove(...r))}_detachContentWhenStable(){this._ngZone.runOutsideAngular(()=>{const e=this._ngZone.onStable.pipe(dn(Ap(this._attachments,this._detachments))).subscribe(()=>{(!this._pane||!this._host||0===this._pane.children.length)&&(this._pane&&this._config.panelClass&&this._toggleClasses(this._pane,this._config.panelClass,!1),this._host&&this._host.parentElement&&(this._previousHostParent=this._host.parentElement,this._host.remove()),e.unsubscribe())})})}_disposeScrollStrategy(){const e=this._scrollStrategy;e&&(e.disable(),e.detach&&e.detach())}_disposeBackdrop(e){e&&(e.removeEventListener("click",this._backdropClickHandler),e.removeEventListener("transitionend",this._backdropTransitionendHandler),e.remove(),this._backdropElement===e&&(this._backdropElement=null)),this._backdropTimeout&&(clearTimeout(this._backdropTimeout),this._backdropTimeout=void 0)}}let m_=(()=>{class n{constructor(t,i){this._platform=i,this._document=t}ngOnDestroy(){var t;null===(t=this._containerElement)||void 0===t||t.remove()}getContainerElement(){return this._containerElement||this._createContainer(),this._containerElement}_createContainer(){const t="cdk-overlay-container";if(this._platform.isBrowser||mf()){const r=this._document.querySelectorAll(`.${t}[platform="server"], .${t}[platform="test"]`);for(let a=0;a<r.length;a++)r[a].remove()}const i=this._document.createElement("div");i.classList.add(t),mf()?i.setAttribute("platform","test"):this._platform.isBrowser||i.setAttribute("platform","server"),this._document.body.appendChild(i),this._containerElement=i}}return n.\u0275fac=function(t){return new(t||n)(v(Q),v(Pt))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})();const b_="cdk-overlay-connected-position-bounding-box",R4=/([A-Za-z%]+)$/;class P4{constructor(e,t,i,r,a){this._viewportRuler=t,this._document=i,this._platform=r,this._overlayContainer=a,this._lastBoundingBoxSize={width:0,height:0},this._isPushed=!1,this._canPush=!0,this._growAfterOpen=!1,this._hasFlexibleDimensions=!0,this._positionLocked=!1,this._viewportMargin=0,this._scrollables=[],this._preferredPositions=[],this._positionChanges=new _e,this._resizeSubscription=Je.EMPTY,this._offsetX=0,this._offsetY=0,this._appliedPanelClasses=[],this.positionChanges=this._positionChanges,this.setOrigin(e)}get positions(){return this._preferredPositions}attach(e){this._validatePositions(),e.hostElement.classList.add(b_),this._overlayRef=e,this._boundingBox=e.hostElement,this._pane=e.overlayElement,this._isDisposed=!1,this._isInitialRender=!0,this._lastPosition=null,this._resizeSubscription.unsubscribe(),this._resizeSubscription=this._viewportRuler.change().subscribe(()=>{this._isInitialRender=!0,this.apply()})}apply(){if(this._isDisposed||!this._platform.isBrowser)return;if(!this._isInitialRender&&this._positionLocked&&this._lastPosition)return void this.reapplyLastPosition();this._clearPanelClasses(),this._resetOverlayElementStyles(),this._resetBoundingBoxStyles(),this._viewportRect=this._getNarrowedViewportRect(),this._originRect=this._getOriginRect(),this._overlayRect=this._pane.getBoundingClientRect(),this._containerRect=this._overlayContainer.getContainerElement().getBoundingClientRect();const e=this._originRect,t=this._overlayRect,i=this._viewportRect,r=this._containerRect,a=[];let o;for(let s of this._preferredPositions){let l=this._getOriginPoint(e,r,s),c=this._getOverlayPoint(l,t,s),d=this._getOverlayFit(c,t,i,s);if(d.isCompletelyWithinViewport)return this._isPushed=!1,void this._applyPosition(s,l);this._canFitWithFlexibleDimensions(d,c,i)?a.push({position:s,origin:l,overlayRect:t,boundingBoxRect:this._calculateBoundingBoxRect(l,s)}):(!o||o.overlayFit.visibleArea<d.visibleArea)&&(o={overlayFit:d,overlayPoint:c,originPoint:l,position:s,overlayRect:t})}if(a.length){let s=null,l=-1;for(const c of a){const d=c.boundingBoxRect.width*c.boundingBoxRect.height*(c.position.weight||1);d>l&&(l=d,s=c)}return this._isPushed=!1,void this._applyPosition(s.position,s.origin)}if(this._canPush)return this._isPushed=!0,void this._applyPosition(o.position,o.originPoint);this._applyPosition(o.position,o.originPoint)}detach(){this._clearPanelClasses(),this._lastPosition=null,this._previousPushAmount=null,this._resizeSubscription.unsubscribe()}dispose(){this._isDisposed||(this._boundingBox&&$i(this._boundingBox.style,{top:"",left:"",right:"",bottom:"",height:"",width:"",alignItems:"",justifyContent:""}),this._pane&&this._resetOverlayElementStyles(),this._overlayRef&&this._overlayRef.hostElement.classList.remove(b_),this.detach(),this._positionChanges.complete(),this._overlayRef=this._boundingBox=null,this._isDisposed=!0)}reapplyLastPosition(){if(this._isDisposed||!this._platform.isBrowser)return;const e=this._lastPosition;if(e){this._originRect=this._getOriginRect(),this._overlayRect=this._pane.getBoundingClientRect(),this._viewportRect=this._getNarrowedViewportRect(),this._containerRect=this._overlayContainer.getContainerElement().getBoundingClientRect();const t=this._getOriginPoint(this._originRect,this._containerRect,e);this._applyPosition(e,t)}else this.apply()}withScrollableContainers(e){return this._scrollables=e,this}withPositions(e){return this._preferredPositions=e,-1===e.indexOf(this._lastPosition)&&(this._lastPosition=null),this._validatePositions(),this}withViewportMargin(e){return this._viewportMargin=e,this}withFlexibleDimensions(e=!0){return this._hasFlexibleDimensions=e,this}withGrowAfterOpen(e=!0){return this._growAfterOpen=e,this}withPush(e=!0){return this._canPush=e,this}withLockedPosition(e=!0){return this._positionLocked=e,this}setOrigin(e){return this._origin=e,this}withDefaultOffsetX(e){return this._offsetX=e,this}withDefaultOffsetY(e){return this._offsetY=e,this}withTransformOriginOn(e){return this._transformOriginSelector=e,this}_getOriginPoint(e,t,i){let r,a;if("center"==i.originX)r=e.left+e.width/2;else{const o=this._isRtl()?e.right:e.left,s=this._isRtl()?e.left:e.right;r="start"==i.originX?o:s}return t.left<0&&(r-=t.left),a="center"==i.originY?e.top+e.height/2:"top"==i.originY?e.top:e.bottom,t.top<0&&(a-=t.top),{x:r,y:a}}_getOverlayPoint(e,t,i){let r,a;return r="center"==i.overlayX?-t.width/2:"start"===i.overlayX?this._isRtl()?-t.width:0:this._isRtl()?0:-t.width,a="center"==i.overlayY?-t.height/2:"top"==i.overlayY?0:-t.height,{x:e.x+r,y:e.y+a}}_getOverlayFit(e,t,i,r){const a=v_(t);let{x:o,y:s}=e,l=this._getOffset(r,"x"),c=this._getOffset(r,"y");l&&(o+=l),c&&(s+=c);let h=0-s,f=s+a.height-i.height,p=this._subtractOverflows(a.width,0-o,o+a.width-i.width),g=this._subtractOverflows(a.height,h,f),m=p*g;return{visibleArea:m,isCompletelyWithinViewport:a.width*a.height===m,fitsInViewportVertically:g===a.height,fitsInViewportHorizontally:p==a.width}}_canFitWithFlexibleDimensions(e,t,i){if(this._hasFlexibleDimensions){const r=i.bottom-t.y,a=i.right-t.x,o=y_(this._overlayRef.getConfig().minHeight),s=y_(this._overlayRef.getConfig().minWidth),c=e.fitsInViewportHorizontally||null!=s&&s<=a;return(e.fitsInViewportVertically||null!=o&&o<=r)&&c}return!1}_pushOverlayOnScreen(e,t,i){if(this._previousPushAmount&&this._positionLocked)return{x:e.x+this._previousPushAmount.x,y:e.y+this._previousPushAmount.y};const r=v_(t),a=this._viewportRect,o=Math.max(e.x+r.width-a.width,0),s=Math.max(e.y+r.height-a.height,0),l=Math.max(a.top-i.top-e.y,0),c=Math.max(a.left-i.left-e.x,0);let d=0,u=0;return d=r.width<=a.width?c||-o:e.x<this._viewportMargin?a.left-i.left-e.x:0,u=r.height<=a.height?l||-s:e.y<this._viewportMargin?a.top-i.top-e.y:0,this._previousPushAmount={x:d,y:u},{x:e.x+d,y:e.y+u}}_applyPosition(e,t){if(this._setTransformOrigin(e),this._setOverlayElementStyles(t,e),this._setBoundingBoxStyles(t,e),e.panelClass&&this._addPanelClasses(e.panelClass),this._lastPosition=e,this._positionChanges.observers.length){const i=this._getScrollVisibility(),r=new I4(e,i);this._positionChanges.next(r)}this._isInitialRender=!1}_setTransformOrigin(e){if(!this._transformOriginSelector)return;const t=this._boundingBox.querySelectorAll(this._transformOriginSelector);let i,r=e.overlayY;i="center"===e.overlayX?"center":this._isRtl()?"start"===e.overlayX?"right":"left":"start"===e.overlayX?"left":"right";for(let a=0;a<t.length;a++)t[a].style.transformOrigin=`${i} ${r}`}_calculateBoundingBoxRect(e,t){const i=this._viewportRect,r=this._isRtl();let a,o,s,d,u,h;if("top"===t.overlayY)o=e.y,a=i.height-o+this._viewportMargin;else if("bottom"===t.overlayY)s=i.height-e.y+2*this._viewportMargin,a=i.height-s+this._viewportMargin;else{const f=Math.min(i.bottom-e.y+i.top,e.y),p=this._lastBoundingBoxSize.height;a=2*f,o=e.y-f,a>p&&!this._isInitialRender&&!this._growAfterOpen&&(o=e.y-p/2)}if("end"===t.overlayX&&!r||"start"===t.overlayX&&r)h=i.width-e.x+this._viewportMargin,d=e.x-this._viewportMargin;else if("start"===t.overlayX&&!r||"end"===t.overlayX&&r)u=e.x,d=i.right-e.x;else{const f=Math.min(i.right-e.x+i.left,e.x),p=this._lastBoundingBoxSize.width;d=2*f,u=e.x-f,d>p&&!this._isInitialRender&&!this._growAfterOpen&&(u=e.x-p/2)}return{top:o,left:u,bottom:s,right:h,width:d,height:a}}_setBoundingBoxStyles(e,t){const i=this._calculateBoundingBoxRect(e,t);!this._isInitialRender&&!this._growAfterOpen&&(i.height=Math.min(i.height,this._lastBoundingBoxSize.height),i.width=Math.min(i.width,this._lastBoundingBoxSize.width));const r={};if(this._hasExactPosition())r.top=r.left="0",r.bottom=r.right=r.maxHeight=r.maxWidth="",r.width=r.height="100%";else{const a=this._overlayRef.getConfig().maxHeight,o=this._overlayRef.getConfig().maxWidth;r.height=Be(i.height),r.top=Be(i.top),r.bottom=Be(i.bottom),r.width=Be(i.width),r.left=Be(i.left),r.right=Be(i.right),r.alignItems="center"===t.overlayX?"center":"end"===t.overlayX?"flex-end":"flex-start",r.justifyContent="center"===t.overlayY?"center":"bottom"===t.overlayY?"flex-end":"flex-start",a&&(r.maxHeight=Be(a)),o&&(r.maxWidth=Be(o))}this._lastBoundingBoxSize=i,$i(this._boundingBox.style,r)}_resetBoundingBoxStyles(){$i(this._boundingBox.style,{top:"0",left:"0",right:"0",bottom:"0",height:"",width:"",alignItems:"",justifyContent:""})}_resetOverlayElementStyles(){$i(this._pane.style,{top:"",left:"",bottom:"",right:"",position:"",transform:""})}_setOverlayElementStyles(e,t){const i={},r=this._hasExactPosition(),a=this._hasFlexibleDimensions,o=this._overlayRef.getConfig();if(r){const d=this._viewportRuler.getViewportScrollPosition();$i(i,this._getExactOverlayY(t,e,d)),$i(i,this._getExactOverlayX(t,e,d))}else i.position="static";let s="",l=this._getOffset(t,"x"),c=this._getOffset(t,"y");l&&(s+=`translateX(${l}px) `),c&&(s+=`translateY(${c}px)`),i.transform=s.trim(),o.maxHeight&&(r?i.maxHeight=Be(o.maxHeight):a&&(i.maxHeight="")),o.maxWidth&&(r?i.maxWidth=Be(o.maxWidth):a&&(i.maxWidth="")),$i(this._pane.style,i)}_getExactOverlayY(e,t,i){let r={top:"",bottom:""},a=this._getOverlayPoint(t,this._overlayRect,e);return this._isPushed&&(a=this._pushOverlayOnScreen(a,this._overlayRect,i)),"bottom"===e.overlayY?r.bottom=this._document.documentElement.clientHeight-(a.y+this._overlayRect.height)+"px":r.top=Be(a.y),r}_getExactOverlayX(e,t,i){let o,r={left:"",right:""},a=this._getOverlayPoint(t,this._overlayRect,e);return this._isPushed&&(a=this._pushOverlayOnScreen(a,this._overlayRect,i)),o=this._isRtl()?"end"===e.overlayX?"left":"right":"end"===e.overlayX?"right":"left","right"===o?r.right=this._document.documentElement.clientWidth-(a.x+this._overlayRect.width)+"px":r.left=Be(a.x),r}_getScrollVisibility(){const e=this._getOriginRect(),t=this._pane.getBoundingClientRect(),i=this._scrollables.map(r=>r.getElementRef().nativeElement.getBoundingClientRect());return{isOriginClipped:g_(e,i),isOriginOutsideView:Mf(e,i),isOverlayClipped:g_(t,i),isOverlayOutsideView:Mf(t,i)}}_subtractOverflows(e,...t){return t.reduce((i,r)=>i-Math.max(r,0),e)}_getNarrowedViewportRect(){const e=this._document.documentElement.clientWidth,t=this._document.documentElement.clientHeight,i=this._viewportRuler.getViewportScrollPosition();return{top:i.top+this._viewportMargin,left:i.left+this._viewportMargin,right:i.left+e-this._viewportMargin,bottom:i.top+t-this._viewportMargin,width:e-2*this._viewportMargin,height:t-2*this._viewportMargin}}_isRtl(){return"rtl"===this._overlayRef.getDirection()}_hasExactPosition(){return!this._hasFlexibleDimensions||this._isPushed}_getOffset(e,t){return"x"===t?null==e.offsetX?this._offsetX:e.offsetX:null==e.offsetY?this._offsetY:e.offsetY}_validatePositions(){}_addPanelClasses(e){this._pane&&$l(e).forEach(t=>{""!==t&&-1===this._appliedPanelClasses.indexOf(t)&&(this._appliedPanelClasses.push(t),this._pane.classList.add(t))})}_clearPanelClasses(){this._pane&&(this._appliedPanelClasses.forEach(e=>{this._pane.classList.remove(e)}),this._appliedPanelClasses=[])}_getOriginRect(){const e=this._origin;if(e instanceof ht)return e.nativeElement.getBoundingClientRect();if(e instanceof Element)return e.getBoundingClientRect();const t=e.width||0,i=e.height||0;return{top:e.y,bottom:e.y+i,left:e.x,right:e.x+t,height:i,width:t}}}function $i(n,e){for(let t in e)e.hasOwnProperty(t)&&(n[t]=e[t]);return n}function y_(n){if("number"!=typeof n&&null!=n){const[e,t]=n.split(R4);return t&&"px"!==t?null:parseFloat(e)}return n||null}function v_(n){return{top:Math.floor(n.top),right:Math.floor(n.right),bottom:Math.floor(n.bottom),left:Math.floor(n.left),width:Math.floor(n.width),height:Math.floor(n.height)}}const w_="cdk-global-overlay-wrapper";class L4{constructor(){this._cssPosition="static",this._topOffset="",this._bottomOffset="",this._alignItems="",this._xPosition="",this._xOffset="",this._width="",this._height="",this._isDisposed=!1}attach(e){const t=e.getConfig();this._overlayRef=e,this._width&&!t.width&&e.updateSize({width:this._width}),this._height&&!t.height&&e.updateSize({height:this._height}),e.hostElement.classList.add(w_),this._isDisposed=!1}top(e=""){return this._bottomOffset="",this._topOffset=e,this._alignItems="flex-start",this}left(e=""){return this._xOffset=e,this._xPosition="left",this}bottom(e=""){return this._topOffset="",this._bottomOffset=e,this._alignItems="flex-end",this}right(e=""){return this._xOffset=e,this._xPosition="right",this}start(e=""){return this._xOffset=e,this._xPosition="start",this}end(e=""){return this._xOffset=e,this._xPosition="end",this}width(e=""){return this._overlayRef?this._overlayRef.updateSize({width:e}):this._width=e,this}height(e=""){return this._overlayRef?this._overlayRef.updateSize({height:e}):this._height=e,this}centerHorizontally(e=""){return this.left(e),this._xPosition="center",this}centerVertically(e=""){return this.top(e),this._alignItems="center",this}apply(){if(!this._overlayRef||!this._overlayRef.hasAttached())return;const e=this._overlayRef.overlayElement.style,t=this._overlayRef.hostElement.style,i=this._overlayRef.getConfig(),{width:r,height:a,maxWidth:o,maxHeight:s}=i,l=!("100%"!==r&&"100vw"!==r||o&&"100%"!==o&&"100vw"!==o),c=!("100%"!==a&&"100vh"!==a||s&&"100%"!==s&&"100vh"!==s),d=this._xPosition,u=this._xOffset,h="rtl"===this._overlayRef.getConfig().direction;let f="",p="",g="";l?g="flex-start":"center"===d?(g="center",h?p=u:f=u):h?"left"===d||"end"===d?(g="flex-end",f=u):("right"===d||"start"===d)&&(g="flex-start",p=u):"left"===d||"start"===d?(g="flex-start",f=u):("right"===d||"end"===d)&&(g="flex-end",p=u),e.position=this._cssPosition,e.marginLeft=l?"0":f,e.marginTop=c?"0":this._topOffset,e.marginBottom=this._bottomOffset,e.marginRight=l?"0":p,t.justifyContent=g,t.alignItems=c?"flex-start":this._alignItems}dispose(){if(this._isDisposed||!this._overlayRef)return;const e=this._overlayRef.overlayElement.style,t=this._overlayRef.hostElement,i=t.style;t.classList.remove(w_),i.justifyContent=i.alignItems=e.marginTop=e.marginBottom=e.marginLeft=e.marginRight=e.position="",this._overlayRef=null,this._isDisposed=!0}}let O4=(()=>{class n{constructor(t,i,r,a){this._viewportRuler=t,this._document=i,this._platform=r,this._overlayContainer=a}global(){return new L4}flexibleConnectedTo(t){return new P4(t,this._viewportRuler,this._document,this._platform,this._overlayContainer)}}return n.\u0275fac=function(t){return new(t||n)(v(d_),v(Q),v(Pt),v(m_))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})(),__=(()=>{class n{constructor(t){this._attachedOverlays=[],this._document=t}ngOnDestroy(){this.detach()}add(t){this.remove(t),this._attachedOverlays.push(t)}remove(t){const i=this._attachedOverlays.indexOf(t);i>-1&&this._attachedOverlays.splice(i,1),0===this._attachedOverlays.length&&this.detach()}}return n.\u0275fac=function(t){return new(t||n)(v(Q))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})(),N4=(()=>{class n extends __{constructor(t,i){super(t),this._ngZone=i,this._keydownListener=r=>{const a=this._attachedOverlays;for(let o=a.length-1;o>-1;o--)if(a[o]._keydownEvents.observers.length>0){const s=a[o]._keydownEvents;this._ngZone?this._ngZone.run(()=>s.next(r)):s.next(r);break}}}add(t){super.add(t),this._isAttached||(this._ngZone?this._ngZone.runOutsideAngular(()=>this._document.body.addEventListener("keydown",this._keydownListener)):this._document.body.addEventListener("keydown",this._keydownListener),this._isAttached=!0)}detach(){this._isAttached&&(this._document.body.removeEventListener("keydown",this._keydownListener),this._isAttached=!1)}}return n.\u0275fac=function(t){return new(t||n)(v(Q),v(ae,8))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})(),F4=(()=>{class n extends __{constructor(t,i,r){super(t),this._platform=i,this._ngZone=r,this._cursorStyleIsSet=!1,this._pointerDownListener=a=>{this._pointerDownEventTarget=Ui(a)},this._clickListener=a=>{const o=Ui(a),s="click"===a.type&&this._pointerDownEventTarget?this._pointerDownEventTarget:o;this._pointerDownEventTarget=null;const l=this._attachedOverlays.slice();for(let c=l.length-1;c>-1;c--){const d=l[c];if(d._outsidePointerEvents.observers.length<1||!d.hasAttached())continue;if(d.overlayElement.contains(o)||d.overlayElement.contains(s))break;const u=d._outsidePointerEvents;this._ngZone?this._ngZone.run(()=>u.next(a)):u.next(a)}}}add(t){if(super.add(t),!this._isAttached){const i=this._document.body;this._ngZone?this._ngZone.runOutsideAngular(()=>this._addEventListeners(i)):this._addEventListeners(i),this._platform.IOS&&!this._cursorStyleIsSet&&(this._cursorOriginalValue=i.style.cursor,i.style.cursor="pointer",this._cursorStyleIsSet=!0),this._isAttached=!0}}detach(){if(this._isAttached){const t=this._document.body;t.removeEventListener("pointerdown",this._pointerDownListener,!0),t.removeEventListener("click",this._clickListener,!0),t.removeEventListener("auxclick",this._clickListener,!0),t.removeEventListener("contextmenu",this._clickListener,!0),this._platform.IOS&&this._cursorStyleIsSet&&(t.style.cursor=this._cursorOriginalValue,this._cursorStyleIsSet=!1),this._isAttached=!1}}_addEventListeners(t){t.addEventListener("pointerdown",this._pointerDownListener,!0),t.addEventListener("click",this._clickListener,!0),t.addEventListener("auxclick",this._clickListener,!0),t.addEventListener("contextmenu",this._clickListener,!0)}}return n.\u0275fac=function(t){return new(t||n)(v(Q),v(Pt),v(ae,8))},n.\u0275prov=k({token:n,factory:n.\u0275fac,providedIn:"root"}),n})(),j4=0,Zl=(()=>{class n{constructor(t,i,r,a,o,s,l,c,d,u,h,f){this.scrollStrategies=t,this._overlayContainer=i,this._componentFactoryResolver=r,this._positionBuilder=a,this._keyboardDispatcher=o,this._injector=s,this._ngZone=l,this._document=c,this._directionality=d,this._location=u,this._outsideClickDispatcher=h,this._animationsModuleType=f}create(t){const i=this._createHostElement(),r=this._createPaneElement(i),a=this._createPortalOutlet(r),o=new M4(t);return o.direction=o.direction||this._directionality.value,new A4(a,i,r,o,this._ngZone,this._keyboardDispatcher,this._document,this._location,this._outsideClickDispatcher,"NoopAnimations"===this._animationsModuleType)}position(){return this._positionBuilder}_createPaneElement(t){const i=this._document.createElement("div");return i.id="cdk-overlay-"+j4++,i.classList.add("cdk-overlay-pane"),t.appendChild(i),i}_createHostElement(){const t=this._document.createElement("div");return this._overlayContainer.getContainerElement().appendChild(t),t}_createPortalOutlet(t){return this._appRef||(this._appRef=this._injector.get(qr)),new S4(t,this._componentFactoryResolver,this._appRef,this._injector,this._document)}}return n.\u0275fac=function(t){return new(t||n)(v(k4),v(m_),v(_r),v(O4),v(N4),v(wt),v(ae),v(Q),v(P1),v(el),v(F4),v(Fi,8))},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})();const z4={provide:new E("cdk-connected-overlay-scroll-strategy"),deps:[Zl],useFactory:function H4(n){return()=>n.scrollStrategies.reposition()}};let W4=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({providers:[Zl,z4],imports:[So,C4,u_,u_]}),n})();class D_{}const Gn="*";function C_(n,e=null){return{type:2,steps:n,options:e}}function hi(n){return{type:6,styles:n,offset:null}}function E_(n){Promise.resolve().then(n)}class ko{constructor(e=0,t=0){this._onDoneFns=[],this._onStartFns=[],this._onDestroyFns=[],this._originalOnDoneFns=[],this._originalOnStartFns=[],this._started=!1,this._destroyed=!1,this._finished=!1,this._position=0,this.parentPlayer=null,this.totalTime=e+t}_onFinish(){this._finished||(this._finished=!0,this._onDoneFns.forEach(e=>e()),this._onDoneFns=[])}onStart(e){this._originalOnStartFns.push(e),this._onStartFns.push(e)}onDone(e){this._originalOnDoneFns.push(e),this._onDoneFns.push(e)}onDestroy(e){this._onDestroyFns.push(e)}hasStarted(){return this._started}init(){}play(){this.hasStarted()||(this._onStart(),this.triggerMicrotask()),this._started=!0}triggerMicrotask(){E_(()=>this._onFinish())}_onStart(){this._onStartFns.forEach(e=>e()),this._onStartFns=[]}pause(){}restart(){}finish(){this._onFinish()}destroy(){this._destroyed||(this._destroyed=!0,this.hasStarted()||this._onStart(),this.finish(),this._onDestroyFns.forEach(e=>e()),this._onDestroyFns=[])}reset(){this._started=!1,this._finished=!1,this._onStartFns=this._originalOnStartFns,this._onDoneFns=this._originalOnDoneFns}setPosition(e){this._position=this.totalTime?e*this.totalTime:1}getPosition(){return this.totalTime?this._position/this.totalTime:1}triggerCallback(e){const t="start"==e?this._onStartFns:this._onDoneFns;t.forEach(i=>i()),t.length=0}}class k_{constructor(e){this._onDoneFns=[],this._onStartFns=[],this._finished=!1,this._started=!1,this._destroyed=!1,this._onDestroyFns=[],this.parentPlayer=null,this.totalTime=0,this.players=e;let t=0,i=0,r=0;const a=this.players.length;0==a?E_(()=>this._onFinish()):this.players.forEach(o=>{o.onDone(()=>{++t==a&&this._onFinish()}),o.onDestroy(()=>{++i==a&&this._onDestroy()}),o.onStart(()=>{++r==a&&this._onStart()})}),this.totalTime=this.players.reduce((o,s)=>Math.max(o,s.totalTime),0)}_onFinish(){this._finished||(this._finished=!0,this._onDoneFns.forEach(e=>e()),this._onDoneFns=[])}init(){this.players.forEach(e=>e.init())}onStart(e){this._onStartFns.push(e)}_onStart(){this.hasStarted()||(this._started=!0,this._onStartFns.forEach(e=>e()),this._onStartFns=[])}onDone(e){this._onDoneFns.push(e)}onDestroy(e){this._onDestroyFns.push(e)}hasStarted(){return this._started}play(){this.parentPlayer||this.init(),this._onStart(),this.players.forEach(e=>e.play())}pause(){this.players.forEach(e=>e.pause())}restart(){this.players.forEach(e=>e.restart())}finish(){this._onFinish(),this.players.forEach(e=>e.finish())}destroy(){this._onDestroy()}_onDestroy(){this._destroyed||(this._destroyed=!0,this._onFinish(),this.players.forEach(e=>e.destroy()),this._onDestroyFns.forEach(e=>e()),this._onDestroyFns=[])}reset(){this.players.forEach(e=>e.reset()),this._destroyed=!1,this._finished=!1,this._started=!1}setPosition(e){const t=e*this.totalTime;this.players.forEach(i=>{const r=i.totalTime?Math.min(1,t/i.totalTime):1;i.setPosition(r)})}getPosition(){const e=this.players.reduce((t,i)=>null===t||i.totalTime>t.totalTime?i:t,null);return null!=e?e.getPosition():0}beforeDestroy(){this.players.forEach(e=>{e.beforeDestroy&&e.beforeDestroy()})}triggerCallback(e){const t="start"==e?this._onStartFns:this._onDoneFns;t.forEach(i=>i()),t.length=0}}const $4=["tooltip"],M_="tooltip-panel",I_=Gl({passive:!0}),A_=new E("mat-tooltip-scroll-strategy"),K4={provide:A_,deps:[Zl],useFactory:function Q4(n){return()=>n.scrollStrategies.reposition({scrollThrottle:20})}},X4=new E("mat-tooltip-default-options",{providedIn:"root",factory:function J4(){return{showDelay:0,hideDelay:0,touchendHideDelay:1500}}});let Z4=(()=>{class n{constructor(t,i,r,a,o,s,l,c,d,u,h,f){this._overlay=t,this._elementRef=i,this._scrollDispatcher=r,this._viewContainerRef=a,this._ngZone=o,this._platform=s,this._ariaDescriber=l,this._focusMonitor=c,this._dir=u,this._defaultOptions=h,this._position="below",this._disabled=!1,this._viewInitialized=!1,this._pointerExitEventsInitialized=!1,this._viewportMargin=8,this._cssClassPrefix="mat",this._showDelay=this._defaultOptions.showDelay,this._hideDelay=this._defaultOptions.hideDelay,this.touchGestures="auto",this._message="",this._passiveListeners=[],this._destroyed=new _e,this._scrollStrategy=d,this._document=f,h&&(h.position&&(this.position=h.position),h.touchGestures&&(this.touchGestures=h.touchGestures)),u.change.pipe(dn(this._destroyed)).subscribe(()=>{this._overlayRef&&this._updatePosition(this._overlayRef)})}get position(){return this._position}set position(t){var i;t!==this._position&&(this._position=t,this._overlayRef&&(this._updatePosition(this._overlayRef),null===(i=this._tooltipInstance)||void 0===i||i.show(0),this._overlayRef.updatePosition()))}get disabled(){return this._disabled}set disabled(t){this._disabled=bf(t),this._disabled?this.hide(0):this._setupPointerEnterEventsIfNeeded()}get showDelay(){return this._showDelay}set showDelay(t){this._showDelay=F1(t)}get hideDelay(){return this._hideDelay}set hideDelay(t){this._hideDelay=F1(t),this._tooltipInstance&&(this._tooltipInstance._mouseLeaveHideDelay=this._hideDelay)}get message(){return this._message}set message(t){this._ariaDescriber.removeDescription(this._elementRef.nativeElement,this._message,"tooltip"),this._message=null!=t?String(t).trim():"",!this._message&&this._isTooltipVisible()?this.hide(0):(this._setupPointerEnterEventsIfNeeded(),this._updateTooltipMessage(),this._ngZone.runOutsideAngular(()=>{Promise.resolve().then(()=>{this._ariaDescriber.describe(this._elementRef.nativeElement,this.message,"tooltip")})}))}get tooltipClass(){return this._tooltipClass}set tooltipClass(t){this._tooltipClass=t,this._tooltipInstance&&this._setTooltipClass(this._tooltipClass)}ngAfterViewInit(){this._viewInitialized=!0,this._setupPointerEnterEventsIfNeeded(),this._focusMonitor.monitor(this._elementRef).pipe(dn(this._destroyed)).subscribe(t=>{t?"keyboard"===t&&this._ngZone.run(()=>this.show()):this._ngZone.run(()=>this.hide(0))})}ngOnDestroy(){const t=this._elementRef.nativeElement;clearTimeout(this._touchstartTimeout),this._overlayRef&&(this._overlayRef.dispose(),this._tooltipInstance=null),this._passiveListeners.forEach(([i,r])=>{t.removeEventListener(i,r,I_)}),this._passiveListeners.length=0,this._destroyed.next(),this._destroyed.complete(),this._ariaDescriber.removeDescription(t,this.message,"tooltip"),this._focusMonitor.stopMonitoring(t)}show(t=this.showDelay){var i;if(this.disabled||!this.message||this._isTooltipVisible())return void(null===(i=this._tooltipInstance)||void 0===i||i._cancelPendingAnimations());const r=this._createOverlay();this._detach(),this._portal=this._portal||new h_(this._tooltipComponent,this._viewContainerRef);const a=this._tooltipInstance=r.attach(this._portal).instance;a._triggerElement=this._elementRef.nativeElement,a._mouseLeaveHideDelay=this._hideDelay,a.afterHidden().pipe(dn(this._destroyed)).subscribe(()=>this._detach()),this._setTooltipClass(this._tooltipClass),this._updateTooltipMessage(),a.show(t)}hide(t=this.hideDelay){const i=this._tooltipInstance;i&&(i.isVisible()?i.hide(t):(i._cancelPendingAnimations(),this._detach()))}toggle(){this._isTooltipVisible()?this.hide():this.show()}_isTooltipVisible(){return!!this._tooltipInstance&&this._tooltipInstance.isVisible()}_createOverlay(){var t;if(this._overlayRef)return this._overlayRef;const i=this._scrollDispatcher.getAncestorScrollContainers(this._elementRef),r=this._overlay.position().flexibleConnectedTo(this._elementRef).withTransformOriginOn(`.${this._cssClassPrefix}-tooltip`).withFlexibleDimensions(!1).withViewportMargin(this._viewportMargin).withScrollableContainers(i);return r.positionChanges.pipe(dn(this._destroyed)).subscribe(a=>{this._updateCurrentPositionClass(a.connectionPair),this._tooltipInstance&&a.scrollableViewProperties.isOverlayClipped&&this._tooltipInstance.isVisible()&&this._ngZone.run(()=>this.hide(0))}),this._overlayRef=this._overlay.create({direction:this._dir,positionStrategy:r,panelClass:`${this._cssClassPrefix}-${M_}`,scrollStrategy:this._scrollStrategy()}),this._updatePosition(this._overlayRef),this._overlayRef.detachments().pipe(dn(this._destroyed)).subscribe(()=>this._detach()),this._overlayRef.outsidePointerEvents().pipe(dn(this._destroyed)).subscribe(()=>{var a;return null===(a=this._tooltipInstance)||void 0===a?void 0:a._handleBodyInteraction()}),this._overlayRef.keydownEvents().pipe(dn(this._destroyed)).subscribe(a=>{this._isTooltipVisible()&&27===a.keyCode&&!function c5(n,...e){return e.length?e.some(t=>n[t]):n.altKey||n.shiftKey||n.ctrlKey||n.metaKey}(a)&&(a.preventDefault(),a.stopPropagation(),this._ngZone.run(()=>this.hide(0)))}),!(null===(t=this._defaultOptions)||void 0===t)&&t.disableTooltipInteractivity&&this._overlayRef.addPanelClass(`${this._cssClassPrefix}-tooltip-panel-non-interactive`),this._overlayRef}_detach(){this._overlayRef&&this._overlayRef.hasAttached()&&this._overlayRef.detach(),this._tooltipInstance=null}_updatePosition(t){const i=t.getConfig().positionStrategy,r=this._getOrigin(),a=this._getOverlayPosition();i.withPositions([this._addOffset(Object.assign(Object.assign({},r.main),a.main)),this._addOffset(Object.assign(Object.assign({},r.fallback),a.fallback))])}_addOffset(t){return t}_getOrigin(){const t=!this._dir||"ltr"==this._dir.value,i=this.position;let r;"above"==i||"below"==i?r={originX:"center",originY:"above"==i?"top":"bottom"}:"before"==i||"left"==i&&t||"right"==i&&!t?r={originX:"start",originY:"center"}:("after"==i||"right"==i&&t||"left"==i&&!t)&&(r={originX:"end",originY:"center"});const{x:a,y:o}=this._invertPosition(r.originX,r.originY);return{main:r,fallback:{originX:a,originY:o}}}_getOverlayPosition(){const t=!this._dir||"ltr"==this._dir.value,i=this.position;let r;"above"==i?r={overlayX:"center",overlayY:"bottom"}:"below"==i?r={overlayX:"center",overlayY:"top"}:"before"==i||"left"==i&&t||"right"==i&&!t?r={overlayX:"end",overlayY:"center"}:("after"==i||"right"==i&&t||"left"==i&&!t)&&(r={overlayX:"start",overlayY:"center"});const{x:a,y:o}=this._invertPosition(r.overlayX,r.overlayY);return{main:r,fallback:{overlayX:a,overlayY:o}}}_updateTooltipMessage(){this._tooltipInstance&&(this._tooltipInstance.message=this.message,this._tooltipInstance._markForCheck(),this._ngZone.onMicrotaskEmpty.pipe(si(1),dn(this._destroyed)).subscribe(()=>{this._tooltipInstance&&this._overlayRef.updatePosition()}))}_setTooltipClass(t){this._tooltipInstance&&(this._tooltipInstance.tooltipClass=t,this._tooltipInstance._markForCheck())}_invertPosition(t,i){return"above"===this.position||"below"===this.position?"top"===i?i="bottom":"bottom"===i&&(i="top"):"end"===t?t="start":"start"===t&&(t="end"),{x:t,y:i}}_updateCurrentPositionClass(t){const{overlayY:i,originX:r,originY:a}=t;let o;if(o="center"===i?this._dir&&"rtl"===this._dir.value?"end"===r?"left":"right":"start"===r?"left":"right":"bottom"===i&&"top"===a?"above":"below",o!==this._currentPosition){const s=this._overlayRef;if(s){const l=`${this._cssClassPrefix}-${M_}-`;s.removePanelClass(l+this._currentPosition),s.addPanelClass(l+o)}this._currentPosition=o}}_setupPointerEnterEventsIfNeeded(){this._disabled||!this.message||!this._viewInitialized||this._passiveListeners.length||(this._platformSupportsMouseEvents()?this._passiveListeners.push(["mouseenter",()=>{this._setupPointerExitEventsIfNeeded(),this.show()}]):"off"!==this.touchGestures&&(this._disableNativeGesturesIfNecessary(),this._passiveListeners.push(["touchstart",()=>{this._setupPointerExitEventsIfNeeded(),clearTimeout(this._touchstartTimeout),this._touchstartTimeout=setTimeout(()=>this.show(),500)}])),this._addListeners(this._passiveListeners))}_setupPointerExitEventsIfNeeded(){if(this._pointerExitEventsInitialized)return;this._pointerExitEventsInitialized=!0;const t=[];if(this._platformSupportsMouseEvents())t.push(["mouseleave",i=>{var r;const a=i.relatedTarget;(!a||null===(r=this._overlayRef)||void 0===r||!r.overlayElement.contains(a))&&this.hide()}],["wheel",i=>this._wheelListener(i)]);else if("off"!==this.touchGestures){this._disableNativeGesturesIfNecessary();const i=()=>{clearTimeout(this._touchstartTimeout),this.hide(this._defaultOptions.touchendHideDelay)};t.push(["touchend",i],["touchcancel",i])}this._addListeners(t),this._passiveListeners.push(...t)}_addListeners(t){t.forEach(([i,r])=>{this._elementRef.nativeElement.addEventListener(i,r,I_)})}_platformSupportsMouseEvents(){return!this._platform.IOS&&!this._platform.ANDROID}_wheelListener(t){if(this._isTooltipVisible()){const i=this._document.elementFromPoint(t.clientX,t.clientY),r=this._elementRef.nativeElement;i!==r&&!r.contains(i)&&this.hide()}}_disableNativeGesturesIfNecessary(){const t=this.touchGestures;if("off"!==t){const i=this._elementRef.nativeElement,r=i.style;("on"===t||"INPUT"!==i.nodeName&&"TEXTAREA"!==i.nodeName)&&(r.userSelect=r.msUserSelect=r.webkitUserSelect=r.MozUserSelect="none"),("on"===t||!i.draggable)&&(r.webkitUserDrag="none"),r.touchAction="none",r.webkitTapHighlightColor="transparent"}}}return n.\u0275fac=function(t){Ds()},n.\u0275dir=Re({type:n,inputs:{position:["matTooltipPosition","position"],disabled:["matTooltipDisabled","disabled"],showDelay:["matTooltipShowDelay","showDelay"],hideDelay:["matTooltipHideDelay","hideDelay"],touchGestures:["matTooltipTouchGestures","touchGestures"],message:["matTooltip","message"],tooltipClass:["matTooltipClass","tooltipClass"]}}),n})(),eL=(()=>{class n extends Z4{constructor(t,i,r,a,o,s,l,c,d,u,h,f){super(t,i,r,a,o,s,l,c,d,u,h,f),this._tooltipComponent=nL}}return n.\u0275fac=function(t){return new(t||n)(C(Zl),C(ht),C(c_),C(Wt),C(ae),C(Pt),C(C5),C(Q1),C(A_),C(P1,8),C(X4,8),C(Q))},n.\u0275dir=Re({type:n,selectors:[["","matTooltip",""]],hostAttrs:[1,"mat-tooltip-trigger"],exportAs:["matTooltip"],features:[kr]}),n})(),tL=(()=>{class n{constructor(t,i){this._changeDetectorRef=t,this._closeOnInteraction=!1,this._isVisible=!1,this._onHide=new _e,this._animationsDisabled="NoopAnimations"===i}show(t){clearTimeout(this._hideTimeoutId),this._showTimeoutId=setTimeout(()=>{this._toggleVisibility(!0),this._showTimeoutId=void 0},t)}hide(t){clearTimeout(this._showTimeoutId),this._hideTimeoutId=setTimeout(()=>{this._toggleVisibility(!1),this._hideTimeoutId=void 0},t)}afterHidden(){return this._onHide}isVisible(){return this._isVisible}ngOnDestroy(){this._cancelPendingAnimations(),this._onHide.complete(),this._triggerElement=null}_handleBodyInteraction(){this._closeOnInteraction&&this.hide(0)}_markForCheck(){this._changeDetectorRef.markForCheck()}_handleMouseLeave({relatedTarget:t}){(!t||!this._triggerElement.contains(t))&&(this.isVisible()?this.hide(this._mouseLeaveHideDelay):this._finalizeAnimation(!1))}_onShow(){}_handleAnimationEnd({animationName:t}){(t===this._showAnimation||t===this._hideAnimation)&&this._finalizeAnimation(t===this._showAnimation)}_cancelPendingAnimations(){clearTimeout(this._showTimeoutId),clearTimeout(this._hideTimeoutId),this._showTimeoutId=this._hideTimeoutId=void 0}_finalizeAnimation(t){t?this._closeOnInteraction=!0:this.isVisible()||this._onHide.next()}_toggleVisibility(t){const i=this._tooltip.nativeElement,r=this._showAnimation,a=this._hideAnimation;if(i.classList.remove(t?a:r),i.classList.add(t?r:a),this._isVisible=t,t&&!this._animationsDisabled&&"function"==typeof getComputedStyle){const o=getComputedStyle(i);("0s"===o.getPropertyValue("animation-duration")||"none"===o.getPropertyValue("animation-name"))&&(this._animationsDisabled=!0)}t&&this._onShow(),this._animationsDisabled&&(i.classList.add("_mat-animation-noopable"),this._finalizeAnimation(t))}}return n.\u0275fac=function(t){return new(t||n)(C(eo),C(Fi,8))},n.\u0275dir=Re({type:n}),n})(),nL=(()=>{class n extends tL{constructor(t,i,r){super(t,r),this._breakpointObserver=i,this._isHandset=this._breakpointObserver.observe("(max-width: 599.98px) and (orientation: portrait), (max-width: 959.98px) and (orientation: landscape)"),this._showAnimation="mat-tooltip-show",this._hideAnimation="mat-tooltip-hide"}}return n.\u0275fac=function(t){return new(t||n)(C(eo),C(H1),C(Fi,8))},n.\u0275cmp=mt({type:n,selectors:[["mat-tooltip-component"]],viewQuery:function(t,i){if(1&t&&Uu($4,7),2&t){let r;Wr(r=Vr())&&(i._tooltip=r.first)}},hostAttrs:["aria-hidden","true"],hostVars:2,hostBindings:function(t,i){1&t&&jr("mouseleave",function(a){return i._handleMouseLeave(a)}),2&t&&xu("zoom",i.isVisible()?1:null)},features:[kr],decls:4,vars:6,consts:[[1,"mat-tooltip",3,"ngClass","animationend"],["tooltip",""]],template:function(t,i){if(1&t&&(Y(0,"div",0,1),jr("animationend",function(a){return i._handleAnimationEnd(a)}),function pv(n,e){const t=ee();let i;const r=n+22;t.firstCreatePass?(i=function mM(n,e){if(e)for(let t=e.length-1;t>=0;t--){const i=e[t];if(n===i.name)return i}}(e,t.pipeRegistry),t.data[r]=i,i.onDestroy&&(t.destroyHooks||(t.destroyHooks=[])).push(r,i.onDestroy)):i=t.data[r];const a=i.factory||(i.factory=Ti(i.type)),o=Ft(C);try{const s=ns(!1),l=a();return ns(s),function oE(n,e,t,i){t>=n.data.length&&(n.data[t]=null,n.blueprint[t]=null),e[t]=i}(t,w(),r,l),l}finally{Ft(o)}}(2,"async"),ue(3),G()),2&t){let r;Li("mat-tooltip-handset",null==(r=function gv(n,e,t){const i=n+22,r=w(),a=function ar(n,e){return n[e]}(r,i);return function Xa(n,e){return n[1].data[e].pure}(r,i)?cv(r,ct(),e,a.transform,t,a):a.transform(t)}(2,4,i._isHandset))?null:r.matches),re("ngClass",i.tooltipClass),ie(3),Rt(i.message)}},dependencies:[P0,B0],styles:[".mat-tooltip{color:#fff;border-radius:4px;margin:14px;max-width:250px;padding-left:8px;padding-right:8px;overflow:hidden;text-overflow:ellipsis;transform:scale(0)}.mat-tooltip._mat-animation-noopable{animation:none;transform:scale(1)}.cdk-high-contrast-active .mat-tooltip{outline:solid 1px}.mat-tooltip-handset{margin:24px;padding-left:16px;padding-right:16px}.mat-tooltip-panel-non-interactive{pointer-events:none}@keyframes mat-tooltip-show{0%{opacity:0;transform:scale(0)}50%{opacity:.5;transform:scale(0.99)}100%{opacity:1;transform:scale(1)}}@keyframes mat-tooltip-hide{0%{opacity:1;transform:scale(1)}100%{opacity:0;transform:scale(1)}}.mat-tooltip-show{animation:mat-tooltip-show 200ms cubic-bezier(0, 0, 0.2, 1) forwards}.mat-tooltip-hide{animation:mat-tooltip-hide 100ms cubic-bezier(0, 0, 0.2, 1) forwards}"],encapsulation:2,changeDetection:0}),n})(),iL=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({providers:[K4],imports:[O5,z0,W4,Dn,Dn,Ef]}),n})();function rL(n,e){if(1&n&&(Y(0,"a",4),ue(1),G()),2&n){const t=Qe().$implicit;re("href",t.url,vr),ie(1),Rt(t.title)}}function aL(n,e){if(1&n&&(Wa(0),ue(1),Va()),2&n){const t=Qe().$implicit;ie(1),Rt(t.title)}}function oL(n,e){1&n&&(Y(0,"button",5),ue(1,"Abstract"),G()),2&n&&re("matTooltip",Qe().$implicit.abstract)}const sL=function(n){return["/","people",n]};function lL(n,e){if(1&n&&(Y(0,"a",7),ue(1),G()),2&n){const t=Qe().$implicit,i=Qe(2);re("routerLink",function lv(n,e,t,i){return cv(w(),ct(),n,e,t,i)}(2,sL,i.peopleMap[t].id)),ie(1),Rt(t)}}function cL(n,e){if(1&n&&(Wa(0),ue(1),Va()),2&n){const t=Qe().$implicit;ie(1),Rt(t)}}function dL(n,e){if(1&n&&(Y(0,"span"),Ne(1,lL,2,4,"a",6),Ne(2,cL,2,1,"ng-container",2),G()),2&n){const t=e.$implicit,i=Qe(2);ie(1),re("ngIf",i.peopleMap[t]),ie(1),re("ngIf",!i.peopleMap[t])}}function uL(n,e){if(1&n&&(Y(0,"div")(1,"h3"),Ne(2,rL,2,2,"a",1),Ne(3,aL,2,1,"ng-container",2),Ne(4,oL,2,1,"button",3),G(),Ne(5,dL,3,2,"span",0),G()),2&n){const t=e.$implicit;ie(2),re("ngIf",t.url),ie(1),re("ngIf",!t.url),ie(1),re("ngIf",t.abstract),ie(1),re("ngForOf",t.authors)}}let R_=(()=>{class n{constructor(){this.publications=[],this.peopleMap=ff}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275cmp=mt({type:n,selectors:[["app-publications-list"]],inputs:{publications:"publications"},decls:1,vars:1,consts:[[4,"ngFor","ngForOf"],["target","_blank",3,"href",4,"ngIf"],[4,"ngIf"],["mat-button","","color","accent","aria-label","Show publication abstract",3,"matTooltip",4,"ngIf"],["target","_blank",3,"href"],["mat-button","","color","accent","aria-label","Show publication abstract",3,"matTooltip"],[3,"routerLink",4,"ngIf"],[3,"routerLink"]],template:function(t,i){1&t&&Ne(0,uL,6,4,"div",0),2&t&&re("ngForOf",i.publications)},dependencies:[dl,ro,Hl,a_,eL],styles:['div[_ngcontent-%COMP%]:not(:last-child){padding-bottom:8px;border-bottom:1px solid rgba(0,0,0,.1019607843)}span[_ngcontent-%COMP%]{font-style:italic;padding:0 2px}span[_ngcontent-%COMP%]:not(:last-child):after{content:","}h3[_ngcontent-%COMP%]{margin:0}']}),n})();function hL(n,e){if(1&n&&(Y(0,"div")(1,"h2"),ue(2),G(),zt(3,"app-publications-list",1),G()),2&n){const t=e.$implicit,i=Qe();ie(2),Rt(t),ie(1),re("publications",i.publicationsMap[t])}}let fL=(()=>{class n{constructor(){this.publicationsMap=Vl,this.years=Object.keys(this.publicationsMap).sort().reverse()}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275cmp=mt({type:n,selectors:[["app-publications"]],decls:1,vars:1,consts:[[4,"ngFor","ngForOf"],[3,"publications"]],template:function(t,i){1&t&&Ne(0,hL,4,2,"div",0),2&t&&re("ngForOf",i.years)},dependencies:[dl,R_],styles:["h2[_ngcontent-%COMP%]{margin:16px 0}"]}),n})();function pL(n,e){if(1&n&&(Y(0,"h6",6),ue(1),G()),2&n){const t=Qe(2);ie(1),Rt(t.person.advisors.join("\n"))}}function gL(n,e){if(1&n&&(Y(0,"div",1),zt(1,"img",2),Y(2,"div",3)(3,"h5",4),ue(4),G(),Ne(5,pL,2,1,"h6",5),G()()),2&n){const t=Qe();ie(1),re("src","assets/people/"+t.person.imagePath+".jpg",vr)("alt","Photo of a "+t.person.name),ie(3),Rt(t.person.name),ie(1),re("ngIf",t.person.advisors)}}let P_=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275cmp=mt({type:n,selectors:[["app-person-card"]],inputs:{person:"person"},decls:1,vars:1,consts:[["class","person-card",4,"ngIf"],[1,"person-card"],[1,"card-image",3,"src","alt"],[1,"container"],[1,"title"],["class","subtitle",4,"ngIf"],[1,"subtitle"]],template:function(t,i){1&t&&Ne(0,gL,6,4,"div",0),2&t&&re("ngIf",i.person)},dependencies:[ro],styles:[".with-hover[_nghost-%COMP%]:hover   .card-image[_ngcontent-%COMP%]{transform:scale(.95);filter:brightness(1.15)}.with-hover[_nghost-%COMP%]:hover   .container[_ngcontent-%COMP%]{transform:translateY(-8px)}.card-image[_ngcontent-%COMP%]{width:100%;background:none no-repeat center center/cover;border-radius:32px;transition:all .25s ease}.container[_ngcontent-%COMP%]{padding:16px;display:flex;flex-direction:column;gap:4px;box-sizing:border-box;transition:all .25s ease}.title[_ngcontent-%COMP%]{font-size:24px;font-weight:500;margin:0;line-height:125%}.subtitle[_ngcontent-%COMP%]{font-size:16px;margin:0;opacity:.5;white-space:pre}"]}),n})();function mL(n,e){if(1&n&&zt(0,"app-person-card",5),2&n){const t=e.$implicit;re("person",t)("routerLink",t.id)}}function bL(n,e){if(1&n&&(Y(0,"div",3),Ne(1,mL,1,2,"app-person-card",4),G()),2&n){const t=e.ngIf;ie(1),re("ngForOf",t)}}function yL(n,e){if(1&n&&(Y(0,"section",1)(1,"h2"),ue(2),G(),Ne(3,bL,2,1,"div",2),G()),2&n){const t=e.$implicit,i=Qe();za("data-group",t),ie(2),Rt(i.groupName(t)),ie(1),re("ngIf",i.groups[t])}}let vL=(()=>{class n{constructor(){this.groupsOrder=["Faculty","PhD Student","MSc Student","Alumni","Employee"],this.groups=A1,this.groupsOrder.forEach(t=>{this.groups[t]=this.groups[t].sort("Alumni"===t?(i,r)=>(i.endYear||0)<(r.endYear||0)?1:-1:(i,r)=>i.name>r.name?1:-1)})}groupName(t){return"Faculty"!==t&&"Alumni"!==t?t+"s":t}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275cmp=mt({type:n,selectors:[["app-people"]],decls:1,vars:1,consts:[["class","people-section",4,"ngFor","ngForOf"],[1,"people-section"],["class","peoples-grid",4,"ngIf"],[1,"peoples-grid"],["class","with-hover",3,"person","routerLink",4,"ngFor","ngForOf"],[1,"with-hover",3,"person","routerLink"]],template:function(t,i){1&t&&Ne(0,yL,4,3,"section",0),2&t&&re("ngForOf",i.groupsOrder)},dependencies:[dl,ro,Do,P_],styles:[".people-section[_ngcontent-%COMP%]{margin:48px 16px 0}@media (min-width: 768px){.people-section[data-group=Faculty][_ngcontent-%COMP%]   app-person-card[_ngcontent-%COMP%]{width:33.3333333333%}}.peoples-grid[_ngcontent-%COMP%]{display:flex;flex-wrap:wrap;align-items:stretch;justify-content:stretch}h2[_ngcontent-%COMP%]{font-size:32px;font-weight:300}app-person-card[_ngcontent-%COMP%]{width:100%;padding:12px;box-sizing:border-box;cursor:pointer}@media (min-width: 768px){app-person-card[_ngcontent-%COMP%]{width:25%}}@media (min-width: 1024px){app-person-card[_ngcontent-%COMP%]{width:20%}}[_nghost-%COMP%]   div[_ngcontent-%COMP%]{margin-top:24px}"]}),n})(),wL=(()=>{class n{constructor(){}ngOnInit(){}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275cmp=mt({type:n,selectors:[["app-resources"]],decls:2,vars:0,template:function(t,i){1&t&&(Y(0,"p"),ue(1,"resources works!"),G())}}),n})();function _L(n,e){if(1&n&&(Y(0,"div"),ue(1),G()),2&n){const t=Qe(2);ie(1),ku("",t.person.current.position," @ ",t.person.current.employer,"")}}function DL(n,e){if(1&n&&(Y(0,"div"),ue(1),G()),2&n){const t=Qe(2);ie(1),Rt(t.person.interests)}}function SL(n,e){if(1&n&&(Y(0,"a",8),ue(1),G()),2&n){const t=Qe(2);re("href",t.person.homepage,vr),ie(1),Rt(t.person.homepage)}}function CL(n,e){if(1&n&&(Y(0,"div",6),Ne(1,_L,2,2,"div",4),Ne(2,DL,2,1,"div",4),Ne(3,SL,2,2,"a",7),G()),2&n){const t=Qe();ie(1),re("ngIf",t.person.current),ie(1),re("ngIf",t.person.interests),ie(1),re("ngIf",t.person.homepage)}}function xL(n,e){1&n&&(Y(0,"div"),ue(1,"No Publications"),G())}function TL(n,e){1&n&&zt(0,"app-publications-list",9),2&n&&re("publications",Qe().publications)}const EL=[{path:"",component:$8},{path:"people",component:vL},{path:"people/:id",component:(()=>{class n{constructor(t,i){this.route=t,this.router=i,this.publications=[]}ngOnInit(){this.route.params.subscribe(t=>{const i=t.id,r=R1.find(a=>a.id===i);if(r){this.person=r;const a=[this.person.name].concat(this.person.aliases||[]);this.publications=I1.filter(o=>a.some(s=>o.authors.indexOf(s)>-1))}else this.router.navigate(["/people"])})}}return n.\u0275fac=function(t){return new(t||n)(C(ui),C(Ye))},n.\u0275cmp=mt({type:n,selectors:[["app-person"]],decls:8,vars:4,consts:[["id","info"],[3,"person"],["id","other-info",4,"ngIf"],[1,"publications"],[4,"ngIf"],[3,"publications",4,"ngIf"],["id","other-info"],["target","_blank",3,"href",4,"ngIf"],["target","_blank",3,"href"],[3,"publications"]],template:function(t,i){1&t&&(Y(0,"div",0),zt(1,"app-person-card",1),Ne(2,CL,4,3,"div",2),G(),Y(3,"div",3)(4,"h3"),ue(5,"Publications"),G(),Ne(6,xL,2,0,"div",4),Ne(7,TL,1,1,"app-publications-list",5),G()),2&t&&(ie(1),re("person",i.person),ie(1),re("ngIf",i.person),ie(4),re("ngIf",0==i.publications.length),ie(1),re("ngIf",i.publications.length>0))},dependencies:[ro,P_,R_],styles:["[_nghost-%COMP%]{display:flex;width:100%;margin:40px 8px}[_nghost-%COMP%] > *[_ngcontent-%COMP%]{padding:8px}@media (max-width: 768px){[_nghost-%COMP%]{display:block}}.publications[_ngcontent-%COMP%]{flex-grow:1}#info[_ngcontent-%COMP%]{width:256px;min-width:256px;margin:0 auto;display:block}#info[_ngcontent-%COMP%]   #other-info[_ngcontent-%COMP%]{padding:0 16px}"]}),n})()},{path:"publications",component:fL},{path:"resources",component:wL}];let kL=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({imports:[k1.forRoot(EL),k1]}),n})();const ML=["*",[["mat-toolbar-row"]]],IL=["*","mat-toolbar-row"],AL=e_(class{constructor(n){this._elementRef=n}});let RL=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275dir=Re({type:n,selectors:[["mat-toolbar-row"]],hostAttrs:[1,"mat-toolbar-row"],exportAs:["matToolbarRow"]}),n})(),PL=(()=>{class n extends AL{constructor(t,i,r){super(t),this._platform=i,this._document=r}ngAfterViewInit(){this._platform.isBrowser&&(this._checkToolbarMixedModes(),this._toolbarRows.changes.subscribe(()=>this._checkToolbarMixedModes()))}_checkToolbarMixedModes(){}}return n.\u0275fac=function(t){return new(t||n)(C(ht),C(Pt),C(Q))},n.\u0275cmp=mt({type:n,selectors:[["mat-toolbar"]],contentQueries:function(t,i,r){if(1&t&&function Vs(n,e,t,i){const r=ee();if(r.firstCreatePass){const a=ze();Dv(r,new vv(e,t,i),a.index),function OM(n,e){const t=n.contentQueries||(n.contentQueries=[]);e!==(t.length?t[t.length-1]:-1)&&t.push(n.queries.length-1,e)}(r,n),2==(2&t)&&(r.staticContentQueries=!0)}_v(r,w(),t)}(r,RL,5),2&t){let a;Wr(a=Vr())&&(i._toolbarRows=a)}},hostAttrs:[1,"mat-toolbar"],hostVars:4,hostBindings:function(t,i){2&t&&Li("mat-toolbar-multiple-rows",i._toolbarRows.length>0)("mat-toolbar-single-row",0===i._toolbarRows.length)},inputs:{color:"color"},exportAs:["matToolbar"],features:[kr],ngContentSelectors:IL,decls:2,vars:0,template:function(t,i){1&t&&(Su(ML),Os(0),Os(1,1))},styles:[".cdk-high-contrast-active .mat-toolbar{outline:solid 1px}.mat-toolbar-row,.mat-toolbar-single-row{display:flex;box-sizing:border-box;padding:0 16px;width:100%;flex-direction:row;align-items:center;white-space:nowrap}.mat-toolbar-multiple-rows{display:flex;box-sizing:border-box;flex-direction:column;width:100%}"],encapsulation:2,changeDetection:0}),n})(),LL=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({imports:[Dn,Dn]}),n})(),OL=(()=>{class n{constructor(){this.title="lab-website"}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275cmp=mt({type:n,selectors:[["app-root"]],decls:13,vars:0,consts:[["color","primary"],[1,"container"],["routerLink","/","id","nav-main"],[1,"nav-spacer"],["mat-button","","aria-label","Navigate to 'people' page","routerLink","/people"],["mat-button","","aria-label","Navigate to 'publications' page","routerLink","/publications"]],template:function(t,i){1&t&&(Y(0,"mat-toolbar",0)(1,"div",1)(2,"span",2),ue(3,"BIU NLP Lab"),G(),zt(4,"span",3),Y(5,"button",4)(6,"span"),ue(7,"People"),G()(),Y(8,"button",5)(9,"span"),ue(10,"Publications"),G()()()(),Y(11,"div",1),zt(12,"router-outlet"),G())},dependencies:[Jh,Do,PL,a_],styles:[".nav-spacer[_ngcontent-%COMP%]{flex:1 1 auto}#nav-main[_ngcontent-%COMP%]{cursor:pointer}"]}),n})(),JL=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({imports:[Dn,Dn]}),n})();function G_(n){return new _(3e3,!1)}function P6(){return"undefined"!=typeof window&&void 0!==window.document}function Lf(){return"undefined"!=typeof process&&"[object process]"==={}.toString.call(process)}function gi(n){switch(n.length){case 0:return new ko;case 1:return n[0];default:return new k_(n)}}function U_(n,e,t,i,r=new Map,a=new Map){const o=[],s=[];let l=-1,c=null;if(i.forEach(d=>{const u=d.get("offset"),h=u==l,f=h&&c||new Map;d.forEach((p,g)=>{let m=g,b=p;if("offset"!==g)switch(m=e.normalizePropertyName(m,o),b){case"!":b=r.get(g);break;case Gn:b=a.get(g);break;default:b=e.normalizeStyleValue(g,m,b,o)}f.set(m,b)}),h||s.push(f),c=f,l=u}),o.length)throw function _6(n){return new _(3502,!1)}();return s}function Of(n,e,t,i){switch(e){case"start":n.onStart(()=>i(t&&Nf(t,"start",n)));break;case"done":n.onDone(()=>i(t&&Nf(t,"done",n)));break;case"destroy":n.onDestroy(()=>i(t&&Nf(t,"destroy",n)))}}function Nf(n,e,t){const i=t.totalTime,a=Ff(n.element,n.triggerName,n.fromState,n.toState,e||n.phaseName,null==i?n.totalTime:i,!!t.disabled),o=n._data;return null!=o&&(a._data=o),a}function Ff(n,e,t,i,r="",a=0,o){return{element:n,triggerName:e,fromState:t,toState:i,phaseName:r,totalTime:a,disabled:!!o}}function Lt(n,e,t){let i=n.get(e);return i||n.set(e,i=t),i}function $_(n){const e=n.indexOf(":");return[n.substring(1,e),n.slice(e+1)]}let jf=(n,e)=>!1,q_=(n,e,t)=>[],Y_=null;function Bf(n){const e=n.parentNode||n.host;return e===Y_?null:e}(Lf()||"undefined"!=typeof Element)&&(P6()?(Y_=(()=>document.documentElement)(),jf=(n,e)=>{for(;e;){if(e===n)return!0;e=Bf(e)}return!1}):jf=(n,e)=>n.contains(e),q_=(n,e,t)=>{if(t)return Array.from(n.querySelectorAll(e));const i=n.querySelector(e);return i?[i]:[]});let Yi=null,Q_=!1;const K_=jf,X_=q_;let J_=(()=>{class n{validateStyleProperty(t){return function O6(n){Yi||(Yi=function N6(){return"undefined"!=typeof document?document.body:null}()||{},Q_=!!Yi.style&&"WebkitAppearance"in Yi.style);let e=!0;return Yi.style&&!function L6(n){return"ebkit"==n.substring(1,6)}(n)&&(e=n in Yi.style,!e&&Q_&&(e="Webkit"+n.charAt(0).toUpperCase()+n.slice(1)in Yi.style)),e}(t)}matchesElement(t,i){return!1}containsElement(t,i){return K_(t,i)}getParentElement(t){return Bf(t)}query(t,i,r){return X_(t,i,r)}computeStyle(t,i,r){return r||""}animate(t,i,r,a,o,s=[],l){return new ko(r,a)}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})(),Hf=(()=>{class n{}return n.NOOP=new J_,n})();const zf="ng-enter",ic="ng-leave",rc="ng-trigger",ac=".ng-trigger",e2="ng-animating",Wf=".ng-animating";function Un(n){if("number"==typeof n)return n;const e=n.match(/^(-?[\.\d]+)(m?s)/);return!e||e.length<2?0:Vf(parseFloat(e[1]),e[2])}function Vf(n,e){return"s"===e?1e3*n:n}function oc(n,e,t){return n.hasOwnProperty("duration")?n:function B6(n,e,t){let r,a=0,o="";if("string"==typeof n){const s=n.match(/^(-?[\.\d]+)(m?s)(?:\s+(-?[\.\d]+)(m?s))?(?:\s+([-a-z]+(?:\(.+?\))?))?$/i);if(null===s)return e.push(G_()),{duration:0,delay:0,easing:""};r=Vf(parseFloat(s[1]),s[2]);const l=s[3];null!=l&&(a=Vf(parseFloat(l),s[4]));const c=s[5];c&&(o=c)}else r=n;if(!t){let s=!1,l=e.length;r<0&&(e.push(function ZL(){return new _(3100,!1)}()),s=!0),a<0&&(e.push(function e6(){return new _(3101,!1)}()),s=!0),s&&e.splice(l,0,G_())}return{duration:r,delay:a,easing:o}}(n,e,t)}function Ao(n,e={}){return Object.keys(n).forEach(t=>{e[t]=n[t]}),e}function t2(n){const e=new Map;return Object.keys(n).forEach(t=>{e.set(t,n[t])}),e}function mi(n,e=new Map,t){if(t)for(let[i,r]of t)e.set(i,r);for(let[i,r]of n)e.set(i,r);return e}function r2(n,e,t){return t?e+":"+t+";":""}function a2(n){let e="";for(let t=0;t<n.style.length;t++){const i=n.style.item(t);e+=r2(0,i,n.style.getPropertyValue(i))}for(const t in n.style)n.style.hasOwnProperty(t)&&!t.startsWith("_")&&(e+=r2(0,V6(t),n.style[t]));n.setAttribute("style",e)}function Sn(n,e,t){n.style&&(e.forEach((i,r)=>{const a=Uf(r);t&&!t.has(r)&&t.set(r,n.style[a]),n.style[a]=i}),Lf()&&a2(n))}function Qi(n,e){n.style&&(e.forEach((t,i)=>{const r=Uf(i);n.style[r]=""}),Lf()&&a2(n))}function Ro(n){return Array.isArray(n)?1==n.length?n[0]:C_(n):n}const Gf=new RegExp("{{\\s*(.+?)\\s*}}","g");function o2(n){let e=[];if("string"==typeof n){let t;for(;t=Gf.exec(n);)e.push(t[1]);Gf.lastIndex=0}return e}function Po(n,e,t){const i=n.toString(),r=i.replace(Gf,(a,o)=>{let s=e[o];return null==s&&(t.push(function n6(n){return new _(3003,!1)}()),s=""),s.toString()});return r==i?n:r}function sc(n){const e=[];let t=n.next();for(;!t.done;)e.push(t.value),t=n.next();return e}const W6=/-+([a-z0-9])/g;function Uf(n){return n.replace(W6,(...e)=>e[1].toUpperCase())}function V6(n){return n.replace(/([a-z])([A-Z])/g,"$1-$2").toLowerCase()}function Ot(n,e,t){switch(e.type){case 7:return n.visitTrigger(e,t);case 0:return n.visitState(e,t);case 1:return n.visitTransition(e,t);case 2:return n.visitSequence(e,t);case 3:return n.visitGroup(e,t);case 4:return n.visitAnimate(e,t);case 5:return n.visitKeyframes(e,t);case 6:return n.visitStyle(e,t);case 8:return n.visitReference(e,t);case 9:return n.visitAnimateChild(e,t);case 10:return n.visitAnimateRef(e,t);case 11:return n.visitQuery(e,t);case 12:return n.visitStagger(e,t);default:throw function i6(n){return new _(3004,!1)}()}}function s2(n,e){return window.getComputedStyle(n)[e]}function Q6(n,e){const t=[];return"string"==typeof n?n.split(/\s*,\s*/).forEach(i=>function K6(n,e,t){if(":"==n[0]){const l=function X6(n,e){switch(n){case":enter":return"void => *";case":leave":return"* => void";case":increment":return(t,i)=>parseFloat(i)>parseFloat(t);case":decrement":return(t,i)=>parseFloat(i)<parseFloat(t);default:return e.push(function b6(n){return new _(3016,!1)}()),"* => *"}}(n,t);if("function"==typeof l)return void e.push(l);n=l}const i=n.match(/^(\*|[-\w]+)\s*(<?[=-]>)\s*(\*|[-\w]+)$/);if(null==i||i.length<4)return t.push(function m6(n){return new _(3015,!1)}()),e;const r=i[1],a=i[2],o=i[3];e.push(l2(r,o));"<"==a[0]&&!("*"==r&&"*"==o)&&e.push(l2(o,r))}(i,t,e)):t.push(n),t}const uc=new Set(["true","1"]),hc=new Set(["false","0"]);function l2(n,e){const t=uc.has(n)||hc.has(n),i=uc.has(e)||hc.has(e);return(r,a)=>{let o="*"==n||n==r,s="*"==e||e==a;return!o&&t&&"boolean"==typeof r&&(o=r?uc.has(n):hc.has(n)),!s&&i&&"boolean"==typeof a&&(s=a?uc.has(e):hc.has(e)),o&&s}}const J6=new RegExp("s*:selfs*,?","g");function $f(n,e,t,i){return new Z6(n).build(e,t,i)}class Z6{constructor(e){this._driver=e}build(e,t,i){const r=new nO(t);return this._resetContextStyleTimingState(r),Ot(this,Ro(e),r)}_resetContextStyleTimingState(e){e.currentQuerySelector="",e.collectedStyles=new Map,e.collectedStyles.set("",new Map),e.currentTime=0}visitTrigger(e,t){let i=t.queryCount=0,r=t.depCount=0;const a=[],o=[];return"@"==e.name.charAt(0)&&t.errors.push(function a6(){return new _(3006,!1)}()),e.definitions.forEach(s=>{if(this._resetContextStyleTimingState(t),0==s.type){const l=s,c=l.name;c.toString().split(/\s*,\s*/).forEach(d=>{l.name=d,a.push(this.visitState(l,t))}),l.name=c}else if(1==s.type){const l=this.visitTransition(s,t);i+=l.queryCount,r+=l.depCount,o.push(l)}else t.errors.push(function o6(){return new _(3007,!1)}())}),{type:7,name:e.name,states:a,transitions:o,queryCount:i,depCount:r,options:null}}visitState(e,t){const i=this.visitStyle(e.styles,t),r=e.options&&e.options.params||null;if(i.containsDynamicStyles){const a=new Set,o=r||{};i.styles.forEach(s=>{s instanceof Map&&s.forEach(l=>{o2(l).forEach(c=>{o.hasOwnProperty(c)||a.add(c)})})}),a.size&&(sc(a.values()),t.errors.push(function s6(n,e){return new _(3008,!1)}()))}return{type:0,name:e.name,style:i,options:r?{params:r}:null}}visitTransition(e,t){t.queryCount=0,t.depCount=0;const i=Ot(this,Ro(e.animation),t);return{type:1,matchers:Q6(e.expr,t.errors),animation:i,queryCount:t.queryCount,depCount:t.depCount,options:Ki(e.options)}}visitSequence(e,t){return{type:2,steps:e.steps.map(i=>Ot(this,i,t)),options:Ki(e.options)}}visitGroup(e,t){const i=t.currentTime;let r=0;const a=e.steps.map(o=>{t.currentTime=i;const s=Ot(this,o,t);return r=Math.max(r,t.currentTime),s});return t.currentTime=r,{type:3,steps:a,options:Ki(e.options)}}visitAnimate(e,t){const i=function rO(n,e){if(n.hasOwnProperty("duration"))return n;if("number"==typeof n)return qf(oc(n,e).duration,0,"");const t=n;if(t.split(/\s+/).some(a=>"{"==a.charAt(0)&&"{"==a.charAt(1))){const a=qf(0,0,"");return a.dynamic=!0,a.strValue=t,a}const r=oc(t,e);return qf(r.duration,r.delay,r.easing)}(e.timings,t.errors);t.currentAnimateTimings=i;let r,a=e.styles?e.styles:hi({});if(5==a.type)r=this.visitKeyframes(a,t);else{let o=e.styles,s=!1;if(!o){s=!0;const c={};i.easing&&(c.easing=i.easing),o=hi(c)}t.currentTime+=i.duration+i.delay;const l=this.visitStyle(o,t);l.isEmptyStep=s,r=l}return t.currentAnimateTimings=null,{type:4,timings:i,style:r,options:null}}visitStyle(e,t){const i=this._makeStyleAst(e,t);return this._validateStyleAst(i,t),i}_makeStyleAst(e,t){const i=[],r=Array.isArray(e.styles)?e.styles:[e.styles];for(let s of r)"string"==typeof s?s===Gn?i.push(s):t.errors.push(new _(3002,!1)):i.push(t2(s));let a=!1,o=null;return i.forEach(s=>{if(s instanceof Map&&(s.has("easing")&&(o=s.get("easing"),s.delete("easing")),!a))for(let l of s.values())if(l.toString().indexOf("{{")>=0){a=!0;break}}),{type:6,styles:i,easing:o,offset:e.offset,containsDynamicStyles:a,options:null}}_validateStyleAst(e,t){const i=t.currentAnimateTimings;let r=t.currentTime,a=t.currentTime;i&&a>0&&(a-=i.duration+i.delay),e.styles.forEach(o=>{"string"!=typeof o&&o.forEach((s,l)=>{const c=t.collectedStyles.get(t.currentQuerySelector),d=c.get(l);let u=!0;d&&(a!=r&&a>=d.startTime&&r<=d.endTime&&(t.errors.push(function c6(n,e,t,i,r){return new _(3010,!1)}()),u=!1),a=d.startTime),u&&c.set(l,{startTime:a,endTime:r}),t.options&&function z6(n,e,t){const i=e.params||{},r=o2(n);r.length&&r.forEach(a=>{i.hasOwnProperty(a)||t.push(function t6(n){return new _(3001,!1)}())})}(s,t.options,t.errors)})})}visitKeyframes(e,t){const i={type:5,styles:[],options:null};if(!t.currentAnimateTimings)return t.errors.push(function d6(){return new _(3011,!1)}()),i;let a=0;const o=[];let s=!1,l=!1,c=0;const d=e.steps.map(b=>{const D=this._makeStyleAst(b,t);let y=null!=D.offset?D.offset:function iO(n){if("string"==typeof n)return null;let e=null;if(Array.isArray(n))n.forEach(t=>{if(t instanceof Map&&t.has("offset")){const i=t;e=parseFloat(i.get("offset")),i.delete("offset")}});else if(n instanceof Map&&n.has("offset")){const t=n;e=parseFloat(t.get("offset")),t.delete("offset")}return e}(D.styles),S=0;return null!=y&&(a++,S=D.offset=y),l=l||S<0||S>1,s=s||S<c,c=S,o.push(S),D});l&&t.errors.push(function u6(){return new _(3012,!1)}()),s&&t.errors.push(function h6(){return new _(3200,!1)}());const u=e.steps.length;let h=0;a>0&&a<u?t.errors.push(function f6(){return new _(3202,!1)}()):0==a&&(h=1/(u-1));const f=u-1,p=t.currentTime,g=t.currentAnimateTimings,m=g.duration;return d.forEach((b,D)=>{const y=h>0?D==f?1:h*D:o[D],S=y*m;t.currentTime=p+g.delay+S,g.duration=S,this._validateStyleAst(b,t),b.offset=y,i.styles.push(b)}),i}visitReference(e,t){return{type:8,animation:Ot(this,Ro(e.animation),t),options:Ki(e.options)}}visitAnimateChild(e,t){return t.depCount++,{type:9,options:Ki(e.options)}}visitAnimateRef(e,t){return{type:10,animation:this.visitReference(e.animation,t),options:Ki(e.options)}}visitQuery(e,t){const i=t.currentQuerySelector,r=e.options||{};t.queryCount++,t.currentQuery=e;const[a,o]=function eO(n){const e=!!n.split(/\s*,\s*/).find(t=>":self"==t);return e&&(n=n.replace(J6,"")),n=n.replace(/@\*/g,ac).replace(/@\w+/g,t=>ac+"-"+t.slice(1)).replace(/:animating/g,Wf),[n,e]}(e.selector);t.currentQuerySelector=i.length?i+" "+a:a,Lt(t.collectedStyles,t.currentQuerySelector,new Map);const s=Ot(this,Ro(e.animation),t);return t.currentQuery=null,t.currentQuerySelector=i,{type:11,selector:a,limit:r.limit||0,optional:!!r.optional,includeSelf:o,animation:s,originalSelector:e.selector,options:Ki(e.options)}}visitStagger(e,t){t.currentQuery||t.errors.push(function p6(){return new _(3013,!1)}());const i="full"===e.timings?{duration:0,delay:0,easing:"full"}:oc(e.timings,t.errors,!0);return{type:12,animation:Ot(this,Ro(e.animation),t),timings:i,options:null}}}class nO{constructor(e){this.errors=e,this.queryCount=0,this.depCount=0,this.currentTransition=null,this.currentQuery=null,this.currentQuerySelector=null,this.currentAnimateTimings=null,this.currentTime=0,this.collectedStyles=new Map,this.options=null,this.unsupportedCSSPropertiesFound=new Set}}function Ki(n){return n?(n=Ao(n)).params&&(n.params=function tO(n){return n?Ao(n):null}(n.params)):n={},n}function qf(n,e,t){return{duration:n,delay:e,easing:t}}function Yf(n,e,t,i,r,a,o=null,s=!1){return{type:1,element:n,keyframes:e,preStyleProps:t,postStyleProps:i,duration:r,delay:a,totalTime:r+a,easing:o,subTimeline:s}}class fc{constructor(){this._map=new Map}get(e){return this._map.get(e)||[]}append(e,t){let i=this._map.get(e);i||this._map.set(e,i=[]),i.push(...t)}has(e){return this._map.has(e)}clear(){this._map.clear()}}const sO=new RegExp(":enter","g"),cO=new RegExp(":leave","g");function Qf(n,e,t,i,r,a=new Map,o=new Map,s,l,c=[]){return(new dO).buildKeyframes(n,e,t,i,r,a,o,s,l,c)}class dO{buildKeyframes(e,t,i,r,a,o,s,l,c,d=[]){c=c||new fc;const u=new Kf(e,t,c,r,a,d,[]);u.options=l;const h=l.delay?Un(l.delay):0;u.currentTimeline.delayNextStep(h),u.currentTimeline.setStyles([o],null,u.errors,l),Ot(this,i,u);const f=u.timelines.filter(p=>p.containsAnimation());if(f.length&&s.size){let p;for(let g=f.length-1;g>=0;g--){const m=f[g];if(m.element===t){p=m;break}}p&&!p.allowOnlyTimelineStyles()&&p.setStyles([s],null,u.errors,l)}return f.length?f.map(p=>p.buildKeyframes()):[Yf(t,[],[],[],0,h,"",!1)]}visitTrigger(e,t){}visitState(e,t){}visitTransition(e,t){}visitAnimateChild(e,t){const i=t.subInstructions.get(t.element);if(i){const r=t.createSubContext(e.options),a=t.currentTimeline.currentTime,o=this._visitSubInstructions(i,r,r.options);a!=o&&t.transformIntoNewTimeline(o)}t.previousNode=e}visitAnimateRef(e,t){const i=t.createSubContext(e.options);i.transformIntoNewTimeline(),this._applyAnimationRefDelays([e.options,e.animation.options],t,i),this.visitReference(e.animation,i),t.transformIntoNewTimeline(i.currentTimeline.currentTime),t.previousNode=e}_applyAnimationRefDelays(e,t,i){var r;for(const a of e){const o=null==a?void 0:a.delay;if(o){const s="number"==typeof o?o:Un(Po(o,null!==(r=null==a?void 0:a.params)&&void 0!==r?r:{},t.errors));i.delayNextStep(s)}}}_visitSubInstructions(e,t,i){let a=t.currentTimeline.currentTime;const o=null!=i.duration?Un(i.duration):null,s=null!=i.delay?Un(i.delay):null;return 0!==o&&e.forEach(l=>{const c=t.appendInstructionToTimeline(l,o,s);a=Math.max(a,c.duration+c.delay)}),a}visitReference(e,t){t.updateOptions(e.options,!0),Ot(this,e.animation,t),t.previousNode=e}visitSequence(e,t){const i=t.subContextCount;let r=t;const a=e.options;if(a&&(a.params||a.delay)&&(r=t.createSubContext(a),r.transformIntoNewTimeline(),null!=a.delay)){6==r.previousNode.type&&(r.currentTimeline.snapshotCurrentStyles(),r.previousNode=pc);const o=Un(a.delay);r.delayNextStep(o)}e.steps.length&&(e.steps.forEach(o=>Ot(this,o,r)),r.currentTimeline.applyStylesToKeyframe(),r.subContextCount>i&&r.transformIntoNewTimeline()),t.previousNode=e}visitGroup(e,t){const i=[];let r=t.currentTimeline.currentTime;const a=e.options&&e.options.delay?Un(e.options.delay):0;e.steps.forEach(o=>{const s=t.createSubContext(e.options);a&&s.delayNextStep(a),Ot(this,o,s),r=Math.max(r,s.currentTimeline.currentTime),i.push(s.currentTimeline)}),i.forEach(o=>t.currentTimeline.mergeTimelineCollectedStyles(o)),t.transformIntoNewTimeline(r),t.previousNode=e}_visitTiming(e,t){if(e.dynamic){const i=e.strValue;return oc(t.params?Po(i,t.params,t.errors):i,t.errors)}return{duration:e.duration,delay:e.delay,easing:e.easing}}visitAnimate(e,t){const i=t.currentAnimateTimings=this._visitTiming(e.timings,t),r=t.currentTimeline;i.delay&&(t.incrementTime(i.delay),r.snapshotCurrentStyles());const a=e.style;5==a.type?this.visitKeyframes(a,t):(t.incrementTime(i.duration),this.visitStyle(a,t),r.applyStylesToKeyframe()),t.currentAnimateTimings=null,t.previousNode=e}visitStyle(e,t){const i=t.currentTimeline,r=t.currentAnimateTimings;!r&&i.hasCurrentStyleProperties()&&i.forwardFrame();const a=r&&r.easing||e.easing;e.isEmptyStep?i.applyEmptyStep(a):i.setStyles(e.styles,a,t.errors,t.options),t.previousNode=e}visitKeyframes(e,t){const i=t.currentAnimateTimings,r=t.currentTimeline.duration,a=i.duration,s=t.createSubContext().currentTimeline;s.easing=i.easing,e.styles.forEach(l=>{s.forwardTime((l.offset||0)*a),s.setStyles(l.styles,l.easing,t.errors,t.options),s.applyStylesToKeyframe()}),t.currentTimeline.mergeTimelineCollectedStyles(s),t.transformIntoNewTimeline(r+a),t.previousNode=e}visitQuery(e,t){const i=t.currentTimeline.currentTime,r=e.options||{},a=r.delay?Un(r.delay):0;a&&(6===t.previousNode.type||0==i&&t.currentTimeline.hasCurrentStyleProperties())&&(t.currentTimeline.snapshotCurrentStyles(),t.previousNode=pc);let o=i;const s=t.invokeQuery(e.selector,e.originalSelector,e.limit,e.includeSelf,!!r.optional,t.errors);t.currentQueryTotal=s.length;let l=null;s.forEach((c,d)=>{t.currentQueryIndex=d;const u=t.createSubContext(e.options,c);a&&u.delayNextStep(a),c===t.element&&(l=u.currentTimeline),Ot(this,e.animation,u),u.currentTimeline.applyStylesToKeyframe(),o=Math.max(o,u.currentTimeline.currentTime)}),t.currentQueryIndex=0,t.currentQueryTotal=0,t.transformIntoNewTimeline(o),l&&(t.currentTimeline.mergeTimelineCollectedStyles(l),t.currentTimeline.snapshotCurrentStyles()),t.previousNode=e}visitStagger(e,t){const i=t.parentContext,r=t.currentTimeline,a=e.timings,o=Math.abs(a.duration),s=o*(t.currentQueryTotal-1);let l=o*t.currentQueryIndex;switch(a.duration<0?"reverse":a.easing){case"reverse":l=s-l;break;case"full":l=i.currentStaggerTime}const d=t.currentTimeline;l&&d.delayNextStep(l);const u=d.currentTime;Ot(this,e.animation,t),t.previousNode=e,i.currentStaggerTime=r.currentTime-u+(r.startTime-i.currentTimeline.startTime)}}const pc={};class Kf{constructor(e,t,i,r,a,o,s,l){this._driver=e,this.element=t,this.subInstructions=i,this._enterClassName=r,this._leaveClassName=a,this.errors=o,this.timelines=s,this.parentContext=null,this.currentAnimateTimings=null,this.previousNode=pc,this.subContextCount=0,this.options={},this.currentQueryIndex=0,this.currentQueryTotal=0,this.currentStaggerTime=0,this.currentTimeline=l||new gc(this._driver,t,0),s.push(this.currentTimeline)}get params(){return this.options.params}updateOptions(e,t){if(!e)return;const i=e;let r=this.options;null!=i.duration&&(r.duration=Un(i.duration)),null!=i.delay&&(r.delay=Un(i.delay));const a=i.params;if(a){let o=r.params;o||(o=this.options.params={}),Object.keys(a).forEach(s=>{(!t||!o.hasOwnProperty(s))&&(o[s]=Po(a[s],o,this.errors))})}}_copyOptions(){const e={};if(this.options){const t=this.options.params;if(t){const i=e.params={};Object.keys(t).forEach(r=>{i[r]=t[r]})}}return e}createSubContext(e=null,t,i){const r=t||this.element,a=new Kf(this._driver,r,this.subInstructions,this._enterClassName,this._leaveClassName,this.errors,this.timelines,this.currentTimeline.fork(r,i||0));return a.previousNode=this.previousNode,a.currentAnimateTimings=this.currentAnimateTimings,a.options=this._copyOptions(),a.updateOptions(e),a.currentQueryIndex=this.currentQueryIndex,a.currentQueryTotal=this.currentQueryTotal,a.parentContext=this,this.subContextCount++,a}transformIntoNewTimeline(e){return this.previousNode=pc,this.currentTimeline=this.currentTimeline.fork(this.element,e),this.timelines.push(this.currentTimeline),this.currentTimeline}appendInstructionToTimeline(e,t,i){const r={duration:null!=t?t:e.duration,delay:this.currentTimeline.currentTime+(null!=i?i:0)+e.delay,easing:""},a=new uO(this._driver,e.element,e.keyframes,e.preStyleProps,e.postStyleProps,r,e.stretchStartingKeyframe);return this.timelines.push(a),r}incrementTime(e){this.currentTimeline.forwardTime(this.currentTimeline.duration+e)}delayNextStep(e){e>0&&this.currentTimeline.delayNextStep(e)}invokeQuery(e,t,i,r,a,o){let s=[];if(r&&s.push(this.element),e.length>0){e=(e=e.replace(sO,"."+this._enterClassName)).replace(cO,"."+this._leaveClassName);let c=this._driver.query(this.element,e,1!=i);0!==i&&(c=i<0?c.slice(c.length+i,c.length):c.slice(0,i)),s.push(...c)}return!a&&0==s.length&&o.push(function g6(n){return new _(3014,!1)}()),s}}class gc{constructor(e,t,i,r){this._driver=e,this.element=t,this.startTime=i,this._elementTimelineStylesLookup=r,this.duration=0,this._previousKeyframe=new Map,this._currentKeyframe=new Map,this._keyframes=new Map,this._styleSummary=new Map,this._localTimelineStyles=new Map,this._pendingStyles=new Map,this._backFill=new Map,this._currentEmptyStepKeyframe=null,this._elementTimelineStylesLookup||(this._elementTimelineStylesLookup=new Map),this._globalTimelineStyles=this._elementTimelineStylesLookup.get(t),this._globalTimelineStyles||(this._globalTimelineStyles=this._localTimelineStyles,this._elementTimelineStylesLookup.set(t,this._localTimelineStyles)),this._loadKeyframe()}containsAnimation(){switch(this._keyframes.size){case 0:return!1;case 1:return this.hasCurrentStyleProperties();default:return!0}}hasCurrentStyleProperties(){return this._currentKeyframe.size>0}get currentTime(){return this.startTime+this.duration}delayNextStep(e){const t=1===this._keyframes.size&&this._pendingStyles.size;this.duration||t?(this.forwardTime(this.currentTime+e),t&&this.snapshotCurrentStyles()):this.startTime+=e}fork(e,t){return this.applyStylesToKeyframe(),new gc(this._driver,e,t||this.currentTime,this._elementTimelineStylesLookup)}_loadKeyframe(){this._currentKeyframe&&(this._previousKeyframe=this._currentKeyframe),this._currentKeyframe=this._keyframes.get(this.duration),this._currentKeyframe||(this._currentKeyframe=new Map,this._keyframes.set(this.duration,this._currentKeyframe))}forwardFrame(){this.duration+=1,this._loadKeyframe()}forwardTime(e){this.applyStylesToKeyframe(),this.duration=e,this._loadKeyframe()}_updateStyle(e,t){this._localTimelineStyles.set(e,t),this._globalTimelineStyles.set(e,t),this._styleSummary.set(e,{time:this.currentTime,value:t})}allowOnlyTimelineStyles(){return this._currentEmptyStepKeyframe!==this._currentKeyframe}applyEmptyStep(e){e&&this._previousKeyframe.set("easing",e);for(let[t,i]of this._globalTimelineStyles)this._backFill.set(t,i||Gn),this._currentKeyframe.set(t,Gn);this._currentEmptyStepKeyframe=this._currentKeyframe}setStyles(e,t,i,r){var a;t&&this._previousKeyframe.set("easing",t);const o=r&&r.params||{},s=function hO(n,e){const t=new Map;let i;return n.forEach(r=>{if("*"===r){i=i||e.keys();for(let a of i)t.set(a,Gn)}else mi(r,t)}),t}(e,this._globalTimelineStyles);for(let[l,c]of s){const d=Po(c,o,i);this._pendingStyles.set(l,d),this._localTimelineStyles.has(l)||this._backFill.set(l,null!==(a=this._globalTimelineStyles.get(l))&&void 0!==a?a:Gn),this._updateStyle(l,d)}}applyStylesToKeyframe(){0!=this._pendingStyles.size&&(this._pendingStyles.forEach((e,t)=>{this._currentKeyframe.set(t,e)}),this._pendingStyles.clear(),this._localTimelineStyles.forEach((e,t)=>{this._currentKeyframe.has(t)||this._currentKeyframe.set(t,e)}))}snapshotCurrentStyles(){for(let[e,t]of this._localTimelineStyles)this._pendingStyles.set(e,t),this._updateStyle(e,t)}getFinalKeyframe(){return this._keyframes.get(this.duration)}get properties(){const e=[];for(let t in this._currentKeyframe)e.push(t);return e}mergeTimelineCollectedStyles(e){e._styleSummary.forEach((t,i)=>{const r=this._styleSummary.get(i);(!r||t.time>r.time)&&this._updateStyle(i,t.value)})}buildKeyframes(){this.applyStylesToKeyframe();const e=new Set,t=new Set,i=1===this._keyframes.size&&0===this.duration;let r=[];this._keyframes.forEach((s,l)=>{const c=mi(s,new Map,this._backFill);c.forEach((d,u)=>{"!"===d?e.add(u):d===Gn&&t.add(u)}),i||c.set("offset",l/this.duration),r.push(c)});const a=e.size?sc(e.values()):[],o=t.size?sc(t.values()):[];if(i){const s=r[0],l=new Map(s);s.set("offset",0),l.set("offset",1),r=[s,l]}return Yf(this.element,r,a,o,this.duration,this.startTime,this.easing,!1)}}class uO extends gc{constructor(e,t,i,r,a,o,s=!1){super(e,t,o.delay),this.keyframes=i,this.preStyleProps=r,this.postStyleProps=a,this._stretchStartingKeyframe=s,this.timings={duration:o.duration,delay:o.delay,easing:o.easing}}containsAnimation(){return this.keyframes.length>1}buildKeyframes(){let e=this.keyframes,{delay:t,duration:i,easing:r}=this.timings;if(this._stretchStartingKeyframe&&t){const a=[],o=i+t,s=t/o,l=mi(e[0]);l.set("offset",0),a.push(l);const c=mi(e[0]);c.set("offset",u2(s)),a.push(c);const d=e.length-1;for(let u=1;u<=d;u++){let h=mi(e[u]);const f=h.get("offset");h.set("offset",u2((t+f*i)/o)),a.push(h)}i=o,t=0,r="",e=a}return Yf(this.element,e,this.preStyleProps,this.postStyleProps,i,t,r,!0)}}function u2(n,e=3){const t=Math.pow(10,e-1);return Math.round(n*t)/t}class Xf{}const fO=new Set(["width","height","minWidth","minHeight","maxWidth","maxHeight","left","top","bottom","right","fontSize","outlineWidth","outlineOffset","paddingTop","paddingLeft","paddingBottom","paddingRight","marginTop","marginLeft","marginBottom","marginRight","borderRadius","borderWidth","borderTopWidth","borderLeftWidth","borderRightWidth","borderBottomWidth","textIndent","perspective"]);class pO extends Xf{normalizePropertyName(e,t){return Uf(e)}normalizeStyleValue(e,t,i,r){let a="";const o=i.toString().trim();if(fO.has(t)&&0!==i&&"0"!==i)if("number"==typeof i)a="px";else{const s=i.match(/^[+-]?[\d\.]+([a-z]*)$/);s&&0==s[1].length&&r.push(function r6(n,e){return new _(3005,!1)}())}return o+a}}function h2(n,e,t,i,r,a,o,s,l,c,d,u,h){return{type:0,element:n,triggerName:e,isRemovalTransition:r,fromState:t,fromStyles:a,toState:i,toStyles:o,timelines:s,queriedElements:l,preStyleProps:c,postStyleProps:d,totalTime:u,errors:h}}const Jf={};class f2{constructor(e,t,i){this._triggerName=e,this.ast=t,this._stateStyles=i}match(e,t,i,r){return function gO(n,e,t,i,r){return n.some(a=>a(e,t,i,r))}(this.ast.matchers,e,t,i,r)}buildStyles(e,t,i){let r=this._stateStyles.get("*");return void 0!==e&&(r=this._stateStyles.get(null==e?void 0:e.toString())||r),r?r.buildStyles(t,i):new Map}build(e,t,i,r,a,o,s,l,c,d){var u;const h=[],f=this.ast.options&&this.ast.options.params||Jf,g=this.buildStyles(i,s&&s.params||Jf,h),m=l&&l.params||Jf,b=this.buildStyles(r,m,h),D=new Set,y=new Map,S=new Map,j="void"===r,ne={params:mO(m,f),delay:null===(u=this.ast.options)||void 0===u?void 0:u.delay},Me=d?[]:Qf(e,t,this.ast.animation,a,o,g,b,ne,c,h);let rt=0;if(Me.forEach(gt=>{rt=Math.max(gt.duration+gt.delay,rt)}),h.length)return h2(t,this._triggerName,i,r,j,g,b,[],[],y,S,rt,h);Me.forEach(gt=>{const $n=gt.element,qn=Lt(y,$n,new Set);gt.preStyleProps.forEach(Cn=>qn.add(Cn));const Yn=Lt(S,$n,new Set);gt.postStyleProps.forEach(Cn=>Yn.add(Cn)),$n!==t&&D.add($n)});const bi=sc(D.values());return h2(t,this._triggerName,i,r,j,g,b,Me,bi,y,S,rt)}}function mO(n,e){const t=Ao(e);for(const i in n)n.hasOwnProperty(i)&&null!=n[i]&&(t[i]=n[i]);return t}class bO{constructor(e,t,i){this.styles=e,this.defaultParams=t,this.normalizer=i}buildStyles(e,t){const i=new Map,r=Ao(this.defaultParams);return Object.keys(e).forEach(a=>{const o=e[a];null!==o&&(r[a]=o)}),this.styles.styles.forEach(a=>{"string"!=typeof a&&a.forEach((o,s)=>{o&&(o=Po(o,r,t));const l=this.normalizer.normalizePropertyName(s,t);o=this.normalizer.normalizeStyleValue(s,l,o,t),i.set(l,o)})}),i}}class vO{constructor(e,t,i){this.name=e,this.ast=t,this._normalizer=i,this.transitionFactories=[],this.states=new Map,t.states.forEach(r=>{this.states.set(r.name,new bO(r.style,r.options&&r.options.params||{},i))}),p2(this.states,"true","1"),p2(this.states,"false","0"),t.transitions.forEach(r=>{this.transitionFactories.push(new f2(e,r,this.states))}),this.fallbackTransition=function wO(n,e,t){return new f2(n,{type:1,animation:{type:2,steps:[],options:null},matchers:[(o,s)=>!0],options:null,queryCount:0,depCount:0},e)}(e,this.states)}get containsQueries(){return this.ast.queryCount>0}matchTransition(e,t,i,r){return this.transitionFactories.find(o=>o.match(e,t,i,r))||null}matchStyles(e,t,i){return this.fallbackTransition.buildStyles(e,t,i)}}function p2(n,e,t){n.has(e)?n.has(t)||n.set(t,n.get(e)):n.has(t)&&n.set(e,n.get(t))}const _O=new fc;class DO{constructor(e,t,i){this.bodyNode=e,this._driver=t,this._normalizer=i,this._animations=new Map,this._playersById=new Map,this.players=[]}register(e,t){const i=[],a=$f(this._driver,t,i,[]);if(i.length)throw function D6(n){return new _(3503,!1)}();this._animations.set(e,a)}_buildPlayer(e,t,i){const r=e.element,a=U_(0,this._normalizer,0,e.keyframes,t,i);return this._driver.animate(r,a,e.duration,e.delay,e.easing,[],!0)}create(e,t,i={}){const r=[],a=this._animations.get(e);let o;const s=new Map;if(a?(o=Qf(this._driver,t,a,zf,ic,new Map,new Map,i,_O,r),o.forEach(d=>{const u=Lt(s,d.element,new Map);d.postStyleProps.forEach(h=>u.set(h,null))})):(r.push(function S6(){return new _(3300,!1)}()),o=[]),r.length)throw function C6(n){return new _(3504,!1)}();s.forEach((d,u)=>{d.forEach((h,f)=>{d.set(f,this._driver.computeStyle(u,f,Gn))})});const c=gi(o.map(d=>{const u=s.get(d.element);return this._buildPlayer(d,new Map,u)}));return this._playersById.set(e,c),c.onDestroy(()=>this.destroy(e)),this.players.push(c),c}destroy(e){const t=this._getPlayer(e);t.destroy(),this._playersById.delete(e);const i=this.players.indexOf(t);i>=0&&this.players.splice(i,1)}_getPlayer(e){const t=this._playersById.get(e);if(!t)throw function x6(n){return new _(3301,!1)}();return t}listen(e,t,i,r){const a=Ff(t,"","","");return Of(this._getPlayer(e),i,a,r),()=>{}}command(e,t,i,r){if("register"==i)return void this.register(e,r[0]);if("create"==i)return void this.create(e,t,r[0]||{});const a=this._getPlayer(e);switch(i){case"play":a.play();break;case"pause":a.pause();break;case"reset":a.reset();break;case"restart":a.restart();break;case"finish":a.finish();break;case"init":a.init();break;case"setPosition":a.setPosition(parseFloat(r[0]));break;case"destroy":this.destroy(e)}}}const g2="ng-animate-queued",Zf="ng-animate-disabled",EO=[],m2={namespaceId:"",setForRemoval:!1,setForMove:!1,hasAnimation:!1,removedBeforeQueried:!1},kO={namespaceId:"",setForMove:!1,setForRemoval:!1,hasAnimation:!1,removedBeforeQueried:!0},qt="__ng_removed";class ep{constructor(e,t=""){this.namespaceId=t;const i=e&&e.hasOwnProperty("value");if(this.value=function RO(n){return null!=n?n:null}(i?e.value:e),i){const a=Ao(e);delete a.value,this.options=a}else this.options={};this.options.params||(this.options.params={})}get params(){return this.options.params}absorbOptions(e){const t=e.params;if(t){const i=this.options.params;Object.keys(t).forEach(r=>{null==i[r]&&(i[r]=t[r])})}}}const Lo="void",tp=new ep(Lo);class MO{constructor(e,t,i){this.id=e,this.hostElement=t,this._engine=i,this.players=[],this._triggers=new Map,this._queue=[],this._elementListeners=new Map,this._hostClassName="ng-tns-"+e,Yt(t,this._hostClassName)}listen(e,t,i,r){if(!this._triggers.has(t))throw function T6(n,e){return new _(3302,!1)}();if(null==i||0==i.length)throw function E6(n){return new _(3303,!1)}();if(!function PO(n){return"start"==n||"done"==n}(i))throw function k6(n,e){return new _(3400,!1)}();const a=Lt(this._elementListeners,e,[]),o={name:t,phase:i,callback:r};a.push(o);const s=Lt(this._engine.statesByElement,e,new Map);return s.has(t)||(Yt(e,rc),Yt(e,rc+"-"+t),s.set(t,tp)),()=>{this._engine.afterFlush(()=>{const l=a.indexOf(o);l>=0&&a.splice(l,1),this._triggers.has(t)||s.delete(t)})}}register(e,t){return!this._triggers.has(e)&&(this._triggers.set(e,t),!0)}_getTrigger(e){const t=this._triggers.get(e);if(!t)throw function M6(n){return new _(3401,!1)}();return t}trigger(e,t,i,r=!0){const a=this._getTrigger(t),o=new np(this.id,t,e);let s=this._engine.statesByElement.get(e);s||(Yt(e,rc),Yt(e,rc+"-"+t),this._engine.statesByElement.set(e,s=new Map));let l=s.get(t);const c=new ep(i,this.id);if(!(i&&i.hasOwnProperty("value"))&&l&&c.absorbOptions(l.options),s.set(t,c),l||(l=tp),c.value!==Lo&&l.value===c.value){if(!function NO(n,e){const t=Object.keys(n),i=Object.keys(e);if(t.length!=i.length)return!1;for(let r=0;r<t.length;r++){const a=t[r];if(!e.hasOwnProperty(a)||n[a]!==e[a])return!1}return!0}(l.params,c.params)){const g=[],m=a.matchStyles(l.value,l.params,g),b=a.matchStyles(c.value,c.params,g);g.length?this._engine.reportError(g):this._engine.afterFlush(()=>{Qi(e,m),Sn(e,b)})}return}const h=Lt(this._engine.playersByElement,e,[]);h.forEach(g=>{g.namespaceId==this.id&&g.triggerName==t&&g.queued&&g.destroy()});let f=a.matchTransition(l.value,c.value,e,c.params),p=!1;if(!f){if(!r)return;f=a.fallbackTransition,p=!0}return this._engine.totalQueuedPlayers++,this._queue.push({element:e,triggerName:t,transition:f,fromState:l,toState:c,player:o,isFallbackTransition:p}),p||(Yt(e,g2),o.onStart(()=>{sa(e,g2)})),o.onDone(()=>{let g=this.players.indexOf(o);g>=0&&this.players.splice(g,1);const m=this._engine.playersByElement.get(e);if(m){let b=m.indexOf(o);b>=0&&m.splice(b,1)}}),this.players.push(o),h.push(o),o}deregister(e){this._triggers.delete(e),this._engine.statesByElement.forEach(t=>t.delete(e)),this._elementListeners.forEach((t,i)=>{this._elementListeners.set(i,t.filter(r=>r.name!=e))})}clearElementCache(e){this._engine.statesByElement.delete(e),this._elementListeners.delete(e);const t=this._engine.playersByElement.get(e);t&&(t.forEach(i=>i.destroy()),this._engine.playersByElement.delete(e))}_signalRemovalForInnerTriggers(e,t){const i=this._engine.driver.query(e,ac,!0);i.forEach(r=>{if(r[qt])return;const a=this._engine.fetchNamespacesByElement(r);a.size?a.forEach(o=>o.triggerLeaveAnimation(r,t,!1,!0)):this.clearElementCache(r)}),this._engine.afterFlushAnimationsDone(()=>i.forEach(r=>this.clearElementCache(r)))}triggerLeaveAnimation(e,t,i,r){const a=this._engine.statesByElement.get(e),o=new Map;if(a){const s=[];if(a.forEach((l,c)=>{if(o.set(c,l.value),this._triggers.has(c)){const d=this.trigger(e,c,Lo,r);d&&s.push(d)}}),s.length)return this._engine.markElementAsRemoved(this.id,e,!0,t,o),i&&gi(s).onDone(()=>this._engine.processLeaveNode(e)),!0}return!1}prepareLeaveAnimationListeners(e){const t=this._elementListeners.get(e),i=this._engine.statesByElement.get(e);if(t&&i){const r=new Set;t.forEach(a=>{const o=a.name;if(r.has(o))return;r.add(o);const l=this._triggers.get(o).fallbackTransition,c=i.get(o)||tp,d=new ep(Lo),u=new np(this.id,o,e);this._engine.totalQueuedPlayers++,this._queue.push({element:e,triggerName:o,transition:l,fromState:c,toState:d,player:u,isFallbackTransition:!0})})}}removeNode(e,t){const i=this._engine;if(e.childElementCount&&this._signalRemovalForInnerTriggers(e,t),this.triggerLeaveAnimation(e,t,!0))return;let r=!1;if(i.totalAnimations){const a=i.players.length?i.playersByQueriedElement.get(e):[];if(a&&a.length)r=!0;else{let o=e;for(;o=o.parentNode;)if(i.statesByElement.get(o)){r=!0;break}}}if(this.prepareLeaveAnimationListeners(e),r)i.markElementAsRemoved(this.id,e,!1,t);else{const a=e[qt];(!a||a===m2)&&(i.afterFlush(()=>this.clearElementCache(e)),i.destroyInnerAnimations(e),i._onRemovalComplete(e,t))}}insertNode(e,t){Yt(e,this._hostClassName)}drainQueuedTransitions(e){const t=[];return this._queue.forEach(i=>{const r=i.player;if(r.destroyed)return;const a=i.element,o=this._elementListeners.get(a);o&&o.forEach(s=>{if(s.name==i.triggerName){const l=Ff(a,i.triggerName,i.fromState.value,i.toState.value);l._data=e,Of(i.player,s.phase,l,s.callback)}}),r.markedForDestroy?this._engine.afterFlush(()=>{r.destroy()}):t.push(i)}),this._queue=[],t.sort((i,r)=>{const a=i.transition.ast.depCount,o=r.transition.ast.depCount;return 0==a||0==o?a-o:this._engine.driver.containsElement(i.element,r.element)?1:-1})}destroy(e){this.players.forEach(t=>t.destroy()),this._signalRemovalForInnerTriggers(this.hostElement,e)}elementContainsData(e){let t=!1;return this._elementListeners.has(e)&&(t=!0),t=!!this._queue.find(i=>i.element===e)||t,t}}class IO{constructor(e,t,i){this.bodyNode=e,this.driver=t,this._normalizer=i,this.players=[],this.newHostElements=new Map,this.playersByElement=new Map,this.playersByQueriedElement=new Map,this.statesByElement=new Map,this.disabledNodes=new Set,this.totalAnimations=0,this.totalQueuedPlayers=0,this._namespaceLookup={},this._namespaceList=[],this._flushFns=[],this._whenQuietFns=[],this.namespacesByHostElement=new Map,this.collectedEnterElements=[],this.collectedLeaveElements=[],this.onRemovalComplete=(r,a)=>{}}_onRemovalComplete(e,t){this.onRemovalComplete(e,t)}get queuedPlayers(){const e=[];return this._namespaceList.forEach(t=>{t.players.forEach(i=>{i.queued&&e.push(i)})}),e}createNamespace(e,t){const i=new MO(e,t,this);return this.bodyNode&&this.driver.containsElement(this.bodyNode,t)?this._balanceNamespaceList(i,t):(this.newHostElements.set(t,i),this.collectEnterElement(t)),this._namespaceLookup[e]=i}_balanceNamespaceList(e,t){const i=this._namespaceList,r=this.namespacesByHostElement;if(i.length-1>=0){let o=!1,s=this.driver.getParentElement(t);for(;s;){const l=r.get(s);if(l){const c=i.indexOf(l);i.splice(c+1,0,e),o=!0;break}s=this.driver.getParentElement(s)}o||i.unshift(e)}else i.push(e);return r.set(t,e),e}register(e,t){let i=this._namespaceLookup[e];return i||(i=this.createNamespace(e,t)),i}registerTrigger(e,t,i){let r=this._namespaceLookup[e];r&&r.register(t,i)&&this.totalAnimations++}destroy(e,t){if(!e)return;const i=this._fetchNamespace(e);this.afterFlush(()=>{this.namespacesByHostElement.delete(i.hostElement),delete this._namespaceLookup[e];const r=this._namespaceList.indexOf(i);r>=0&&this._namespaceList.splice(r,1)}),this.afterFlushAnimationsDone(()=>i.destroy(t))}_fetchNamespace(e){return this._namespaceLookup[e]}fetchNamespacesByElement(e){const t=new Set,i=this.statesByElement.get(e);if(i)for(let r of i.values())if(r.namespaceId){const a=this._fetchNamespace(r.namespaceId);a&&t.add(a)}return t}trigger(e,t,i,r){if(mc(t)){const a=this._fetchNamespace(e);if(a)return a.trigger(t,i,r),!0}return!1}insertNode(e,t,i,r){if(!mc(t))return;const a=t[qt];if(a&&a.setForRemoval){a.setForRemoval=!1,a.setForMove=!0;const o=this.collectedLeaveElements.indexOf(t);o>=0&&this.collectedLeaveElements.splice(o,1)}if(e){const o=this._fetchNamespace(e);o&&o.insertNode(t,i)}r&&this.collectEnterElement(t)}collectEnterElement(e){this.collectedEnterElements.push(e)}markElementAsDisabled(e,t){t?this.disabledNodes.has(e)||(this.disabledNodes.add(e),Yt(e,Zf)):this.disabledNodes.has(e)&&(this.disabledNodes.delete(e),sa(e,Zf))}removeNode(e,t,i,r){if(mc(t)){const a=e?this._fetchNamespace(e):null;if(a?a.removeNode(t,r):this.markElementAsRemoved(e,t,!1,r),i){const o=this.namespacesByHostElement.get(t);o&&o.id!==e&&o.removeNode(t,r)}}else this._onRemovalComplete(t,r)}markElementAsRemoved(e,t,i,r,a){this.collectedLeaveElements.push(t),t[qt]={namespaceId:e,setForRemoval:r,hasAnimation:i,removedBeforeQueried:!1,previousTriggersValues:a}}listen(e,t,i,r,a){return mc(t)?this._fetchNamespace(e).listen(t,i,r,a):()=>{}}_buildInstruction(e,t,i,r,a){return e.transition.build(this.driver,e.element,e.fromState.value,e.toState.value,i,r,e.fromState.options,e.toState.options,t,a)}destroyInnerAnimations(e){let t=this.driver.query(e,ac,!0);t.forEach(i=>this.destroyActiveAnimationsForElement(i)),0!=this.playersByQueriedElement.size&&(t=this.driver.query(e,Wf,!0),t.forEach(i=>this.finishActiveQueriedAnimationOnElement(i)))}destroyActiveAnimationsForElement(e){const t=this.playersByElement.get(e);t&&t.forEach(i=>{i.queued?i.markedForDestroy=!0:i.destroy()})}finishActiveQueriedAnimationOnElement(e){const t=this.playersByQueriedElement.get(e);t&&t.forEach(i=>i.finish())}whenRenderingDone(){return new Promise(e=>{if(this.players.length)return gi(this.players).onDone(()=>e());e()})}processLeaveNode(e){var t;const i=e[qt];if(i&&i.setForRemoval){if(e[qt]=m2,i.namespaceId){this.destroyInnerAnimations(e);const r=this._fetchNamespace(i.namespaceId);r&&r.clearElementCache(e)}this._onRemovalComplete(e,i.setForRemoval)}!(null===(t=e.classList)||void 0===t)&&t.contains(Zf)&&this.markElementAsDisabled(e,!1),this.driver.query(e,".ng-animate-disabled",!0).forEach(r=>{this.markElementAsDisabled(r,!1)})}flush(e=-1){let t=[];if(this.newHostElements.size&&(this.newHostElements.forEach((i,r)=>this._balanceNamespaceList(i,r)),this.newHostElements.clear()),this.totalAnimations&&this.collectedEnterElements.length)for(let i=0;i<this.collectedEnterElements.length;i++)Yt(this.collectedEnterElements[i],"ng-star-inserted");if(this._namespaceList.length&&(this.totalQueuedPlayers||this.collectedLeaveElements.length)){const i=[];try{t=this._flushAnimations(i,e)}finally{for(let r=0;r<i.length;r++)i[r]()}}else for(let i=0;i<this.collectedLeaveElements.length;i++)this.processLeaveNode(this.collectedLeaveElements[i]);if(this.totalQueuedPlayers=0,this.collectedEnterElements.length=0,this.collectedLeaveElements.length=0,this._flushFns.forEach(i=>i()),this._flushFns=[],this._whenQuietFns.length){const i=this._whenQuietFns;this._whenQuietFns=[],t.length?gi(t).onDone(()=>{i.forEach(r=>r())}):i.forEach(r=>r())}}reportError(e){throw function I6(n){return new _(3402,!1)}()}_flushAnimations(e,t){const i=new fc,r=[],a=new Map,o=[],s=new Map,l=new Map,c=new Map,d=new Set;this.disabledNodes.forEach(M=>{d.add(M);const R=this.driver.query(M,".ng-animate-queued",!0);for(let L=0;L<R.length;L++)d.add(R[L])});const u=this.bodyNode,h=Array.from(this.statesByElement.keys()),f=v2(h,this.collectedEnterElements),p=new Map;let g=0;f.forEach((M,R)=>{const L=zf+g++;p.set(R,L),M.forEach(J=>Yt(J,L))});const m=[],b=new Set,D=new Set;for(let M=0;M<this.collectedLeaveElements.length;M++){const R=this.collectedLeaveElements[M],L=R[qt];L&&L.setForRemoval&&(m.push(R),b.add(R),L.hasAnimation?this.driver.query(R,".ng-star-inserted",!0).forEach(J=>b.add(J)):D.add(R))}const y=new Map,S=v2(h,Array.from(b));S.forEach((M,R)=>{const L=ic+g++;y.set(R,L),M.forEach(J=>Yt(J,L))}),e.push(()=>{f.forEach((M,R)=>{const L=p.get(R);M.forEach(J=>sa(J,L))}),S.forEach((M,R)=>{const L=y.get(R);M.forEach(J=>sa(J,L))}),m.forEach(M=>{this.processLeaveNode(M)})});const j=[],ne=[];for(let M=this._namespaceList.length-1;M>=0;M--)this._namespaceList[M].drainQueuedTransitions(t).forEach(L=>{const J=L.player,Ie=L.element;if(j.push(J),this.collectedEnterElements.length){const at=Ie[qt];if(at&&at.setForMove){if(at.previousTriggersValues&&at.previousTriggersValues.has(L.triggerName)){const Xi=at.previousTriggersValues.get(L.triggerName),Qt=this.statesByElement.get(L.element);if(Qt&&Qt.has(L.triggerName)){const vc=Qt.get(L.triggerName);vc.value=Xi,Qt.set(L.triggerName,vc)}}return void J.destroy()}}const xn=!u||!this.driver.containsElement(u,Ie),Nt=y.get(Ie),yi=p.get(Ie),Ce=this._buildInstruction(L,i,yi,Nt,xn);if(Ce.errors&&Ce.errors.length)return void ne.push(Ce);if(xn)return J.onStart(()=>Qi(Ie,Ce.fromStyles)),J.onDestroy(()=>Sn(Ie,Ce.toStyles)),void r.push(J);if(L.isFallbackTransition)return J.onStart(()=>Qi(Ie,Ce.fromStyles)),J.onDestroy(()=>Sn(Ie,Ce.toStyles)),void r.push(J);const k2=[];Ce.timelines.forEach(at=>{at.stretchStartingKeyframe=!0,this.disabledNodes.has(at.element)||k2.push(at)}),Ce.timelines=k2,i.append(Ie,Ce.timelines),o.push({instruction:Ce,player:J,element:Ie}),Ce.queriedElements.forEach(at=>Lt(s,at,[]).push(J)),Ce.preStyleProps.forEach((at,Xi)=>{if(at.size){let Qt=l.get(Xi);Qt||l.set(Xi,Qt=new Set),at.forEach((vc,ap)=>Qt.add(ap))}}),Ce.postStyleProps.forEach((at,Xi)=>{let Qt=c.get(Xi);Qt||c.set(Xi,Qt=new Set),at.forEach((vc,ap)=>Qt.add(ap))})});if(ne.length){const M=[];ne.forEach(R=>{M.push(function A6(n,e){return new _(3505,!1)}())}),j.forEach(R=>R.destroy()),this.reportError(M)}const Me=new Map,rt=new Map;o.forEach(M=>{const R=M.element;i.has(R)&&(rt.set(R,R),this._beforeAnimationBuild(M.player.namespaceId,M.instruction,Me))}),r.forEach(M=>{const R=M.element;this._getPreviousPlayers(R,!1,M.namespaceId,M.triggerName,null).forEach(J=>{Lt(Me,R,[]).push(J),J.destroy()})});const bi=m.filter(M=>_2(M,l,c)),gt=new Map;y2(gt,this.driver,D,c,Gn).forEach(M=>{_2(M,l,c)&&bi.push(M)});const qn=new Map;f.forEach((M,R)=>{y2(qn,this.driver,new Set(M),l,"!")}),bi.forEach(M=>{var R,L;const J=gt.get(M),Ie=qn.get(M);gt.set(M,new Map([...Array.from(null!==(R=null==J?void 0:J.entries())&&void 0!==R?R:[]),...Array.from(null!==(L=null==Ie?void 0:Ie.entries())&&void 0!==L?L:[])]))});const Yn=[],Cn=[],la={};o.forEach(M=>{const{element:R,player:L,instruction:J}=M;if(i.has(R)){if(d.has(R))return L.onDestroy(()=>Sn(R,J.toStyles)),L.disabled=!0,L.overrideTotalTime(J.totalTime),void r.push(L);let Ie=la;if(rt.size>1){let Nt=R;const yi=[];for(;Nt=Nt.parentNode;){const Ce=rt.get(Nt);if(Ce){Ie=Ce;break}yi.push(Nt)}yi.forEach(Ce=>rt.set(Ce,Ie))}const xn=this._buildAnimation(L.namespaceId,J,Me,a,qn,gt);if(L.setRealPlayer(xn),Ie===la)Yn.push(L);else{const Nt=this.playersByElement.get(Ie);Nt&&Nt.length&&(L.parentPlayer=gi(Nt)),r.push(L)}}else Qi(R,J.fromStyles),L.onDestroy(()=>Sn(R,J.toStyles)),Cn.push(L),d.has(R)&&r.push(L)}),Cn.forEach(M=>{const R=a.get(M.element);if(R&&R.length){const L=gi(R);M.setRealPlayer(L)}}),r.forEach(M=>{M.parentPlayer?M.syncPlayerEvents(M.parentPlayer):M.destroy()});for(let M=0;M<m.length;M++){const R=m[M],L=R[qt];if(sa(R,ic),L&&L.hasAnimation)continue;let J=[];if(s.size){let xn=s.get(R);xn&&xn.length&&J.push(...xn);let Nt=this.driver.query(R,Wf,!0);for(let yi=0;yi<Nt.length;yi++){let Ce=s.get(Nt[yi]);Ce&&Ce.length&&J.push(...Ce)}}const Ie=J.filter(xn=>!xn.destroyed);Ie.length?LO(this,R,Ie):this.processLeaveNode(R)}return m.length=0,Yn.forEach(M=>{this.players.push(M),M.onDone(()=>{M.destroy();const R=this.players.indexOf(M);this.players.splice(R,1)}),M.play()}),Yn}elementContainsData(e,t){let i=!1;const r=t[qt];return r&&r.setForRemoval&&(i=!0),this.playersByElement.has(t)&&(i=!0),this.playersByQueriedElement.has(t)&&(i=!0),this.statesByElement.has(t)&&(i=!0),this._fetchNamespace(e).elementContainsData(t)||i}afterFlush(e){this._flushFns.push(e)}afterFlushAnimationsDone(e){this._whenQuietFns.push(e)}_getPreviousPlayers(e,t,i,r,a){let o=[];if(t){const s=this.playersByQueriedElement.get(e);s&&(o=s)}else{const s=this.playersByElement.get(e);if(s){const l=!a||a==Lo;s.forEach(c=>{c.queued||!l&&c.triggerName!=r||o.push(c)})}}return(i||r)&&(o=o.filter(s=>!(i&&i!=s.namespaceId||r&&r!=s.triggerName))),o}_beforeAnimationBuild(e,t,i){const a=t.element,o=t.isRemovalTransition?void 0:e,s=t.isRemovalTransition?void 0:t.triggerName;for(const l of t.timelines){const c=l.element,d=c!==a,u=Lt(i,c,[]);this._getPreviousPlayers(c,d,o,s,t.toState).forEach(f=>{const p=f.getRealPlayer();p.beforeDestroy&&p.beforeDestroy(),f.destroy(),u.push(f)})}Qi(a,t.fromStyles)}_buildAnimation(e,t,i,r,a,o){const s=t.triggerName,l=t.element,c=[],d=new Set,u=new Set,h=t.timelines.map(p=>{const g=p.element;d.add(g);const m=g[qt];if(m&&m.removedBeforeQueried)return new ko(p.duration,p.delay);const b=g!==l,D=function OO(n){const e=[];return w2(n,e),e}((i.get(g)||EO).map(Me=>Me.getRealPlayer())).filter(Me=>!!Me.element&&Me.element===g),y=a.get(g),S=o.get(g),j=U_(0,this._normalizer,0,p.keyframes,y,S),ne=this._buildPlayer(p,j,D);if(p.subTimeline&&r&&u.add(g),b){const Me=new np(e,s,g);Me.setRealPlayer(ne),c.push(Me)}return ne});c.forEach(p=>{Lt(this.playersByQueriedElement,p.element,[]).push(p),p.onDone(()=>function AO(n,e,t){let i=n.get(e);if(i){if(i.length){const r=i.indexOf(t);i.splice(r,1)}0==i.length&&n.delete(e)}return i}(this.playersByQueriedElement,p.element,p))}),d.forEach(p=>Yt(p,e2));const f=gi(h);return f.onDestroy(()=>{d.forEach(p=>sa(p,e2)),Sn(l,t.toStyles)}),u.forEach(p=>{Lt(r,p,[]).push(f)}),f}_buildPlayer(e,t,i){return t.length>0?this.driver.animate(e.element,t,e.duration,e.delay,e.easing,i):new ko(e.duration,e.delay)}}class np{constructor(e,t,i){this.namespaceId=e,this.triggerName=t,this.element=i,this._player=new ko,this._containsRealPlayer=!1,this._queuedCallbacks=new Map,this.destroyed=!1,this.markedForDestroy=!1,this.disabled=!1,this.queued=!0,this.totalTime=0}setRealPlayer(e){this._containsRealPlayer||(this._player=e,this._queuedCallbacks.forEach((t,i)=>{t.forEach(r=>Of(e,i,void 0,r))}),this._queuedCallbacks.clear(),this._containsRealPlayer=!0,this.overrideTotalTime(e.totalTime),this.queued=!1)}getRealPlayer(){return this._player}overrideTotalTime(e){this.totalTime=e}syncPlayerEvents(e){const t=this._player;t.triggerCallback&&e.onStart(()=>t.triggerCallback("start")),e.onDone(()=>this.finish()),e.onDestroy(()=>this.destroy())}_queueEvent(e,t){Lt(this._queuedCallbacks,e,[]).push(t)}onDone(e){this.queued&&this._queueEvent("done",e),this._player.onDone(e)}onStart(e){this.queued&&this._queueEvent("start",e),this._player.onStart(e)}onDestroy(e){this.queued&&this._queueEvent("destroy",e),this._player.onDestroy(e)}init(){this._player.init()}hasStarted(){return!this.queued&&this._player.hasStarted()}play(){!this.queued&&this._player.play()}pause(){!this.queued&&this._player.pause()}restart(){!this.queued&&this._player.restart()}finish(){this._player.finish()}destroy(){this.destroyed=!0,this._player.destroy()}reset(){!this.queued&&this._player.reset()}setPosition(e){this.queued||this._player.setPosition(e)}getPosition(){return this.queued?0:this._player.getPosition()}triggerCallback(e){const t=this._player;t.triggerCallback&&t.triggerCallback(e)}}function mc(n){return n&&1===n.nodeType}function b2(n,e){const t=n.style.display;return n.style.display=null!=e?e:"none",t}function y2(n,e,t,i,r){const a=[];t.forEach(l=>a.push(b2(l)));const o=[];i.forEach((l,c)=>{const d=new Map;l.forEach(u=>{const h=e.computeStyle(c,u,r);d.set(u,h),(!h||0==h.length)&&(c[qt]=kO,o.push(c))}),n.set(c,d)});let s=0;return t.forEach(l=>b2(l,a[s++])),o}function v2(n,e){const t=new Map;if(n.forEach(s=>t.set(s,[])),0==e.length)return t;const r=new Set(e),a=new Map;function o(s){if(!s)return 1;let l=a.get(s);if(l)return l;const c=s.parentNode;return l=t.has(c)?c:r.has(c)?1:o(c),a.set(s,l),l}return e.forEach(s=>{const l=o(s);1!==l&&t.get(l).push(s)}),t}function Yt(n,e){var t;null===(t=n.classList)||void 0===t||t.add(e)}function sa(n,e){var t;null===(t=n.classList)||void 0===t||t.remove(e)}function LO(n,e,t){gi(t).onDone(()=>n.processLeaveNode(e))}function w2(n,e){for(let t=0;t<n.length;t++){const i=n[t];i instanceof k_?w2(i.players,e):e.push(i)}}function _2(n,e,t){const i=t.get(n);if(!i)return!1;let r=e.get(n);return r?i.forEach(a=>r.add(a)):e.set(n,i),t.delete(n),!0}class bc{constructor(e,t,i){this.bodyNode=e,this._driver=t,this._normalizer=i,this._triggerCache={},this.onRemovalComplete=(r,a)=>{},this._transitionEngine=new IO(e,t,i),this._timelineEngine=new DO(e,t,i),this._transitionEngine.onRemovalComplete=(r,a)=>this.onRemovalComplete(r,a)}registerTrigger(e,t,i,r,a){const o=e+"-"+r;let s=this._triggerCache[o];if(!s){const l=[],d=$f(this._driver,a,l,[]);if(l.length)throw function w6(n,e){return new _(3404,!1)}();s=function yO(n,e,t){return new vO(n,e,t)}(r,d,this._normalizer),this._triggerCache[o]=s}this._transitionEngine.registerTrigger(t,r,s)}register(e,t){this._transitionEngine.register(e,t)}destroy(e,t){this._transitionEngine.destroy(e,t)}onInsert(e,t,i,r){this._transitionEngine.insertNode(e,t,i,r)}onRemove(e,t,i,r){this._transitionEngine.removeNode(e,t,r||!1,i)}disableAnimations(e,t){this._transitionEngine.markElementAsDisabled(e,t)}process(e,t,i,r){if("@"==i.charAt(0)){const[a,o]=$_(i);this._timelineEngine.command(a,t,o,r)}else this._transitionEngine.trigger(e,t,i,r)}listen(e,t,i,r,a){if("@"==i.charAt(0)){const[o,s]=$_(i);return this._timelineEngine.listen(o,t,s,a)}return this._transitionEngine.listen(e,t,i,r,a)}flush(e=-1){this._transitionEngine.flush(e)}get players(){return this._transitionEngine.players.concat(this._timelineEngine.players)}whenRenderingDone(){return this._transitionEngine.whenRenderingDone()}}let jO=(()=>{class n{constructor(t,i,r){this._element=t,this._startStyles=i,this._endStyles=r,this._state=0;let a=n.initialStylesByElement.get(t);a||n.initialStylesByElement.set(t,a=new Map),this._initialStyles=a}start(){this._state<1&&(this._startStyles&&Sn(this._element,this._startStyles,this._initialStyles),this._state=1)}finish(){this.start(),this._state<2&&(Sn(this._element,this._initialStyles),this._endStyles&&(Sn(this._element,this._endStyles),this._endStyles=null),this._state=1)}destroy(){this.finish(),this._state<3&&(n.initialStylesByElement.delete(this._element),this._startStyles&&(Qi(this._element,this._startStyles),this._endStyles=null),this._endStyles&&(Qi(this._element,this._endStyles),this._endStyles=null),Sn(this._element,this._initialStyles),this._state=3)}}return n.initialStylesByElement=new WeakMap,n})();function ip(n){let e=null;return n.forEach((t,i)=>{(function BO(n){return"display"===n||"position"===n})(i)&&(e=e||new Map,e.set(i,t))}),e}class D2{constructor(e,t,i,r){this.element=e,this.keyframes=t,this.options=i,this._specialStyles=r,this._onDoneFns=[],this._onStartFns=[],this._onDestroyFns=[],this._initialized=!1,this._finished=!1,this._started=!1,this._destroyed=!1,this._originalOnDoneFns=[],this._originalOnStartFns=[],this.time=0,this.parentPlayer=null,this.currentSnapshot=new Map,this._duration=i.duration,this._delay=i.delay||0,this.time=this._duration+this._delay}_onFinish(){this._finished||(this._finished=!0,this._onDoneFns.forEach(e=>e()),this._onDoneFns=[])}init(){this._buildPlayer(),this._preparePlayerBeforeStart()}_buildPlayer(){if(this._initialized)return;this._initialized=!0;const e=this.keyframes;this.domPlayer=this._triggerWebAnimation(this.element,e,this.options),this._finalKeyframe=e.length?e[e.length-1]:new Map,this.domPlayer.addEventListener("finish",()=>this._onFinish())}_preparePlayerBeforeStart(){this._delay?this._resetDomPlayerState():this.domPlayer.pause()}_convertKeyframesToObject(e){const t=[];return e.forEach(i=>{t.push(Object.fromEntries(i))}),t}_triggerWebAnimation(e,t,i){return e.animate(this._convertKeyframesToObject(t),i)}onStart(e){this._originalOnStartFns.push(e),this._onStartFns.push(e)}onDone(e){this._originalOnDoneFns.push(e),this._onDoneFns.push(e)}onDestroy(e){this._onDestroyFns.push(e)}play(){this._buildPlayer(),this.hasStarted()||(this._onStartFns.forEach(e=>e()),this._onStartFns=[],this._started=!0,this._specialStyles&&this._specialStyles.start()),this.domPlayer.play()}pause(){this.init(),this.domPlayer.pause()}finish(){this.init(),this._specialStyles&&this._specialStyles.finish(),this._onFinish(),this.domPlayer.finish()}reset(){this._resetDomPlayerState(),this._destroyed=!1,this._finished=!1,this._started=!1,this._onStartFns=this._originalOnStartFns,this._onDoneFns=this._originalOnDoneFns}_resetDomPlayerState(){this.domPlayer&&this.domPlayer.cancel()}restart(){this.reset(),this.play()}hasStarted(){return this._started}destroy(){this._destroyed||(this._destroyed=!0,this._resetDomPlayerState(),this._onFinish(),this._specialStyles&&this._specialStyles.destroy(),this._onDestroyFns.forEach(e=>e()),this._onDestroyFns=[])}setPosition(e){void 0===this.domPlayer&&this.init(),this.domPlayer.currentTime=e*this.time}getPosition(){return this.domPlayer.currentTime/this.time}get totalTime(){return this._delay+this._duration}beforeDestroy(){const e=new Map;this.hasStarted()&&this._finalKeyframe.forEach((i,r)=>{"offset"!==r&&e.set(r,this._finished?i:s2(this.element,r))}),this.currentSnapshot=e}triggerCallback(e){const t="start"===e?this._onStartFns:this._onDoneFns;t.forEach(i=>i()),t.length=0}}class HO{validateStyleProperty(e){return!0}validateAnimatableStyleProperty(e){return!0}matchesElement(e,t){return!1}containsElement(e,t){return K_(e,t)}getParentElement(e){return Bf(e)}query(e,t,i){return X_(e,t,i)}computeStyle(e,t,i){return window.getComputedStyle(e)[t]}animate(e,t,i,r,a,o=[]){const l={duration:i,delay:r,fill:0==r?"both":"forwards"};a&&(l.easing=a);const c=new Map,d=o.filter(f=>f instanceof D2);(function G6(n,e){return 0===n||0===e})(i,r)&&d.forEach(f=>{f.currentSnapshot.forEach((p,g)=>c.set(g,p))});let u=function H6(n){return n.length?n[0]instanceof Map?n:n.map(e=>t2(e)):[]}(t).map(f=>mi(f));u=function U6(n,e,t){if(t.size&&e.length){let i=e[0],r=[];if(t.forEach((a,o)=>{i.has(o)||r.push(o),i.set(o,a)}),r.length)for(let a=1;a<e.length;a++){let o=e[a];r.forEach(s=>o.set(s,s2(n,s)))}}return e}(e,u,c);const h=function FO(n,e){let t=null,i=null;return Array.isArray(e)&&e.length?(t=ip(e[0]),e.length>1&&(i=ip(e[e.length-1]))):e instanceof Map&&(t=ip(e)),t||i?new jO(n,t,i):null}(e,u);return new D2(e,u,l,h)}}let zO=(()=>{class n extends D_{constructor(t,i){super(),this._nextAnimationId=0,this._renderer=t.createRenderer(i.body,{id:"0",encapsulation:Xt.None,styles:[],data:{animation:[]}})}build(t){const i=this._nextAnimationId.toString();this._nextAnimationId++;const r=Array.isArray(t)?C_(t):t;return S2(this._renderer,null,i,"register",[r]),new WO(i,this._renderer)}}return n.\u0275fac=function(t){return new(t||n)(v(Na),v(Q))},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})();class WO extends class V4{}{constructor(e,t){super(),this._id=e,this._renderer=t}create(e,t){return new VO(this._id,e,t||{},this._renderer)}}class VO{constructor(e,t,i,r){this.id=e,this.element=t,this._renderer=r,this.parentPlayer=null,this._started=!1,this.totalTime=0,this._command("create",i)}_listen(e,t){return this._renderer.listen(this.element,`@@${this.id}:${e}`,t)}_command(e,...t){return S2(this._renderer,this.element,this.id,e,t)}onDone(e){this._listen("done",e)}onStart(e){this._listen("start",e)}onDestroy(e){this._listen("destroy",e)}init(){this._command("init")}hasStarted(){return this._started}play(){this._command("play"),this._started=!0}pause(){this._command("pause")}restart(){this._command("restart")}finish(){this._command("finish")}destroy(){this._command("destroy")}reset(){this._command("reset"),this._started=!1}setPosition(e){this._command("setPosition",e)}getPosition(){var e,t;return null!==(t=null===(e=this._renderer.engine.players[+this.id])||void 0===e?void 0:e.getPosition())&&void 0!==t?t:0}}function S2(n,e,t,i,r){return n.setProperty(e,`@@${t}:${i}`,r)}const C2="@.disabled";let GO=(()=>{class n{constructor(t,i,r){this.delegate=t,this.engine=i,this._zone=r,this._currentId=0,this._microtaskId=1,this._animationCallbacksBuffer=[],this._rendererCache=new Map,this._cdRecurDepth=0,this.promise=Promise.resolve(0),i.onRemovalComplete=(a,o)=>{const s=null==o?void 0:o.parentNode(a);s&&o.removeChild(s,a)}}createRenderer(t,i){const a=this.delegate.createRenderer(t,i);if(!(t&&i&&i.data&&i.data.animation)){let d=this._rendererCache.get(a);return d||(d=new x2("",a,this.engine,()=>this._rendererCache.delete(a)),this._rendererCache.set(a,d)),d}const o=i.id,s=i.id+"-"+this._currentId;this._currentId++,this.engine.register(s,t);const l=d=>{Array.isArray(d)?d.forEach(l):this.engine.registerTrigger(o,s,t,d.name,d)};return i.data.animation.forEach(l),new UO(this,s,a,this.engine)}begin(){this._cdRecurDepth++,this.delegate.begin&&this.delegate.begin()}_scheduleCountTask(){this.promise.then(()=>{this._microtaskId++})}scheduleListenerCallback(t,i,r){t>=0&&t<this._microtaskId?this._zone.run(()=>i(r)):(0==this._animationCallbacksBuffer.length&&Promise.resolve(null).then(()=>{this._zone.run(()=>{this._animationCallbacksBuffer.forEach(a=>{const[o,s]=a;o(s)}),this._animationCallbacksBuffer=[]})}),this._animationCallbacksBuffer.push([i,r]))}end(){this._cdRecurDepth--,0==this._cdRecurDepth&&this._zone.runOutsideAngular(()=>{this._scheduleCountTask(),this.engine.flush(this._microtaskId)}),this.delegate.end&&this.delegate.end()}whenRenderingDone(){return this.engine.whenRenderingDone()}}return n.\u0275fac=function(t){return new(t||n)(v(Na),v(bc),v(ae))},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})();class x2{constructor(e,t,i,r){this.namespaceId=e,this.delegate=t,this.engine=i,this._onDestroy=r,this.destroyNode=this.delegate.destroyNode?a=>t.destroyNode(a):null}get data(){return this.delegate.data}destroy(){var e;this.engine.destroy(this.namespaceId,this.delegate),this.delegate.destroy(),null===(e=this._onDestroy)||void 0===e||e.call(this)}createElement(e,t){return this.delegate.createElement(e,t)}createComment(e){return this.delegate.createComment(e)}createText(e){return this.delegate.createText(e)}appendChild(e,t){this.delegate.appendChild(e,t),this.engine.onInsert(this.namespaceId,t,e,!1)}insertBefore(e,t,i,r=!0){this.delegate.insertBefore(e,t,i),this.engine.onInsert(this.namespaceId,t,e,r)}removeChild(e,t,i){this.engine.onRemove(this.namespaceId,t,this.delegate,i)}selectRootElement(e,t){return this.delegate.selectRootElement(e,t)}parentNode(e){return this.delegate.parentNode(e)}nextSibling(e){return this.delegate.nextSibling(e)}setAttribute(e,t,i,r){this.delegate.setAttribute(e,t,i,r)}removeAttribute(e,t,i){this.delegate.removeAttribute(e,t,i)}addClass(e,t){this.delegate.addClass(e,t)}removeClass(e,t){this.delegate.removeClass(e,t)}setStyle(e,t,i,r){this.delegate.setStyle(e,t,i,r)}removeStyle(e,t,i){this.delegate.removeStyle(e,t,i)}setProperty(e,t,i){"@"==t.charAt(0)&&t==C2?this.disableAnimations(e,!!i):this.delegate.setProperty(e,t,i)}setValue(e,t){this.delegate.setValue(e,t)}listen(e,t,i){return this.delegate.listen(e,t,i)}disableAnimations(e,t){this.engine.disableAnimations(e,t)}}class UO extends x2{constructor(e,t,i,r,a){super(t,i,r,a),this.factory=e,this.namespaceId=t}setProperty(e,t,i){"@"==t.charAt(0)?"."==t.charAt(1)&&t==C2?this.disableAnimations(e,i=void 0===i||!!i):this.engine.process(this.namespaceId,e,t.slice(1),i):this.delegate.setProperty(e,t,i)}listen(e,t,i){if("@"==t.charAt(0)){const r=function $O(n){switch(n){case"body":return document.body;case"document":return document;case"window":return window;default:return n}}(e);let a=t.slice(1),o="";return"@"!=a.charAt(0)&&([a,o]=function qO(n){const e=n.indexOf(".");return[n.substring(0,e),n.slice(e+1)]}(a)),this.engine.listen(this.namespaceId,r,a,o,s=>{this.factory.scheduleListenerCallback(s._data||-1,i,s)})}return this.delegate.listen(e,t,i)}}const T2=[{provide:D_,useClass:zO},{provide:Xf,useFactory:function QO(){return new pO}},{provide:bc,useClass:(()=>{class n extends bc{constructor(t,i,r,a){super(t.body,i,r)}ngOnDestroy(){this.flush()}}return n.\u0275fac=function(t){return new(t||n)(v(Q),v(Hf),v(Xf),v(qr))},n.\u0275prov=k({token:n,factory:n.\u0275fac}),n})()},{provide:Na,useFactory:function KO(n,e,t){return new GO(n,e,t)},deps:[bl,bc,ae]}],rp=[{provide:Hf,useFactory:()=>new HO},{provide:Fi,useValue:"BrowserAnimations"},...T2],E2=[{provide:Hf,useClass:J_},{provide:Fi,useValue:"NoopAnimations"},...T2];let XO=(()=>{class n{static withConfig(t){return{ngModule:n,providers:t.disableAnimations?E2:rp}}}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n}),n.\u0275inj=Te({providers:rp,imports:[cw]}),n})(),JO=(()=>{class n{}return n.\u0275fac=function(t){return new(t||n)},n.\u0275mod=Ae({type:n,bootstrap:[OL]}),n.\u0275inj=Te({imports:[cw,XO,kL,LL,J5,JL,iL]}),n})();(function _I(){l0=!1})(),oP().bootstrapModule(JO).catch(n=>console.error(n))}},te=>{te(te.s=297)}]);